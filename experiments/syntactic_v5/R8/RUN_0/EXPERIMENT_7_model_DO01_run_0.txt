
==========: 253146848622000
Epoch:0001, train_loss=2.46296, train_acc=0.03504, val_loss=2.08640, val_acc=0.32847, time=1.15399
Epoch:0002, train_loss=2.11366, train_acc=0.35366, val_loss=2.07132, val_acc=0.45620, time=1.14202
Epoch:0003, train_loss=1.95404, train_acc=0.49261, val_loss=2.06851, val_acc=0.47263, time=1.25501
Epoch:0004, train_loss=1.90990, train_acc=0.51752, val_loss=2.06758, val_acc=0.47628, time=1.21901
Epoch:0005, train_loss=1.88548, train_acc=0.52805, val_loss=2.06952, val_acc=0.39781, time=1.25800
Epoch:0006, train_loss=1.88828, train_acc=0.49585, val_loss=2.07325, val_acc=0.35766, time=1.16901
Epoch:0007, train_loss=1.90704, train_acc=0.44967, val_loss=2.07419, val_acc=0.35036, time=1.30801
Epoch:0008, train_loss=1.90095, train_acc=0.47276, val_loss=2.07308, val_acc=0.38504, time=1.20800
Epoch:0009, train_loss=1.87702, train_acc=0.53859, val_loss=2.07210, val_acc=0.41606, time=1.30901
Epoch:0010, train_loss=1.85537, train_acc=0.57383, val_loss=2.07165, val_acc=0.46168, time=1.17002
Epoch:0011, train_loss=1.83999, train_acc=0.57687, val_loss=2.07085, val_acc=0.45985, time=1.31699
Epoch:0012, train_loss=1.82314, train_acc=0.58031, val_loss=2.06939, val_acc=0.45073, time=1.30802
Epoch:0013, train_loss=1.80195, train_acc=0.58619, val_loss=2.06778, val_acc=0.44343, time=1.27301
Epoch:0014, train_loss=1.78050, train_acc=0.60138, val_loss=2.06669, val_acc=0.41788, time=1.17900
Epoch:0015, train_loss=1.76422, train_acc=0.62487, val_loss=2.06638, val_acc=0.40328, time=1.13202
Epoch:0016, train_loss=1.75509, train_acc=0.64169, val_loss=2.06664, val_acc=0.39416, time=1.17000
Epoch:0017, train_loss=1.75063, train_acc=0.65100, val_loss=2.06708, val_acc=0.38321, time=1.16301
Epoch:0018, train_loss=1.74695, train_acc=0.66174, val_loss=2.06748, val_acc=0.38869, time=1.18201
Epoch:0019, train_loss=1.74198, train_acc=0.67065, val_loss=2.06784, val_acc=0.39781, time=1.26400
Epoch:0020, train_loss=1.73578, train_acc=0.67632, val_loss=2.06823, val_acc=0.40511, time=1.26102
Early stopping...

Optimization Finished!

Test set results: loss= 2.01798, accuracy= 0.42028, time= 0.37899

Test Precision, Recall and F1-Score...
              precision    recall  f1-score   support

           0     0.3149    0.1882    0.2356       696
           1     0.4887    0.7202    0.5823      1083
           2     0.0185    0.0115    0.0142        87
           3     0.0455    0.0165    0.0242       121
           4     0.0851    0.0533    0.0656        75
           5     0.0000    0.0000    0.0000        10
           6     0.0345    0.0123    0.0182        81
           7     0.3333    0.0278    0.0513        36

    accuracy                         0.4203      2189
   macro avg     0.1651    0.1287    0.1239      2189
weighted avg     0.3548    0.4203    0.3687      2189


Macro average Test Precision, Recall and F1-Score...
(0.16506514869636685, 0.12873999575870648, 0.1239228503824549, None)

Micro average Test Precision, Recall and F1-Score...
(0.4202832343535861, 0.4202832343535861, 0.4202832343535861, None)

Embeddings:
Word_embeddings:7688
Train_doc_embeddings:5485
Test_doc_embeddings:2189
