
==========: 253358624647200
Epoch:0001, train_loss=2.50366, train_acc=0.03869, val_loss=2.08938, val_acc=0.15876, time=1.22502
Epoch:0002, train_loss=2.16584, train_acc=0.16609, val_loss=2.06803, val_acc=0.36861, time=1.29100
Epoch:0003, train_loss=1.95601, train_acc=0.38728, val_loss=2.06149, val_acc=0.44526, time=1.31102
Epoch:0004, train_loss=1.87873, train_acc=0.49605, val_loss=2.06452, val_acc=0.47445, time=1.21201
Epoch:0005, train_loss=1.88639, train_acc=0.51712, val_loss=2.06820, val_acc=0.47445, time=1.18700
Epoch:0006, train_loss=1.90103, train_acc=0.52927, val_loss=2.06983, val_acc=0.47445, time=1.30200
Epoch:0007, train_loss=1.89905, train_acc=0.54000, val_loss=2.07085, val_acc=0.45255, time=1.19801
Epoch:0008, train_loss=1.89351, train_acc=0.55803, val_loss=2.07172, val_acc=0.40146, time=1.22500
Epoch:0009, train_loss=1.88857, train_acc=0.55682, val_loss=2.07141, val_acc=0.40328, time=1.40901
Epoch:0010, train_loss=1.87489, train_acc=0.57039, val_loss=2.07020, val_acc=0.42518, time=1.24700
Epoch:0011, train_loss=1.85439, train_acc=0.59753, val_loss=2.06898, val_acc=0.43796, time=1.18502
Epoch:0012, train_loss=1.83442, train_acc=0.60604, val_loss=2.06812, val_acc=0.44161, time=1.27599
Epoch:0013, train_loss=1.81723, train_acc=0.60320, val_loss=2.06742, val_acc=0.44161, time=1.22101
Epoch:0014, train_loss=1.80088, train_acc=0.60462, val_loss=2.06675, val_acc=0.43978, time=1.20302
Epoch:0015, train_loss=1.78422, train_acc=0.61090, val_loss=2.06620, val_acc=0.42883, time=1.21900
Epoch:0016, train_loss=1.76841, train_acc=0.62265, val_loss=2.06594, val_acc=0.43978, time=1.22501
Epoch:0017, train_loss=1.75532, train_acc=0.63804, val_loss=2.06604, val_acc=0.42701, time=1.15102
Epoch:0018, train_loss=1.74573, train_acc=0.65141, val_loss=2.06641, val_acc=0.41058, time=1.19701
Epoch:0019, train_loss=1.73893, train_acc=0.66113, val_loss=2.06691, val_acc=0.41058, time=1.21500
Epoch:0020, train_loss=1.73355, train_acc=0.66579, val_loss=2.06744, val_acc=0.41241, time=1.29901
Early stopping...

Optimization Finished!

Test set results: loss= 2.01467, accuracy= 0.42714, time= 0.35601

Test Precision, Recall and F1-Score...
              precision    recall  f1-score   support

           0     0.3172    0.2170    0.2577       696
           1     0.4866    0.7193    0.5805      1083
           2     0.0667    0.0345    0.0455        87
           3     0.0286    0.0083    0.0128       121
           4     0.0588    0.0133    0.0217        75
           5     0.0000    0.0000    0.0000        10
           6     0.0000    0.0000    0.0000        81
           7     0.0000    0.0000    0.0000        36

    accuracy                         0.4271      2189
   macro avg     0.1197    0.1240    0.1148      2189
weighted avg     0.3478    0.4271    0.3724      2189


Macro average Test Precision, Recall and F1-Score...
(0.1197324260747397, 0.12404160292081015, 0.11477128371828052, None)

Micro average Test Precision, Recall and F1-Score...
(0.4271356783919598, 0.4271356783919598, 0.4271356783919598, None)

Embeddings:
Word_embeddings:7688
Train_doc_embeddings:5485
Test_doc_embeddings:2189
