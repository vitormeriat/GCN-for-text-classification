
==========: 253276223937100
Epoch:0001, train_loss=2.30151, train_acc=0.15029, val_loss=2.07504, val_acc=0.34489, time=1.15902
Epoch:0002, train_loss=2.02417, train_acc=0.34758, val_loss=2.06481, val_acc=0.45620, time=1.18900
Epoch:0003, train_loss=1.90305, train_acc=0.49119, val_loss=2.06967, val_acc=0.48540, time=1.26901
Epoch:0004, train_loss=1.92124, train_acc=0.51995, val_loss=2.07221, val_acc=0.48540, time=1.37902
Epoch:0005, train_loss=1.92173, train_acc=0.52603, val_loss=2.07120, val_acc=0.46715, time=1.31300
Epoch:0006, train_loss=1.89376, train_acc=0.53818, val_loss=2.07052, val_acc=0.45803, time=1.25201
Epoch:0007, train_loss=1.87158, train_acc=0.54750, val_loss=2.07156, val_acc=0.39234, time=1.21102
Epoch:0008, train_loss=1.86781, train_acc=0.51853, val_loss=2.07150, val_acc=0.37044, time=1.30400
Epoch:0009, train_loss=1.85615, train_acc=0.51286, val_loss=2.06978, val_acc=0.39051, time=1.26201
Epoch:0010, train_loss=1.83076, train_acc=0.55276, val_loss=2.06779, val_acc=0.42701, time=1.33600
Epoch:0011, train_loss=1.80441, train_acc=0.59368, val_loss=2.06659, val_acc=0.43978, time=1.23500
Epoch:0012, train_loss=1.78618, train_acc=0.60583, val_loss=2.06614, val_acc=0.44708, time=1.34400
Epoch:0013, train_loss=1.77542, train_acc=0.60057, val_loss=2.06596, val_acc=0.45438, time=1.31201
Epoch:0014, train_loss=1.76744, train_acc=0.60340, val_loss=2.06577, val_acc=0.45073, time=1.28803
Epoch:0015, train_loss=1.75946, train_acc=0.60786, val_loss=2.06560, val_acc=0.43248, time=1.30700
Epoch:0016, train_loss=1.75142, train_acc=0.62568, val_loss=2.06559, val_acc=0.42883, time=1.18801
Epoch:0017, train_loss=1.74430, train_acc=0.64229, val_loss=2.06579, val_acc=0.41971, time=1.30801
Epoch:0018, train_loss=1.73856, train_acc=0.65728, val_loss=2.06619, val_acc=0.41423, time=1.35102
Epoch:0019, train_loss=1.73370, train_acc=0.66498, val_loss=2.06666, val_acc=0.41606, time=1.30700
Early stopping...

Optimization Finished!

Test set results: loss= 2.01572, accuracy= 0.43307, time= 0.50201

Test Precision, Recall and F1-Score...
              precision    recall  f1-score   support

           0     0.3253    0.2328    0.2714       696
           1     0.4962    0.7221    0.5882      1083
           2     0.0000    0.0000    0.0000        87
           3     0.0385    0.0083    0.0136       121
           4     0.0566    0.0400    0.0469        75
           5     0.0000    0.0000    0.0000        10
           6     0.0000    0.0000    0.0000        81
           7     0.0000    0.0000    0.0000        36

    accuracy                         0.4331      2189
   macro avg     0.1146    0.1254    0.1150      2189
weighted avg     0.3530    0.4331    0.3796      2189


Macro average Test Precision, Recall and F1-Score...
(0.11456992628334205, 0.12538642652701257, 0.11500353442038878, None)

Micro average Test Precision, Recall and F1-Score...
(0.433074463225217, 0.433074463225217, 0.4330744632252169, None)

Embeddings:
Word_embeddings:7688
Train_doc_embeddings:5485
Test_doc_embeddings:2189
