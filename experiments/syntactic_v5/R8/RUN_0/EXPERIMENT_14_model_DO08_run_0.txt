
==========: 253328096071600
Epoch:0001, train_loss=2.24991, train_acc=0.05590, val_loss=2.07131, val_acc=0.34124, time=1.40901
Epoch:0002, train_loss=1.98438, train_acc=0.38201, val_loss=2.06309, val_acc=0.45620, time=1.23602
Epoch:0003, train_loss=1.88376, train_acc=0.49970, val_loss=2.06554, val_acc=0.46350, time=1.26100
Epoch:0004, train_loss=1.87970, train_acc=0.52623, val_loss=2.06857, val_acc=0.45803, time=1.33002
Epoch:0005, train_loss=1.88223, train_acc=0.53899, val_loss=2.07112, val_acc=0.45985, time=1.37200
Epoch:0006, train_loss=1.88266, train_acc=0.55418, val_loss=2.07259, val_acc=0.43431, time=1.33901
Epoch:0007, train_loss=1.87636, train_acc=0.56411, val_loss=2.07242, val_acc=0.45620, time=1.40799
Epoch:0008, train_loss=1.85834, train_acc=0.58436, val_loss=2.07164, val_acc=0.46168, time=1.32400
Epoch:0009, train_loss=1.83718, train_acc=0.59206, val_loss=2.07069, val_acc=0.45985, time=1.30001
Epoch:0010, train_loss=1.81638, train_acc=0.59753, val_loss=2.06965, val_acc=0.45438, time=1.27002
Epoch:0011, train_loss=1.79653, train_acc=0.60583, val_loss=2.06884, val_acc=0.45255, time=1.23201
Epoch:0012, train_loss=1.78030, train_acc=0.62366, val_loss=2.06841, val_acc=0.45073, time=1.27301
Epoch:0013, train_loss=1.76864, train_acc=0.64169, val_loss=2.06816, val_acc=0.43613, time=1.31102
Epoch:0014, train_loss=1.75929, train_acc=0.65161, val_loss=2.06793, val_acc=0.42883, time=1.39201
Epoch:0015, train_loss=1.75029, train_acc=0.66214, val_loss=2.06774, val_acc=0.43066, time=1.32701
Epoch:0016, train_loss=1.74151, train_acc=0.67065, val_loss=2.06770, val_acc=0.42883, time=1.26401
Epoch:0017, train_loss=1.73346, train_acc=0.67085, val_loss=2.06781, val_acc=0.43066, time=1.33702
Epoch:0018, train_loss=1.72624, train_acc=0.67146, val_loss=2.06805, val_acc=0.43248, time=1.26501
Epoch:0019, train_loss=1.71957, train_acc=0.67490, val_loss=2.06837, val_acc=0.43066, time=1.27901
Epoch:0020, train_loss=1.71317, train_acc=0.68078, val_loss=2.06876, val_acc=0.43066, time=1.33200
Early stopping...

Optimization Finished!

Test set results: loss= 2.02410, accuracy= 0.42439, time= 0.38499

Test Precision, Recall and F1-Score...
              precision    recall  f1-score   support

           0     0.2902    0.1739    0.2174       696
           1     0.4841    0.7433    0.5863      1083
           2     0.0714    0.0115    0.0198        87
           3     0.0000    0.0000    0.0000       121
           4     0.0323    0.0133    0.0189        75
           5     0.0000    0.0000    0.0000        10
           6     0.0345    0.0123    0.0182        81
           7     0.0000    0.0000    0.0000        36

    accuracy                         0.4244      2189
   macro avg     0.1141    0.1193    0.1076      2189
weighted avg     0.3370    0.4244    0.3613      2189


Macro average Test Precision, Recall and F1-Score...
(0.11405027539339346, 0.11929118405427429, 0.10757368092953885, None)

Micro average Test Precision, Recall and F1-Score...
(0.42439470077661035, 0.42439470077661035, 0.4243947007766103, None)

Embeddings:
Word_embeddings:7688
Train_doc_embeddings:5485
Test_doc_embeddings:2189
