
==========: 256693808066500
Epoch:0001, train_loss=2.37506, train_acc=0.03686, val_loss=2.07903, val_acc=0.26642, time=1.42101
Epoch:0002, train_loss=2.04405, train_acc=0.31679, val_loss=2.06552, val_acc=0.41606, time=1.41501
Epoch:0003, train_loss=1.90040, train_acc=0.47478, val_loss=2.06559, val_acc=0.45438, time=1.26901
Epoch:0004, train_loss=1.87748, train_acc=0.51286, val_loss=2.06900, val_acc=0.46168, time=1.26201
Epoch:0005, train_loss=1.88544, train_acc=0.52664, val_loss=2.07144, val_acc=0.44708, time=1.29201
Epoch:0006, train_loss=1.88725, train_acc=0.54345, val_loss=2.07299, val_acc=0.44708, time=1.29100
Epoch:0007, train_loss=1.88390, train_acc=0.55884, val_loss=2.07334, val_acc=0.44708, time=1.25802
Epoch:0008, train_loss=1.87285, train_acc=0.57667, val_loss=2.07264, val_acc=0.45620, time=1.27399
Epoch:0009, train_loss=1.85460, train_acc=0.59206, val_loss=2.07167, val_acc=0.45073, time=1.36601
Epoch:0010, train_loss=1.83539, train_acc=0.60057, val_loss=2.07093, val_acc=0.45255, time=1.23801
Epoch:0011, train_loss=1.81865, train_acc=0.60523, val_loss=2.07045, val_acc=0.45803, time=1.25001
Epoch:0012, train_loss=1.80397, train_acc=0.61454, val_loss=2.07016, val_acc=0.45620, time=1.25001
Epoch:0013, train_loss=1.79020, train_acc=0.62811, val_loss=2.07007, val_acc=0.44161, time=1.20402
Epoch:0014, train_loss=1.77766, train_acc=0.64047, val_loss=2.07024, val_acc=0.43978, time=1.30602
Epoch:0015, train_loss=1.76709, train_acc=0.65546, val_loss=2.07063, val_acc=0.43248, time=1.23099
Epoch:0016, train_loss=1.75837, train_acc=0.66640, val_loss=2.07113, val_acc=0.43248, time=1.16801
Epoch:0017, train_loss=1.75060, train_acc=0.67085, val_loss=2.07161, val_acc=0.42701, time=1.32801
Epoch:0018, train_loss=1.74288, train_acc=0.67754, val_loss=2.07201, val_acc=0.42701, time=1.23201
Epoch:0019, train_loss=1.73491, train_acc=0.67855, val_loss=2.07234, val_acc=0.42701, time=1.21901
Epoch:0020, train_loss=1.72685, train_acc=0.68220, val_loss=2.07262, val_acc=0.42518, time=1.37202
Epoch:0021, train_loss=1.71900, train_acc=0.68584, val_loss=2.07288, val_acc=0.42153, time=1.41301
Epoch:0022, train_loss=1.71163, train_acc=0.68847, val_loss=2.07313, val_acc=0.41971, time=1.27200
Epoch:0023, train_loss=1.70484, train_acc=0.69192, val_loss=2.07338, val_acc=0.41788, time=1.21601
Epoch:0024, train_loss=1.69866, train_acc=0.69779, val_loss=2.07363, val_acc=0.42153, time=1.24402
Epoch:0025, train_loss=1.69296, train_acc=0.70488, val_loss=2.07387, val_acc=0.41971, time=1.21399
Epoch:0026, train_loss=1.68758, train_acc=0.71035, val_loss=2.07410, val_acc=0.41971, time=1.31201
Epoch:0027, train_loss=1.68245, train_acc=0.71501, val_loss=2.07433, val_acc=0.41606, time=1.34202
Epoch:0028, train_loss=1.67759, train_acc=0.72068, val_loss=2.07458, val_acc=0.41606, time=1.35300
Epoch:0029, train_loss=1.67310, train_acc=0.72635, val_loss=2.07488, val_acc=0.41058, time=1.14201
Epoch:0030, train_loss=1.66902, train_acc=0.73020, val_loss=2.07522, val_acc=0.40693, time=1.12102
Epoch:0031, train_loss=1.66533, train_acc=0.73405, val_loss=2.07561, val_acc=0.40693, time=1.34500
Epoch:0032, train_loss=1.66194, train_acc=0.73729, val_loss=2.07604, val_acc=0.40511, time=1.38601
Epoch:0033, train_loss=1.65871, train_acc=0.74134, val_loss=2.07651, val_acc=0.40328, time=1.35501
Epoch:0034, train_loss=1.65551, train_acc=0.74357, val_loss=2.07701, val_acc=0.40146, time=1.23100
Epoch:0035, train_loss=1.65223, train_acc=0.74742, val_loss=2.07754, val_acc=0.40146, time=1.17802
Epoch:0036, train_loss=1.64881, train_acc=0.75309, val_loss=2.07810, val_acc=0.39781, time=1.31699
Epoch:0037, train_loss=1.64527, train_acc=0.75836, val_loss=2.07869, val_acc=0.39781, time=1.23801
Epoch:0038, train_loss=1.64173, train_acc=0.76403, val_loss=2.07930, val_acc=0.39416, time=1.29502
Epoch:0039, train_loss=1.63829, train_acc=0.76666, val_loss=2.07995, val_acc=0.39234, time=1.32499
Epoch:0040, train_loss=1.63502, train_acc=0.76929, val_loss=2.08060, val_acc=0.39416, time=1.35701
Epoch:0041, train_loss=1.63196, train_acc=0.77213, val_loss=2.08125, val_acc=0.39051, time=1.27100
Epoch:0042, train_loss=1.62905, train_acc=0.77577, val_loss=2.08188, val_acc=0.38686, time=1.33701
Epoch:0043, train_loss=1.62627, train_acc=0.78064, val_loss=2.08249, val_acc=0.38686, time=1.32800
Epoch:0044, train_loss=1.62358, train_acc=0.78489, val_loss=2.08308, val_acc=0.39234, time=1.18401
Epoch:0045, train_loss=1.62092, train_acc=0.78813, val_loss=2.08363, val_acc=0.39051, time=1.21701
Epoch:0046, train_loss=1.61828, train_acc=0.79117, val_loss=2.08416, val_acc=0.39416, time=1.27501
Epoch:0047, train_loss=1.61566, train_acc=0.79400, val_loss=2.08466, val_acc=0.39234, time=1.22501
Epoch:0048, train_loss=1.61309, train_acc=0.79684, val_loss=2.08514, val_acc=0.39416, time=1.23000
Epoch:0049, train_loss=1.61058, train_acc=0.80069, val_loss=2.08561, val_acc=0.39599, time=1.30103
Epoch:0050, train_loss=1.60814, train_acc=0.80271, val_loss=2.08607, val_acc=0.39416, time=1.28598
Epoch:0051, train_loss=1.60579, train_acc=0.80758, val_loss=2.08653, val_acc=0.39051, time=1.27601
Epoch:0052, train_loss=1.60350, train_acc=0.81163, val_loss=2.08701, val_acc=0.39051, time=1.28201
Epoch:0053, train_loss=1.60127, train_acc=0.81547, val_loss=2.08750, val_acc=0.39599, time=1.35501
Epoch:0054, train_loss=1.59906, train_acc=0.81831, val_loss=2.08802, val_acc=0.39781, time=1.28602
Epoch:0055, train_loss=1.59688, train_acc=0.82115, val_loss=2.08856, val_acc=0.39599, time=1.22799
Epoch:0056, train_loss=1.59471, train_acc=0.82378, val_loss=2.08911, val_acc=0.39416, time=1.35302
Epoch:0057, train_loss=1.59258, train_acc=0.82560, val_loss=2.08968, val_acc=0.39234, time=1.19301
Epoch:0058, train_loss=1.59049, train_acc=0.82641, val_loss=2.09024, val_acc=0.39051, time=1.29301
Epoch:0059, train_loss=1.58844, train_acc=0.82864, val_loss=2.09080, val_acc=0.39051, time=1.41101
Epoch:0060, train_loss=1.58644, train_acc=0.83168, val_loss=2.09135, val_acc=0.39234, time=1.20301
Epoch:0061, train_loss=1.58449, train_acc=0.83431, val_loss=2.09189, val_acc=0.39234, time=1.26601
Epoch:0062, train_loss=1.58255, train_acc=0.83654, val_loss=2.09242, val_acc=0.39051, time=1.44202
Epoch:0063, train_loss=1.58065, train_acc=0.83816, val_loss=2.09294, val_acc=0.39051, time=1.29600
Epoch:0064, train_loss=1.57876, train_acc=0.84059, val_loss=2.09344, val_acc=0.39051, time=1.27102
Epoch:0065, train_loss=1.57692, train_acc=0.84343, val_loss=2.09393, val_acc=0.39234, time=1.29300
Epoch:0066, train_loss=1.57510, train_acc=0.84606, val_loss=2.09442, val_acc=0.39234, time=1.29200
Epoch:0067, train_loss=1.57333, train_acc=0.84910, val_loss=2.09492, val_acc=0.39234, time=1.20400
Epoch:0068, train_loss=1.57158, train_acc=0.85112, val_loss=2.09544, val_acc=0.39234, time=1.19202
Epoch:0069, train_loss=1.56987, train_acc=0.85214, val_loss=2.09597, val_acc=0.39234, time=1.24400
Epoch:0070, train_loss=1.56817, train_acc=0.85335, val_loss=2.09651, val_acc=0.39234, time=1.31101
Epoch:0071, train_loss=1.56649, train_acc=0.85497, val_loss=2.09706, val_acc=0.39234, time=1.24400
Epoch:0072, train_loss=1.56484, train_acc=0.85659, val_loss=2.09762, val_acc=0.39051, time=1.34002
Epoch:0073, train_loss=1.56321, train_acc=0.85923, val_loss=2.09819, val_acc=0.39051, time=1.25400
Epoch:0074, train_loss=1.56162, train_acc=0.86044, val_loss=2.09877, val_acc=0.39051, time=1.34002
Epoch:0075, train_loss=1.56006, train_acc=0.86226, val_loss=2.09934, val_acc=0.39234, time=1.34400
Epoch:0076, train_loss=1.55852, train_acc=0.86328, val_loss=2.09991, val_acc=0.39234, time=1.25502
Epoch:0077, train_loss=1.55700, train_acc=0.86449, val_loss=2.10047, val_acc=0.39051, time=1.25500
Epoch:0078, train_loss=1.55550, train_acc=0.86652, val_loss=2.10102, val_acc=0.38869, time=1.25502
Epoch:0079, train_loss=1.55402, train_acc=0.86854, val_loss=2.10156, val_acc=0.38869, time=1.25400
Epoch:0080, train_loss=1.55256, train_acc=0.87016, val_loss=2.10211, val_acc=0.38869, time=1.25000
Epoch:0081, train_loss=1.55113, train_acc=0.87239, val_loss=2.10266, val_acc=0.38686, time=1.25799
Epoch:0082, train_loss=1.54972, train_acc=0.87462, val_loss=2.10323, val_acc=0.38504, time=1.37101
Epoch:0083, train_loss=1.54833, train_acc=0.87563, val_loss=2.10380, val_acc=0.38321, time=1.25900
Epoch:0084, train_loss=1.54697, train_acc=0.87705, val_loss=2.10437, val_acc=0.38321, time=1.28301
Epoch:0085, train_loss=1.54562, train_acc=0.87968, val_loss=2.10496, val_acc=0.38321, time=1.24601
Epoch:0086, train_loss=1.54429, train_acc=0.88049, val_loss=2.10555, val_acc=0.38321, time=1.31402
Epoch:0087, train_loss=1.54298, train_acc=0.88130, val_loss=2.10614, val_acc=0.38139, time=1.37301
Epoch:0088, train_loss=1.54169, train_acc=0.88211, val_loss=2.10672, val_acc=0.38139, time=1.26701
Epoch:0089, train_loss=1.54042, train_acc=0.88394, val_loss=2.10730, val_acc=0.38139, time=1.15300
Epoch:0090, train_loss=1.53917, train_acc=0.88455, val_loss=2.10787, val_acc=0.38321, time=1.24298
Epoch:0091, train_loss=1.53793, train_acc=0.88536, val_loss=2.10844, val_acc=0.38321, time=1.28202
Epoch:0092, train_loss=1.53672, train_acc=0.88617, val_loss=2.10900, val_acc=0.38321, time=1.19300
Epoch:0093, train_loss=1.53552, train_acc=0.88718, val_loss=2.10956, val_acc=0.38321, time=1.31901
Epoch:0094, train_loss=1.53434, train_acc=0.88799, val_loss=2.11012, val_acc=0.38504, time=1.29201
Epoch:0095, train_loss=1.53318, train_acc=0.88880, val_loss=2.11069, val_acc=0.38504, time=1.40301
Epoch:0096, train_loss=1.53203, train_acc=0.89001, val_loss=2.11126, val_acc=0.38686, time=1.23000
Epoch:0097, train_loss=1.53090, train_acc=0.89082, val_loss=2.11185, val_acc=0.38686, time=1.26502
Epoch:0098, train_loss=1.52979, train_acc=0.89123, val_loss=2.11243, val_acc=0.39051, time=1.25401
Epoch:0099, train_loss=1.52869, train_acc=0.89244, val_loss=2.11302, val_acc=0.39051, time=1.31800
Epoch:0100, train_loss=1.52761, train_acc=0.89447, val_loss=2.11360, val_acc=0.38869, time=1.32600
Epoch:0101, train_loss=1.52654, train_acc=0.89690, val_loss=2.11419, val_acc=0.38869, time=1.24901
Epoch:0102, train_loss=1.52549, train_acc=0.89832, val_loss=2.11477, val_acc=0.38321, time=1.35202
Early stopping...

Optimization Finished!

Test set results: loss= 2.19016, accuracy= 0.39698, time= 0.34500

Test Precision, Recall and F1-Score...
              precision    recall  f1-score   support

           0     0.3285    0.2960    0.3114       696
           1     0.4914    0.6057    0.5426      1083
           2     0.0204    0.0115    0.0147        87
           3     0.0196    0.0083    0.0116       121
           4     0.0588    0.0533    0.0559        75
           5     0.0000    0.0000    0.0000        10
           6     0.0000    0.0000    0.0000        81
           7     0.0476    0.0278    0.0351        36

    accuracy                         0.3970      2189
   macro avg     0.1208    0.1253    0.1214      2189
weighted avg     0.3523    0.3970    0.3712      2189


Macro average Test Precision, Recall and F1-Score...
(0.12079912444521919, 0.12532145958758292, 0.12142202582513276, None)

Micro average Test Precision, Recall and F1-Score...
(0.3969849246231156, 0.3969849246231156, 0.3969849246231156, None)

Embeddings:
Word_embeddings:7688
Train_doc_embeddings:5485
Test_doc_embeddings:2189
