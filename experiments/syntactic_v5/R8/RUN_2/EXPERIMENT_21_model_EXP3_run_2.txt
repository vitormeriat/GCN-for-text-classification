
==========: 260092104038300
Epoch:0001, train_loss=2.45279, train_acc=0.03504, val_loss=2.08579, val_acc=0.18431, time=1.35602
Epoch:0002, train_loss=2.11831, train_acc=0.21936, val_loss=2.06703, val_acc=0.38504, time=1.35901
Epoch:0003, train_loss=1.92461, train_acc=0.44237, val_loss=2.06451, val_acc=0.44891, time=1.33802
Epoch:0004, train_loss=1.87684, train_acc=0.51266, val_loss=2.06877, val_acc=0.46898, time=1.27501
Epoch:0005, train_loss=1.89169, train_acc=0.53008, val_loss=2.07183, val_acc=0.47445, time=1.28400
Epoch:0006, train_loss=1.89967, train_acc=0.54183, val_loss=2.07360, val_acc=0.45073, time=1.24801
Epoch:0007, train_loss=1.89980, train_acc=0.55439, val_loss=2.07488, val_acc=0.45073, time=1.29901
Epoch:0008, train_loss=1.89789, train_acc=0.56228, val_loss=2.07475, val_acc=0.44891, time=1.33702
Epoch:0009, train_loss=1.88415, train_acc=0.57646, val_loss=2.07356, val_acc=0.46533, time=1.30099
Epoch:0010, train_loss=1.86173, train_acc=0.59490, val_loss=2.07212, val_acc=0.47080, time=1.32103
Epoch:0011, train_loss=1.83834, train_acc=0.60016, val_loss=2.07063, val_acc=0.47445, time=1.30801
Epoch:0012, train_loss=1.81601, train_acc=0.60462, val_loss=2.06906, val_acc=0.46898, time=1.25101
Epoch:0013, train_loss=1.79449, train_acc=0.61171, val_loss=2.06768, val_acc=0.46350, time=1.18801
Epoch:0014, train_loss=1.77563, train_acc=0.62285, val_loss=2.06684, val_acc=0.45620, time=1.37201
Epoch:0015, train_loss=1.76226, train_acc=0.63682, val_loss=2.06668, val_acc=0.44343, time=1.26401
Epoch:0016, train_loss=1.75481, train_acc=0.65404, val_loss=2.06697, val_acc=0.43796, time=1.29701
Epoch:0017, train_loss=1.75070, train_acc=0.66579, val_loss=2.06738, val_acc=0.42701, time=1.24002
Epoch:0018, train_loss=1.74668, train_acc=0.67227, val_loss=2.06775, val_acc=0.42701, time=1.19900
Epoch:0019, train_loss=1.74104, train_acc=0.67774, val_loss=2.06805, val_acc=0.41788, time=1.36600
Epoch:0020, train_loss=1.73387, train_acc=0.68220, val_loss=2.06834, val_acc=0.41423, time=1.29900
Early stopping...

Optimization Finished!

Test set results: loss= 2.02321, accuracy= 0.42622, time= 0.32299

Test Precision, Recall and F1-Score...
              precision    recall  f1-score   support

           0     0.2992    0.1552    0.2044       696
           1     0.4887    0.7562    0.5937      1083
           2     0.0556    0.0230    0.0325        87
           3     0.0000    0.0000    0.0000       121
           4     0.0385    0.0267    0.0315        75
           5     0.0000    0.0000    0.0000        10
           6     0.0625    0.0123    0.0206        81
           7     0.1111    0.0278    0.0444        36

    accuracy                         0.4262      2189
   macro avg     0.1319    0.1251    0.1159      2189
weighted avg     0.3446    0.4262    0.3626      2189


Macro average Test Precision, Recall and F1-Score...
(0.13193258308554134, 0.12514796624720367, 0.11589058699426821, None)

Micro average Test Precision, Recall and F1-Score...
(0.4262220191868433, 0.4262220191868433, 0.4262220191868433, None)

Embeddings:
Word_embeddings:7688
Train_doc_embeddings:5485
Test_doc_embeddings:2189
