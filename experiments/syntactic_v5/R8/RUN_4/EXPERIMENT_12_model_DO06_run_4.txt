
==========: 266477513530000
Epoch:0001, train_loss=2.28607, train_acc=0.04841, val_loss=2.06976, val_acc=0.36314, time=1.33000
Epoch:0002, train_loss=1.98860, train_acc=0.39396, val_loss=2.06217, val_acc=0.46168, time=1.17601
Epoch:0003, train_loss=1.89136, train_acc=0.49483, val_loss=2.06471, val_acc=0.46533, time=1.32101
Epoch:0004, train_loss=1.88720, train_acc=0.51752, val_loss=2.06780, val_acc=0.46350, time=1.28801
Epoch:0005, train_loss=1.89090, train_acc=0.52866, val_loss=2.07058, val_acc=0.44708, time=1.20201
Epoch:0006, train_loss=1.89425, train_acc=0.53940, val_loss=2.07183, val_acc=0.44161, time=1.30401
Epoch:0007, train_loss=1.88600, train_acc=0.55439, val_loss=2.07179, val_acc=0.45620, time=1.25201
Epoch:0008, train_loss=1.86814, train_acc=0.57930, val_loss=2.07129, val_acc=0.46533, time=1.24803
Epoch:0009, train_loss=1.84819, train_acc=0.58477, val_loss=2.07047, val_acc=0.46350, time=1.29301
Epoch:0010, train_loss=1.82731, train_acc=0.59368, val_loss=2.06951, val_acc=0.46350, time=1.31200
Epoch:0011, train_loss=1.80689, train_acc=0.60806, val_loss=2.06883, val_acc=0.45255, time=1.28602
Epoch:0012, train_loss=1.79054, train_acc=0.62224, val_loss=2.06858, val_acc=0.42883, time=1.33699
Epoch:0013, train_loss=1.77922, train_acc=0.63905, val_loss=2.06853, val_acc=0.42701, time=1.22702
Epoch:0014, train_loss=1.77014, train_acc=0.64837, val_loss=2.06845, val_acc=0.41971, time=1.30101
Epoch:0015, train_loss=1.76064, train_acc=0.65667, val_loss=2.06833, val_acc=0.41788, time=1.28601
Epoch:0016, train_loss=1.75032, train_acc=0.66417, val_loss=2.06825, val_acc=0.41788, time=1.27800
Epoch:0017, train_loss=1.74006, train_acc=0.66842, val_loss=2.06829, val_acc=0.43066, time=1.19901
Epoch:0018, train_loss=1.73072, train_acc=0.67065, val_loss=2.06848, val_acc=0.42701, time=1.22900
Epoch:0019, train_loss=1.72259, train_acc=0.67207, val_loss=2.06878, val_acc=0.42518, time=1.23701
Early stopping...

Optimization Finished!

Test set results: loss= 2.02088, accuracy= 0.44130, time= 0.54101

Test Precision, Recall and F1-Score...
              precision    recall  f1-score   support

           0     0.3226    0.1724    0.2247       696
           1     0.4939    0.7793    0.6046      1083
           2     0.0000    0.0000    0.0000        87
           3     0.0000    0.0000    0.0000       121
           4     0.0294    0.0133    0.0183        75
           5     0.0000    0.0000    0.0000        10
           6     0.0000    0.0000    0.0000        81
           7     0.2500    0.0278    0.0500        36

    accuracy                         0.4413      2189
   macro avg     0.1370    0.1241    0.1122      2189
weighted avg     0.3520    0.4413    0.3720      2189


Macro average Test Precision, Recall and F1-Score...
(0.1369810582550467, 0.12410520213115972, 0.1122065315246796, None)

Micro average Test Precision, Recall and F1-Score...
(0.4412973960712654, 0.4412973960712654, 0.4412973960712654, None)

Embeddings:
Word_embeddings:7688
Train_doc_embeddings:5485
Test_doc_embeddings:2189
