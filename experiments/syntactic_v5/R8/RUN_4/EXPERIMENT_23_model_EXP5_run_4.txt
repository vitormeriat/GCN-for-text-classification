
==========: 266717059421000
Epoch:0001, train_loss=2.30152, train_acc=0.12923, val_loss=2.07878, val_acc=0.45803, time=1.47501
Epoch:0002, train_loss=2.06763, train_acc=0.48147, val_loss=2.07060, val_acc=0.48723, time=1.22300
Epoch:0003, train_loss=1.96867, train_acc=0.51266, val_loss=2.06462, val_acc=0.48905, time=1.24701
Epoch:0004, train_loss=1.89617, train_acc=0.52177, val_loss=2.06313, val_acc=0.43248, time=1.35600
Epoch:0005, train_loss=1.86921, train_acc=0.47478, val_loss=2.06749, val_acc=0.36314, time=1.36701
Epoch:0006, train_loss=1.89634, train_acc=0.41240, val_loss=2.06893, val_acc=0.35949, time=1.45799
Epoch:0007, train_loss=1.89314, train_acc=0.42779, val_loss=2.06772, val_acc=0.42883, time=1.28302
Epoch:0008, train_loss=1.86335, train_acc=0.49686, val_loss=2.06714, val_acc=0.44891, time=1.21301
Epoch:0009, train_loss=1.83953, train_acc=0.55317, val_loss=2.06772, val_acc=0.47445, time=1.29001
Epoch:0010, train_loss=1.82911, train_acc=0.56775, val_loss=2.06779, val_acc=0.47445, time=1.23000
Epoch:0011, train_loss=1.81769, train_acc=0.56735, val_loss=2.06668, val_acc=0.47445, time=1.24600
Epoch:0012, train_loss=1.79876, train_acc=0.57201, val_loss=2.06505, val_acc=0.47263, time=1.37400
Epoch:0013, train_loss=1.77788, train_acc=0.59611, val_loss=2.06389, val_acc=0.45255, time=1.26200
Epoch:0014, train_loss=1.76288, train_acc=0.61778, val_loss=2.06365, val_acc=0.45985, time=1.34001
Epoch:0015, train_loss=1.75644, train_acc=0.63601, val_loss=2.06405, val_acc=0.43066, time=1.27001
Epoch:0016, train_loss=1.75491, train_acc=0.63541, val_loss=2.06456, val_acc=0.41241, time=1.33802
Epoch:0017, train_loss=1.75277, train_acc=0.63763, val_loss=2.06488, val_acc=0.41788, time=1.23501
Epoch:0018, train_loss=1.74722, train_acc=0.65141, val_loss=2.06505, val_acc=0.43248, time=1.41302
Epoch:0019, train_loss=1.73887, train_acc=0.67247, val_loss=2.06527, val_acc=0.43248, time=1.25601
Epoch:0020, train_loss=1.72996, train_acc=0.67369, val_loss=2.06569, val_acc=0.43796, time=1.20801
Early stopping...

Optimization Finished!

Test set results: loss= 2.01976, accuracy= 0.44587, time= 0.41301

Test Precision, Recall and F1-Score...
              precision    recall  f1-score   support

           0     0.3300    0.1422    0.1988       696
           1     0.4974    0.8033    0.6144      1083
           2     0.0345    0.0115    0.0172        87
           3     0.0769    0.0248    0.0375       121
           4     0.0263    0.0133    0.0177        75
           5     0.0000    0.0000    0.0000        10
           6     0.0333    0.0123    0.0180        81
           7     0.3333    0.0278    0.0513        36

    accuracy                         0.4459      2189
   macro avg     0.1665    0.1294    0.1194      2189
weighted avg     0.3643    0.4459    0.3721      2189


Macro average Test Precision, Recall and F1-Score...
(0.16647692411060044, 0.12941373880751358, 0.11936781550482131, None)

Micro average Test Precision, Recall and F1-Score...
(0.4458656920968479, 0.4458656920968479, 0.4458656920968479, None)

Embeddings:
Word_embeddings:7688
Train_doc_embeddings:5485
Test_doc_embeddings:2189
