
==========: 263198260448200
Epoch:0001, train_loss=2.41882, train_acc=0.04152, val_loss=2.08251, val_acc=0.27372, time=1.39502
Epoch:0002, train_loss=2.10415, train_acc=0.28783, val_loss=2.06616, val_acc=0.39781, time=1.32600
Epoch:0003, train_loss=1.93223, train_acc=0.43225, val_loss=2.06437, val_acc=0.46533, time=1.19302
Epoch:0004, train_loss=1.89431, train_acc=0.50557, val_loss=2.06851, val_acc=0.48723, time=1.33501
Epoch:0005, train_loss=1.91252, train_acc=0.52258, val_loss=2.07000, val_acc=0.48540, time=1.40101
Epoch:0006, train_loss=1.90788, train_acc=0.52947, val_loss=2.06914, val_acc=0.47628, time=1.29099
Epoch:0007, train_loss=1.88325, train_acc=0.53636, val_loss=2.06845, val_acc=0.47263, time=1.34601
Epoch:0008, train_loss=1.86193, train_acc=0.54588, val_loss=2.06889, val_acc=0.39416, time=1.23502
Epoch:0009, train_loss=1.85367, train_acc=0.53251, val_loss=2.06905, val_acc=0.37044, time=1.29100
Epoch:0010, train_loss=1.84556, train_acc=0.52380, val_loss=2.06816, val_acc=0.38504, time=1.26701
Epoch:0011, train_loss=1.82935, train_acc=0.55236, val_loss=2.06684, val_acc=0.43431, time=1.22900
Epoch:0012, train_loss=1.80943, train_acc=0.59591, val_loss=2.06592, val_acc=0.44891, time=1.28701
Epoch:0013, train_loss=1.79284, train_acc=0.61292, val_loss=2.06568, val_acc=0.44708, time=1.23200
Epoch:0014, train_loss=1.78207, train_acc=0.61049, val_loss=2.06585, val_acc=0.44526, time=1.26802
Epoch:0015, train_loss=1.77456, train_acc=0.60725, val_loss=2.06602, val_acc=0.44343, time=1.23001
Epoch:0016, train_loss=1.76681, train_acc=0.61069, val_loss=2.06608, val_acc=0.43613, time=1.29700
Epoch:0017, train_loss=1.75777, train_acc=0.62589, val_loss=2.06616, val_acc=0.42701, time=1.25401
Epoch:0018, train_loss=1.74886, train_acc=0.64270, val_loss=2.06645, val_acc=0.42336, time=1.24502
Epoch:0019, train_loss=1.74187, train_acc=0.65404, val_loss=2.06700, val_acc=0.43248, time=1.22501
Early stopping...

Optimization Finished!

Test set results: loss= 2.01501, accuracy= 0.43079, time= 0.39800

Test Precision, Recall and F1-Score...
              precision    recall  f1-score   support

           0     0.3111    0.2141    0.2536       696
           1     0.4880    0.7295    0.5848      1083
           2     0.0000    0.0000    0.0000        87
           3     0.0000    0.0000    0.0000       121
           4     0.0588    0.0400    0.0476        75
           5     0.0000    0.0000    0.0000        10
           6     0.0000    0.0000    0.0000        81
           7     1.0000    0.0278    0.0541        36

    accuracy                         0.4308      2189
   macro avg     0.2322    0.1264    0.1175      2189
weighted avg     0.3588    0.4308    0.3725      2189


Macro average Test Precision, Recall and F1-Score...
(0.23223047195979646, 0.12641418181721698, 0.11750526980986688, None)

Micro average Test Precision, Recall and F1-Score...
(0.4307903152124258, 0.4307903152124258, 0.4307903152124258, None)

Embeddings:
Word_embeddings:7688
Train_doc_embeddings:5485
Test_doc_embeddings:2189
