
==========: 261704390694000
Epoch:0001, train_loss=2.10393, train_acc=0.16143, val_loss=2.08162, val_acc=0.20073, time=1.33501
Epoch:0002, train_loss=2.08973, train_acc=0.20316, val_loss=2.08027, val_acc=0.25182, time=1.24201
Epoch:0003, train_loss=2.07611, train_acc=0.24873, val_loss=2.07899, val_acc=0.28102, time=1.26901
Epoch:0004, train_loss=2.06309, train_acc=0.28965, val_loss=2.07777, val_acc=0.31569, time=1.23501
Epoch:0005, train_loss=2.05068, train_acc=0.31922, val_loss=2.07662, val_acc=0.33577, time=1.22302
Epoch:0006, train_loss=2.03889, train_acc=0.35143, val_loss=2.07555, val_acc=0.35584, time=1.24399
Epoch:0007, train_loss=2.02771, train_acc=0.37492, val_loss=2.07454, val_acc=0.37226, time=1.32300
Epoch:0008, train_loss=2.01713, train_acc=0.39376, val_loss=2.07359, val_acc=0.39416, time=1.32902
Epoch:0009, train_loss=2.00713, train_acc=0.40875, val_loss=2.07271, val_acc=0.41058, time=1.23801
Epoch:0010, train_loss=1.99772, train_acc=0.42090, val_loss=2.07189, val_acc=0.41606, time=1.33301
Epoch:0011, train_loss=1.98886, train_acc=0.43245, val_loss=2.07113, val_acc=0.42883, time=1.34101
Epoch:0012, train_loss=1.98054, train_acc=0.44420, val_loss=2.07042, val_acc=0.43066, time=1.35000
Epoch:0013, train_loss=1.97273, train_acc=0.45513, val_loss=2.06977, val_acc=0.43431, time=1.26201
Epoch:0014, train_loss=1.96540, train_acc=0.46344, val_loss=2.06916, val_acc=0.43978, time=1.29402
Epoch:0015, train_loss=1.95853, train_acc=0.46992, val_loss=2.06861, val_acc=0.44161, time=1.39098
Epoch:0016, train_loss=1.95209, train_acc=0.47458, val_loss=2.06809, val_acc=0.44343, time=1.35502
Epoch:0017, train_loss=1.94606, train_acc=0.47823, val_loss=2.06762, val_acc=0.44343, time=1.24200
Epoch:0018, train_loss=1.94041, train_acc=0.48126, val_loss=2.06718, val_acc=0.44526, time=1.32901
Epoch:0019, train_loss=1.93509, train_acc=0.48511, val_loss=2.06678, val_acc=0.44708, time=1.26701
Epoch:0020, train_loss=1.93010, train_acc=0.48592, val_loss=2.06641, val_acc=0.45255, time=1.26100
Epoch:0021, train_loss=1.92541, train_acc=0.48734, val_loss=2.06606, val_acc=0.45255, time=1.31001
Epoch:0022, train_loss=1.92100, train_acc=0.48937, val_loss=2.06575, val_acc=0.45255, time=1.21701
Epoch:0023, train_loss=1.91685, train_acc=0.49240, val_loss=2.06546, val_acc=0.45620, time=1.33701
Epoch:0024, train_loss=1.91295, train_acc=0.49281, val_loss=2.06519, val_acc=0.45438, time=1.21801
Epoch:0025, train_loss=1.90926, train_acc=0.49423, val_loss=2.06495, val_acc=0.45620, time=1.32601
Epoch:0026, train_loss=1.90579, train_acc=0.49666, val_loss=2.06472, val_acc=0.45438, time=1.30801
Epoch:0027, train_loss=1.90252, train_acc=0.49868, val_loss=2.06452, val_acc=0.45438, time=1.28602
Epoch:0028, train_loss=1.89942, train_acc=0.49990, val_loss=2.06433, val_acc=0.45255, time=1.18000
Epoch:0029, train_loss=1.89650, train_acc=0.50030, val_loss=2.06416, val_acc=0.45438, time=1.30100
Epoch:0030, train_loss=1.89373, train_acc=0.50132, val_loss=2.06401, val_acc=0.45620, time=1.33901
Epoch:0031, train_loss=1.89111, train_acc=0.50213, val_loss=2.06387, val_acc=0.45620, time=1.33201
Epoch:0032, train_loss=1.88862, train_acc=0.50132, val_loss=2.06374, val_acc=0.45073, time=1.42601
Epoch:0033, train_loss=1.88626, train_acc=0.50294, val_loss=2.06362, val_acc=0.45073, time=1.35503
Epoch:0034, train_loss=1.88400, train_acc=0.50253, val_loss=2.06351, val_acc=0.44708, time=1.28799
Epoch:0035, train_loss=1.88184, train_acc=0.50334, val_loss=2.06342, val_acc=0.44708, time=1.28901
Epoch:0036, train_loss=1.87977, train_acc=0.50375, val_loss=2.06333, val_acc=0.44891, time=1.22301
Epoch:0037, train_loss=1.87778, train_acc=0.50395, val_loss=2.06325, val_acc=0.44708, time=1.27801
Epoch:0038, train_loss=1.87585, train_acc=0.50375, val_loss=2.06317, val_acc=0.44526, time=1.24999
Epoch:0039, train_loss=1.87399, train_acc=0.50577, val_loss=2.06310, val_acc=0.44708, time=1.33000
Epoch:0040, train_loss=1.87218, train_acc=0.50638, val_loss=2.06304, val_acc=0.44526, time=1.28199
Epoch:0041, train_loss=1.87042, train_acc=0.50719, val_loss=2.06298, val_acc=0.44526, time=1.30602
Epoch:0042, train_loss=1.86871, train_acc=0.50739, val_loss=2.06292, val_acc=0.44708, time=1.36500
Epoch:0043, train_loss=1.86703, train_acc=0.50901, val_loss=2.06287, val_acc=0.44708, time=1.32901
Epoch:0044, train_loss=1.86539, train_acc=0.51104, val_loss=2.06282, val_acc=0.44708, time=1.27702
Epoch:0045, train_loss=1.86378, train_acc=0.51306, val_loss=2.06277, val_acc=0.44708, time=1.24800
Epoch:0046, train_loss=1.86220, train_acc=0.51489, val_loss=2.06273, val_acc=0.44708, time=1.25401
Epoch:0047, train_loss=1.86065, train_acc=0.51671, val_loss=2.06269, val_acc=0.44708, time=1.24100
Epoch:0048, train_loss=1.85913, train_acc=0.51934, val_loss=2.06265, val_acc=0.44891, time=1.37001
Epoch:0049, train_loss=1.85764, train_acc=0.52157, val_loss=2.06261, val_acc=0.44891, time=1.35103
Epoch:0050, train_loss=1.85616, train_acc=0.52258, val_loss=2.06257, val_acc=0.44708, time=1.31400
Epoch:0051, train_loss=1.85471, train_acc=0.52299, val_loss=2.06254, val_acc=0.45073, time=1.38401
Epoch:0052, train_loss=1.85328, train_acc=0.52522, val_loss=2.06251, val_acc=0.45073, time=1.29199
Epoch:0053, train_loss=1.85188, train_acc=0.52643, val_loss=2.06248, val_acc=0.45255, time=1.24601
Epoch:0054, train_loss=1.85049, train_acc=0.52785, val_loss=2.06245, val_acc=0.45255, time=1.34301
Epoch:0055, train_loss=1.84912, train_acc=0.52947, val_loss=2.06242, val_acc=0.45255, time=1.26100
Epoch:0056, train_loss=1.84777, train_acc=0.53028, val_loss=2.06239, val_acc=0.45438, time=1.23802
Epoch:0057, train_loss=1.84644, train_acc=0.53129, val_loss=2.06237, val_acc=0.45438, time=1.23199
Epoch:0058, train_loss=1.84512, train_acc=0.53231, val_loss=2.06234, val_acc=0.45438, time=1.30700
Epoch:0059, train_loss=1.84383, train_acc=0.53413, val_loss=2.06232, val_acc=0.45438, time=1.26201
Epoch:0060, train_loss=1.84254, train_acc=0.53575, val_loss=2.06230, val_acc=0.45620, time=1.28501
Epoch:0061, train_loss=1.84127, train_acc=0.53595, val_loss=2.06228, val_acc=0.45438, time=1.26600
Epoch:0062, train_loss=1.84002, train_acc=0.53697, val_loss=2.06226, val_acc=0.45255, time=1.30202
Epoch:0063, train_loss=1.83878, train_acc=0.53757, val_loss=2.06224, val_acc=0.45255, time=1.27500
Epoch:0064, train_loss=1.83756, train_acc=0.53919, val_loss=2.06222, val_acc=0.45438, time=1.23401
Epoch:0065, train_loss=1.83634, train_acc=0.54081, val_loss=2.06221, val_acc=0.45438, time=1.29800
Epoch:0066, train_loss=1.83515, train_acc=0.54264, val_loss=2.06219, val_acc=0.45438, time=1.24900
Epoch:0067, train_loss=1.83396, train_acc=0.54284, val_loss=2.06218, val_acc=0.45438, time=1.28502
Epoch:0068, train_loss=1.83279, train_acc=0.54487, val_loss=2.06217, val_acc=0.45255, time=1.25199
Epoch:0069, train_loss=1.83163, train_acc=0.54608, val_loss=2.06216, val_acc=0.45255, time=1.24601
Epoch:0070, train_loss=1.83048, train_acc=0.54649, val_loss=2.06215, val_acc=0.45438, time=1.32900
Epoch:0071, train_loss=1.82934, train_acc=0.54831, val_loss=2.06214, val_acc=0.45438, time=1.32302
Epoch:0072, train_loss=1.82821, train_acc=0.55033, val_loss=2.06214, val_acc=0.45255, time=1.28601
Epoch:0073, train_loss=1.82710, train_acc=0.55135, val_loss=2.06213, val_acc=0.45255, time=1.30700
Epoch:0074, train_loss=1.82599, train_acc=0.55155, val_loss=2.06213, val_acc=0.45255, time=1.24400
Epoch:0075, train_loss=1.82489, train_acc=0.55276, val_loss=2.06212, val_acc=0.45073, time=1.22201
Epoch:0076, train_loss=1.82381, train_acc=0.55418, val_loss=2.06212, val_acc=0.45073, time=1.28501
Epoch:0077, train_loss=1.82273, train_acc=0.55540, val_loss=2.06212, val_acc=0.45073, time=1.22501
Epoch:0078, train_loss=1.82167, train_acc=0.55682, val_loss=2.06212, val_acc=0.45073, time=1.33601
Epoch:0079, train_loss=1.82061, train_acc=0.55783, val_loss=2.06212, val_acc=0.45255, time=1.41000
Epoch:0080, train_loss=1.81956, train_acc=0.55965, val_loss=2.06212, val_acc=0.45255, time=1.33201
Epoch:0081, train_loss=1.81852, train_acc=0.56066, val_loss=2.06213, val_acc=0.45073, time=1.23101
Epoch:0082, train_loss=1.81749, train_acc=0.56208, val_loss=2.06213, val_acc=0.44891, time=1.21999
Early stopping...

Optimization Finished!

Test set results: loss= 1.99836, accuracy= 0.47465, time= 0.39101

Test Precision, Recall and F1-Score...
              precision    recall  f1-score   support

           0     0.3664    0.1753    0.2371       696
           1     0.5008    0.8467    0.6294      1083
           2     0.0000    0.0000    0.0000        87
           3     0.0000    0.0000    0.0000       121
           4     0.0000    0.0000    0.0000        75
           5     0.0000    0.0000    0.0000        10
           6     0.0000    0.0000    0.0000        81
           7     0.0000    0.0000    0.0000        36

    accuracy                         0.4746      2189
   macro avg     0.1084    0.1278    0.1083      2189
weighted avg     0.3643    0.4746    0.3868      2189


Macro average Test Precision, Recall and F1-Score...
(0.1083981988542338, 0.12775117808131944, 0.10831235622006424, None)

Micro average Test Precision, Recall and F1-Score...
(0.4746459570580174, 0.4746459570580174, 0.4746459570580174, None)

Embeddings:
Word_embeddings:7688
Train_doc_embeddings:5485
Test_doc_embeddings:2189
