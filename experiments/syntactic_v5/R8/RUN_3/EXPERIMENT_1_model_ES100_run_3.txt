
==========: 260218254680000
Epoch:0001, train_loss=2.38101, train_acc=0.03403, val_loss=2.08117, val_acc=0.25730, time=1.43500
Epoch:0002, train_loss=2.07773, train_acc=0.25501, val_loss=2.06590, val_acc=0.39051, time=1.43001
Epoch:0003, train_loss=1.91811, train_acc=0.44501, val_loss=2.06402, val_acc=0.45803, time=1.41901
Epoch:0004, train_loss=1.87945, train_acc=0.50456, val_loss=2.06700, val_acc=0.46533, time=1.42802
Epoch:0005, train_loss=1.88754, train_acc=0.52137, val_loss=2.06830, val_acc=0.46350, time=1.36301
Epoch:0006, train_loss=1.88535, train_acc=0.53150, val_loss=2.06818, val_acc=0.46168, time=1.34001
Epoch:0007, train_loss=1.87366, train_acc=0.54446, val_loss=2.06803, val_acc=0.42153, time=1.31701
Epoch:0008, train_loss=1.86391, train_acc=0.55864, val_loss=2.06740, val_acc=0.41788, time=1.25799
Epoch:0009, train_loss=1.85100, train_acc=0.56472, val_loss=2.06601, val_acc=0.42153, time=1.40702
Epoch:0010, train_loss=1.83212, train_acc=0.59247, val_loss=2.06474, val_acc=0.42883, time=1.31200
Epoch:0011, train_loss=1.81457, train_acc=0.60502, val_loss=2.06422, val_acc=0.42883, time=1.29801
Epoch:0012, train_loss=1.80356, train_acc=0.60523, val_loss=2.06431, val_acc=0.43248, time=1.33102
Epoch:0013, train_loss=1.79682, train_acc=0.60583, val_loss=2.06456, val_acc=0.42518, time=1.26400
Epoch:0014, train_loss=1.78975, train_acc=0.61191, val_loss=2.06475, val_acc=0.41971, time=1.43001
Epoch:0015, train_loss=1.78027, train_acc=0.62710, val_loss=2.06489, val_acc=0.42336, time=1.35900
Epoch:0016, train_loss=1.76896, train_acc=0.63925, val_loss=2.06512, val_acc=0.41788, time=1.31601
Epoch:0017, train_loss=1.75748, train_acc=0.64958, val_loss=2.06555, val_acc=0.41058, time=1.36401
Epoch:0018, train_loss=1.74708, train_acc=0.65526, val_loss=2.06613, val_acc=0.40693, time=1.45501
Epoch:0019, train_loss=1.73804, train_acc=0.66457, val_loss=2.06682, val_acc=0.40693, time=1.25501
Epoch:0020, train_loss=1.73017, train_acc=0.67106, val_loss=2.06755, val_acc=0.40693, time=1.45301
Epoch:0021, train_loss=1.72341, train_acc=0.67288, val_loss=2.06832, val_acc=0.40693, time=1.37700
Epoch:0022, train_loss=1.71775, train_acc=0.67450, val_loss=2.06907, val_acc=0.41423, time=1.30998
Epoch:0023, train_loss=1.71286, train_acc=0.67531, val_loss=2.06973, val_acc=0.41423, time=1.29701
Epoch:0024, train_loss=1.70811, train_acc=0.67754, val_loss=2.07024, val_acc=0.41423, time=1.38900
Epoch:0025, train_loss=1.70303, train_acc=0.68017, val_loss=2.07061, val_acc=0.42336, time=1.39502
Epoch:0026, train_loss=1.69758, train_acc=0.68888, val_loss=2.07089, val_acc=0.41788, time=1.41401
Epoch:0027, train_loss=1.69210, train_acc=0.69739, val_loss=2.07114, val_acc=0.41058, time=1.40800
Epoch:0028, train_loss=1.68696, train_acc=0.70691, val_loss=2.07141, val_acc=0.40146, time=1.27901
Epoch:0029, train_loss=1.68231, train_acc=0.71379, val_loss=2.07172, val_acc=0.40146, time=1.33700
Epoch:0030, train_loss=1.67808, train_acc=0.71866, val_loss=2.07206, val_acc=0.39781, time=1.33402
Epoch:0031, train_loss=1.67413, train_acc=0.72352, val_loss=2.07245, val_acc=0.39781, time=1.32401
Epoch:0032, train_loss=1.67039, train_acc=0.72797, val_loss=2.07289, val_acc=0.40146, time=1.38100
Epoch:0033, train_loss=1.66686, train_acc=0.73040, val_loss=2.07339, val_acc=0.40693, time=1.30201
Epoch:0034, train_loss=1.66354, train_acc=0.73142, val_loss=2.07392, val_acc=0.40328, time=1.29201
Epoch:0035, train_loss=1.66039, train_acc=0.73283, val_loss=2.07447, val_acc=0.39964, time=1.25000
Epoch:0036, train_loss=1.65727, train_acc=0.73385, val_loss=2.07502, val_acc=0.40146, time=1.40700
Epoch:0037, train_loss=1.65410, train_acc=0.73871, val_loss=2.07557, val_acc=0.39964, time=1.35699
Epoch:0038, train_loss=1.65088, train_acc=0.74337, val_loss=2.07612, val_acc=0.39599, time=1.30499
Epoch:0039, train_loss=1.64770, train_acc=0.74944, val_loss=2.07668, val_acc=0.39599, time=1.26500
Epoch:0040, train_loss=1.64460, train_acc=0.75471, val_loss=2.07723, val_acc=0.39234, time=1.35400
Epoch:0041, train_loss=1.64158, train_acc=0.75856, val_loss=2.07779, val_acc=0.39234, time=1.35701
Epoch:0042, train_loss=1.63855, train_acc=0.76119, val_loss=2.07832, val_acc=0.39599, time=1.32601
Epoch:0043, train_loss=1.63550, train_acc=0.76504, val_loss=2.07885, val_acc=0.39599, time=1.31202
Epoch:0044, train_loss=1.63249, train_acc=0.76646, val_loss=2.07936, val_acc=0.39599, time=1.40301
Epoch:0045, train_loss=1.62960, train_acc=0.76788, val_loss=2.07986, val_acc=0.39051, time=1.35201
Epoch:0046, train_loss=1.62685, train_acc=0.77091, val_loss=2.08035, val_acc=0.38869, time=1.36300
Epoch:0047, train_loss=1.62421, train_acc=0.77476, val_loss=2.08081, val_acc=0.39051, time=1.38001
Epoch:0048, train_loss=1.62165, train_acc=0.77800, val_loss=2.08126, val_acc=0.38869, time=1.30301
Epoch:0049, train_loss=1.61918, train_acc=0.78165, val_loss=2.08172, val_acc=0.38869, time=1.29402
Epoch:0050, train_loss=1.61682, train_acc=0.78712, val_loss=2.08219, val_acc=0.39416, time=1.20100
Epoch:0051, train_loss=1.61451, train_acc=0.79097, val_loss=2.08269, val_acc=0.39599, time=1.33902
Epoch:0052, train_loss=1.61220, train_acc=0.79481, val_loss=2.08320, val_acc=0.39416, time=1.30900
Epoch:0053, train_loss=1.60986, train_acc=0.79826, val_loss=2.08373, val_acc=0.39416, time=1.35701
Epoch:0054, train_loss=1.60752, train_acc=0.80251, val_loss=2.08427, val_acc=0.39051, time=1.29401
Epoch:0055, train_loss=1.60524, train_acc=0.80454, val_loss=2.08482, val_acc=0.39234, time=1.27401
Epoch:0056, train_loss=1.60300, train_acc=0.80656, val_loss=2.08536, val_acc=0.38869, time=1.37301
Epoch:0057, train_loss=1.60081, train_acc=0.80940, val_loss=2.08589, val_acc=0.38869, time=1.31801
Epoch:0058, train_loss=1.59865, train_acc=0.81284, val_loss=2.08641, val_acc=0.38686, time=1.26801
Epoch:0059, train_loss=1.59655, train_acc=0.81730, val_loss=2.08692, val_acc=0.38686, time=1.31699
Epoch:0060, train_loss=1.59450, train_acc=0.81973, val_loss=2.08743, val_acc=0.38686, time=1.32701
Epoch:0061, train_loss=1.59246, train_acc=0.82196, val_loss=2.08794, val_acc=0.38504, time=1.30998
Epoch:0062, train_loss=1.59044, train_acc=0.82418, val_loss=2.08845, val_acc=0.38321, time=1.26801
Epoch:0063, train_loss=1.58846, train_acc=0.82601, val_loss=2.08895, val_acc=0.38504, time=1.28001
Epoch:0064, train_loss=1.58652, train_acc=0.82905, val_loss=2.08946, val_acc=0.38686, time=1.34801
Epoch:0065, train_loss=1.58462, train_acc=0.83067, val_loss=2.08997, val_acc=0.38869, time=1.26401
Epoch:0066, train_loss=1.58273, train_acc=0.83249, val_loss=2.09049, val_acc=0.38869, time=1.22999
Epoch:0067, train_loss=1.58087, train_acc=0.83391, val_loss=2.09103, val_acc=0.39051, time=1.35900
Epoch:0068, train_loss=1.57905, train_acc=0.83674, val_loss=2.09159, val_acc=0.39051, time=1.35701
Epoch:0069, train_loss=1.57724, train_acc=0.83857, val_loss=2.09217, val_acc=0.38504, time=1.37101
Epoch:0070, train_loss=1.57545, train_acc=0.83897, val_loss=2.09276, val_acc=0.38504, time=1.33801
Epoch:0071, train_loss=1.57371, train_acc=0.84059, val_loss=2.09334, val_acc=0.38504, time=1.39000
Epoch:0072, train_loss=1.57200, train_acc=0.84221, val_loss=2.09391, val_acc=0.38321, time=1.28401
Epoch:0073, train_loss=1.57031, train_acc=0.84383, val_loss=2.09446, val_acc=0.38321, time=1.31101
Epoch:0074, train_loss=1.56865, train_acc=0.84768, val_loss=2.09500, val_acc=0.38686, time=1.27200
Epoch:0075, train_loss=1.56701, train_acc=0.85092, val_loss=2.09554, val_acc=0.38504, time=1.31801
Epoch:0076, train_loss=1.56538, train_acc=0.85274, val_loss=2.09607, val_acc=0.38686, time=1.23903
Epoch:0077, train_loss=1.56377, train_acc=0.85477, val_loss=2.09660, val_acc=0.38504, time=1.36900
Epoch:0078, train_loss=1.56219, train_acc=0.85558, val_loss=2.09714, val_acc=0.38504, time=1.31202
Epoch:0079, train_loss=1.56064, train_acc=0.85781, val_loss=2.09767, val_acc=0.38504, time=1.33201
Epoch:0080, train_loss=1.55911, train_acc=0.86024, val_loss=2.09822, val_acc=0.38686, time=1.23199
Epoch:0081, train_loss=1.55761, train_acc=0.86348, val_loss=2.09878, val_acc=0.38504, time=1.37201
Epoch:0082, train_loss=1.55612, train_acc=0.86611, val_loss=2.09936, val_acc=0.38504, time=1.29901
Epoch:0083, train_loss=1.55466, train_acc=0.86733, val_loss=2.09994, val_acc=0.38321, time=1.26003
Epoch:0084, train_loss=1.55321, train_acc=0.86814, val_loss=2.10053, val_acc=0.38504, time=1.24901
Epoch:0085, train_loss=1.55179, train_acc=0.86996, val_loss=2.10110, val_acc=0.38504, time=1.25102
Epoch:0086, train_loss=1.55038, train_acc=0.87097, val_loss=2.10167, val_acc=0.38504, time=1.37201
Epoch:0087, train_loss=1.54900, train_acc=0.87320, val_loss=2.10224, val_acc=0.38504, time=1.32203
Epoch:0088, train_loss=1.54763, train_acc=0.87340, val_loss=2.10280, val_acc=0.38686, time=1.30199
Epoch:0089, train_loss=1.54628, train_acc=0.87543, val_loss=2.10335, val_acc=0.38504, time=1.26500
Epoch:0090, train_loss=1.54495, train_acc=0.87604, val_loss=2.10391, val_acc=0.38504, time=1.27001
Epoch:0091, train_loss=1.54364, train_acc=0.87827, val_loss=2.10447, val_acc=0.38504, time=1.26900
Epoch:0092, train_loss=1.54235, train_acc=0.87928, val_loss=2.10504, val_acc=0.38504, time=1.20302
Epoch:0093, train_loss=1.54107, train_acc=0.88211, val_loss=2.10563, val_acc=0.38504, time=1.33800
Epoch:0094, train_loss=1.53982, train_acc=0.88292, val_loss=2.10622, val_acc=0.38504, time=1.35100
Epoch:0095, train_loss=1.53858, train_acc=0.88313, val_loss=2.10682, val_acc=0.38321, time=1.25001
Epoch:0096, train_loss=1.53735, train_acc=0.88556, val_loss=2.10741, val_acc=0.38321, time=1.28401
Epoch:0097, train_loss=1.53615, train_acc=0.88738, val_loss=2.10800, val_acc=0.38321, time=1.40400
Epoch:0098, train_loss=1.53496, train_acc=0.88819, val_loss=2.10859, val_acc=0.38321, time=1.42101
Epoch:0099, train_loss=1.53378, train_acc=0.88961, val_loss=2.10917, val_acc=0.38321, time=1.33002
Epoch:0100, train_loss=1.53263, train_acc=0.89062, val_loss=2.10975, val_acc=0.38321, time=1.23400
Epoch:0101, train_loss=1.53149, train_acc=0.89163, val_loss=2.11033, val_acc=0.38321, time=1.34101
Epoch:0102, train_loss=1.53037, train_acc=0.89224, val_loss=2.11091, val_acc=0.38321, time=1.22900
Early stopping...

Optimization Finished!

Test set results: loss= 2.17402, accuracy= 0.39333, time= 0.37300

Test Precision, Recall and F1-Score...
              precision    recall  f1-score   support

           0     0.3214    0.2845    0.3018       696
           1     0.4947    0.6048    0.5442      1083
           2     0.0328    0.0230    0.0270        87
           3     0.0167    0.0083    0.0110       121
           4     0.0492    0.0400    0.0441        75
           5     0.0000    0.0000    0.0000        10
           6     0.0233    0.0123    0.0161        81
           7     0.0500    0.0278    0.0357        36

    accuracy                         0.3933      2189
   macro avg     0.1235    0.1251    0.1225      2189
weighted avg     0.3525    0.3933    0.3696      2189


Macro average Test Precision, Recall and F1-Score...
(0.12350390701250456, 0.12508258266818895, 0.12251411667778614, None)

Micro average Test Precision, Recall and F1-Score...
(0.3933302878026496, 0.3933302878026496, 0.39333028780264967, None)

Embeddings:
Word_embeddings:7688
Train_doc_embeddings:5485
Test_doc_embeddings:2189
