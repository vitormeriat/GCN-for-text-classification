
==========: 256328699796400
Epoch:0001, train_loss=2.25993, train_acc=0.05368, val_loss=2.07011, val_acc=0.34854, time=1.25701
Epoch:0002, train_loss=1.98889, train_acc=0.36237, val_loss=2.06205, val_acc=0.45438, time=1.25101
Epoch:0003, train_loss=1.88945, train_acc=0.49970, val_loss=2.06438, val_acc=0.47263, time=1.13301
Epoch:0004, train_loss=1.88321, train_acc=0.52603, val_loss=2.06695, val_acc=0.47445, time=1.29503
Epoch:0005, train_loss=1.88277, train_acc=0.53575, val_loss=2.06865, val_acc=0.46168, time=1.34400
Epoch:0006, train_loss=1.87798, train_acc=0.54406, val_loss=2.06965, val_acc=0.45073, time=1.20101
Epoch:0007, train_loss=1.86968, train_acc=0.55216, val_loss=2.06947, val_acc=0.45073, time=1.23801
Epoch:0008, train_loss=1.85302, train_acc=0.57383, val_loss=2.06904, val_acc=0.45438, time=1.24100
Epoch:0009, train_loss=1.83599, train_acc=0.59084, val_loss=2.06886, val_acc=0.45620, time=1.25499
Epoch:0010, train_loss=1.82298, train_acc=0.59388, val_loss=2.06858, val_acc=0.45620, time=1.36001
Epoch:0011, train_loss=1.81085, train_acc=0.60300, val_loss=2.06796, val_acc=0.45438, time=1.22302
Epoch:0012, train_loss=1.79662, train_acc=0.61697, val_loss=2.06716, val_acc=0.45438, time=1.40100
Epoch:0013, train_loss=1.78126, train_acc=0.63196, val_loss=2.06645, val_acc=0.45803, time=1.16801
Epoch:0014, train_loss=1.76686, train_acc=0.64108, val_loss=2.06596, val_acc=0.45438, time=1.33699
Epoch:0015, train_loss=1.75441, train_acc=0.65080, val_loss=2.06575, val_acc=0.45255, time=1.31900
Epoch:0016, train_loss=1.74429, train_acc=0.65708, val_loss=2.06586, val_acc=0.45438, time=1.39602
Epoch:0017, train_loss=1.73673, train_acc=0.66235, val_loss=2.06625, val_acc=0.44891, time=1.27400
Epoch:0018, train_loss=1.73133, train_acc=0.66498, val_loss=2.06676, val_acc=0.44343, time=1.17600
Epoch:0019, train_loss=1.72680, train_acc=0.66619, val_loss=2.06726, val_acc=0.44343, time=1.24901
Early stopping...

Optimization Finished!

Test set results: loss= 2.02172, accuracy= 0.44632, time= 0.53701

Test Precision, Recall and F1-Score...
              precision    recall  f1-score   support

           0     0.3127    0.1595    0.2112       696
           1     0.4909    0.7987    0.6081      1083
           2     0.0000    0.0000    0.0000        87
           3     0.0000    0.0000    0.0000       121
           4     0.0385    0.0133    0.0198        75
           5     0.0000    0.0000    0.0000        10
           6     0.0000    0.0000    0.0000        81
           7     0.0000    0.0000    0.0000        36

    accuracy                         0.4463      2189
   macro avg     0.1053    0.1214    0.1049      2189
weighted avg     0.3436    0.4463    0.3687      2189


Macro average Test Precision, Recall and F1-Score...
(0.10525712557015014, 0.1214404233132741, 0.10488921764944753, None)

Micro average Test Precision, Recall and F1-Score...
(0.4463225216994061, 0.4463225216994061, 0.4463225216994061, None)

Embeddings:
Word_embeddings:7688
Train_doc_embeddings:5485
Test_doc_embeddings:2189
