
==========: 256475515487900
Epoch:0001, train_loss=2.37152, train_acc=0.08244, val_loss=2.07976, val_acc=0.29380, time=1.42200
Epoch:0002, train_loss=2.06002, train_acc=0.31234, val_loss=2.06638, val_acc=0.41241, time=1.28201
Epoch:0003, train_loss=1.91030, train_acc=0.46790, val_loss=2.06904, val_acc=0.45803, time=1.29800
Epoch:0004, train_loss=1.90670, train_acc=0.51651, val_loss=2.07280, val_acc=0.46898, time=1.25800
Epoch:0005, train_loss=1.91930, train_acc=0.52684, val_loss=2.07260, val_acc=0.46533, time=1.17900
Epoch:0006, train_loss=1.90092, train_acc=0.53433, val_loss=2.07140, val_acc=0.45255, time=1.20600
Epoch:0007, train_loss=1.87669, train_acc=0.54547, val_loss=2.07209, val_acc=0.39599, time=1.28803
Epoch:0008, train_loss=1.87152, train_acc=0.52846, val_loss=2.07308, val_acc=0.37774, time=1.24099
Epoch:0009, train_loss=1.86948, train_acc=0.50456, val_loss=2.07250, val_acc=0.36496, time=1.19800
Epoch:0010, train_loss=1.85305, train_acc=0.53129, val_loss=2.07131, val_acc=0.39599, time=1.25502
Epoch:0011, train_loss=1.83091, train_acc=0.57707, val_loss=2.07062, val_acc=0.43066, time=1.24701
Epoch:0012, train_loss=1.81383, train_acc=0.60158, val_loss=2.07052, val_acc=0.44343, time=1.33101
Epoch:0013, train_loss=1.80284, train_acc=0.60340, val_loss=2.07046, val_acc=0.44343, time=1.30600
Epoch:0014, train_loss=1.79303, train_acc=0.60340, val_loss=2.07005, val_acc=0.44343, time=1.28101
Epoch:0015, train_loss=1.78075, train_acc=0.60705, val_loss=2.06930, val_acc=0.44161, time=1.32501
Epoch:0016, train_loss=1.76610, train_acc=0.62102, val_loss=2.06852, val_acc=0.44161, time=1.28101
Epoch:0017, train_loss=1.75174, train_acc=0.63399, val_loss=2.06802, val_acc=0.43066, time=1.22601
Epoch:0018, train_loss=1.74044, train_acc=0.64877, val_loss=2.06794, val_acc=0.41971, time=1.25301
Epoch:0019, train_loss=1.73300, train_acc=0.65748, val_loss=2.06815, val_acc=0.41788, time=1.34000
Epoch:0020, train_loss=1.72803, train_acc=0.66194, val_loss=2.06845, val_acc=0.41423, time=1.41302
Epoch:0021, train_loss=1.72355, train_acc=0.66579, val_loss=2.06875, val_acc=0.41788, time=1.22300
Epoch:0022, train_loss=1.71862, train_acc=0.67065, val_loss=2.06909, val_acc=0.41606, time=1.36702
Early stopping...

Optimization Finished!

Test set results: loss= 2.01774, accuracy= 0.42759, time= 0.36600

Test Precision, Recall and F1-Score...
              precision    recall  f1-score   support

           0     0.3062    0.2213    0.2569       696
           1     0.4884    0.7184    0.5815      1083
           2     0.0000    0.0000    0.0000        87
           3     0.0526    0.0165    0.0252       121
           4     0.0000    0.0000    0.0000        75
           5     0.0000    0.0000    0.0000        10
           6     0.0370    0.0123    0.0185        81
           7     0.2500    0.0278    0.0500        36

    accuracy                         0.4276      2189
   macro avg     0.1418    0.1245    0.1165      2189
weighted avg     0.3474    0.4276    0.3723      2189


Macro average Test Precision, Recall and F1-Score...
(0.14177729120371438, 0.1245364543507401, 0.11650266976407109, None)

Micro average Test Precision, Recall and F1-Score...
(0.42759250799451803, 0.42759250799451803, 0.42759250799451803, None)

Embeddings:
Word_embeddings:7688
Train_doc_embeddings:5485
Test_doc_embeddings:2189
