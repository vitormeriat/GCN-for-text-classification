
==========: 255052407004700
Epoch:0001, train_loss=2.35088, train_acc=0.03727, val_loss=2.09342, val_acc=0.14234, time=1.45601
Epoch:0002, train_loss=2.17637, train_acc=0.16508, val_loss=2.07918, val_acc=0.31204, time=1.32199
Epoch:0003, train_loss=2.04125, train_acc=0.32975, val_loss=2.07002, val_acc=0.39599, time=1.22301
Epoch:0004, train_loss=1.95098, train_acc=0.44035, val_loss=2.06572, val_acc=0.43978, time=1.32002
Epoch:0005, train_loss=1.90349, train_acc=0.48957, val_loss=2.06482, val_acc=0.46533, time=1.22300
Epoch:0006, train_loss=1.88614, train_acc=0.51327, val_loss=2.06519, val_acc=0.46898, time=1.36601
Epoch:0007, train_loss=1.88086, train_acc=0.52400, val_loss=2.06553, val_acc=0.47263, time=1.36800
Epoch:0008, train_loss=1.87612, train_acc=0.53048, val_loss=2.06555, val_acc=0.47993, time=1.26400
Epoch:0009, train_loss=1.86943, train_acc=0.53514, val_loss=2.06550, val_acc=0.46715, time=1.36401
Epoch:0010, train_loss=1.86287, train_acc=0.54203, val_loss=2.06560, val_acc=0.45620, time=1.27101
Epoch:0011, train_loss=1.85806, train_acc=0.55114, val_loss=2.06570, val_acc=0.44526, time=1.24401
Epoch:0012, train_loss=1.85320, train_acc=0.54608, val_loss=2.06555, val_acc=0.45803, time=1.30401
Epoch:0013, train_loss=1.84572, train_acc=0.55236, val_loss=2.06514, val_acc=0.45620, time=1.25800
Epoch:0014, train_loss=1.83569, train_acc=0.56856, val_loss=2.06468, val_acc=0.46168, time=1.17701
Epoch:0015, train_loss=1.82503, train_acc=0.58274, val_loss=2.06434, val_acc=0.46898, time=1.24701
Epoch:0016, train_loss=1.81541, train_acc=0.58841, val_loss=2.06415, val_acc=0.46533, time=1.24301
Epoch:0017, train_loss=1.80720, train_acc=0.58902, val_loss=2.06405, val_acc=0.47263, time=1.31201
Epoch:0018, train_loss=1.79984, train_acc=0.59186, val_loss=2.06398, val_acc=0.47080, time=1.21402
Epoch:0019, train_loss=1.79273, train_acc=0.59611, val_loss=2.06391, val_acc=0.46350, time=1.27800
Epoch:0020, train_loss=1.78578, train_acc=0.60462, val_loss=2.06390, val_acc=0.44708, time=1.24001
Epoch:0021, train_loss=1.77923, train_acc=0.61515, val_loss=2.06396, val_acc=0.43796, time=1.28802
Epoch:0022, train_loss=1.77331, train_acc=0.62811, val_loss=2.06409, val_acc=0.43796, time=1.26601
Epoch:0023, train_loss=1.76795, train_acc=0.63439, val_loss=2.06428, val_acc=0.42701, time=1.35000
Early stopping...

Optimization Finished!

Test set results: loss= 2.00705, accuracy= 0.44312, time= 0.39199

Test Precision, Recall and F1-Score...
              precision    recall  f1-score   support

           0     0.3290    0.2170    0.2615       696
           1     0.4937    0.7553    0.5971      1083
           2     0.0000    0.0000    0.0000        87
           3     0.0000    0.0000    0.0000       121
           4     0.1111    0.0133    0.0238        75
           5     0.0000    0.0000    0.0000        10
           6     0.0000    0.0000    0.0000        81
           7     0.0000    0.0000    0.0000        36

    accuracy                         0.4431      2189
   macro avg     0.1167    0.1232    0.1103      2189
weighted avg     0.3526    0.4431    0.3794      2189


Macro average Test Precision, Recall and F1-Score...
(0.1167187991001403, 0.12319958528353551, 0.11029520965652352, None)

Micro average Test Precision, Recall and F1-Score...
(0.4431247144814984, 0.4431247144814984, 0.4431247144814984, None)

Embeddings:
Word_embeddings:7688
Train_doc_embeddings:5485
Test_doc_embeddings:2189
