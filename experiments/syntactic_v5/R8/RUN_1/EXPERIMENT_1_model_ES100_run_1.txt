
==========: 253623096417800
Epoch:0001, train_loss=2.15090, train_acc=0.08345, val_loss=2.06534, val_acc=0.42518, time=1.53703
Epoch:0002, train_loss=1.94084, train_acc=0.45108, val_loss=2.06231, val_acc=0.46350, time=1.44700
Epoch:0003, train_loss=1.88320, train_acc=0.50213, val_loss=2.06486, val_acc=0.45073, time=1.22699
Epoch:0004, train_loss=1.87681, train_acc=0.51833, val_loss=2.06786, val_acc=0.44343, time=1.32704
Epoch:0005, train_loss=1.87758, train_acc=0.53838, val_loss=2.06964, val_acc=0.43978, time=1.27400
Epoch:0006, train_loss=1.87096, train_acc=0.55398, val_loss=2.07019, val_acc=0.43978, time=1.24699
Epoch:0007, train_loss=1.85649, train_acc=0.56978, val_loss=2.06993, val_acc=0.45438, time=1.24201
Epoch:0008, train_loss=1.83750, train_acc=0.58011, val_loss=2.06932, val_acc=0.45255, time=1.17103
Epoch:0009, train_loss=1.81789, train_acc=0.59692, val_loss=2.06871, val_acc=0.44526, time=1.32599
Epoch:0010, train_loss=1.80049, train_acc=0.61353, val_loss=2.06817, val_acc=0.43796, time=1.18601
Epoch:0011, train_loss=1.78557, train_acc=0.62791, val_loss=2.06772, val_acc=0.43248, time=1.28801
Epoch:0012, train_loss=1.77249, train_acc=0.63743, val_loss=2.06742, val_acc=0.42701, time=1.15901
Epoch:0013, train_loss=1.76088, train_acc=0.64695, val_loss=2.06728, val_acc=0.43066, time=1.32401
Epoch:0014, train_loss=1.75043, train_acc=0.65627, val_loss=2.06733, val_acc=0.43066, time=1.23301
Epoch:0015, train_loss=1.74105, train_acc=0.66498, val_loss=2.06754, val_acc=0.43066, time=1.36801
Epoch:0016, train_loss=1.73269, train_acc=0.67268, val_loss=2.06787, val_acc=0.43066, time=1.23199
Epoch:0017, train_loss=1.72505, train_acc=0.67956, val_loss=2.06827, val_acc=0.42153, time=1.30301
Epoch:0018, train_loss=1.71781, train_acc=0.68523, val_loss=2.06870, val_acc=0.42336, time=1.23801
Epoch:0019, train_loss=1.71079, train_acc=0.68827, val_loss=2.06915, val_acc=0.41606, time=1.26602
Epoch:0020, train_loss=1.70405, train_acc=0.69374, val_loss=2.06964, val_acc=0.41241, time=1.28701
Epoch:0021, train_loss=1.69771, train_acc=0.69799, val_loss=2.07015, val_acc=0.41423, time=1.21400
Epoch:0022, train_loss=1.69183, train_acc=0.70367, val_loss=2.07068, val_acc=0.41241, time=1.25702
Epoch:0023, train_loss=1.68641, train_acc=0.70731, val_loss=2.07121, val_acc=0.41606, time=1.22501
Epoch:0024, train_loss=1.68137, train_acc=0.71278, val_loss=2.07174, val_acc=0.40876, time=1.29900
Epoch:0025, train_loss=1.67658, train_acc=0.71886, val_loss=2.07226, val_acc=0.41058, time=1.19999
Epoch:0026, train_loss=1.67196, train_acc=0.72514, val_loss=2.07279, val_acc=0.41241, time=1.24401
Epoch:0027, train_loss=1.66750, train_acc=0.73101, val_loss=2.07335, val_acc=0.40876, time=1.25000
Epoch:0028, train_loss=1.66320, train_acc=0.73607, val_loss=2.07393, val_acc=0.40328, time=1.18902
Epoch:0029, train_loss=1.65905, train_acc=0.74296, val_loss=2.07455, val_acc=0.40328, time=1.29202
Epoch:0030, train_loss=1.65501, train_acc=0.74701, val_loss=2.07520, val_acc=0.40146, time=1.25101
Epoch:0031, train_loss=1.65104, train_acc=0.75066, val_loss=2.07586, val_acc=0.40146, time=1.20100
Epoch:0032, train_loss=1.64704, train_acc=0.75430, val_loss=2.07651, val_acc=0.40511, time=1.16401
Epoch:0033, train_loss=1.64300, train_acc=0.75876, val_loss=2.07718, val_acc=0.40876, time=1.27600
Epoch:0034, train_loss=1.63900, train_acc=0.76261, val_loss=2.07786, val_acc=0.40511, time=1.31301
Epoch:0035, train_loss=1.63514, train_acc=0.76767, val_loss=2.07857, val_acc=0.40328, time=1.29601
Epoch:0036, train_loss=1.63152, train_acc=0.77193, val_loss=2.07932, val_acc=0.40693, time=1.27900
Epoch:0037, train_loss=1.62815, train_acc=0.77699, val_loss=2.08008, val_acc=0.41058, time=1.29800
Epoch:0038, train_loss=1.62502, train_acc=0.78084, val_loss=2.08083, val_acc=0.40693, time=1.30601
Epoch:0039, train_loss=1.62204, train_acc=0.78367, val_loss=2.08155, val_acc=0.40693, time=1.23600
Epoch:0040, train_loss=1.61914, train_acc=0.78752, val_loss=2.08222, val_acc=0.40328, time=1.21800
Epoch:0041, train_loss=1.61622, train_acc=0.78995, val_loss=2.08284, val_acc=0.40328, time=1.36400
Epoch:0042, train_loss=1.61327, train_acc=0.79400, val_loss=2.08343, val_acc=0.39964, time=1.23401
Epoch:0043, train_loss=1.61028, train_acc=0.79866, val_loss=2.08399, val_acc=0.40328, time=1.32401
Epoch:0044, train_loss=1.60731, train_acc=0.80312, val_loss=2.08455, val_acc=0.40328, time=1.22301
Epoch:0045, train_loss=1.60442, train_acc=0.80677, val_loss=2.08510, val_acc=0.39599, time=1.27201
Epoch:0046, train_loss=1.60163, train_acc=0.80920, val_loss=2.08566, val_acc=0.39781, time=1.24599
Epoch:0047, train_loss=1.59895, train_acc=0.81345, val_loss=2.08624, val_acc=0.39964, time=1.16501
Epoch:0048, train_loss=1.59636, train_acc=0.81791, val_loss=2.08684, val_acc=0.40693, time=1.29499
Epoch:0049, train_loss=1.59383, train_acc=0.82135, val_loss=2.08747, val_acc=0.40328, time=1.23701
Epoch:0050, train_loss=1.59134, train_acc=0.82256, val_loss=2.08812, val_acc=0.40146, time=1.11000
Epoch:0051, train_loss=1.58888, train_acc=0.82439, val_loss=2.08878, val_acc=0.40146, time=1.19002
Epoch:0052, train_loss=1.58646, train_acc=0.82722, val_loss=2.08943, val_acc=0.40328, time=1.27100
Epoch:0053, train_loss=1.58409, train_acc=0.82864, val_loss=2.09008, val_acc=0.40511, time=1.16301
Epoch:0054, train_loss=1.58178, train_acc=0.83208, val_loss=2.09072, val_acc=0.40328, time=1.28400
Epoch:0055, train_loss=1.57952, train_acc=0.83249, val_loss=2.09136, val_acc=0.40328, time=1.26101
Epoch:0056, train_loss=1.57729, train_acc=0.83654, val_loss=2.09199, val_acc=0.39781, time=1.24401
Epoch:0057, train_loss=1.57510, train_acc=0.83978, val_loss=2.09261, val_acc=0.40146, time=1.21100
Epoch:0058, train_loss=1.57294, train_acc=0.84201, val_loss=2.09322, val_acc=0.39781, time=1.25001
Epoch:0059, train_loss=1.57084, train_acc=0.84424, val_loss=2.09385, val_acc=0.39781, time=1.24201
Epoch:0060, train_loss=1.56877, train_acc=0.84687, val_loss=2.09450, val_acc=0.39964, time=1.19001
Epoch:0061, train_loss=1.56675, train_acc=0.85193, val_loss=2.09517, val_acc=0.39599, time=1.18001
Epoch:0062, train_loss=1.56476, train_acc=0.85355, val_loss=2.09586, val_acc=0.39234, time=1.24601
Epoch:0063, train_loss=1.56280, train_acc=0.85599, val_loss=2.09658, val_acc=0.39051, time=1.18801
Epoch:0064, train_loss=1.56087, train_acc=0.85882, val_loss=2.09731, val_acc=0.39051, time=1.17500
Epoch:0065, train_loss=1.55898, train_acc=0.86024, val_loss=2.09804, val_acc=0.39051, time=1.27401
Epoch:0066, train_loss=1.55714, train_acc=0.86206, val_loss=2.09875, val_acc=0.38686, time=1.25600
Epoch:0067, train_loss=1.55533, train_acc=0.86348, val_loss=2.09943, val_acc=0.38869, time=1.19000
Epoch:0068, train_loss=1.55356, train_acc=0.86611, val_loss=2.10010, val_acc=0.38869, time=1.24501
Epoch:0069, train_loss=1.55181, train_acc=0.86794, val_loss=2.10075, val_acc=0.38686, time=1.21701
Epoch:0070, train_loss=1.55008, train_acc=0.86996, val_loss=2.10139, val_acc=0.38686, time=1.19101
Epoch:0071, train_loss=1.54838, train_acc=0.87259, val_loss=2.10201, val_acc=0.38321, time=1.27000
Epoch:0072, train_loss=1.54673, train_acc=0.87482, val_loss=2.10265, val_acc=0.38321, time=1.31000
Epoch:0073, train_loss=1.54511, train_acc=0.87806, val_loss=2.10330, val_acc=0.38321, time=1.40301
Epoch:0074, train_loss=1.54352, train_acc=0.88009, val_loss=2.10397, val_acc=0.38321, time=1.37700
Epoch:0075, train_loss=1.54196, train_acc=0.88171, val_loss=2.10467, val_acc=0.38321, time=1.30800
Epoch:0076, train_loss=1.54043, train_acc=0.88374, val_loss=2.10539, val_acc=0.38321, time=1.27301
Epoch:0077, train_loss=1.53892, train_acc=0.88495, val_loss=2.10612, val_acc=0.38504, time=1.34901
Epoch:0078, train_loss=1.53743, train_acc=0.88617, val_loss=2.10684, val_acc=0.38321, time=1.24801
Epoch:0079, train_loss=1.53597, train_acc=0.88677, val_loss=2.10756, val_acc=0.38504, time=1.15300
Epoch:0080, train_loss=1.53455, train_acc=0.88819, val_loss=2.10827, val_acc=0.38504, time=1.21302
Epoch:0081, train_loss=1.53315, train_acc=0.88900, val_loss=2.10897, val_acc=0.38139, time=1.19102
Epoch:0082, train_loss=1.53177, train_acc=0.89062, val_loss=2.10966, val_acc=0.38139, time=1.32400
Epoch:0083, train_loss=1.53042, train_acc=0.89204, val_loss=2.11036, val_acc=0.38139, time=1.27302
Epoch:0084, train_loss=1.52909, train_acc=0.89366, val_loss=2.11105, val_acc=0.38139, time=1.24600
Epoch:0085, train_loss=1.52778, train_acc=0.89447, val_loss=2.11175, val_acc=0.38321, time=1.21601
Epoch:0086, train_loss=1.52650, train_acc=0.89832, val_loss=2.11246, val_acc=0.38504, time=1.26902
Epoch:0087, train_loss=1.52525, train_acc=0.89913, val_loss=2.11318, val_acc=0.38504, time=1.25298
Epoch:0088, train_loss=1.52401, train_acc=0.89933, val_loss=2.11390, val_acc=0.38321, time=1.12301
Epoch:0089, train_loss=1.52280, train_acc=0.89994, val_loss=2.11461, val_acc=0.38321, time=1.36301
Epoch:0090, train_loss=1.52161, train_acc=0.90075, val_loss=2.11532, val_acc=0.38321, time=1.17901
Epoch:0091, train_loss=1.52044, train_acc=0.90277, val_loss=2.11602, val_acc=0.38321, time=1.22401
Epoch:0092, train_loss=1.51929, train_acc=0.90298, val_loss=2.11672, val_acc=0.38321, time=1.37801
Epoch:0093, train_loss=1.51816, train_acc=0.90379, val_loss=2.11742, val_acc=0.38321, time=1.19002
Epoch:0094, train_loss=1.51705, train_acc=0.90480, val_loss=2.11812, val_acc=0.38321, time=1.22700
Epoch:0095, train_loss=1.51596, train_acc=0.90622, val_loss=2.11883, val_acc=0.37956, time=1.22900
Epoch:0096, train_loss=1.51489, train_acc=0.90703, val_loss=2.11955, val_acc=0.37956, time=1.28201
Epoch:0097, train_loss=1.51384, train_acc=0.90784, val_loss=2.12028, val_acc=0.37774, time=1.28001
Epoch:0098, train_loss=1.51281, train_acc=0.90926, val_loss=2.12101, val_acc=0.37591, time=1.22100
Epoch:0099, train_loss=1.51179, train_acc=0.91027, val_loss=2.12174, val_acc=0.37409, time=1.24702
Epoch:0100, train_loss=1.51079, train_acc=0.91027, val_loss=2.12247, val_acc=0.37409, time=1.35800
Epoch:0101, train_loss=1.50982, train_acc=0.91290, val_loss=2.12319, val_acc=0.37409, time=1.27901
Epoch:0102, train_loss=1.50885, train_acc=0.91432, val_loss=2.12391, val_acc=0.37409, time=1.30700
Early stopping...

Optimization Finished!

Test set results: loss= 2.23526, accuracy= 0.39242, time= 0.40200

Test Precision, Recall and F1-Score...
              precision    recall  f1-score   support

           0     0.3247    0.3075    0.3159       696
           1     0.4980    0.5891    0.5398      1083
           2     0.0130    0.0115    0.0122        87
           3     0.0182    0.0083    0.0114       121
           4     0.0667    0.0533    0.0593        75
           5     0.0000    0.0000    0.0000        10
           6     0.0000    0.0000    0.0000        81
           7     0.0556    0.0278    0.0370        36

    accuracy                         0.3924      2189
   macro avg     0.1220    0.1247    0.1219      2189
weighted avg     0.3544    0.3924    0.3712      2189


Macro average Test Precision, Recall and F1-Score...
(0.12202173740116226, 0.12468067886990854, 0.12193566583123915, None)

Micro average Test Precision, Recall and F1-Score...
(0.3924166285975331, 0.3924166285975331, 0.39241662859753307, None)

Embeddings:
Word_embeddings:7688
Train_doc_embeddings:5485
Test_doc_embeddings:2189
