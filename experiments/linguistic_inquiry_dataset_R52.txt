
==================== Torch Seed: 47832545164300
Epoch:0001, train_loss=4.08585, train_acc=0.00867, val_loss=3.92820, val_acc=0.39051, time=0.82398
Epoch:0002, train_loss=3.74515, train_acc=0.35754, val_loss=3.89766, val_acc=0.52986, time=0.70201
Epoch:0003, train_loss=3.47754, train_acc=0.51641, val_loss=3.87729, val_acc=0.61256, time=0.76599
Epoch:0004, train_loss=3.29964, train_acc=0.58292, val_loss=3.86553, val_acc=0.65237, time=0.70999
Epoch:0005, train_loss=3.19590, train_acc=0.62970, val_loss=3.85877, val_acc=0.68453, time=0.69800
Epoch:0006, train_loss=3.13336, train_acc=0.65504, val_loss=3.85406, val_acc=0.70597, time=0.77698
Epoch:0007, train_loss=3.08604, train_acc=0.68362, val_loss=3.85009, val_acc=0.73047, time=0.78599
Epoch:0008, train_loss=3.04329, train_acc=0.72070, val_loss=3.84663, val_acc=0.75957, time=0.77099
Epoch:0009, train_loss=3.00455, train_acc=0.75880, val_loss=3.84373, val_acc=0.78254, time=0.80899
Epoch:0010, train_loss=2.97118, train_acc=0.79078, val_loss=3.84134, val_acc=0.80092, time=0.90499
Epoch:0011, train_loss=2.94312, train_acc=0.82089, val_loss=3.83934, val_acc=0.81317, time=0.80899
Epoch:0012, train_loss=2.91935, train_acc=0.84487, val_loss=3.83761, val_acc=0.81930, time=0.83700
Epoch:0013, train_loss=2.89885, train_acc=0.86545, val_loss=3.83607, val_acc=0.83614, time=0.80599
Epoch:0014, train_loss=2.88097, train_acc=0.87906, val_loss=3.83469, val_acc=0.84839, time=0.76298
Epoch:0015, train_loss=2.86528, train_acc=0.88774, val_loss=3.83343, val_acc=0.85299, time=0.68299
Epoch:0016, train_loss=2.85131, train_acc=0.89454, val_loss=3.83227, val_acc=0.85911, time=0.70499
Epoch:0017, train_loss=2.83866, train_acc=0.90441, val_loss=3.83119, val_acc=0.86217, time=0.85700
Epoch:0018, train_loss=2.82704, train_acc=0.91325, val_loss=3.83015, val_acc=0.87443, time=0.87298
Epoch:0019, train_loss=2.81620, train_acc=0.91903, val_loss=3.82917, val_acc=0.87749, time=0.72599
Epoch:0020, train_loss=2.80589, train_acc=0.92414, val_loss=3.82824, val_acc=0.88208, time=0.72601
Epoch:0021, train_loss=2.79610, train_acc=0.92958, val_loss=3.82738, val_acc=0.88208, time=0.74398
Epoch:0022, train_loss=2.78697, train_acc=0.93536, val_loss=3.82662, val_acc=0.88974, time=0.72899
Epoch:0023, train_loss=2.77869, train_acc=0.94081, val_loss=3.82595, val_acc=0.89587, time=0.72901
Epoch:0024, train_loss=2.77129, train_acc=0.94387, val_loss=3.82535, val_acc=0.90046, time=0.75998
Epoch:0025, train_loss=2.76463, train_acc=0.94676, val_loss=3.82481, val_acc=0.90199, time=0.71500
Epoch:0026, train_loss=2.75849, train_acc=0.94795, val_loss=3.82431, val_acc=0.90505, time=0.73299
Epoch:0027, train_loss=2.75273, train_acc=0.95084, val_loss=3.82384, val_acc=0.90505, time=0.96400
Epoch:0028, train_loss=2.74727, train_acc=0.95220, val_loss=3.82340, val_acc=0.90505, time=0.66500
Epoch:0029, train_loss=2.74212, train_acc=0.95543, val_loss=3.82299, val_acc=0.90505, time=0.74599
Epoch:0030, train_loss=2.73730, train_acc=0.95782, val_loss=3.82260, val_acc=0.90812, time=0.71099
Epoch:0031, train_loss=2.73280, train_acc=0.96003, val_loss=3.82224, val_acc=0.90965, time=0.73200
Epoch:0032, train_loss=2.72862, train_acc=0.96224, val_loss=3.82190, val_acc=0.90965, time=0.74398
Epoch:0033, train_loss=2.72475, train_acc=0.96530, val_loss=3.82158, val_acc=0.91424, time=0.74100
Epoch:0034, train_loss=2.72115, train_acc=0.96751, val_loss=3.82128, val_acc=0.91577, time=0.73198
Epoch:0035, train_loss=2.71782, train_acc=0.97074, val_loss=3.82100, val_acc=0.91730, time=0.71599
Epoch:0036, train_loss=2.71473, train_acc=0.97278, val_loss=3.82073, val_acc=0.91884, time=0.80699
Epoch:0037, train_loss=2.71188, train_acc=0.97517, val_loss=3.82047, val_acc=0.91730, time=0.69900
Epoch:0038, train_loss=2.70923, train_acc=0.97687, val_loss=3.82024, val_acc=0.92037, time=0.81198
Epoch:0039, train_loss=2.70677, train_acc=0.97840, val_loss=3.82002, val_acc=0.92190, time=0.72200
Epoch:0040, train_loss=2.70448, train_acc=0.97959, val_loss=3.81982, val_acc=0.92343, time=0.69899
Epoch:0041, train_loss=2.70234, train_acc=0.98095, val_loss=3.81964, val_acc=0.92496, time=0.70699
Epoch:0042, train_loss=2.70033, train_acc=0.98316, val_loss=3.81948, val_acc=0.92496, time=0.80899
Epoch:0043, train_loss=2.69845, train_acc=0.98435, val_loss=3.81933, val_acc=0.92496, time=0.70700
Epoch:0044, train_loss=2.69668, train_acc=0.98469, val_loss=3.81920, val_acc=0.92649, time=0.74299
Epoch:0045, train_loss=2.69502, train_acc=0.98520, val_loss=3.81908, val_acc=0.92802, time=0.68301
Epoch:0046, train_loss=2.69347, train_acc=0.98622, val_loss=3.81898, val_acc=0.92802, time=0.74598
Epoch:0047, train_loss=2.69202, train_acc=0.98622, val_loss=3.81889, val_acc=0.92802, time=0.72101
Epoch:0048, train_loss=2.69066, train_acc=0.98639, val_loss=3.81880, val_acc=0.92802, time=0.72899
Epoch:0049, train_loss=2.68938, train_acc=0.98622, val_loss=3.81872, val_acc=0.92802, time=0.76999
Epoch:0050, train_loss=2.68818, train_acc=0.98690, val_loss=3.81864, val_acc=0.92956, time=0.78799
Epoch:0051, train_loss=2.68704, train_acc=0.98741, val_loss=3.81856, val_acc=0.92956, time=0.87699
Epoch:0052, train_loss=2.68597, train_acc=0.98792, val_loss=3.81848, val_acc=0.93109, time=0.72998
Epoch:0053, train_loss=2.68494, train_acc=0.98877, val_loss=3.81840, val_acc=0.93262, time=0.81800
Epoch:0054, train_loss=2.68398, train_acc=0.98911, val_loss=3.81831, val_acc=0.93109, time=0.89000
Epoch:0055, train_loss=2.68306, train_acc=0.98996, val_loss=3.81823, val_acc=0.93109, time=0.77698
Epoch:0056, train_loss=2.68219, train_acc=0.99098, val_loss=3.81815, val_acc=0.93262, time=0.87499
Epoch:0057, train_loss=2.68137, train_acc=0.99081, val_loss=3.81807, val_acc=0.93415, time=0.76400
Epoch:0058, train_loss=2.68059, train_acc=0.99115, val_loss=3.81799, val_acc=0.93415, time=0.80899
Epoch:0059, train_loss=2.67985, train_acc=0.99150, val_loss=3.81792, val_acc=0.93415, time=0.80099
Epoch:0060, train_loss=2.67915, train_acc=0.99201, val_loss=3.81786, val_acc=0.93568, time=0.77600
Epoch:0061, train_loss=2.67848, train_acc=0.99252, val_loss=3.81780, val_acc=0.93721, time=0.74199
Epoch:0062, train_loss=2.67784, train_acc=0.99252, val_loss=3.81774, val_acc=0.93721, time=0.72800
Epoch:0063, train_loss=2.67723, train_acc=0.99286, val_loss=3.81769, val_acc=0.93721, time=0.79299
Epoch:0064, train_loss=2.67665, train_acc=0.99303, val_loss=3.81764, val_acc=0.93721, time=0.78097
Epoch:0065, train_loss=2.67610, train_acc=0.99337, val_loss=3.81759, val_acc=0.93721, time=0.83599
Epoch:0066, train_loss=2.67558, train_acc=0.99388, val_loss=3.81755, val_acc=0.93721, time=0.74501
Epoch:0067, train_loss=2.67508, train_acc=0.99405, val_loss=3.81751, val_acc=0.93721, time=0.73600
Epoch:0068, train_loss=2.67461, train_acc=0.99405, val_loss=3.81747, val_acc=0.93721, time=0.66998
Epoch:0069, train_loss=2.67417, train_acc=0.99422, val_loss=3.81743, val_acc=0.93874, time=0.86900
Epoch:0070, train_loss=2.67375, train_acc=0.99422, val_loss=3.81740, val_acc=0.93874, time=0.89099
Epoch:0071, train_loss=2.67335, train_acc=0.99422, val_loss=3.81737, val_acc=0.93874, time=0.74701
Epoch:0072, train_loss=2.67297, train_acc=0.99439, val_loss=3.81734, val_acc=0.93874, time=0.80298
Epoch:0073, train_loss=2.67261, train_acc=0.99456, val_loss=3.81731, val_acc=0.93874, time=0.88499
Epoch:0074, train_loss=2.67228, train_acc=0.99490, val_loss=3.81729, val_acc=0.93874, time=0.67398
Epoch:0075, train_loss=2.67196, train_acc=0.99490, val_loss=3.81728, val_acc=0.93874, time=0.89399
Epoch:0076, train_loss=2.67165, train_acc=0.99490, val_loss=3.81726, val_acc=0.93874, time=0.70001
Epoch:0077, train_loss=2.67136, train_acc=0.99524, val_loss=3.81725, val_acc=0.93874, time=0.69200
Epoch:0078, train_loss=2.67108, train_acc=0.99575, val_loss=3.81724, val_acc=0.93874, time=0.73099
Epoch:0079, train_loss=2.67081, train_acc=0.99592, val_loss=3.81724, val_acc=0.93874, time=0.89697
Epoch:0080, train_loss=2.67056, train_acc=0.99592, val_loss=3.81723, val_acc=0.93721, time=0.69200
Epoch:0081, train_loss=2.67031, train_acc=0.99609, val_loss=3.81723, val_acc=0.93721, time=0.69599
Epoch:0082, train_loss=2.67007, train_acc=0.99609, val_loss=3.81722, val_acc=0.93721, time=0.72799
Epoch:0083, train_loss=2.66985, train_acc=0.99609, val_loss=3.81722, val_acc=0.93721, time=0.86599
Epoch:0084, train_loss=2.66963, train_acc=0.99609, val_loss=3.81721, val_acc=0.93721, time=0.70400
Epoch:0085, train_loss=2.66942, train_acc=0.99643, val_loss=3.81720, val_acc=0.93721, time=0.82899
Epoch:0086, train_loss=2.66922, train_acc=0.99643, val_loss=3.81720, val_acc=0.93721, time=0.75899
Epoch:0087, train_loss=2.66902, train_acc=0.99643, val_loss=3.81719, val_acc=0.93721, time=0.68499
Epoch:0088, train_loss=2.66884, train_acc=0.99660, val_loss=3.81718, val_acc=0.93874, time=0.79000
Epoch:0089, train_loss=2.66866, train_acc=0.99660, val_loss=3.81717, val_acc=0.93874, time=0.81099
Epoch:0090, train_loss=2.66848, train_acc=0.99677, val_loss=3.81717, val_acc=0.93874, time=0.81900
Epoch:0091, train_loss=2.66832, train_acc=0.99711, val_loss=3.81716, val_acc=0.93874, time=0.68999
Epoch:0092, train_loss=2.66816, train_acc=0.99711, val_loss=3.81716, val_acc=0.93874, time=0.80698
Epoch:0093, train_loss=2.66800, train_acc=0.99711, val_loss=3.81715, val_acc=0.93874, time=0.75400
Epoch:0094, train_loss=2.66785, train_acc=0.99711, val_loss=3.81715, val_acc=0.93874, time=0.74298
Epoch:0095, train_loss=2.66771, train_acc=0.99745, val_loss=3.81714, val_acc=0.94028, time=0.80601
Epoch:0096, train_loss=2.66757, train_acc=0.99745, val_loss=3.81714, val_acc=0.94028, time=0.85399
Epoch:0097, train_loss=2.66744, train_acc=0.99745, val_loss=3.81714, val_acc=0.94028, time=0.85200
Epoch:0098, train_loss=2.66731, train_acc=0.99745, val_loss=3.81713, val_acc=0.94028, time=0.77800
Epoch:0099, train_loss=2.66719, train_acc=0.99745, val_loss=3.81713, val_acc=0.94028, time=0.71399
Epoch:0100, train_loss=2.66707, train_acc=0.99745, val_loss=3.81713, val_acc=0.94028, time=0.75498
Epoch:0101, train_loss=2.66695, train_acc=0.99745, val_loss=3.81712, val_acc=0.94028, time=0.84799
Epoch:0102, train_loss=2.66684, train_acc=0.99745, val_loss=3.81712, val_acc=0.93874, time=0.70499
Epoch:0103, train_loss=2.66673, train_acc=0.99745, val_loss=3.81712, val_acc=0.93874, time=0.75900
Epoch:0104, train_loss=2.66663, train_acc=0.99762, val_loss=3.81712, val_acc=0.93874, time=0.78898
Epoch:0105, train_loss=2.66653, train_acc=0.99762, val_loss=3.81712, val_acc=0.93874, time=0.69700
Epoch:0106, train_loss=2.66643, train_acc=0.99762, val_loss=3.81711, val_acc=0.93874, time=0.77699
Epoch:0107, train_loss=2.66633, train_acc=0.99762, val_loss=3.81711, val_acc=0.93874, time=0.67499
Epoch:0108, train_loss=2.66624, train_acc=0.99779, val_loss=3.81711, val_acc=0.93874, time=0.86998
Epoch:0109, train_loss=2.66615, train_acc=0.99796, val_loss=3.81711, val_acc=0.93874, time=0.86099
Epoch:0110, train_loss=2.66606, train_acc=0.99796, val_loss=3.81711, val_acc=0.93874, time=0.74499
Epoch:0111, train_loss=2.66598, train_acc=0.99796, val_loss=3.81711, val_acc=0.93874, time=0.74599
Epoch:0112, train_loss=2.66590, train_acc=0.99796, val_loss=3.81711, val_acc=0.93874, time=0.92200
Epoch:0113, train_loss=2.66582, train_acc=0.99796, val_loss=3.81711, val_acc=0.93874, time=0.83899
Epoch:0114, train_loss=2.66574, train_acc=0.99796, val_loss=3.81711, val_acc=0.93874, time=0.70798
Epoch:0115, train_loss=2.66566, train_acc=0.99796, val_loss=3.81711, val_acc=0.93874, time=0.75401
Epoch:0116, train_loss=2.66559, train_acc=0.99796, val_loss=3.81711, val_acc=0.93874, time=0.66800
Epoch:0117, train_loss=2.66551, train_acc=0.99796, val_loss=3.81711, val_acc=0.93874, time=0.83599
Epoch:0118, train_loss=2.66544, train_acc=0.99796, val_loss=3.81711, val_acc=0.93874, time=0.75299
Epoch:0119, train_loss=2.66537, train_acc=0.99796, val_loss=3.81711, val_acc=0.93874, time=0.80299
Epoch:0120, train_loss=2.66531, train_acc=0.99796, val_loss=3.81711, val_acc=0.93874, time=0.75701
Epoch:0121, train_loss=2.66524, train_acc=0.99796, val_loss=3.81711, val_acc=0.93874, time=0.73599
Epoch:0122, train_loss=2.66518, train_acc=0.99796, val_loss=3.81711, val_acc=0.93874, time=0.74199
Epoch:0123, train_loss=2.66512, train_acc=0.99796, val_loss=3.81710, val_acc=0.93874, time=0.67298
Epoch:0124, train_loss=2.66505, train_acc=0.99796, val_loss=3.81710, val_acc=0.94028, time=0.73899
Epoch:0125, train_loss=2.66500, train_acc=0.99796, val_loss=3.81711, val_acc=0.94028, time=0.82901
Epoch:0126, train_loss=2.66494, train_acc=0.99796, val_loss=3.81711, val_acc=0.94028, time=0.82299
Epoch:0127, train_loss=2.66488, train_acc=0.99796, val_loss=3.81711, val_acc=0.94028, time=0.87398
Epoch:0128, train_loss=2.66483, train_acc=0.99796, val_loss=3.81711, val_acc=0.94028, time=0.83399
Early stopping...

Optimization Finished!

Test set results: loss= 3.44178, accuracy= 0.91900, time= 0.26500

Test Precision, Recall and F1-Score...
              precision    recall  f1-score   support

           0     0.9167    1.0000    0.9565        22
           1     0.9571    0.9898    0.9732      1083
           2     0.8621    0.6944    0.7692        36
           3     0.8750    0.9333    0.9032        15
           4     0.9559    0.9339    0.9448       696
           5     0.6875    1.0000    0.8148        11
           6     0.8750    0.7368    0.8000        19
           7     1.0000    0.7647    0.8667        17
           8     0.7500    0.9000    0.8182        10
           9     0.8444    0.9421    0.8906       121
          10     0.7857    0.9167    0.8462        12
          11     0.8333    0.7407    0.7843        81
          12     0.8229    0.9080    0.8634        87
          13     0.8125    0.9286    0.8667        28
          14     0.6762    0.9467    0.7889        75
          15     0.9375    0.7500    0.8333        20
          16     0.8571    0.5455    0.6667        11
          17     1.0000    0.9167    0.9565        12
          18     0.0000    0.0000    0.0000         1
          19     0.8333    0.7692    0.8000        13
          20     1.0000    0.8889    0.9412         9
          21     1.0000    0.7778    0.8750         9
          22     0.7143    0.5556    0.6250         9
          23     1.0000    0.1667    0.2857        12
          24     1.0000    1.0000    1.0000         1
          25     1.0000    0.7778    0.8750         9
          26     0.0000    0.0000    0.0000         6
          27     0.8846    0.9200    0.9020        25
          28     1.0000    0.5000    0.6667         4
          29     0.9000    0.7500    0.8182        12
          30     1.0000    0.9333    0.9655        15
          31     0.8000    0.8000    0.8000         5
          32     0.6000    0.6000    0.6000         5
          33     1.0000    1.0000    1.0000         2
          34     0.6000    0.7500    0.6667         4
          35     0.9000    0.9000    0.9000        10
          36     0.8750    0.8750    0.8750         8
          37     1.0000    0.2000    0.3333         5
          38     0.0000    0.0000    0.0000         1
          39     1.0000    0.3333    0.5000         3
          40     1.0000    1.0000    1.0000         9
          41     1.0000    1.0000    1.0000         3
          42     0.3333    0.1667    0.2222         6
          43     1.0000    0.3333    0.5000         3
          44     0.0000    0.0000    0.0000         4
          45     0.0000    0.0000    0.0000         1
          46     0.0000    0.0000    0.0000         1
          47     1.0000    0.7500    0.8571         4
          48     1.0000    1.0000    1.0000         1
          49     0.0000    0.0000    0.0000         3
          50     0.0000    0.0000    0.0000         2
          51     1.0000    0.1429    0.2500         7

    accuracy                         0.9190      2568
   macro avg     0.7479    0.6430    0.6654      2568
weighted avg     0.9162    0.9190    0.9122      2568


Macro average Test Precision, Recall and F1-Score...
(0.7478763590639955, 0.6430472987985598, 0.6654186074162876, None)

Micro average Test Precision, Recall and F1-Score...
(0.9190031152647975, 0.9190031152647975, 0.9190031152647975, None)

Embeddings:
Word_embeddings:8892
Train_doc_embeddings:6532
Test_doc_embeddings:2568
