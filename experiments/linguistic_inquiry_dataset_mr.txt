
==================== Torch Seed: 48079086829700
Epoch:0001, train_loss=0.87103, train_acc=0.50547, val_loss=0.70582, val_acc=0.50141, time=1.83198
Epoch:0002, train_loss=0.77438, train_acc=0.52501, val_loss=0.70639, val_acc=0.51690, time=1.69699
Epoch:0003, train_loss=0.75009, train_acc=0.57471, val_loss=0.70268, val_acc=0.56197, time=1.62197
Epoch:0004, train_loss=0.69273, train_acc=0.66396, val_loss=0.69928, val_acc=0.61549, time=1.53698
Epoch:0005, train_loss=0.64378, train_acc=0.77977, val_loss=0.69860, val_acc=0.65775, time=1.52199
Epoch:0006, train_loss=0.62205, train_acc=0.84808, val_loss=0.69909, val_acc=0.64789, time=1.57898
Epoch:0007, train_loss=0.61265, train_acc=0.86527, val_loss=0.69897, val_acc=0.66056, time=1.59397
Epoch:0008, train_loss=0.60207, train_acc=0.88668, val_loss=0.69807, val_acc=0.68028, time=1.55699
Epoch:0009, train_loss=0.59005, train_acc=0.91826, val_loss=0.69698, val_acc=0.68873, time=1.47898
Epoch:0010, train_loss=0.58009, train_acc=0.94576, val_loss=0.69619, val_acc=0.70986, time=1.69098
Epoch:0011, train_loss=0.57372, train_acc=0.95827, val_loss=0.69584, val_acc=0.71127, time=1.72397
Epoch:0012, train_loss=0.57036, train_acc=0.96311, val_loss=0.69583, val_acc=0.72113, time=1.58496
Epoch:0013, train_loss=0.56862, train_acc=0.96186, val_loss=0.69596, val_acc=0.72676, time=1.68296
Epoch:0014, train_loss=0.56733, train_acc=0.96280, val_loss=0.69610, val_acc=0.72254, time=1.51298
Epoch:0015, train_loss=0.56581, train_acc=0.96655, val_loss=0.69618, val_acc=0.72676, time=1.72197
Epoch:0016, train_loss=0.56392, train_acc=0.97015, val_loss=0.69621, val_acc=0.72394, time=1.61197
Epoch:0017, train_loss=0.56187, train_acc=0.97671, val_loss=0.69623, val_acc=0.72676, time=1.59496
Epoch:0018, train_loss=0.55992, train_acc=0.98078, val_loss=0.69629, val_acc=0.72394, time=1.48398
Epoch:0019, train_loss=0.55828, train_acc=0.98328, val_loss=0.69642, val_acc=0.72113, time=1.66897
Early stopping...

Optimization Finished!

Test set results: loss= 0.70221, accuracy= 0.73101, time= 0.51600

Test Precision, Recall and F1-Score...
              precision    recall  f1-score   support

           0     0.7315    0.7299    0.7307      1777
           1     0.7305    0.7321    0.7313      1777

    accuracy                         0.7310      3554
   macro avg     0.7310    0.7310    0.7310      3554
weighted avg     0.7310    0.7310    0.7310      3554


Macro average Test Precision, Recall and F1-Score...
(0.7310084862050478, 0.7310073157006189, 0.7310069749582373, None)

Micro average Test Precision, Recall and F1-Score...
(0.731007315700619, 0.731007315700619, 0.731007315700619, None)

Embeddings:
Word_embeddings:18764
Train_doc_embeddings:7108
Test_doc_embeddings:3554
