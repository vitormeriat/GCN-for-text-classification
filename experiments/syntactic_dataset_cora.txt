
==================== Torch Seed: 49158191854600
Epoch:0001, train_loss=2.07853, train_acc=0.11541, val_loss=1.94250, val_acc=0.27513, time=0.07101
Epoch:0002, train_loss=1.88005, train_acc=0.32045, val_loss=1.93721, val_acc=0.32275, time=0.06000
Epoch:0003, train_loss=1.81984, train_acc=0.38664, val_loss=1.93268, val_acc=0.38095, time=0.05802
Epoch:0004, train_loss=1.76736, train_acc=0.46104, val_loss=1.92646, val_acc=0.41799, time=0.06799
Epoch:0005, train_loss=1.70170, train_acc=0.53486, val_loss=1.91985, val_acc=0.51323, time=0.05801
Epoch:0006, train_loss=1.63347, train_acc=0.61804, val_loss=1.91414, val_acc=0.58201, time=0.06900
Epoch:0007, train_loss=1.57327, train_acc=0.69127, val_loss=1.90966, val_acc=0.64021, time=0.06799
Epoch:0008, train_loss=1.52399, train_acc=0.74165, val_loss=1.90617, val_acc=0.67725, time=0.06800
Epoch:0009, train_loss=1.48391, train_acc=0.78852, val_loss=1.90359, val_acc=0.69312, time=0.06702
Epoch:0010, train_loss=1.45236, train_acc=0.82191, val_loss=1.90181, val_acc=0.70899, time=0.06799
Epoch:0011, train_loss=1.42822, train_acc=0.84241, val_loss=1.90035, val_acc=0.71958, time=0.06799
Epoch:0012, train_loss=1.40800, train_acc=0.85120, val_loss=1.89873, val_acc=0.71429, time=0.07100
Epoch:0013, train_loss=1.38817, train_acc=0.85764, val_loss=1.89684, val_acc=0.72487, time=0.06900
Epoch:0014, train_loss=1.36767, train_acc=0.86819, val_loss=1.89488, val_acc=0.74074, time=0.06801
Epoch:0015, train_loss=1.34770, train_acc=0.87698, val_loss=1.89313, val_acc=0.74074, time=0.06900
Epoch:0016, train_loss=1.32985, train_acc=0.88518, val_loss=1.89179, val_acc=0.74603, time=0.07102
Epoch:0017, train_loss=1.31494, train_acc=0.88928, val_loss=1.89090, val_acc=0.74603, time=0.07000
Epoch:0018, train_loss=1.30289, train_acc=0.89338, val_loss=1.89037, val_acc=0.73545, time=0.07000
Epoch:0019, train_loss=1.29293, train_acc=0.89455, val_loss=1.89006, val_acc=0.73545, time=0.06601
Epoch:0020, train_loss=1.28400, train_acc=0.89690, val_loss=1.88980, val_acc=0.74074, time=0.06700
Epoch:0021, train_loss=1.27514, train_acc=0.90334, val_loss=1.88952, val_acc=0.74603, time=0.07001
Epoch:0022, train_loss=1.26592, train_acc=0.90978, val_loss=1.88922, val_acc=0.75132, time=0.05399
Epoch:0023, train_loss=1.25651, train_acc=0.91798, val_loss=1.88896, val_acc=0.75661, time=0.05200
Epoch:0024, train_loss=1.24740, train_acc=0.92677, val_loss=1.88882, val_acc=0.77249, time=0.06801
Epoch:0025, train_loss=1.23911, train_acc=0.93322, val_loss=1.88882, val_acc=0.78307, time=0.06901
Epoch:0026, train_loss=1.23189, train_acc=0.93966, val_loss=1.88895, val_acc=0.77249, time=0.07000
Epoch:0027, train_loss=1.22564, train_acc=0.94845, val_loss=1.88916, val_acc=0.77249, time=0.06999
Epoch:0028, train_loss=1.22007, train_acc=0.95313, val_loss=1.88938, val_acc=0.77778, time=0.07001
Early stopping...

Optimization Finished!

Test set results: loss= 1.72751, accuracy= 0.70813, time= 0.01800

Test Precision, Recall and F1-Score...
              precision    recall  f1-score   support

           0     0.8175    0.7357    0.7744       140
           1     0.7273    0.7273    0.7273       121
           2     0.6126    0.5862    0.5991       116
           3     0.7306    0.7682    0.7490       233
           4     0.8200    0.6308    0.7130        65
           5     0.6404    0.7935    0.7087        92
           6     0.5111    0.5111    0.5111        45

    accuracy                         0.7081       812
   macro avg     0.6942    0.6790    0.6832       812
weighted avg     0.7130    0.7081    0.7081       812


Macro average Test Precision, Recall and F1-Score...
(0.6942028415068157, 0.6789704079480404, 0.6832391697963738, None)

Micro average Test Precision, Recall and F1-Score...
(0.708128078817734, 0.708128078817734, 0.708128078817734, None)

Embeddings:
Word_embeddings:1343
Train_doc_embeddings:1896
Test_doc_embeddings:812
