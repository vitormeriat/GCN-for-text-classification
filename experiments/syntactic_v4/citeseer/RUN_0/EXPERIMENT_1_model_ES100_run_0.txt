
==========: 232692549837400
Epoch:0001, train_loss=1.83520, train_acc=0.17433, val_loss=1.79037, val_acc=0.27706, time=0.34099
Epoch:0002, train_loss=1.76149, train_acc=0.30220, val_loss=1.78474, val_acc=0.35498, time=0.34501
Epoch:0003, train_loss=1.69679, train_acc=0.40996, val_loss=1.77845, val_acc=0.52814, time=0.38199
Epoch:0004, train_loss=1.63065, train_acc=0.60201, val_loss=1.77426, val_acc=0.58009, time=0.35301
Epoch:0005, train_loss=1.58473, train_acc=0.67433, val_loss=1.77131, val_acc=0.59740, time=0.33300
Epoch:0006, train_loss=1.54984, train_acc=0.70546, val_loss=1.76852, val_acc=0.65368, time=0.33900
Epoch:0007, train_loss=1.51635, train_acc=0.75431, val_loss=1.76594, val_acc=0.69264, time=0.32700
Epoch:0008, train_loss=1.48546, train_acc=0.79646, val_loss=1.76403, val_acc=0.72294, time=0.32000
Epoch:0009, train_loss=1.46124, train_acc=0.82423, val_loss=1.76278, val_acc=0.72294, time=0.32201
Epoch:0010, train_loss=1.44323, train_acc=0.83142, val_loss=1.76182, val_acc=0.72727, time=0.32099
Epoch:0011, train_loss=1.42793, train_acc=0.84195, val_loss=1.76091, val_acc=0.74026, time=0.31800
Epoch:0012, train_loss=1.41319, train_acc=0.85297, val_loss=1.76008, val_acc=0.73593, time=0.35401
Epoch:0013, train_loss=1.39925, train_acc=0.86542, val_loss=1.75944, val_acc=0.73160, time=0.32500
Epoch:0014, train_loss=1.38708, train_acc=0.87261, val_loss=1.75903, val_acc=0.73593, time=0.32199
Epoch:0015, train_loss=1.37695, train_acc=0.88506, val_loss=1.75876, val_acc=0.74892, time=0.31601
Epoch:0016, train_loss=1.36821, train_acc=0.89368, val_loss=1.75853, val_acc=0.74026, time=0.30400
Epoch:0017, train_loss=1.36004, train_acc=0.89799, val_loss=1.75831, val_acc=0.74459, time=0.31502
Epoch:0018, train_loss=1.35205, train_acc=0.90900, val_loss=1.75812, val_acc=0.74892, time=0.30100
Epoch:0019, train_loss=1.34435, train_acc=0.91523, val_loss=1.75802, val_acc=0.74892, time=0.32500
Epoch:0020, train_loss=1.33723, train_acc=0.92577, val_loss=1.75804, val_acc=0.74892, time=0.32000
Epoch:0021, train_loss=1.33082, train_acc=0.93151, val_loss=1.75817, val_acc=0.74459, time=0.30099
Epoch:0022, train_loss=1.32500, train_acc=0.93534, val_loss=1.75837, val_acc=0.74892, time=0.30801
Epoch:0023, train_loss=1.31957, train_acc=0.94349, val_loss=1.75860, val_acc=0.74892, time=0.31700
Epoch:0024, train_loss=1.31443, train_acc=0.94971, val_loss=1.75882, val_acc=0.74892, time=0.31801
Epoch:0025, train_loss=1.30952, train_acc=0.95307, val_loss=1.75901, val_acc=0.74459, time=0.31499
Epoch:0026, train_loss=1.30491, train_acc=0.95738, val_loss=1.75917, val_acc=0.74459, time=0.32302
Epoch:0027, train_loss=1.30063, train_acc=0.96408, val_loss=1.75931, val_acc=0.74459, time=0.32302
Epoch:0028, train_loss=1.29671, train_acc=0.97031, val_loss=1.75944, val_acc=0.74459, time=0.32701
Epoch:0029, train_loss=1.29314, train_acc=0.97557, val_loss=1.75958, val_acc=0.74459, time=0.31501
Epoch:0030, train_loss=1.28986, train_acc=0.98084, val_loss=1.75974, val_acc=0.74459, time=0.28900
Epoch:0031, train_loss=1.28680, train_acc=0.98228, val_loss=1.75992, val_acc=0.74459, time=0.32702
Epoch:0032, train_loss=1.28391, train_acc=0.98515, val_loss=1.76015, val_acc=0.74026, time=0.34399
Epoch:0033, train_loss=1.28119, train_acc=0.98803, val_loss=1.76043, val_acc=0.74026, time=0.31200
Epoch:0034, train_loss=1.27870, train_acc=0.99138, val_loss=1.76074, val_acc=0.73593, time=0.32404
Epoch:0035, train_loss=1.27644, train_acc=0.99377, val_loss=1.76107, val_acc=0.73160, time=0.32099
Epoch:0036, train_loss=1.27439, train_acc=0.99521, val_loss=1.76139, val_acc=0.73593, time=0.31299
Epoch:0037, train_loss=1.27249, train_acc=0.99617, val_loss=1.76168, val_acc=0.74459, time=0.29801
Epoch:0038, train_loss=1.27073, train_acc=0.99617, val_loss=1.76194, val_acc=0.74026, time=0.31901
Epoch:0039, train_loss=1.26910, train_acc=0.99665, val_loss=1.76216, val_acc=0.73593, time=0.32800
Epoch:0040, train_loss=1.26759, train_acc=0.99761, val_loss=1.76236, val_acc=0.73593, time=0.31800
Epoch:0041, train_loss=1.26621, train_acc=0.99808, val_loss=1.76253, val_acc=0.73593, time=0.28799
Epoch:0042, train_loss=1.26493, train_acc=0.99808, val_loss=1.76270, val_acc=0.73160, time=0.32101
Epoch:0043, train_loss=1.26377, train_acc=0.99808, val_loss=1.76288, val_acc=0.73593, time=0.31100
Epoch:0044, train_loss=1.26270, train_acc=0.99904, val_loss=1.76308, val_acc=0.74026, time=0.32301
Epoch:0045, train_loss=1.26172, train_acc=0.99904, val_loss=1.76328, val_acc=0.74026, time=0.33000
Epoch:0046, train_loss=1.26080, train_acc=0.99904, val_loss=1.76351, val_acc=0.74026, time=0.32099
Epoch:0047, train_loss=1.25994, train_acc=0.99904, val_loss=1.76375, val_acc=0.73593, time=0.30701
Epoch:0048, train_loss=1.25915, train_acc=0.99904, val_loss=1.76401, val_acc=0.73593, time=0.31302
Epoch:0049, train_loss=1.25841, train_acc=0.99904, val_loss=1.76426, val_acc=0.73160, time=0.32898
Epoch:0050, train_loss=1.25773, train_acc=0.99904, val_loss=1.76451, val_acc=0.73160, time=0.30499
Epoch:0051, train_loss=1.25709, train_acc=0.99904, val_loss=1.76474, val_acc=0.72727, time=0.31101
Epoch:0052, train_loss=1.25651, train_acc=0.99904, val_loss=1.76497, val_acc=0.72727, time=0.32200
Epoch:0053, train_loss=1.25596, train_acc=0.99904, val_loss=1.76517, val_acc=0.72727, time=0.31401
Epoch:0054, train_loss=1.25545, train_acc=0.99904, val_loss=1.76536, val_acc=0.72727, time=0.32401
Epoch:0055, train_loss=1.25497, train_acc=0.99952, val_loss=1.76553, val_acc=0.72727, time=0.33601
Epoch:0056, train_loss=1.25452, train_acc=0.99952, val_loss=1.76569, val_acc=0.72727, time=0.32300
Epoch:0057, train_loss=1.25409, train_acc=0.99952, val_loss=1.76585, val_acc=0.72727, time=0.31701
Epoch:0058, train_loss=1.25370, train_acc=0.99952, val_loss=1.76600, val_acc=0.72727, time=0.32000
Epoch:0059, train_loss=1.25333, train_acc=0.99952, val_loss=1.76615, val_acc=0.72727, time=0.33202
Epoch:0060, train_loss=1.25299, train_acc=0.99952, val_loss=1.76631, val_acc=0.72727, time=0.32301
Epoch:0061, train_loss=1.25266, train_acc=0.99952, val_loss=1.76647, val_acc=0.72727, time=0.31601
Epoch:0062, train_loss=1.25235, train_acc=0.99952, val_loss=1.76664, val_acc=0.72727, time=0.32200
Epoch:0063, train_loss=1.25206, train_acc=0.99952, val_loss=1.76680, val_acc=0.72727, time=0.32602
Epoch:0064, train_loss=1.25179, train_acc=0.99952, val_loss=1.76697, val_acc=0.72727, time=0.31599
Epoch:0065, train_loss=1.25153, train_acc=0.99952, val_loss=1.76714, val_acc=0.72727, time=0.31402
Epoch:0066, train_loss=1.25129, train_acc=0.99952, val_loss=1.76731, val_acc=0.72294, time=0.33501
Epoch:0067, train_loss=1.25106, train_acc=0.99952, val_loss=1.76747, val_acc=0.72294, time=0.32199
Epoch:0068, train_loss=1.25085, train_acc=0.99952, val_loss=1.76763, val_acc=0.72294, time=0.30701
Epoch:0069, train_loss=1.25064, train_acc=0.99952, val_loss=1.76778, val_acc=0.72294, time=0.32000
Epoch:0070, train_loss=1.25044, train_acc=0.99952, val_loss=1.76792, val_acc=0.72294, time=0.32099
Epoch:0071, train_loss=1.25026, train_acc=0.99952, val_loss=1.76806, val_acc=0.72294, time=0.30700
Epoch:0072, train_loss=1.25008, train_acc=0.99952, val_loss=1.76819, val_acc=0.72294, time=0.31202
Epoch:0073, train_loss=1.24992, train_acc=0.99952, val_loss=1.76832, val_acc=0.72294, time=0.33001
Epoch:0074, train_loss=1.24976, train_acc=0.99952, val_loss=1.76844, val_acc=0.71861, time=0.32201
Epoch:0075, train_loss=1.24960, train_acc=0.99952, val_loss=1.76856, val_acc=0.71861, time=0.31200
Epoch:0076, train_loss=1.24946, train_acc=0.99952, val_loss=1.76869, val_acc=0.71861, time=0.33200
Epoch:0077, train_loss=1.24932, train_acc=0.99952, val_loss=1.76881, val_acc=0.71861, time=0.29501
Epoch:0078, train_loss=1.24919, train_acc=0.99952, val_loss=1.76892, val_acc=0.71861, time=0.32300
Epoch:0079, train_loss=1.24906, train_acc=0.99952, val_loss=1.76904, val_acc=0.71861, time=0.31899
Epoch:0080, train_loss=1.24894, train_acc=0.99952, val_loss=1.76916, val_acc=0.71861, time=0.30602
Epoch:0081, train_loss=1.24882, train_acc=0.99952, val_loss=1.76927, val_acc=0.71861, time=0.31900
Epoch:0082, train_loss=1.24871, train_acc=0.99952, val_loss=1.76939, val_acc=0.71861, time=0.31401
Epoch:0083, train_loss=1.24861, train_acc=0.99952, val_loss=1.76950, val_acc=0.71429, time=0.32100
Epoch:0084, train_loss=1.24850, train_acc=0.99952, val_loss=1.76961, val_acc=0.71429, time=0.30001
Epoch:0085, train_loss=1.24840, train_acc=0.99952, val_loss=1.76971, val_acc=0.71429, time=0.31000
Epoch:0086, train_loss=1.24831, train_acc=0.99952, val_loss=1.76982, val_acc=0.71429, time=0.32501
Epoch:0087, train_loss=1.24822, train_acc=0.99952, val_loss=1.76992, val_acc=0.71429, time=0.31600
Epoch:0088, train_loss=1.24813, train_acc=0.99952, val_loss=1.77003, val_acc=0.71429, time=0.31300
Epoch:0089, train_loss=1.24804, train_acc=0.99952, val_loss=1.77013, val_acc=0.71429, time=0.31900
Epoch:0090, train_loss=1.24796, train_acc=0.99952, val_loss=1.77023, val_acc=0.71429, time=0.33201
Epoch:0091, train_loss=1.24788, train_acc=0.99952, val_loss=1.77033, val_acc=0.71429, time=0.33800
Epoch:0092, train_loss=1.24781, train_acc=0.99952, val_loss=1.77043, val_acc=0.71429, time=0.31401
Epoch:0093, train_loss=1.24773, train_acc=0.99952, val_loss=1.77052, val_acc=0.71429, time=0.31501
Epoch:0094, train_loss=1.24766, train_acc=0.99952, val_loss=1.77062, val_acc=0.71429, time=0.34200
Epoch:0095, train_loss=1.24759, train_acc=0.99952, val_loss=1.77071, val_acc=0.71429, time=0.30700
Epoch:0096, train_loss=1.24753, train_acc=0.99952, val_loss=1.77081, val_acc=0.71429, time=0.31301
Epoch:0097, train_loss=1.24746, train_acc=0.99952, val_loss=1.77090, val_acc=0.71429, time=0.32001
Epoch:0098, train_loss=1.24740, train_acc=0.99952, val_loss=1.77099, val_acc=0.71429, time=0.31901
Epoch:0099, train_loss=1.24734, train_acc=0.99952, val_loss=1.77108, val_acc=0.71429, time=0.31500
Epoch:0100, train_loss=1.24728, train_acc=0.99952, val_loss=1.77116, val_acc=0.71429, time=0.31800
Epoch:0101, train_loss=1.24722, train_acc=0.99952, val_loss=1.77125, val_acc=0.71429, time=0.30500
Epoch:0102, train_loss=1.24716, train_acc=0.99952, val_loss=1.77134, val_acc=0.71429, time=0.31700
Early stopping...

Optimization Finished!

Test set results: loss= 1.72932, accuracy= 0.69990, time= 0.10000

Test Precision, Recall and F1-Score...
              precision    recall  f1-score   support

           0     0.7485    0.7225    0.7353       173
           1     0.7407    0.7692    0.7547       208
           2     0.7466    0.7267    0.7365       150
           3     0.6762    0.7513    0.7118       189
           4     0.6869    0.6667    0.6766       204
           5     0.4107    0.3333    0.3680        69

    accuracy                         0.6999       993
   macro avg     0.6683    0.6616    0.6638       993
weighted avg     0.6967    0.6999    0.6975       993


Macro average Test Precision, Recall and F1-Score...
(0.6682654209986532, 0.6616272566368905, 0.6638156582183433, None)

Micro average Test Precision, Recall and F1-Score...
(0.6998992950654582, 0.6998992950654582, 0.6998992950654582, None)

Embeddings:
Word_embeddings:3515
Train_doc_embeddings:2319
Test_doc_embeddings:993
