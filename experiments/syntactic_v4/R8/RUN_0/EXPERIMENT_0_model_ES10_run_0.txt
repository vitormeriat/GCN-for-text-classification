
==========: 233006947244100
Epoch:0001, train_loss=2.33116, train_acc=0.05489, val_loss=2.07256, val_acc=0.49088, time=1.29401
Epoch:0002, train_loss=2.01183, train_acc=0.48086, val_loss=2.05328, val_acc=0.57482, time=1.36901
Epoch:0003, train_loss=1.83935, train_acc=0.58416, val_loss=2.04239, val_acc=0.70073, time=1.33601
Epoch:0004, train_loss=1.74328, train_acc=0.69232, val_loss=2.03648, val_acc=0.75182, time=1.15700
Epoch:0005, train_loss=1.69253, train_acc=0.74701, val_loss=2.03281, val_acc=0.76095, time=1.13401
Epoch:0006, train_loss=1.65972, train_acc=0.76261, val_loss=2.02947, val_acc=0.78102, time=1.19600
Epoch:0007, train_loss=1.62709, train_acc=0.78570, val_loss=2.02613, val_acc=0.80292, time=1.09601
Epoch:0008, train_loss=1.59306, train_acc=0.82155, val_loss=2.02322, val_acc=0.84489, time=1.14300
Epoch:0009, train_loss=1.56297, train_acc=0.85923, val_loss=2.02103, val_acc=0.86679, time=1.08802
Epoch:0010, train_loss=1.54014, train_acc=0.88860, val_loss=2.01939, val_acc=0.89051, time=1.19299
Epoch:0011, train_loss=1.52298, train_acc=0.90865, val_loss=2.01805, val_acc=0.89964, time=1.36800
Epoch:0012, train_loss=1.50869, train_acc=0.92445, val_loss=2.01686, val_acc=0.90876, time=1.31901
Epoch:0013, train_loss=1.49596, train_acc=0.93944, val_loss=2.01579, val_acc=0.91606, time=1.24100
Epoch:0014, train_loss=1.48462, train_acc=0.94673, val_loss=2.01485, val_acc=0.92518, time=1.25101
Epoch:0015, train_loss=1.47490, train_acc=0.95564, val_loss=2.01406, val_acc=0.93796, time=1.22902
Epoch:0016, train_loss=1.46699, train_acc=0.96536, val_loss=2.01343, val_acc=0.94343, time=1.12001
Epoch:0017, train_loss=1.46092, train_acc=0.96759, val_loss=2.01297, val_acc=0.94891, time=1.07301
Epoch:0018, train_loss=1.45650, train_acc=0.97205, val_loss=2.01265, val_acc=0.95620, time=1.39800
Epoch:0019, train_loss=1.45339, train_acc=0.97428, val_loss=2.01242, val_acc=0.95803, time=1.22702
Epoch:0020, train_loss=1.45115, train_acc=0.97610, val_loss=2.01225, val_acc=0.95255, time=1.25300
Epoch:0021, train_loss=1.44933, train_acc=0.97671, val_loss=2.01210, val_acc=0.95620, time=1.14500
Epoch:0022, train_loss=1.44761, train_acc=0.97752, val_loss=2.01195, val_acc=0.95985, time=1.22002
Epoch:0023, train_loss=1.44579, train_acc=0.97914, val_loss=2.01179, val_acc=0.95803, time=1.22500
Epoch:0024, train_loss=1.44382, train_acc=0.97893, val_loss=2.01162, val_acc=0.95985, time=1.20400
Epoch:0025, train_loss=1.44175, train_acc=0.98055, val_loss=2.01144, val_acc=0.96168, time=1.17401
Epoch:0026, train_loss=1.43968, train_acc=0.98197, val_loss=2.01128, val_acc=0.96168, time=1.09201
Epoch:0027, train_loss=1.43772, train_acc=0.98319, val_loss=2.01113, val_acc=0.95985, time=1.27301
Epoch:0028, train_loss=1.43593, train_acc=0.98501, val_loss=2.01099, val_acc=0.95803, time=1.25302
Epoch:0029, train_loss=1.43435, train_acc=0.98542, val_loss=2.01087, val_acc=0.95620, time=1.28800
Epoch:0030, train_loss=1.43297, train_acc=0.98562, val_loss=2.01077, val_acc=0.95803, time=1.11800
Epoch:0031, train_loss=1.43177, train_acc=0.98602, val_loss=2.01067, val_acc=0.95803, time=1.25200
Epoch:0032, train_loss=1.43071, train_acc=0.98663, val_loss=2.01058, val_acc=0.95620, time=1.24201
Epoch:0033, train_loss=1.42976, train_acc=0.98704, val_loss=2.01049, val_acc=0.95620, time=1.16202
Epoch:0034, train_loss=1.42889, train_acc=0.98764, val_loss=2.01041, val_acc=0.95985, time=1.20400
Epoch:0035, train_loss=1.42810, train_acc=0.98785, val_loss=2.01033, val_acc=0.95985, time=1.10300
Epoch:0036, train_loss=1.42735, train_acc=0.98926, val_loss=2.01026, val_acc=0.95985, time=1.11998
Epoch:0037, train_loss=1.42666, train_acc=0.98926, val_loss=2.01020, val_acc=0.95985, time=1.29002
Epoch:0038, train_loss=1.42601, train_acc=0.98967, val_loss=2.01015, val_acc=0.96168, time=1.21101
Epoch:0039, train_loss=1.42540, train_acc=0.99068, val_loss=2.01011, val_acc=0.96715, time=1.27901
Epoch:0040, train_loss=1.42483, train_acc=0.99170, val_loss=2.01007, val_acc=0.96898, time=1.26501
Epoch:0041, train_loss=1.42429, train_acc=0.99271, val_loss=2.01004, val_acc=0.97080, time=1.03000
Epoch:0042, train_loss=1.42378, train_acc=0.99372, val_loss=2.01002, val_acc=0.97263, time=1.39401
Epoch:0043, train_loss=1.42330, train_acc=0.99433, val_loss=2.01001, val_acc=0.97263, time=1.35801
Epoch:0044, train_loss=1.42284, train_acc=0.99473, val_loss=2.01001, val_acc=0.97263, time=1.10901
Epoch:0045, train_loss=1.42241, train_acc=0.99473, val_loss=2.01000, val_acc=0.97080, time=1.29701
Epoch:0046, train_loss=1.42200, train_acc=0.99514, val_loss=2.01001, val_acc=0.96898, time=1.20401
Epoch:0047, train_loss=1.42162, train_acc=0.99534, val_loss=2.01001, val_acc=0.96715, time=1.22301
Epoch:0048, train_loss=1.42127, train_acc=0.99514, val_loss=2.01002, val_acc=0.96715, time=1.13401
Epoch:0049, train_loss=1.42095, train_acc=0.99534, val_loss=2.01003, val_acc=0.96533, time=1.24300
Early stopping...

Optimization Finished!

Test set results: loss= 1.80428, accuracy= 0.95112, time= 0.42601

Test Precision, Recall and F1-Score...
              precision    recall  f1-score   support

           0     0.9736    0.9526    0.9630       696
           1     0.9055    0.9504    0.9274       121
           2     0.9700    0.9861    0.9780      1083
           3     0.9259    0.6944    0.7937        36
           4     0.8630    0.7778    0.8182        81
           5     0.8000    0.8000    0.8000        10
           6     0.8315    0.8506    0.8409        87
           7     0.8148    0.8800    0.8462        75

    accuracy                         0.9511      2189
   macro avg     0.8855    0.8615    0.8709      2189
weighted avg     0.9513    0.9511    0.9507      2189


Macro average Test Precision, Recall and F1-Score...
(0.8855403068058165, 0.8614932436738103, 0.8709124805899, None)

Micro average Test Precision, Recall and F1-Score...
(0.9511192325262677, 0.9511192325262677, 0.9511192325262677, None)

Embeddings:
Word_embeddings:7688
Train_doc_embeddings:5485
Test_doc_embeddings:2189
