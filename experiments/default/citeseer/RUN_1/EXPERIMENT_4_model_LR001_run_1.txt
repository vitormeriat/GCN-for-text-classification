
==========: 304765064487500
Epoch:0001, train_loss=1.80174, train_acc=0.17193, val_loss=1.79190, val_acc=0.18615, time=0.32202
Epoch:0002, train_loss=1.78317, train_acc=0.19109, val_loss=1.79227, val_acc=0.17316, time=0.29700
Epoch:0003, train_loss=1.77631, train_acc=0.22366, val_loss=1.79294, val_acc=0.17316, time=0.31901
Epoch:0004, train_loss=1.77420, train_acc=0.25000, val_loss=1.79342, val_acc=0.18182, time=0.33000
Epoch:0005, train_loss=1.77235, train_acc=0.26820, val_loss=1.79362, val_acc=0.19481, time=0.33400
Epoch:0006, train_loss=1.76941, train_acc=0.28017, val_loss=1.79359, val_acc=0.19913, time=0.33301
Epoch:0007, train_loss=1.76551, train_acc=0.28688, val_loss=1.79344, val_acc=0.19913, time=0.30500
Epoch:0008, train_loss=1.76108, train_acc=0.30508, val_loss=1.79323, val_acc=0.17749, time=0.30101
Epoch:0009, train_loss=1.75653, train_acc=0.34148, val_loss=1.79300, val_acc=0.16883, time=0.27700
Epoch:0010, train_loss=1.75210, train_acc=0.38075, val_loss=1.79279, val_acc=0.17316, time=0.27100
Epoch:0011, train_loss=1.74789, train_acc=0.41284, val_loss=1.79259, val_acc=0.16883, time=0.31402
Epoch:0012, train_loss=1.74387, train_acc=0.42385, val_loss=1.79242, val_acc=0.17316, time=0.32901
Epoch:0013, train_loss=1.74003, train_acc=0.43199, val_loss=1.79229, val_acc=0.18615, time=0.30001
Epoch:0014, train_loss=1.73634, train_acc=0.43726, val_loss=1.79220, val_acc=0.18615, time=0.29000
Epoch:0015, train_loss=1.73277, train_acc=0.44301, val_loss=1.79216, val_acc=0.17749, time=0.30299
Epoch:0016, train_loss=1.72920, train_acc=0.45115, val_loss=1.79216, val_acc=0.17316, time=0.22902
Epoch:0017, train_loss=1.72551, train_acc=0.45594, val_loss=1.79220, val_acc=0.17316, time=0.31200
Epoch:0018, train_loss=1.72159, train_acc=0.46264, val_loss=1.79228, val_acc=0.18182, time=0.30400
Epoch:0019, train_loss=1.71743, train_acc=0.47653, val_loss=1.79240, val_acc=0.17749, time=0.30700
Epoch:0020, train_loss=1.71311, train_acc=0.49138, val_loss=1.79258, val_acc=0.17316, time=0.31900
Early stopping...

Optimization Finished!

Test set results: loss= 1.78792, accuracy= 0.20141, time= 0.06200

Test Precision, Recall and F1-Score...
              precision    recall  f1-score   support

           0     0.1818    0.0423    0.0687       189
           1     0.1429    0.0533    0.0777       150
           2     0.1869    0.2941    0.2286       204
           3     0.2123    0.3654    0.2686       208
           4     0.2243    0.2775    0.2481       173
           5     0.0000    0.0000    0.0000        69

    accuracy                         0.2014       993
   macro avg     0.1580    0.1721    0.1486       993
weighted avg     0.1781    0.2014    0.1712       993


Macro average Test Precision, Recall and F1-Score...
(0.1580301301232748, 0.17210338091727642, 0.14858735193900638, None)

Micro average Test Precision, Recall and F1-Score...
(0.2014098690835851, 0.2014098690835851, 0.2014098690835851, None)

Embeddings:
Word_embeddings:3515
Train_doc_embeddings:2319
Test_doc_embeddings:993
