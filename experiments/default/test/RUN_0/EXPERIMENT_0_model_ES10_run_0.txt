
==========: 292166015840100
Epoch:0001, train_loss=0.68261, train_acc=0.85517, val_loss=0.69017, val_acc=1.00000, time=0.12700
Epoch:0002, train_loss=0.66605, train_acc=1.00000, val_loss=0.68994, val_acc=1.00000, time=0.09101
Epoch:0003, train_loss=0.66375, train_acc=1.00000, val_loss=0.68987, val_acc=1.00000, time=0.12999
Epoch:0004, train_loss=0.66321, train_acc=1.00000, val_loss=0.68984, val_acc=1.00000, time=0.11500
Epoch:0005, train_loss=0.66303, train_acc=1.00000, val_loss=0.68983, val_acc=1.00000, time=0.12401
Epoch:0006, train_loss=0.66297, train_acc=1.00000, val_loss=0.68982, val_acc=1.00000, time=0.12201
Epoch:0007, train_loss=0.66293, train_acc=1.00000, val_loss=0.68982, val_acc=1.00000, time=0.13801
Epoch:0008, train_loss=0.66292, train_acc=1.00000, val_loss=0.68981, val_acc=1.00000, time=0.12400
Epoch:0009, train_loss=0.66291, train_acc=1.00000, val_loss=0.68981, val_acc=1.00000, time=0.13099
Epoch:0010, train_loss=0.66290, train_acc=1.00000, val_loss=0.68981, val_acc=1.00000, time=0.11601
Epoch:0011, train_loss=0.66290, train_acc=1.00000, val_loss=0.68981, val_acc=1.00000, time=0.12799
Epoch:0012, train_loss=0.66290, train_acc=1.00000, val_loss=0.68981, val_acc=1.00000, time=0.15501
Epoch:0013, train_loss=0.66290, train_acc=1.00000, val_loss=0.68981, val_acc=1.00000, time=0.12400
Epoch:0014, train_loss=0.66290, train_acc=1.00000, val_loss=0.68981, val_acc=1.00000, time=0.11901
Epoch:0015, train_loss=0.66290, train_acc=1.00000, val_loss=0.68981, val_acc=1.00000, time=0.13100
Epoch:0016, train_loss=0.66289, train_acc=1.00000, val_loss=0.68981, val_acc=1.00000, time=0.12000
Epoch:0017, train_loss=0.66289, train_acc=1.00000, val_loss=0.68981, val_acc=1.00000, time=0.12401
Epoch:0018, train_loss=0.66289, train_acc=1.00000, val_loss=0.68981, val_acc=1.00000, time=0.12201
Epoch:0019, train_loss=0.66289, train_acc=1.00000, val_loss=0.68981, val_acc=1.00000, time=0.12399
Epoch:0020, train_loss=0.66289, train_acc=1.00000, val_loss=0.68981, val_acc=1.00000, time=0.12301
Epoch:0021, train_loss=0.66289, train_acc=1.00000, val_loss=0.68981, val_acc=1.00000, time=0.12101
Epoch:0022, train_loss=0.66289, train_acc=1.00000, val_loss=0.68981, val_acc=1.00000, time=0.11299
Epoch:0023, train_loss=0.66289, train_acc=1.00000, val_loss=0.68981, val_acc=1.00000, time=0.12800
Epoch:0024, train_loss=0.66289, train_acc=1.00000, val_loss=0.68981, val_acc=1.00000, time=0.12000
Epoch:0025, train_loss=0.66289, train_acc=1.00000, val_loss=0.68981, val_acc=1.00000, time=0.12501
Epoch:0026, train_loss=0.66289, train_acc=1.00000, val_loss=0.68981, val_acc=1.00000, time=0.12101
Epoch:0027, train_loss=0.66289, train_acc=1.00000, val_loss=0.68981, val_acc=1.00000, time=0.12600
Epoch:0028, train_loss=0.66289, train_acc=1.00000, val_loss=0.68981, val_acc=1.00000, time=0.12800
Epoch:0029, train_loss=0.66289, train_acc=1.00000, val_loss=0.68981, val_acc=1.00000, time=0.12200
Epoch:0030, train_loss=0.66289, train_acc=1.00000, val_loss=0.68981, val_acc=1.00000, time=0.13701
Epoch:0031, train_loss=0.66289, train_acc=1.00000, val_loss=0.68981, val_acc=1.00000, time=0.13299
Epoch:0032, train_loss=0.66289, train_acc=1.00000, val_loss=0.68981, val_acc=1.00000, time=0.12300
Epoch:0033, train_loss=0.66289, train_acc=1.00000, val_loss=0.68981, val_acc=1.00000, time=0.12801
Epoch:0034, train_loss=0.66289, train_acc=1.00000, val_loss=0.68981, val_acc=1.00000, time=0.12299
Epoch:0035, train_loss=0.66289, train_acc=1.00000, val_loss=0.68981, val_acc=1.00000, time=0.12301
Epoch:0036, train_loss=0.66289, train_acc=1.00000, val_loss=0.68981, val_acc=1.00000, time=0.12500
Epoch:0037, train_loss=0.66289, train_acc=1.00000, val_loss=0.68981, val_acc=1.00000, time=0.13500
Epoch:0038, train_loss=0.66289, train_acc=1.00000, val_loss=0.68981, val_acc=1.00000, time=0.13201
Epoch:0039, train_loss=0.66289, train_acc=1.00000, val_loss=0.68981, val_acc=1.00000, time=0.11600
Epoch:0040, train_loss=0.66289, train_acc=1.00000, val_loss=0.68981, val_acc=1.00000, time=0.12500
Epoch:0041, train_loss=0.66289, train_acc=1.00000, val_loss=0.68981, val_acc=1.00000, time=0.12001
Epoch:0042, train_loss=0.66289, train_acc=1.00000, val_loss=0.68981, val_acc=1.00000, time=0.12200
Epoch:0043, train_loss=0.66289, train_acc=1.00000, val_loss=0.68981, val_acc=1.00000, time=0.12502
Epoch:0044, train_loss=0.66289, train_acc=1.00000, val_loss=0.68981, val_acc=1.00000, time=0.13000
Epoch:0045, train_loss=0.66289, train_acc=1.00000, val_loss=0.68981, val_acc=1.00000, time=0.12399
Epoch:0046, train_loss=0.66289, train_acc=1.00000, val_loss=0.68981, val_acc=1.00000, time=0.12700
Epoch:0047, train_loss=0.66289, train_acc=1.00000, val_loss=0.68981, val_acc=1.00000, time=0.12701
Early stopping...

Optimization Finished!

Test set results: loss= 0.89861, accuracy= 0.00000, time= 0.04099

Test Precision, Recall and F1-Score...
              precision    recall  f1-score   support

           0     0.0000    0.0000    0.0000       0.0
           1     0.0000    0.0000    0.0000      39.0

    accuracy                         0.0000      39.0
   macro avg     0.0000    0.0000    0.0000      39.0
weighted avg     0.0000    0.0000    0.0000      39.0


Macro average Test Precision, Recall and F1-Score...
(0.0, 0.0, 0.0, None)

Micro average Test Precision, Recall and F1-Score...
(0.0, 0.0, 0.0, None)

Embeddings:
Word_embeddings:3122
Train_doc_embeddings:161
Test_doc_embeddings:39
