
==========: 78505139791400
Epoch:0001, train_loss=2.08145, train_acc=0.05570, val_loss=2.04689, val_acc=0.71898, time=1.31001
Epoch:0002, train_loss=1.80287, train_acc=0.71622, val_loss=2.03833, val_acc=0.76825, time=1.27699
Epoch:0003, train_loss=1.75073, train_acc=0.73263, val_loss=2.02332, val_acc=0.83577, time=1.13600
Epoch:0004, train_loss=1.59277, train_acc=0.81649, val_loss=2.02254, val_acc=0.85766, time=1.24401
Epoch:0005, train_loss=1.57096, train_acc=0.85882, val_loss=2.02207, val_acc=0.87409, time=1.26601
Epoch:0006, train_loss=1.55965, train_acc=0.89569, val_loss=2.01931, val_acc=0.90876, time=1.50000
Epoch:0007, train_loss=1.53420, train_acc=0.92627, val_loss=2.01672, val_acc=0.92883, time=0.99801
Epoch:0008, train_loss=1.51276, train_acc=0.94187, val_loss=2.01497, val_acc=0.93978, time=1.11000
Epoch:0009, train_loss=1.49922, train_acc=0.94045, val_loss=2.01385, val_acc=0.94161, time=0.97002
Epoch:0010, train_loss=1.49055, train_acc=0.94004, val_loss=2.01313, val_acc=0.94526, time=1.12900
Epoch:0011, train_loss=1.48415, train_acc=0.93863, val_loss=2.01257, val_acc=0.94708, time=1.05500
Epoch:0012, train_loss=1.47752, train_acc=0.94085, val_loss=2.01205, val_acc=0.94708, time=1.16802
Epoch:0013, train_loss=1.46964, train_acc=0.94774, val_loss=2.01165, val_acc=0.93978, time=1.13300
Epoch:0014, train_loss=1.46209, train_acc=0.95402, val_loss=2.01145, val_acc=0.94343, time=1.21800
Epoch:0015, train_loss=1.45646, train_acc=0.95584, val_loss=2.01138, val_acc=0.94161, time=1.00102
Epoch:0016, train_loss=1.45269, train_acc=0.95767, val_loss=2.01127, val_acc=0.93796, time=0.99001
Epoch:0017, train_loss=1.44954, train_acc=0.96131, val_loss=2.01101, val_acc=0.93978, time=1.06100
Epoch:0018, train_loss=1.44605, train_acc=0.96354, val_loss=2.01061, val_acc=0.94526, time=0.98201
Epoch:0019, train_loss=1.44221, train_acc=0.96901, val_loss=2.01019, val_acc=0.94891, time=1.09501
Epoch:0020, train_loss=1.43855, train_acc=0.97266, val_loss=2.00982, val_acc=0.95073, time=1.18500
Epoch:0021, train_loss=1.43545, train_acc=0.97752, val_loss=2.00955, val_acc=0.95255, time=1.08201
Epoch:0022, train_loss=1.43304, train_acc=0.97995, val_loss=2.00941, val_acc=0.95255, time=0.99501
Epoch:0023, train_loss=1.43129, train_acc=0.98137, val_loss=2.00937, val_acc=0.95803, time=1.08700
Epoch:0024, train_loss=1.43006, train_acc=0.98319, val_loss=2.00940, val_acc=0.95620, time=1.22801
Epoch:0025, train_loss=1.42911, train_acc=0.98461, val_loss=2.00946, val_acc=0.95803, time=1.11599
Epoch:0026, train_loss=1.42822, train_acc=0.98542, val_loss=2.00952, val_acc=0.95985, time=0.94602
Early stopping...

Optimization Finished!

Test set results: loss= 1.79977, accuracy= 0.97213, time= 0.33601

Test Precision, Recall and F1-Score...
              precision    recall  f1-score   support

           0     0.8791    0.9195    0.8989        87
           1     0.9907    0.9889    0.9898      1083
           2     0.9812    0.9770    0.9791       696
           3     0.8333    1.0000    0.9091        10
           4     0.8902    0.9733    0.9299        75
           5     0.9440    0.9752    0.9593       121
           6     0.9615    0.6944    0.8065        36
           7     0.8987    0.8765    0.8875        81

    accuracy                         0.9721      2189
   macro avg     0.9224    0.9256    0.9200      2189
weighted avg     0.9727    0.9721    0.9719      2189


Macro average Test Precision, Recall and F1-Score...
(0.9223701301357292, 0.925624873869066, 0.9200200171735151, None)

Micro average Test Precision, Recall and F1-Score...
(0.972133394243947, 0.972133394243947, 0.972133394243947, None)

Embeddings:
Word_embeddings:7688
Train_doc_embeddings:5485
Test_doc_embeddings:2189
