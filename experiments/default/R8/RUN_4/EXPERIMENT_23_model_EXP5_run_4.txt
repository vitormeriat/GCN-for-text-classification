
==========: 78703772703500
Epoch:0001, train_loss=2.08546, train_acc=0.07029, val_loss=2.06057, val_acc=0.72080, time=1.32200
Epoch:0002, train_loss=1.91794, train_acc=0.68807, val_loss=2.04798, val_acc=0.77007, time=1.52902
Epoch:0003, train_loss=1.80972, train_acc=0.74296, val_loss=2.04017, val_acc=0.78650, time=1.11401
Epoch:0004, train_loss=1.74147, train_acc=0.76605, val_loss=2.03510, val_acc=0.79380, time=1.05000
Epoch:0005, train_loss=1.69592, train_acc=0.77861, val_loss=2.03132, val_acc=0.80474, time=1.11800
Epoch:0006, train_loss=1.66111, train_acc=0.79036, val_loss=2.02815, val_acc=0.81934, time=1.17401
Epoch:0007, train_loss=1.63156, train_acc=0.80717, val_loss=2.02541, val_acc=0.84489, time=1.14800
Epoch:0008, train_loss=1.60617, train_acc=0.82945, val_loss=2.02314, val_acc=0.86861, time=1.06402
Epoch:0009, train_loss=1.58506, train_acc=0.85153, val_loss=2.02127, val_acc=0.88321, time=1.01699
Epoch:0010, train_loss=1.56748, train_acc=0.86551, val_loss=2.01968, val_acc=0.89599, time=1.30301
Epoch:0011, train_loss=1.55233, train_acc=0.88211, val_loss=2.01834, val_acc=0.90693, time=1.11701
Epoch:0012, train_loss=1.53917, train_acc=0.89650, val_loss=2.01726, val_acc=0.91788, time=1.14899
Epoch:0013, train_loss=1.52806, train_acc=0.90885, val_loss=2.01640, val_acc=0.92518, time=0.99401
Epoch:0014, train_loss=1.51890, train_acc=0.92465, val_loss=2.01570, val_acc=0.92883, time=1.08000
Epoch:0015, train_loss=1.51117, train_acc=0.92992, val_loss=2.01509, val_acc=0.92883, time=1.07002
Epoch:0016, train_loss=1.50419, train_acc=0.93640, val_loss=2.01453, val_acc=0.93613, time=1.11699
Epoch:0017, train_loss=1.49760, train_acc=0.94166, val_loss=2.01399, val_acc=0.93796, time=1.02301
Epoch:0018, train_loss=1.49144, train_acc=0.94875, val_loss=2.01351, val_acc=0.93978, time=1.05301
Epoch:0019, train_loss=1.48591, train_acc=0.95503, val_loss=2.01310, val_acc=0.94526, time=1.09301
Epoch:0020, train_loss=1.48107, train_acc=0.95625, val_loss=2.01274, val_acc=0.94526, time=0.97301
Epoch:0021, train_loss=1.47678, train_acc=0.95767, val_loss=2.01241, val_acc=0.94526, time=1.13101
Epoch:0022, train_loss=1.47284, train_acc=0.95949, val_loss=2.01212, val_acc=0.94891, time=1.20400
Epoch:0023, train_loss=1.46916, train_acc=0.96152, val_loss=2.01184, val_acc=0.95255, time=1.04000
Epoch:0024, train_loss=1.46567, train_acc=0.96374, val_loss=2.01157, val_acc=0.95073, time=1.23901
Epoch:0025, train_loss=1.46233, train_acc=0.96516, val_loss=2.01132, val_acc=0.94891, time=1.24499
Epoch:0026, train_loss=1.45913, train_acc=0.96557, val_loss=2.01108, val_acc=0.95255, time=1.18702
Epoch:0027, train_loss=1.45611, train_acc=0.96719, val_loss=2.01086, val_acc=0.95438, time=1.12001
Epoch:0028, train_loss=1.45334, train_acc=0.96881, val_loss=2.01066, val_acc=0.95438, time=1.05500
Epoch:0029, train_loss=1.45086, train_acc=0.96982, val_loss=2.01049, val_acc=0.95438, time=1.09500
Epoch:0030, train_loss=1.44866, train_acc=0.97083, val_loss=2.01034, val_acc=0.95620, time=1.05701
Epoch:0031, train_loss=1.44669, train_acc=0.97347, val_loss=2.01021, val_acc=0.95985, time=1.16001
Epoch:0032, train_loss=1.44492, train_acc=0.97549, val_loss=2.01010, val_acc=0.95803, time=1.12500
Epoch:0033, train_loss=1.44331, train_acc=0.97711, val_loss=2.01000, val_acc=0.95620, time=1.07701
Epoch:0034, train_loss=1.44188, train_acc=0.98015, val_loss=2.00991, val_acc=0.95803, time=1.03001
Epoch:0035, train_loss=1.44061, train_acc=0.98157, val_loss=2.00984, val_acc=0.95803, time=1.25001
Epoch:0036, train_loss=1.43950, train_acc=0.98177, val_loss=2.00977, val_acc=0.95803, time=1.05200
Epoch:0037, train_loss=1.43850, train_acc=0.98278, val_loss=2.00971, val_acc=0.95803, time=1.13401
Epoch:0038, train_loss=1.43755, train_acc=0.98359, val_loss=2.00966, val_acc=0.95620, time=1.15601
Epoch:0039, train_loss=1.43660, train_acc=0.98359, val_loss=2.00960, val_acc=0.95620, time=1.01300
Epoch:0040, train_loss=1.43564, train_acc=0.98400, val_loss=2.00955, val_acc=0.95620, time=1.14402
Epoch:0041, train_loss=1.43469, train_acc=0.98481, val_loss=2.00951, val_acc=0.96168, time=1.14700
Epoch:0042, train_loss=1.43378, train_acc=0.98461, val_loss=2.00947, val_acc=0.95985, time=1.24101
Epoch:0043, train_loss=1.43294, train_acc=0.98440, val_loss=2.00944, val_acc=0.95620, time=1.35102
Epoch:0044, train_loss=1.43217, train_acc=0.98542, val_loss=2.00941, val_acc=0.95620, time=1.13100
Epoch:0045, train_loss=1.43145, train_acc=0.98643, val_loss=2.00938, val_acc=0.95803, time=1.18101
Epoch:0046, train_loss=1.43075, train_acc=0.98663, val_loss=2.00935, val_acc=0.95803, time=1.07501
Epoch:0047, train_loss=1.43007, train_acc=0.98805, val_loss=2.00931, val_acc=0.95985, time=1.04401
Epoch:0048, train_loss=1.42942, train_acc=0.98866, val_loss=2.00928, val_acc=0.95985, time=1.04200
Epoch:0049, train_loss=1.42882, train_acc=0.98906, val_loss=2.00925, val_acc=0.96533, time=1.06702
Epoch:0050, train_loss=1.42827, train_acc=0.98947, val_loss=2.00922, val_acc=0.96533, time=1.00100
Epoch:0051, train_loss=1.42778, train_acc=0.98967, val_loss=2.00920, val_acc=0.96350, time=1.32302
Epoch:0052, train_loss=1.42732, train_acc=0.99048, val_loss=2.00918, val_acc=0.96168, time=1.22099
Epoch:0053, train_loss=1.42687, train_acc=0.99089, val_loss=2.00916, val_acc=0.96168, time=1.03502
Epoch:0054, train_loss=1.42643, train_acc=0.99109, val_loss=2.00915, val_acc=0.96168, time=0.98200
Epoch:0055, train_loss=1.42599, train_acc=0.99109, val_loss=2.00915, val_acc=0.96168, time=0.97901
Epoch:0056, train_loss=1.42556, train_acc=0.99149, val_loss=2.00914, val_acc=0.96168, time=1.04601
Epoch:0057, train_loss=1.42516, train_acc=0.99129, val_loss=2.00915, val_acc=0.96168, time=0.99100
Epoch:0058, train_loss=1.42479, train_acc=0.99149, val_loss=2.00915, val_acc=0.95985, time=1.07001
Epoch:0059, train_loss=1.42443, train_acc=0.99190, val_loss=2.00915, val_acc=0.95803, time=1.20101
Epoch:0060, train_loss=1.42408, train_acc=0.99210, val_loss=2.00915, val_acc=0.95803, time=1.18200
Epoch:0061, train_loss=1.42374, train_acc=0.99230, val_loss=2.00915, val_acc=0.95985, time=1.04701
Epoch:0062, train_loss=1.42341, train_acc=0.99291, val_loss=2.00914, val_acc=0.96168, time=1.23101
Epoch:0063, train_loss=1.42309, train_acc=0.99332, val_loss=2.00913, val_acc=0.96168, time=1.01101
Epoch:0064, train_loss=1.42279, train_acc=0.99372, val_loss=2.00913, val_acc=0.96168, time=1.07601
Epoch:0065, train_loss=1.42250, train_acc=0.99392, val_loss=2.00912, val_acc=0.96168, time=1.03602
Epoch:0066, train_loss=1.42222, train_acc=0.99413, val_loss=2.00912, val_acc=0.96168, time=1.07201
Epoch:0067, train_loss=1.42195, train_acc=0.99473, val_loss=2.00912, val_acc=0.96350, time=1.10300
Epoch:0068, train_loss=1.42168, train_acc=0.99494, val_loss=2.00912, val_acc=0.96168, time=1.05601
Epoch:0069, train_loss=1.42143, train_acc=0.99494, val_loss=2.00912, val_acc=0.96168, time=1.15800
Epoch:0070, train_loss=1.42118, train_acc=0.99534, val_loss=2.00912, val_acc=0.96168, time=1.18901
Epoch:0071, train_loss=1.42095, train_acc=0.99534, val_loss=2.00913, val_acc=0.95985, time=1.12001
Early stopping...

Optimization Finished!

Test set results: loss= 1.79875, accuracy= 0.97168, time= 0.35200

Test Precision, Recall and F1-Score...
              precision    recall  f1-score   support

           0     0.8710    0.9310    0.9000        87
           1     0.9808    0.9926    0.9867      1083
           2     0.9825    0.9698    0.9761       696
           3     1.0000    1.0000    1.0000        10
           4     0.9125    0.9733    0.9419        75
           5     0.9593    0.9752    0.9672       121
           6     1.0000    0.7222    0.8387        36
           7     0.9324    0.8519    0.8903        81

    accuracy                         0.9717      2189
   macro avg     0.9548    0.9270    0.9376      2189
weighted avg     0.9721    0.9717    0.9713      2189


Macro average Test Precision, Recall and F1-Score...
(0.9548277418767436, 0.9270111499587321, 0.9376263535061261, None)

Micro average Test Precision, Recall and F1-Score...
(0.9716765646413887, 0.9716765646413887, 0.9716765646413887, None)

Embeddings:
Word_embeddings:7688
Train_doc_embeddings:5485
Test_doc_embeddings:2189
