
==========: 77667981028300
Epoch:0001, train_loss=2.09716, train_acc=0.06704, val_loss=2.06151, val_acc=0.72263, time=1.34503
Epoch:0002, train_loss=1.92783, train_acc=0.70589, val_loss=2.04872, val_acc=0.79015, time=1.31001
Epoch:0003, train_loss=1.81755, train_acc=0.75815, val_loss=2.04068, val_acc=0.80292, time=1.31101
Epoch:0004, train_loss=1.74706, train_acc=0.77638, val_loss=2.03559, val_acc=0.81022, time=1.05401
Epoch:0005, train_loss=1.70142, train_acc=0.78509, val_loss=2.03193, val_acc=0.81569, time=1.03101
Epoch:0006, train_loss=1.66817, train_acc=0.79157, val_loss=2.02882, val_acc=0.82482, time=1.15300
Epoch:0007, train_loss=1.63999, train_acc=0.80393, val_loss=2.02604, val_acc=0.82847, time=1.04202
Epoch:0008, train_loss=1.61472, train_acc=0.82216, val_loss=2.02362, val_acc=0.85766, time=1.10501
Epoch:0009, train_loss=1.59251, train_acc=0.84059, val_loss=2.02161, val_acc=0.88321, time=1.12300
Epoch:0010, train_loss=1.57362, train_acc=0.85862, val_loss=2.01995, val_acc=0.88869, time=1.00801
Epoch:0011, train_loss=1.55758, train_acc=0.87057, val_loss=2.01857, val_acc=0.89599, time=1.12100
Epoch:0012, train_loss=1.54358, train_acc=0.88758, val_loss=2.01739, val_acc=0.90876, time=1.16901
Epoch:0013, train_loss=1.53116, train_acc=0.89974, val_loss=2.01642, val_acc=0.91788, time=1.19201
Epoch:0014, train_loss=1.52032, train_acc=0.91493, val_loss=2.01564, val_acc=0.93796, time=1.09902
Epoch:0015, train_loss=1.51130, train_acc=0.92951, val_loss=2.01505, val_acc=0.93796, time=1.16000
Epoch:0016, train_loss=1.50411, train_acc=0.93701, val_loss=2.01460, val_acc=0.93431, time=1.22499
Epoch:0017, train_loss=1.49827, train_acc=0.94045, val_loss=2.01419, val_acc=0.93978, time=1.03801
Epoch:0018, train_loss=1.49301, train_acc=0.94308, val_loss=2.01379, val_acc=0.94161, time=1.19801
Epoch:0019, train_loss=1.48784, train_acc=0.94632, val_loss=2.01339, val_acc=0.94343, time=1.15801
Epoch:0020, train_loss=1.48278, train_acc=0.95058, val_loss=2.01300, val_acc=0.94343, time=1.24500
Epoch:0021, train_loss=1.47806, train_acc=0.95665, val_loss=2.01264, val_acc=0.94161, time=1.16401
Epoch:0022, train_loss=1.47376, train_acc=0.96010, val_loss=2.01229, val_acc=0.94891, time=1.00700
Epoch:0023, train_loss=1.46974, train_acc=0.96152, val_loss=2.01196, val_acc=0.94708, time=1.06001
Epoch:0024, train_loss=1.46586, train_acc=0.96374, val_loss=2.01164, val_acc=0.94708, time=1.15202
Epoch:0025, train_loss=1.46212, train_acc=0.96617, val_loss=2.01135, val_acc=0.94891, time=1.13301
Epoch:0026, train_loss=1.45860, train_acc=0.96759, val_loss=2.01109, val_acc=0.95255, time=1.16000
Epoch:0027, train_loss=1.45544, train_acc=0.96840, val_loss=2.01087, val_acc=0.95438, time=1.06803
Epoch:0028, train_loss=1.45270, train_acc=0.97185, val_loss=2.01070, val_acc=0.95620, time=0.99401
Epoch:0029, train_loss=1.45041, train_acc=0.97225, val_loss=2.01056, val_acc=0.95255, time=1.12100
Epoch:0030, train_loss=1.44850, train_acc=0.97428, val_loss=2.01045, val_acc=0.95255, time=1.10701
Epoch:0031, train_loss=1.44688, train_acc=0.97387, val_loss=2.01035, val_acc=0.95438, time=1.14600
Epoch:0032, train_loss=1.44541, train_acc=0.97529, val_loss=2.01025, val_acc=0.95255, time=1.02401
Epoch:0033, train_loss=1.44399, train_acc=0.97731, val_loss=2.01014, val_acc=0.95620, time=1.02001
Epoch:0034, train_loss=1.44256, train_acc=0.97954, val_loss=2.01002, val_acc=0.95803, time=1.20100
Epoch:0035, train_loss=1.44114, train_acc=0.98096, val_loss=2.00990, val_acc=0.95620, time=1.17001
Epoch:0036, train_loss=1.43975, train_acc=0.98177, val_loss=2.00980, val_acc=0.95620, time=1.18199
Epoch:0037, train_loss=1.43845, train_acc=0.98177, val_loss=2.00971, val_acc=0.95438, time=1.04401
Epoch:0038, train_loss=1.43725, train_acc=0.98238, val_loss=2.00964, val_acc=0.95438, time=1.12300
Epoch:0039, train_loss=1.43614, train_acc=0.98299, val_loss=2.00959, val_acc=0.95620, time=1.24602
Epoch:0040, train_loss=1.43511, train_acc=0.98400, val_loss=2.00954, val_acc=0.95438, time=1.26301
Epoch:0041, train_loss=1.43416, train_acc=0.98440, val_loss=2.00951, val_acc=0.95438, time=1.15501
Epoch:0042, train_loss=1.43328, train_acc=0.98481, val_loss=2.00948, val_acc=0.95620, time=1.18999
Epoch:0043, train_loss=1.43250, train_acc=0.98602, val_loss=2.00946, val_acc=0.95438, time=1.16201
Epoch:0044, train_loss=1.43180, train_acc=0.98663, val_loss=2.00943, val_acc=0.95438, time=1.15501
Epoch:0045, train_loss=1.43115, train_acc=0.98663, val_loss=2.00940, val_acc=0.95073, time=1.11600
Epoch:0046, train_loss=1.43053, train_acc=0.98683, val_loss=2.00936, val_acc=0.95255, time=1.13801
Epoch:0047, train_loss=1.42990, train_acc=0.98724, val_loss=2.00931, val_acc=0.95438, time=1.10000
Epoch:0048, train_loss=1.42927, train_acc=0.98724, val_loss=2.00927, val_acc=0.95985, time=1.11300
Epoch:0049, train_loss=1.42867, train_acc=0.98785, val_loss=2.00923, val_acc=0.95985, time=1.12501
Epoch:0050, train_loss=1.42811, train_acc=0.98906, val_loss=2.00920, val_acc=0.96168, time=1.07501
Epoch:0051, train_loss=1.42759, train_acc=0.98947, val_loss=2.00918, val_acc=0.95985, time=1.09500
Epoch:0052, train_loss=1.42710, train_acc=0.98987, val_loss=2.00917, val_acc=0.95985, time=1.06901
Epoch:0053, train_loss=1.42662, train_acc=0.98987, val_loss=2.00916, val_acc=0.95803, time=1.13300
Epoch:0054, train_loss=1.42617, train_acc=0.99109, val_loss=2.00916, val_acc=0.95985, time=1.02001
Epoch:0055, train_loss=1.42572, train_acc=0.99230, val_loss=2.00917, val_acc=0.95803, time=1.12400
Epoch:0056, train_loss=1.42531, train_acc=0.99190, val_loss=2.00918, val_acc=0.95620, time=1.32101
Epoch:0057, train_loss=1.42491, train_acc=0.99251, val_loss=2.00918, val_acc=0.95620, time=1.17702
Epoch:0058, train_loss=1.42453, train_acc=0.99230, val_loss=2.00919, val_acc=0.95620, time=1.03700
Epoch:0059, train_loss=1.42417, train_acc=0.99230, val_loss=2.00918, val_acc=0.95438, time=1.17301
Early stopping...

Optimization Finished!

Test set results: loss= 1.79959, accuracy= 0.96985, time= 0.30901

Test Precision, Recall and F1-Score...
              precision    recall  f1-score   support

           0     0.8526    0.9310    0.8901        87
           1     0.9808    0.9926    0.9867      1083
           2     0.9825    0.9684    0.9754       696
           3     1.0000    1.0000    1.0000        10
           4     0.8916    0.9867    0.9367        75
           5     0.9516    0.9752    0.9633       121
           6     1.0000    0.6944    0.8197        36
           7     0.9429    0.8148    0.8742        81

    accuracy                         0.9698      2189
   macro avg     0.9503    0.9204    0.9308      2189
weighted avg     0.9706    0.9698    0.9694      2189


Macro average Test Precision, Recall and F1-Score...
(0.9502518243473363, 0.9203963670723976, 0.9307521862808318, None)

Micro average Test Precision, Recall and F1-Score...
(0.9698492462311558, 0.9698492462311558, 0.9698492462311558, None)

Embeddings:
Word_embeddings:7688
Train_doc_embeddings:5485
Test_doc_embeddings:2189
