
==========: 78619210572900
Epoch:0001, train_loss=2.08725, train_acc=0.04213, val_loss=2.05944, val_acc=0.67336, time=1.46703
Epoch:0002, train_loss=1.90679, train_acc=0.65991, val_loss=2.04702, val_acc=0.73905, time=1.29899
Epoch:0003, train_loss=1.79930, train_acc=0.72230, val_loss=2.03983, val_acc=0.77920, time=1.11399
Epoch:0004, train_loss=1.73783, train_acc=0.76261, val_loss=2.03523, val_acc=0.79562, time=1.11802
Epoch:0005, train_loss=1.69838, train_acc=0.77699, val_loss=2.03170, val_acc=0.80109, time=1.17701
Epoch:0006, train_loss=1.66709, train_acc=0.78367, val_loss=2.02861, val_acc=0.80839, time=1.26903
Epoch:0007, train_loss=1.63871, train_acc=0.79340, val_loss=2.02584, val_acc=0.82482, time=1.10902
Epoch:0008, train_loss=1.61263, train_acc=0.81304, val_loss=2.02348, val_acc=0.85219, time=1.13700
Epoch:0009, train_loss=1.58987, train_acc=0.84221, val_loss=2.02157, val_acc=0.88504, time=1.31101
Epoch:0010, train_loss=1.57116, train_acc=0.87158, val_loss=2.02005, val_acc=0.89416, time=1.34101
Epoch:0011, train_loss=1.55606, train_acc=0.88839, val_loss=2.01881, val_acc=0.90146, time=1.34502
Epoch:0012, train_loss=1.54346, train_acc=0.89852, val_loss=2.01775, val_acc=0.91241, time=1.13601
Epoch:0013, train_loss=1.53230, train_acc=0.90946, val_loss=2.01681, val_acc=0.92153, time=1.07201
Epoch:0014, train_loss=1.52214, train_acc=0.92323, val_loss=2.01600, val_acc=0.92518, time=1.00201
Epoch:0015, train_loss=1.51310, train_acc=0.93296, val_loss=2.01533, val_acc=0.93431, time=1.10201
Epoch:0016, train_loss=1.50531, train_acc=0.93802, val_loss=2.01478, val_acc=0.93066, time=1.14401
Epoch:0017, train_loss=1.49866, train_acc=0.94187, val_loss=2.01431, val_acc=0.93431, time=1.09401
Epoch:0018, train_loss=1.49280, train_acc=0.94450, val_loss=2.01390, val_acc=0.93796, time=1.15301
Epoch:0019, train_loss=1.48742, train_acc=0.94794, val_loss=2.01351, val_acc=0.94161, time=1.02200
Epoch:0020, train_loss=1.48243, train_acc=0.95179, val_loss=2.01315, val_acc=0.93796, time=1.09102
Epoch:0021, train_loss=1.47786, train_acc=0.95463, val_loss=2.01281, val_acc=0.93796, time=1.11902
Epoch:0022, train_loss=1.47369, train_acc=0.95787, val_loss=2.01248, val_acc=0.94343, time=1.28901
Epoch:0023, train_loss=1.46982, train_acc=0.96010, val_loss=2.01215, val_acc=0.94526, time=1.04300
Epoch:0024, train_loss=1.46616, train_acc=0.96172, val_loss=2.01182, val_acc=0.94708, time=1.13299
Epoch:0025, train_loss=1.46266, train_acc=0.96374, val_loss=2.01151, val_acc=0.94891, time=1.01202
Epoch:0026, train_loss=1.45935, train_acc=0.96476, val_loss=2.01121, val_acc=0.94708, time=1.12500
Epoch:0027, train_loss=1.45628, train_acc=0.96719, val_loss=2.01095, val_acc=0.94708, time=1.05000
Epoch:0028, train_loss=1.45350, train_acc=0.96901, val_loss=2.01073, val_acc=0.95255, time=1.00801
Epoch:0029, train_loss=1.45106, train_acc=0.97104, val_loss=2.01055, val_acc=0.95438, time=1.20901
Epoch:0030, train_loss=1.44897, train_acc=0.97245, val_loss=2.01041, val_acc=0.95438, time=1.19501
Epoch:0031, train_loss=1.44715, train_acc=0.97387, val_loss=2.01029, val_acc=0.95255, time=1.07001
Epoch:0032, train_loss=1.44554, train_acc=0.97428, val_loss=2.01020, val_acc=0.95255, time=1.15401
Epoch:0033, train_loss=1.44404, train_acc=0.97650, val_loss=2.01011, val_acc=0.94891, time=1.11601
Epoch:0034, train_loss=1.44262, train_acc=0.97873, val_loss=2.01002, val_acc=0.94891, time=1.00401
Epoch:0035, train_loss=1.44125, train_acc=0.97995, val_loss=2.00994, val_acc=0.94891, time=1.08101
Epoch:0036, train_loss=1.43997, train_acc=0.98096, val_loss=2.00987, val_acc=0.95073, time=1.11001
Epoch:0037, train_loss=1.43879, train_acc=0.98096, val_loss=2.00981, val_acc=0.95073, time=1.28200
Epoch:0038, train_loss=1.43772, train_acc=0.98076, val_loss=2.00975, val_acc=0.94891, time=1.16801
Epoch:0039, train_loss=1.43673, train_acc=0.98177, val_loss=2.00970, val_acc=0.95073, time=1.17401
Epoch:0040, train_loss=1.43578, train_acc=0.98258, val_loss=2.00964, val_acc=0.95073, time=1.04501
Epoch:0041, train_loss=1.43486, train_acc=0.98278, val_loss=2.00959, val_acc=0.95620, time=1.04500
Epoch:0042, train_loss=1.43396, train_acc=0.98339, val_loss=2.00955, val_acc=0.96168, time=1.06400
Epoch:0043, train_loss=1.43311, train_acc=0.98420, val_loss=2.00951, val_acc=0.95803, time=1.06600
Epoch:0044, train_loss=1.43231, train_acc=0.98420, val_loss=2.00948, val_acc=0.95985, time=1.09202
Epoch:0045, train_loss=1.43159, train_acc=0.98542, val_loss=2.00945, val_acc=0.95620, time=1.06900
Epoch:0046, train_loss=1.43091, train_acc=0.98643, val_loss=2.00942, val_acc=0.95803, time=1.04902
Epoch:0047, train_loss=1.43027, train_acc=0.98704, val_loss=2.00939, val_acc=0.95803, time=1.03101
Epoch:0048, train_loss=1.42965, train_acc=0.98704, val_loss=2.00935, val_acc=0.95985, time=0.99400
Epoch:0049, train_loss=1.42907, train_acc=0.98744, val_loss=2.00932, val_acc=0.96168, time=1.14502
Epoch:0050, train_loss=1.42852, train_acc=0.98805, val_loss=2.00929, val_acc=0.96350, time=1.13401
Epoch:0051, train_loss=1.42801, train_acc=0.98886, val_loss=2.00927, val_acc=0.96350, time=1.10500
Epoch:0052, train_loss=1.42754, train_acc=0.98967, val_loss=2.00925, val_acc=0.96350, time=1.10000
Epoch:0053, train_loss=1.42708, train_acc=0.98987, val_loss=2.00923, val_acc=0.96350, time=1.11801
Epoch:0054, train_loss=1.42664, train_acc=0.99089, val_loss=2.00922, val_acc=0.96350, time=1.27900
Epoch:0055, train_loss=1.42620, train_acc=0.99129, val_loss=2.00921, val_acc=0.96350, time=1.15403
Epoch:0056, train_loss=1.42576, train_acc=0.99109, val_loss=2.00920, val_acc=0.96533, time=1.01699
Epoch:0057, train_loss=1.42534, train_acc=0.99149, val_loss=2.00921, val_acc=0.96350, time=1.03201
Epoch:0058, train_loss=1.42495, train_acc=0.99190, val_loss=2.00921, val_acc=0.96350, time=1.06801
Epoch:0059, train_loss=1.42457, train_acc=0.99251, val_loss=2.00921, val_acc=0.96350, time=1.07101
Epoch:0060, train_loss=1.42422, train_acc=0.99271, val_loss=2.00921, val_acc=0.96350, time=1.05799
Epoch:0061, train_loss=1.42387, train_acc=0.99311, val_loss=2.00921, val_acc=0.96350, time=1.21902
Epoch:0062, train_loss=1.42354, train_acc=0.99332, val_loss=2.00921, val_acc=0.96168, time=1.22001
Epoch:0063, train_loss=1.42322, train_acc=0.99352, val_loss=2.00921, val_acc=0.96168, time=1.21400
Epoch:0064, train_loss=1.42291, train_acc=0.99392, val_loss=2.00920, val_acc=0.96168, time=1.38301
Epoch:0065, train_loss=1.42261, train_acc=0.99453, val_loss=2.00920, val_acc=0.96168, time=1.04700
Epoch:0066, train_loss=1.42233, train_acc=0.99453, val_loss=2.00920, val_acc=0.96168, time=1.11301
Epoch:0067, train_loss=1.42206, train_acc=0.99473, val_loss=2.00920, val_acc=0.96168, time=1.23599
Epoch:0068, train_loss=1.42179, train_acc=0.99494, val_loss=2.00920, val_acc=0.96168, time=1.08302
Epoch:0069, train_loss=1.42154, train_acc=0.99494, val_loss=2.00920, val_acc=0.96168, time=1.06801
Epoch:0070, train_loss=1.42128, train_acc=0.99494, val_loss=2.00921, val_acc=0.96168, time=1.24101
Early stopping...

Optimization Finished!

Test set results: loss= 1.79889, accuracy= 0.97122, time= 0.38000

Test Precision, Recall and F1-Score...
              precision    recall  f1-score   support

           0     0.8617    0.9310    0.8950        87
           1     0.9808    0.9917    0.9862      1083
           2     0.9839    0.9684    0.9761       696
           3     1.0000    1.0000    1.0000        10
           4     0.9136    0.9867    0.9487        75
           5     0.9520    0.9835    0.9675       121
           6     1.0000    0.7222    0.8387        36
           7     0.9315    0.8395    0.8831        81

    accuracy                         0.9712      2189
   macro avg     0.9529    0.9279    0.9369      2189
weighted avg     0.9718    0.9712    0.9709      2189


Macro average Test Precision, Recall and F1-Score...
(0.9529440934419823, 0.9278726467696754, 0.9369227469929506, None)

Micro average Test Precision, Recall and F1-Score...
(0.9712197350388305, 0.9712197350388305, 0.9712197350388305, None)

Embeddings:
Word_embeddings:7688
Train_doc_embeddings:5485
Test_doc_embeddings:2189
