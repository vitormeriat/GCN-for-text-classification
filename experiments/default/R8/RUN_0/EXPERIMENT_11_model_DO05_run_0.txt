
==========: 24295009588400
Epoch:0001, train_loss=2.06493, train_acc=0.37796, val_loss=2.05946, val_acc=0.51825, time=1.22701
Epoch:0002, train_loss=1.90084, train_acc=0.53919, val_loss=2.04683, val_acc=0.68796, time=1.25600
Epoch:0003, train_loss=1.79501, train_acc=0.67288, val_loss=2.03938, val_acc=0.75547, time=1.10701
Epoch:0004, train_loss=1.73346, train_acc=0.74478, val_loss=2.03477, val_acc=0.78650, time=1.02000
Epoch:0005, train_loss=1.69482, train_acc=0.76099, val_loss=2.03128, val_acc=0.79927, time=0.99001
Epoch:0006, train_loss=1.66393, train_acc=0.77112, val_loss=2.02824, val_acc=0.81022, time=0.98402
Epoch:0007, train_loss=1.63560, train_acc=0.78975, val_loss=2.02557, val_acc=0.83394, time=0.98401
Epoch:0008, train_loss=1.60976, train_acc=0.81203, val_loss=2.02333, val_acc=0.86131, time=1.17601
Epoch:0009, train_loss=1.58753, train_acc=0.84505, val_loss=2.02152, val_acc=0.88686, time=1.12801
Epoch:0010, train_loss=1.56935, train_acc=0.87624, val_loss=2.02004, val_acc=0.90511, time=1.16801
Epoch:0011, train_loss=1.55449, train_acc=0.89407, val_loss=2.01881, val_acc=0.91606, time=1.03901
Epoch:0012, train_loss=1.54194, train_acc=0.90602, val_loss=2.01773, val_acc=0.92336, time=1.02700
Epoch:0013, train_loss=1.53091, train_acc=0.91412, val_loss=2.01677, val_acc=0.92883, time=1.02401
Epoch:0014, train_loss=1.52093, train_acc=0.92303, val_loss=2.01591, val_acc=0.93431, time=0.98701
Epoch:0015, train_loss=1.51170, train_acc=0.92931, val_loss=2.01514, val_acc=0.93613, time=0.94300
Epoch:0016, train_loss=1.50313, train_acc=0.93620, val_loss=2.01446, val_acc=0.93796, time=0.92501
Epoch:0017, train_loss=1.49534, train_acc=0.94187, val_loss=2.01389, val_acc=0.94343, time=1.02401
Epoch:0018, train_loss=1.48858, train_acc=0.94632, val_loss=2.01345, val_acc=0.94526, time=0.97300
Epoch:0019, train_loss=1.48305, train_acc=0.95179, val_loss=2.01313, val_acc=0.94708, time=1.05701
Epoch:0020, train_loss=1.47869, train_acc=0.95301, val_loss=2.01288, val_acc=0.94708, time=1.13800
Epoch:0021, train_loss=1.47510, train_acc=0.95443, val_loss=2.01264, val_acc=0.94526, time=1.02401
Epoch:0022, train_loss=1.47171, train_acc=0.95605, val_loss=2.01237, val_acc=0.94526, time=1.01101
Epoch:0023, train_loss=1.46811, train_acc=0.95807, val_loss=2.01205, val_acc=0.94708, time=1.02800
Epoch:0024, train_loss=1.46427, train_acc=0.96030, val_loss=2.01172, val_acc=0.94708, time=1.24701
Epoch:0025, train_loss=1.46046, train_acc=0.96314, val_loss=2.01142, val_acc=0.94891, time=1.00100
Epoch:0026, train_loss=1.45697, train_acc=0.96719, val_loss=2.01115, val_acc=0.95255, time=1.02900
Epoch:0027, train_loss=1.45397, train_acc=0.96941, val_loss=2.01093, val_acc=0.95438, time=0.95101
Epoch:0028, train_loss=1.45145, train_acc=0.97225, val_loss=2.01075, val_acc=0.95438, time=1.09401
Epoch:0029, train_loss=1.44933, train_acc=0.97266, val_loss=2.01060, val_acc=0.95803, time=1.01301
Epoch:0030, train_loss=1.44752, train_acc=0.97488, val_loss=2.01047, val_acc=0.95803, time=1.12500
Epoch:0031, train_loss=1.44592, train_acc=0.97590, val_loss=2.01036, val_acc=0.95620, time=1.04202
Epoch:0032, train_loss=1.44445, train_acc=0.97752, val_loss=2.01026, val_acc=0.95620, time=1.12501
Epoch:0033, train_loss=1.44302, train_acc=0.97853, val_loss=2.01016, val_acc=0.95803, time=1.01100
Epoch:0034, train_loss=1.44157, train_acc=0.97873, val_loss=2.01007, val_acc=0.95803, time=1.37100
Epoch:0035, train_loss=1.44010, train_acc=0.97954, val_loss=2.00998, val_acc=0.95620, time=1.13201
Epoch:0036, train_loss=1.43866, train_acc=0.98076, val_loss=2.00990, val_acc=0.95620, time=1.04601
Epoch:0037, train_loss=1.43731, train_acc=0.98096, val_loss=2.00983, val_acc=0.95438, time=1.06401
Epoch:0038, train_loss=1.43610, train_acc=0.98218, val_loss=2.00978, val_acc=0.95438, time=0.98201
Epoch:0039, train_loss=1.43505, train_acc=0.98258, val_loss=2.00975, val_acc=0.95438, time=1.14100
Epoch:0040, train_loss=1.43414, train_acc=0.98319, val_loss=2.00972, val_acc=0.95255, time=1.04400
Epoch:0041, train_loss=1.43333, train_acc=0.98461, val_loss=2.00969, val_acc=0.95438, time=1.23101
Epoch:0042, train_loss=1.43258, train_acc=0.98521, val_loss=2.00966, val_acc=0.95438, time=1.06501
Epoch:0043, train_loss=1.43184, train_acc=0.98542, val_loss=2.00962, val_acc=0.95255, time=0.93099
Epoch:0044, train_loss=1.43110, train_acc=0.98623, val_loss=2.00958, val_acc=0.95255, time=0.97001
Epoch:0045, train_loss=1.43039, train_acc=0.98663, val_loss=2.00955, val_acc=0.95255, time=0.97101
Epoch:0046, train_loss=1.42970, train_acc=0.98744, val_loss=2.00951, val_acc=0.95073, time=0.93201
Epoch:0047, train_loss=1.42906, train_acc=0.98785, val_loss=2.00948, val_acc=0.95255, time=0.94801
Epoch:0048, train_loss=1.42847, train_acc=0.98866, val_loss=2.00945, val_acc=0.95255, time=1.07000
Epoch:0049, train_loss=1.42792, train_acc=0.98906, val_loss=2.00943, val_acc=0.95255, time=1.06500
Epoch:0050, train_loss=1.42740, train_acc=0.98926, val_loss=2.00941, val_acc=0.95255, time=1.14700
Epoch:0051, train_loss=1.42690, train_acc=0.99007, val_loss=2.00940, val_acc=0.95620, time=0.97203
Epoch:0052, train_loss=1.42642, train_acc=0.99089, val_loss=2.00939, val_acc=0.95803, time=0.97300
Epoch:0053, train_loss=1.42595, train_acc=0.99149, val_loss=2.00938, val_acc=0.95803, time=1.10800
Epoch:0054, train_loss=1.42550, train_acc=0.99149, val_loss=2.00937, val_acc=0.95803, time=0.90702
Epoch:0055, train_loss=1.42507, train_acc=0.99190, val_loss=2.00937, val_acc=0.95803, time=1.14900
Epoch:0056, train_loss=1.42466, train_acc=0.99210, val_loss=2.00937, val_acc=0.95803, time=0.99201
Epoch:0057, train_loss=1.42427, train_acc=0.99251, val_loss=2.00937, val_acc=0.95985, time=0.98101
Epoch:0058, train_loss=1.42391, train_acc=0.99251, val_loss=2.00937, val_acc=0.95985, time=1.11001
Epoch:0059, train_loss=1.42357, train_acc=0.99291, val_loss=2.00937, val_acc=0.95803, time=1.02100
Epoch:0060, train_loss=1.42324, train_acc=0.99372, val_loss=2.00937, val_acc=0.95803, time=1.06801
Epoch:0061, train_loss=1.42292, train_acc=0.99392, val_loss=2.00938, val_acc=0.95803, time=1.18100
Early stopping...

Optimization Finished!

Test set results: loss= 1.79829, accuracy= 0.97213, time= 0.27900

Test Precision, Recall and F1-Score...
              precision    recall  f1-score   support

           0     0.8901    0.9310    0.9101        87
           1     0.9817    0.9926    0.9871      1083
           2     0.9825    0.9698    0.9761       696
           3     0.9091    1.0000    0.9524        10
           4     0.9125    0.9733    0.9419        75
           5     0.9590    0.9669    0.9630       121
           6     0.9630    0.7222    0.8254        36
           7     0.9342    0.8765    0.9045        81

    accuracy                         0.9721      2189
   macro avg     0.9415    0.9291    0.9326      2189
weighted avg     0.9723    0.9721    0.9718      2189


Macro average Test Precision, Recall and F1-Score...
(0.9415198241039037, 0.9290645118605789, 0.9325662725605177, None)

Micro average Test Precision, Recall and F1-Score...
(0.972133394243947, 0.972133394243947, 0.972133394243947, None)

Embeddings:
Word_embeddings:7688
Train_doc_embeddings:5485
Test_doc_embeddings:2189
