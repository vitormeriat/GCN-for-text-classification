
==========: 14270074329200
Epoch:0001, train_loss=2.10620, train_acc=0.12862, val_loss=2.04711, val_acc=0.70438, time=1.22001
Epoch:0002, train_loss=1.81565, train_acc=0.70772, val_loss=2.04020, val_acc=0.76277, time=1.22400
Epoch:0003, train_loss=1.77345, train_acc=0.72676, val_loss=2.02452, val_acc=0.82482, time=0.90801
Epoch:0004, train_loss=1.60644, train_acc=0.80778, val_loss=2.02252, val_acc=0.86679, time=0.99300
Epoch:0005, train_loss=1.57308, train_acc=0.85983, val_loss=2.02165, val_acc=0.88321, time=1.08600
Epoch:0006, train_loss=1.56010, train_acc=0.87786, val_loss=2.01898, val_acc=0.90328, time=1.21801
Epoch:0007, train_loss=1.53535, train_acc=0.91128, val_loss=2.01637, val_acc=0.93978, time=0.92501
Epoch:0008, train_loss=1.51233, train_acc=0.93863, val_loss=2.01467, val_acc=0.94343, time=0.96602
Epoch:0009, train_loss=1.49750, train_acc=0.94531, val_loss=2.01374, val_acc=0.94161, time=0.95199
Epoch:0010, train_loss=1.48954, train_acc=0.94227, val_loss=2.01317, val_acc=0.93796, time=1.10101
Epoch:0011, train_loss=1.48422, train_acc=0.94126, val_loss=2.01261, val_acc=0.94161, time=0.98601
Epoch:0012, train_loss=1.47776, train_acc=0.94349, val_loss=2.01196, val_acc=0.94891, time=0.93501
Epoch:0013, train_loss=1.46928, train_acc=0.94997, val_loss=2.01136, val_acc=0.95073, time=1.03300
Epoch:0014, train_loss=1.46057, train_acc=0.95746, val_loss=2.01097, val_acc=0.95438, time=0.96402
Epoch:0015, train_loss=1.45366, train_acc=0.96273, val_loss=2.01083, val_acc=0.95073, time=1.06100
Epoch:0016, train_loss=1.44919, train_acc=0.96395, val_loss=2.01082, val_acc=0.94343, time=1.09400
Epoch:0017, train_loss=1.44645, train_acc=0.96759, val_loss=2.01076, val_acc=0.94343, time=0.90399
Epoch:0018, train_loss=1.44416, train_acc=0.96779, val_loss=2.01056, val_acc=0.94343, time=1.16900
Epoch:0019, train_loss=1.44161, train_acc=0.96860, val_loss=2.01022, val_acc=0.94343, time=1.05400
Epoch:0020, train_loss=1.43876, train_acc=0.97205, val_loss=2.00982, val_acc=0.95255, time=1.04400
Epoch:0021, train_loss=1.43595, train_acc=0.97691, val_loss=2.00947, val_acc=0.95438, time=1.07602
Epoch:0022, train_loss=1.43357, train_acc=0.97954, val_loss=2.00922, val_acc=0.95803, time=0.97000
Epoch:0023, train_loss=1.43196, train_acc=0.98218, val_loss=2.00910, val_acc=0.96168, time=0.94102
Epoch:0024, train_loss=1.43103, train_acc=0.98116, val_loss=2.00907, val_acc=0.96168, time=1.15699
Epoch:0025, train_loss=1.43028, train_acc=0.98238, val_loss=2.00909, val_acc=0.96350, time=1.04901
Epoch:0026, train_loss=1.42920, train_acc=0.98400, val_loss=2.00913, val_acc=0.96168, time=1.09101
Epoch:0027, train_loss=1.42777, train_acc=0.98562, val_loss=2.00921, val_acc=0.96168, time=1.10700
Epoch:0028, train_loss=1.42633, train_acc=0.98704, val_loss=2.00934, val_acc=0.96168, time=1.07900
Epoch:0029, train_loss=1.42522, train_acc=0.98785, val_loss=2.00949, val_acc=0.95985, time=1.04102
Early stopping...

Optimization Finished!

Test set results: loss= 1.80051, accuracy= 0.97031, time= 0.31700

Test Precision, Recall and F1-Score...
              precision    recall  f1-score   support

           0     0.8283    0.9425    0.8817        87
           1     0.9862    0.9917    0.9890      1083
           2     0.9855    0.9741    0.9798       696
           3     0.8333    1.0000    0.9091        10
           4     0.9000    0.9600    0.9290        75
           5     0.9508    0.9587    0.9547       121
           6     0.9333    0.7778    0.8485        36
           7     0.9275    0.7901    0.8533        81

    accuracy                         0.9703      2189
   macro avg     0.9181    0.9244    0.9181      2189
weighted avg     0.9711    0.9703    0.9701      2189


Macro average Test Precision, Recall and F1-Score...
(0.9181245513200718, 0.9243669172346877, 0.9181391689674326, None)

Micro average Test Precision, Recall and F1-Score...
(0.970306075833714, 0.970306075833714, 0.970306075833714, None)

Embeddings:
Word_embeddings:7688
Train_doc_embeddings:5485
Test_doc_embeddings:2189
