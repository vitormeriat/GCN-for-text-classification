
==========: 24364536054900
Epoch:0001, train_loss=2.03928, train_acc=0.40713, val_loss=2.05600, val_acc=0.59307, time=1.31200
Epoch:0002, train_loss=1.87425, train_acc=0.60280, val_loss=2.04431, val_acc=0.72445, time=1.19901
Epoch:0003, train_loss=1.77375, train_acc=0.71703, val_loss=2.03739, val_acc=0.77190, time=0.97002
Epoch:0004, train_loss=1.71457, train_acc=0.76261, val_loss=2.03286, val_acc=0.79562, time=0.99800
Epoch:0005, train_loss=1.67531, train_acc=0.77456, val_loss=2.02933, val_acc=0.80474, time=1.16501
Epoch:0006, train_loss=1.64349, train_acc=0.78327, val_loss=2.02630, val_acc=0.81934, time=1.08300
Epoch:0007, train_loss=1.61527, train_acc=0.80677, val_loss=2.02379, val_acc=0.85219, time=1.10499
Epoch:0008, train_loss=1.59113, train_acc=0.83978, val_loss=2.02181, val_acc=0.88139, time=1.01501
Epoch:0009, train_loss=1.57166, train_acc=0.86733, val_loss=2.02024, val_acc=0.89964, time=1.25401
Epoch:0010, train_loss=1.55599, train_acc=0.88536, val_loss=2.01893, val_acc=0.90511, time=0.96500
Epoch:0011, train_loss=1.54266, train_acc=0.89852, val_loss=2.01778, val_acc=0.91788, time=0.94199
Epoch:0012, train_loss=1.53069, train_acc=0.91169, val_loss=2.01675, val_acc=0.92883, time=1.04302
Epoch:0013, train_loss=1.51981, train_acc=0.92506, val_loss=2.01587, val_acc=0.93248, time=1.14700
Epoch:0014, train_loss=1.51016, train_acc=0.93316, val_loss=2.01512, val_acc=0.93248, time=1.08200
Epoch:0015, train_loss=1.50184, train_acc=0.93802, val_loss=2.01451, val_acc=0.93431, time=1.09202
Epoch:0016, train_loss=1.49476, train_acc=0.94450, val_loss=2.01400, val_acc=0.93796, time=1.10701
Epoch:0017, train_loss=1.48877, train_acc=0.94855, val_loss=2.01357, val_acc=0.94343, time=1.21701
Epoch:0018, train_loss=1.48363, train_acc=0.95139, val_loss=2.01320, val_acc=0.94708, time=0.96900
Epoch:0019, train_loss=1.47901, train_acc=0.95220, val_loss=2.01285, val_acc=0.95073, time=0.95401
Epoch:0020, train_loss=1.47463, train_acc=0.95503, val_loss=2.01252, val_acc=0.95255, time=1.09201
Epoch:0021, train_loss=1.47035, train_acc=0.95706, val_loss=2.01219, val_acc=0.94708, time=1.01301
Epoch:0022, train_loss=1.46622, train_acc=0.95888, val_loss=2.01188, val_acc=0.94708, time=1.07201
Epoch:0023, train_loss=1.46236, train_acc=0.96131, val_loss=2.01158, val_acc=0.94891, time=1.18202
Epoch:0024, train_loss=1.45884, train_acc=0.96476, val_loss=2.01131, val_acc=0.94891, time=1.01800
Epoch:0025, train_loss=1.45572, train_acc=0.96800, val_loss=2.01106, val_acc=0.94708, time=1.34701
Epoch:0026, train_loss=1.45300, train_acc=0.97144, val_loss=2.01084, val_acc=0.95438, time=0.99201
Epoch:0027, train_loss=1.45065, train_acc=0.97347, val_loss=2.01066, val_acc=0.95438, time=0.98800
Epoch:0028, train_loss=1.44862, train_acc=0.97306, val_loss=2.01050, val_acc=0.95438, time=1.00501
Epoch:0029, train_loss=1.44682, train_acc=0.97407, val_loss=2.01036, val_acc=0.95438, time=1.15600
Epoch:0030, train_loss=1.44516, train_acc=0.97650, val_loss=2.01024, val_acc=0.95255, time=0.93701
Epoch:0031, train_loss=1.44356, train_acc=0.97731, val_loss=2.01012, val_acc=0.95255, time=1.10001
Epoch:0032, train_loss=1.44200, train_acc=0.97893, val_loss=2.01002, val_acc=0.95620, time=1.02301
Epoch:0033, train_loss=1.44050, train_acc=0.98096, val_loss=2.00994, val_acc=0.95438, time=0.99501
Epoch:0034, train_loss=1.43910, train_acc=0.98137, val_loss=2.00988, val_acc=0.95620, time=1.10401
Epoch:0035, train_loss=1.43782, train_acc=0.98238, val_loss=2.00983, val_acc=0.95620, time=1.10802
Epoch:0036, train_loss=1.43666, train_acc=0.98218, val_loss=2.00979, val_acc=0.95438, time=1.01600
Epoch:0037, train_loss=1.43561, train_acc=0.98299, val_loss=2.00976, val_acc=0.95438, time=1.02901
Epoch:0038, train_loss=1.43464, train_acc=0.98319, val_loss=2.00974, val_acc=0.95438, time=0.90100
Epoch:0039, train_loss=1.43375, train_acc=0.98319, val_loss=2.00971, val_acc=0.95620, time=0.96601
Epoch:0040, train_loss=1.43291, train_acc=0.98380, val_loss=2.00968, val_acc=0.95620, time=0.99102
Epoch:0041, train_loss=1.43212, train_acc=0.98461, val_loss=2.00964, val_acc=0.95438, time=1.21801
Epoch:0042, train_loss=1.43138, train_acc=0.98562, val_loss=2.00959, val_acc=0.95438, time=1.02200
Epoch:0043, train_loss=1.43066, train_acc=0.98663, val_loss=2.00954, val_acc=0.95438, time=0.92100
Epoch:0044, train_loss=1.42998, train_acc=0.98704, val_loss=2.00949, val_acc=0.95255, time=1.26700
Epoch:0045, train_loss=1.42933, train_acc=0.98764, val_loss=2.00945, val_acc=0.95438, time=1.08000
Epoch:0046, train_loss=1.42873, train_acc=0.98785, val_loss=2.00941, val_acc=0.95620, time=1.13301
Epoch:0047, train_loss=1.42817, train_acc=0.98805, val_loss=2.00938, val_acc=0.95803, time=1.16001
Epoch:0048, train_loss=1.42766, train_acc=0.98886, val_loss=2.00936, val_acc=0.95985, time=1.14501
Epoch:0049, train_loss=1.42718, train_acc=0.98947, val_loss=2.00935, val_acc=0.95985, time=1.03100
Epoch:0050, train_loss=1.42671, train_acc=0.98987, val_loss=2.00935, val_acc=0.95985, time=1.04099
Epoch:0051, train_loss=1.42625, train_acc=0.99028, val_loss=2.00935, val_acc=0.95803, time=1.11501
Epoch:0052, train_loss=1.42579, train_acc=0.99089, val_loss=2.00935, val_acc=0.95803, time=1.09401
Epoch:0053, train_loss=1.42535, train_acc=0.99109, val_loss=2.00936, val_acc=0.95803, time=1.01801
Epoch:0054, train_loss=1.42492, train_acc=0.99210, val_loss=2.00937, val_acc=0.95803, time=0.92801
Epoch:0055, train_loss=1.42452, train_acc=0.99251, val_loss=2.00938, val_acc=0.95803, time=0.98701
Early stopping...

Optimization Finished!

Test set results: loss= 1.79901, accuracy= 0.97031, time= 0.29401

Test Precision, Recall and F1-Score...
              precision    recall  f1-score   support

           0     0.8696    0.9195    0.8939        87
           1     0.9808    0.9917    0.9862      1083
           2     0.9840    0.9698    0.9768       696
           3     1.0000    1.0000    1.0000        10
           4     0.9125    0.9733    0.9419        75
           5     0.9512    0.9669    0.9590       121
           6     0.9286    0.7222    0.8125        36
           7     0.9200    0.8519    0.8846        81

    accuracy                         0.9703      2189
   macro avg     0.9433    0.9244    0.9319      2189
weighted avg     0.9704    0.9703    0.9700      2189


Macro average Test Precision, Recall and F1-Score...
(0.9433303863179168, 0.9244258903690266, 0.9318741322253528, None)

Micro average Test Precision, Recall and F1-Score...
(0.970306075833714, 0.970306075833714, 0.970306075833714, None)

Embeddings:
Word_embeddings:7688
Train_doc_embeddings:5485
Test_doc_embeddings:2189
