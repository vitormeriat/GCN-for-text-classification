
==========: 240041348372000
Epoch:0001, train_loss=2.07423, train_acc=0.08041, val_loss=2.05773, val_acc=0.72810, time=1.39002
Epoch:0002, train_loss=1.89897, train_acc=0.70306, val_loss=2.04538, val_acc=0.77190, time=1.12301
Epoch:0003, train_loss=1.78973, train_acc=0.75025, val_loss=2.03819, val_acc=0.79197, time=1.25501
Epoch:0004, train_loss=1.72494, train_acc=0.77172, val_loss=2.03361, val_acc=0.79562, time=1.24099
Epoch:0005, train_loss=1.68333, train_acc=0.78185, val_loss=2.03007, val_acc=0.81569, time=1.24001
Epoch:0006, train_loss=1.65117, train_acc=0.79198, val_loss=2.02702, val_acc=0.82299, time=1.29999
Epoch:0007, train_loss=1.62350, train_acc=0.81142, val_loss=2.02441, val_acc=0.85584, time=1.26000
Epoch:0008, train_loss=1.59975, train_acc=0.83735, val_loss=2.02229, val_acc=0.88321, time=1.25600
Epoch:0009, train_loss=1.58017, train_acc=0.85801, val_loss=2.02059, val_acc=0.88869, time=1.27302
Epoch:0010, train_loss=1.56408, train_acc=0.87482, val_loss=2.01917, val_acc=0.90328, time=1.22500
Epoch:0011, train_loss=1.55028, train_acc=0.88617, val_loss=2.01798, val_acc=0.91058, time=1.20500
Epoch:0012, train_loss=1.53807, train_acc=0.89508, val_loss=2.01697, val_acc=0.91971, time=1.20803
Epoch:0013, train_loss=1.52716, train_acc=0.90703, val_loss=2.01611, val_acc=0.92518, time=1.26299
Epoch:0014, train_loss=1.51737, train_acc=0.92100, val_loss=2.01538, val_acc=0.93066, time=1.17400
Epoch:0015, train_loss=1.50861, train_acc=0.93012, val_loss=2.01475, val_acc=0.93431, time=1.23601
Epoch:0016, train_loss=1.50068, train_acc=0.93559, val_loss=2.01420, val_acc=0.93431, time=1.20800
Epoch:0017, train_loss=1.49348, train_acc=0.94065, val_loss=2.01373, val_acc=0.93431, time=1.16902
Epoch:0018, train_loss=1.48708, train_acc=0.94592, val_loss=2.01334, val_acc=0.93796, time=1.25501
Epoch:0019, train_loss=1.48170, train_acc=0.95179, val_loss=2.01305, val_acc=0.94161, time=1.22401
Epoch:0020, train_loss=1.47743, train_acc=0.95625, val_loss=2.01281, val_acc=0.94343, time=1.21703
Epoch:0021, train_loss=1.47396, train_acc=0.95807, val_loss=2.01258, val_acc=0.94161, time=1.15701
Epoch:0022, train_loss=1.47073, train_acc=0.95868, val_loss=2.01231, val_acc=0.94161, time=1.23300
Epoch:0023, train_loss=1.46732, train_acc=0.96152, val_loss=2.01200, val_acc=0.94526, time=1.28102
Epoch:0024, train_loss=1.46368, train_acc=0.96415, val_loss=2.01167, val_acc=0.94708, time=1.25198
Epoch:0025, train_loss=1.46001, train_acc=0.96597, val_loss=2.01136, val_acc=0.94708, time=1.27202
Epoch:0026, train_loss=1.45658, train_acc=0.96800, val_loss=2.01109, val_acc=0.94526, time=1.27203
Epoch:0027, train_loss=1.45358, train_acc=0.96921, val_loss=2.01087, val_acc=0.94891, time=1.23401
Epoch:0028, train_loss=1.45107, train_acc=0.96982, val_loss=2.01069, val_acc=0.95255, time=1.15101
Epoch:0029, train_loss=1.44899, train_acc=0.97347, val_loss=2.01055, val_acc=0.95255, time=1.20202
Epoch:0030, train_loss=1.44720, train_acc=0.97488, val_loss=2.01043, val_acc=0.95255, time=1.26500
Epoch:0031, train_loss=1.44559, train_acc=0.97630, val_loss=2.01032, val_acc=0.95255, time=1.17401
Epoch:0032, train_loss=1.44409, train_acc=0.97792, val_loss=2.01021, val_acc=0.95255, time=1.20501
Epoch:0033, train_loss=1.44266, train_acc=0.97873, val_loss=2.01011, val_acc=0.95255, time=1.22999
Epoch:0034, train_loss=1.44130, train_acc=0.98096, val_loss=2.01001, val_acc=0.95438, time=1.12801
Epoch:0035, train_loss=1.44003, train_acc=0.98157, val_loss=2.00993, val_acc=0.95438, time=1.21200
Epoch:0036, train_loss=1.43883, train_acc=0.98177, val_loss=2.00985, val_acc=0.95803, time=1.23000
Epoch:0037, train_loss=1.43770, train_acc=0.98258, val_loss=2.00978, val_acc=0.95985, time=1.16401
Epoch:0038, train_loss=1.43660, train_acc=0.98258, val_loss=2.00973, val_acc=0.95803, time=1.23801
Epoch:0039, train_loss=1.43553, train_acc=0.98299, val_loss=2.00968, val_acc=0.95803, time=1.15898
Epoch:0040, train_loss=1.43451, train_acc=0.98380, val_loss=2.00965, val_acc=0.95620, time=1.23401
Epoch:0041, train_loss=1.43355, train_acc=0.98542, val_loss=2.00962, val_acc=0.95438, time=1.28701
Epoch:0042, train_loss=1.43269, train_acc=0.98542, val_loss=2.00961, val_acc=0.95438, time=1.23199
Epoch:0043, train_loss=1.43193, train_acc=0.98521, val_loss=2.00960, val_acc=0.95073, time=1.17999
Epoch:0044, train_loss=1.43125, train_acc=0.98582, val_loss=2.00959, val_acc=0.95255, time=1.22500
Epoch:0045, train_loss=1.43061, train_acc=0.98683, val_loss=2.00958, val_acc=0.95438, time=1.19000
Epoch:0046, train_loss=1.42998, train_acc=0.98744, val_loss=2.00955, val_acc=0.95620, time=1.17600
Epoch:0047, train_loss=1.42935, train_acc=0.98805, val_loss=2.00952, val_acc=0.95803, time=1.26402
Epoch:0048, train_loss=1.42873, train_acc=0.98866, val_loss=2.00949, val_acc=0.95985, time=1.19400
Epoch:0049, train_loss=1.42816, train_acc=0.98825, val_loss=2.00945, val_acc=0.96350, time=1.14601
Epoch:0050, train_loss=1.42764, train_acc=0.98906, val_loss=2.00942, val_acc=0.96350, time=1.21701
Epoch:0051, train_loss=1.42715, train_acc=0.98967, val_loss=2.00939, val_acc=0.96350, time=1.23899
Epoch:0052, train_loss=1.42669, train_acc=0.99007, val_loss=2.00937, val_acc=0.96350, time=1.23601
Epoch:0053, train_loss=1.42623, train_acc=0.99089, val_loss=2.00935, val_acc=0.96168, time=1.17002
Epoch:0054, train_loss=1.42579, train_acc=0.99109, val_loss=2.00934, val_acc=0.96168, time=1.19500
Epoch:0055, train_loss=1.42536, train_acc=0.99149, val_loss=2.00934, val_acc=0.96168, time=1.18000
Epoch:0056, train_loss=1.42496, train_acc=0.99230, val_loss=2.00934, val_acc=0.96168, time=1.31601
Epoch:0057, train_loss=1.42457, train_acc=0.99251, val_loss=2.00934, val_acc=0.95985, time=1.18501
Epoch:0058, train_loss=1.42421, train_acc=0.99251, val_loss=2.00935, val_acc=0.95985, time=1.42501
Epoch:0059, train_loss=1.42385, train_acc=0.99271, val_loss=2.00936, val_acc=0.95985, time=1.23201
Epoch:0060, train_loss=1.42350, train_acc=0.99291, val_loss=2.00936, val_acc=0.96168, time=1.15500
Early stopping...

Optimization Finished!

Test set results: loss= 1.79883, accuracy= 0.96802, time= 0.38400

Test Precision, Recall and F1-Score...
              precision    recall  f1-score   support

           0     0.8617    0.9310    0.8950        87
           1     0.9781    0.9917    0.9849      1083
           2     0.9825    0.9670    0.9747       696
           3     1.0000    1.0000    1.0000        10
           4     0.8889    0.9600    0.9231        75
           5     0.9508    0.9587    0.9547       121
           6     0.9286    0.7222    0.8125        36
           7     0.9437    0.8272    0.8816        81

    accuracy                         0.9680      2189
   macro avg     0.9418    0.9197    0.9283      2189
weighted avg     0.9683    0.9680    0.9677      2189


Macro average Test Precision, Recall and F1-Score...
(0.9417834896761968, 0.9197173323049304, 0.9283051721729101, None)

Micro average Test Precision, Recall and F1-Score...
(0.9680219278209228, 0.9680219278209228, 0.9680219278209228, None)

Embeddings:
Word_embeddings:7688
Train_doc_embeddings:5485
Test_doc_embeddings:2189
