
==========: 239943623278800
Epoch:0001, train_loss=2.05585, train_acc=0.31193, val_loss=2.05744, val_acc=0.54380, time=1.22800
Epoch:0002, train_loss=1.89133, train_acc=0.55236, val_loss=2.04586, val_acc=0.67153, time=1.20000
Epoch:0003, train_loss=1.79107, train_acc=0.65850, val_loss=2.03885, val_acc=0.75912, time=1.24001
Epoch:0004, train_loss=1.73082, train_acc=0.74904, val_loss=2.03432, val_acc=0.78467, time=1.22101
Epoch:0005, train_loss=1.69097, train_acc=0.77091, val_loss=2.03082, val_acc=0.80109, time=1.17400
Epoch:0006, train_loss=1.65873, train_acc=0.77841, val_loss=2.02777, val_acc=0.80474, time=1.15201
Epoch:0007, train_loss=1.62934, train_acc=0.79016, val_loss=2.02510, val_acc=0.82482, time=1.26102
Epoch:0008, train_loss=1.60296, train_acc=0.81629, val_loss=2.02293, val_acc=0.85949, time=1.23500
Epoch:0009, train_loss=1.58111, train_acc=0.84991, val_loss=2.02122, val_acc=0.87774, time=1.20501
Epoch:0010, train_loss=1.56388, train_acc=0.87178, val_loss=2.01983, val_acc=0.88504, time=1.26600
Epoch:0011, train_loss=1.54985, train_acc=0.88880, val_loss=2.01861, val_acc=0.89416, time=1.12301
Epoch:0012, train_loss=1.53763, train_acc=0.90440, val_loss=2.01751, val_acc=0.91606, time=1.23300
Epoch:0013, train_loss=1.52664, train_acc=0.91999, val_loss=2.01656, val_acc=0.92701, time=1.14101
Epoch:0014, train_loss=1.51691, train_acc=0.93255, val_loss=2.01575, val_acc=0.93978, time=1.30499
Epoch:0015, train_loss=1.50850, train_acc=0.94126, val_loss=2.01507, val_acc=0.93978, time=1.18502
Epoch:0016, train_loss=1.50123, train_acc=0.94632, val_loss=2.01447, val_acc=0.94161, time=1.27600
Epoch:0017, train_loss=1.49475, train_acc=0.94855, val_loss=2.01394, val_acc=0.93978, time=1.10603
Epoch:0018, train_loss=1.48873, train_acc=0.95139, val_loss=2.01346, val_acc=0.93796, time=1.21400
Epoch:0019, train_loss=1.48305, train_acc=0.95362, val_loss=2.01302, val_acc=0.94161, time=1.24001
Epoch:0020, train_loss=1.47775, train_acc=0.95503, val_loss=2.01265, val_acc=0.94161, time=1.24199
Epoch:0021, train_loss=1.47296, train_acc=0.95787, val_loss=2.01233, val_acc=0.93978, time=1.04601
Epoch:0022, train_loss=1.46876, train_acc=0.96091, val_loss=2.01206, val_acc=0.94343, time=1.17501
Epoch:0023, train_loss=1.46516, train_acc=0.96374, val_loss=2.01183, val_acc=0.94526, time=1.17500
Epoch:0024, train_loss=1.46201, train_acc=0.96496, val_loss=2.01161, val_acc=0.94891, time=1.33400
Epoch:0025, train_loss=1.45915, train_acc=0.96536, val_loss=2.01139, val_acc=0.94891, time=1.22702
Epoch:0026, train_loss=1.45645, train_acc=0.96698, val_loss=2.01117, val_acc=0.94891, time=1.16799
Epoch:0027, train_loss=1.45384, train_acc=0.97022, val_loss=2.01096, val_acc=0.94891, time=1.22501
Epoch:0028, train_loss=1.45133, train_acc=0.97306, val_loss=2.01076, val_acc=0.95073, time=1.12100
Epoch:0029, train_loss=1.44899, train_acc=0.97468, val_loss=2.01058, val_acc=0.95073, time=1.23100
Epoch:0030, train_loss=1.44687, train_acc=0.97610, val_loss=2.01044, val_acc=0.95255, time=1.20197
Epoch:0031, train_loss=1.44501, train_acc=0.97731, val_loss=2.01033, val_acc=0.95255, time=1.10600
Epoch:0032, train_loss=1.44339, train_acc=0.97833, val_loss=2.01024, val_acc=0.95255, time=1.10901
Epoch:0033, train_loss=1.44195, train_acc=0.97914, val_loss=2.01016, val_acc=0.94891, time=1.23599
Epoch:0034, train_loss=1.44063, train_acc=0.97954, val_loss=2.01009, val_acc=0.94891, time=1.24902
Epoch:0035, train_loss=1.43939, train_acc=0.98076, val_loss=2.01002, val_acc=0.95073, time=1.23400
Epoch:0036, train_loss=1.43819, train_acc=0.98116, val_loss=2.00995, val_acc=0.95073, time=1.22501
Epoch:0037, train_loss=1.43705, train_acc=0.98157, val_loss=2.00988, val_acc=0.95255, time=1.11801
Epoch:0038, train_loss=1.43596, train_acc=0.98278, val_loss=2.00981, val_acc=0.95255, time=1.22801
Epoch:0039, train_loss=1.43493, train_acc=0.98339, val_loss=2.00975, val_acc=0.95255, time=1.31302
Epoch:0040, train_loss=1.43398, train_acc=0.98380, val_loss=2.00969, val_acc=0.95438, time=1.18701
Epoch:0041, train_loss=1.43310, train_acc=0.98420, val_loss=2.00963, val_acc=0.95438, time=1.19900
Epoch:0042, train_loss=1.43229, train_acc=0.98440, val_loss=2.00958, val_acc=0.95438, time=1.14400
Epoch:0043, train_loss=1.43153, train_acc=0.98521, val_loss=2.00954, val_acc=0.95620, time=1.14401
Epoch:0044, train_loss=1.43082, train_acc=0.98542, val_loss=2.00952, val_acc=0.95985, time=1.22601
Epoch:0045, train_loss=1.43016, train_acc=0.98562, val_loss=2.00950, val_acc=0.95985, time=1.19501
Epoch:0046, train_loss=1.42953, train_acc=0.98683, val_loss=2.00949, val_acc=0.95985, time=1.19700
Epoch:0047, train_loss=1.42894, train_acc=0.98724, val_loss=2.00948, val_acc=0.96168, time=1.34600
Epoch:0048, train_loss=1.42838, train_acc=0.98764, val_loss=2.00948, val_acc=0.96168, time=1.38199
Epoch:0049, train_loss=1.42784, train_acc=0.98805, val_loss=2.00949, val_acc=0.96168, time=1.07700
Epoch:0050, train_loss=1.42734, train_acc=0.98926, val_loss=2.00949, val_acc=0.95985, time=1.12301
Epoch:0051, train_loss=1.42685, train_acc=0.98967, val_loss=2.00949, val_acc=0.95985, time=1.11204
Epoch:0052, train_loss=1.42639, train_acc=0.99048, val_loss=2.00949, val_acc=0.95803, time=1.20800
Epoch:0053, train_loss=1.42594, train_acc=0.99048, val_loss=2.00949, val_acc=0.95620, time=1.17301
Epoch:0054, train_loss=1.42551, train_acc=0.99129, val_loss=2.00948, val_acc=0.95620, time=1.14502
Epoch:0055, train_loss=1.42509, train_acc=0.99230, val_loss=2.00947, val_acc=0.95620, time=1.19902
Epoch:0056, train_loss=1.42468, train_acc=0.99230, val_loss=2.00945, val_acc=0.95620, time=1.24401
Epoch:0057, train_loss=1.42430, train_acc=0.99251, val_loss=2.00944, val_acc=0.95620, time=1.18601
Epoch:0058, train_loss=1.42393, train_acc=0.99271, val_loss=2.00942, val_acc=0.95620, time=1.22302
Epoch:0059, train_loss=1.42359, train_acc=0.99291, val_loss=2.00941, val_acc=0.95620, time=1.29201
Epoch:0060, train_loss=1.42326, train_acc=0.99291, val_loss=2.00940, val_acc=0.95620, time=1.20101
Epoch:0061, train_loss=1.42294, train_acc=0.99332, val_loss=2.00940, val_acc=0.95620, time=1.18901
Epoch:0062, train_loss=1.42264, train_acc=0.99392, val_loss=2.00939, val_acc=0.95620, time=1.22100
Epoch:0063, train_loss=1.42235, train_acc=0.99392, val_loss=2.00940, val_acc=0.95620, time=1.18401
Epoch:0064, train_loss=1.42206, train_acc=0.99453, val_loss=2.00940, val_acc=0.95620, time=1.24101
Epoch:0065, train_loss=1.42179, train_acc=0.99453, val_loss=2.00941, val_acc=0.95620, time=1.25901
Epoch:0066, train_loss=1.42152, train_acc=0.99453, val_loss=2.00942, val_acc=0.95620, time=1.26001
Early stopping...

Optimization Finished!

Test set results: loss= 1.79862, accuracy= 0.97122, time= 0.38200

Test Precision, Recall and F1-Score...
              precision    recall  f1-score   support

           0     0.8791    0.9195    0.8989        87
           1     0.9808    0.9917    0.9862      1083
           2     0.9839    0.9684    0.9761       696
           3     1.0000    1.0000    1.0000        10
           4     0.9024    0.9867    0.9427        75
           5     0.9444    0.9835    0.9636       121
           6     1.0000    0.6944    0.8197        36
           7     0.9333    0.8642    0.8974        81

    accuracy                         0.9712      2189
   macro avg     0.9530    0.9261    0.9356      2189
weighted avg     0.9717    0.9712    0.9708      2189


Macro average Test Precision, Recall and F1-Score...
(0.9530126506170671, 0.9260500626913442, 0.9355690641166505, None)

Micro average Test Precision, Recall and F1-Score...
(0.9712197350388305, 0.9712197350388305, 0.9712197350388305, None)

Embeddings:
Word_embeddings:7688
Train_doc_embeddings:5485
Test_doc_embeddings:2189
