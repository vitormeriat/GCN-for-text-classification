
==========: 24730376147300
Epoch:0001, train_loss=2.12244, train_acc=0.06340, val_loss=2.06283, val_acc=0.52007, time=1.28801
Epoch:0002, train_loss=1.92949, train_acc=0.53717, val_loss=2.04880, val_acc=0.61861, time=1.20400
Epoch:0003, train_loss=1.80990, train_acc=0.63379, val_loss=2.04073, val_acc=0.73358, time=1.12301
Epoch:0004, train_loss=1.74259, train_acc=0.73020, val_loss=2.03589, val_acc=0.78285, time=1.26300
Epoch:0005, train_loss=1.70237, train_acc=0.75937, val_loss=2.03242, val_acc=0.79380, time=0.97300
Epoch:0006, train_loss=1.67222, train_acc=0.77274, val_loss=2.02943, val_acc=0.80839, time=1.02201
Epoch:0007, train_loss=1.64471, train_acc=0.78246, val_loss=2.02671, val_acc=0.81752, time=0.99201
Epoch:0008, train_loss=1.61866, train_acc=0.80049, val_loss=2.02434, val_acc=0.84307, time=1.01099
Epoch:0009, train_loss=1.59531, train_acc=0.82581, val_loss=2.02239, val_acc=0.87409, time=1.07601
Epoch:0010, train_loss=1.57588, train_acc=0.86470, val_loss=2.02083, val_acc=0.89781, time=1.07200
Epoch:0011, train_loss=1.56040, train_acc=0.88232, val_loss=2.01957, val_acc=0.90693, time=1.04300
Epoch:0012, train_loss=1.54792, train_acc=0.89407, val_loss=2.01850, val_acc=0.91241, time=1.12501
Epoch:0013, train_loss=1.53729, train_acc=0.90743, val_loss=2.01754, val_acc=0.91788, time=0.95401
Epoch:0014, train_loss=1.52773, train_acc=0.91675, val_loss=2.01668, val_acc=0.93248, time=1.12400
Epoch:0015, train_loss=1.51896, train_acc=0.92789, val_loss=2.01591, val_acc=0.93613, time=0.95801
Epoch:0016, train_loss=1.51093, train_acc=0.93437, val_loss=2.01523, val_acc=0.93431, time=0.97800
Epoch:0017, train_loss=1.50357, train_acc=0.93842, val_loss=2.01463, val_acc=0.93796, time=1.03602
Epoch:0018, train_loss=1.49689, train_acc=0.94268, val_loss=2.01412, val_acc=0.93978, time=1.04601
Epoch:0019, train_loss=1.49094, train_acc=0.94774, val_loss=2.01370, val_acc=0.93431, time=1.19500
Epoch:0020, train_loss=1.48576, train_acc=0.95200, val_loss=2.01335, val_acc=0.93431, time=1.24301
Epoch:0021, train_loss=1.48130, train_acc=0.95625, val_loss=2.01306, val_acc=0.93248, time=1.00701
Epoch:0022, train_loss=1.47737, train_acc=0.95665, val_loss=2.01279, val_acc=0.93613, time=1.09399
Epoch:0023, train_loss=1.47369, train_acc=0.95888, val_loss=2.01252, val_acc=0.94161, time=1.17599
Epoch:0024, train_loss=1.47008, train_acc=0.95888, val_loss=2.01225, val_acc=0.94343, time=1.06800
Epoch:0025, train_loss=1.46649, train_acc=0.96050, val_loss=2.01198, val_acc=0.94343, time=1.09801
Epoch:0026, train_loss=1.46299, train_acc=0.96111, val_loss=2.01170, val_acc=0.94708, time=1.01001
Epoch:0027, train_loss=1.45968, train_acc=0.96374, val_loss=2.01144, val_acc=0.94891, time=1.17601
Epoch:0028, train_loss=1.45666, train_acc=0.96455, val_loss=2.01120, val_acc=0.94526, time=0.97499
Epoch:0029, train_loss=1.45397, train_acc=0.96638, val_loss=2.01099, val_acc=0.94526, time=1.06102
Epoch:0030, train_loss=1.45159, train_acc=0.96779, val_loss=2.01079, val_acc=0.94526, time=1.00401
Epoch:0031, train_loss=1.44946, train_acc=0.96779, val_loss=2.01062, val_acc=0.94708, time=0.92801
Epoch:0032, train_loss=1.44752, train_acc=0.96962, val_loss=2.01046, val_acc=0.95255, time=1.14900
Epoch:0033, train_loss=1.44572, train_acc=0.97225, val_loss=2.01032, val_acc=0.95255, time=1.13202
Epoch:0034, train_loss=1.44405, train_acc=0.97448, val_loss=2.01021, val_acc=0.95073, time=1.04401
Epoch:0035, train_loss=1.44253, train_acc=0.97671, val_loss=2.01012, val_acc=0.94891, time=1.01700
Epoch:0036, train_loss=1.44118, train_acc=0.97934, val_loss=2.01006, val_acc=0.95073, time=1.15201
Epoch:0037, train_loss=1.44000, train_acc=0.98096, val_loss=2.01001, val_acc=0.95073, time=1.14502
Epoch:0038, train_loss=1.43896, train_acc=0.98076, val_loss=2.00998, val_acc=0.95073, time=1.04400
Epoch:0039, train_loss=1.43800, train_acc=0.98197, val_loss=2.00995, val_acc=0.95073, time=1.10001
Epoch:0040, train_loss=1.43706, train_acc=0.98238, val_loss=2.00992, val_acc=0.95073, time=1.08401
Epoch:0041, train_loss=1.43614, train_acc=0.98258, val_loss=2.00989, val_acc=0.95073, time=1.20301
Epoch:0042, train_loss=1.43522, train_acc=0.98319, val_loss=2.00986, val_acc=0.95073, time=1.11499
Epoch:0043, train_loss=1.43434, train_acc=0.98420, val_loss=2.00983, val_acc=0.95073, time=1.10402
Epoch:0044, train_loss=1.43353, train_acc=0.98400, val_loss=2.00980, val_acc=0.95255, time=1.01700
Epoch:0045, train_loss=1.43277, train_acc=0.98440, val_loss=2.00977, val_acc=0.95073, time=1.07801
Epoch:0046, train_loss=1.43205, train_acc=0.98542, val_loss=2.00973, val_acc=0.94891, time=1.02800
Epoch:0047, train_loss=1.43135, train_acc=0.98582, val_loss=2.00968, val_acc=0.95073, time=0.98901
Epoch:0048, train_loss=1.43066, train_acc=0.98764, val_loss=2.00964, val_acc=0.95073, time=1.01601
Epoch:0049, train_loss=1.43000, train_acc=0.98704, val_loss=2.00959, val_acc=0.95255, time=1.07701
Epoch:0050, train_loss=1.42938, train_acc=0.98744, val_loss=2.00955, val_acc=0.95620, time=1.09000
Epoch:0051, train_loss=1.42880, train_acc=0.98886, val_loss=2.00952, val_acc=0.95803, time=1.05201
Epoch:0052, train_loss=1.42827, train_acc=0.98967, val_loss=2.00949, val_acc=0.95620, time=1.29801
Epoch:0053, train_loss=1.42778, train_acc=0.98987, val_loss=2.00947, val_acc=0.95620, time=1.08700
Epoch:0054, train_loss=1.42730, train_acc=0.99007, val_loss=2.00945, val_acc=0.95620, time=1.00302
Epoch:0055, train_loss=1.42685, train_acc=0.99028, val_loss=2.00944, val_acc=0.95438, time=1.03100
Epoch:0056, train_loss=1.42641, train_acc=0.99028, val_loss=2.00943, val_acc=0.95255, time=1.10001
Epoch:0057, train_loss=1.42599, train_acc=0.98987, val_loss=2.00943, val_acc=0.95255, time=1.10499
Epoch:0058, train_loss=1.42559, train_acc=0.99048, val_loss=2.00942, val_acc=0.95255, time=1.03500
Epoch:0059, train_loss=1.42520, train_acc=0.99109, val_loss=2.00942, val_acc=0.95255, time=1.02002
Epoch:0060, train_loss=1.42483, train_acc=0.99149, val_loss=2.00941, val_acc=0.95620, time=1.12300
Epoch:0061, train_loss=1.42448, train_acc=0.99170, val_loss=2.00940, val_acc=0.95620, time=1.11001
Epoch:0062, train_loss=1.42413, train_acc=0.99149, val_loss=2.00940, val_acc=0.95438, time=1.08901
Epoch:0063, train_loss=1.42379, train_acc=0.99251, val_loss=2.00939, val_acc=0.95438, time=0.94701
Epoch:0064, train_loss=1.42348, train_acc=0.99352, val_loss=2.00938, val_acc=0.95255, time=0.97900
Epoch:0065, train_loss=1.42318, train_acc=0.99372, val_loss=2.00938, val_acc=0.95255, time=1.10800
Epoch:0066, train_loss=1.42289, train_acc=0.99392, val_loss=2.00938, val_acc=0.95255, time=1.04301
Epoch:0067, train_loss=1.42260, train_acc=0.99392, val_loss=2.00938, val_acc=0.95255, time=1.00001
Epoch:0068, train_loss=1.42233, train_acc=0.99392, val_loss=2.00938, val_acc=0.95255, time=1.20100
Epoch:0069, train_loss=1.42206, train_acc=0.99433, val_loss=2.00938, val_acc=0.95255, time=1.02601
Epoch:0070, train_loss=1.42180, train_acc=0.99473, val_loss=2.00939, val_acc=0.95255, time=1.02099
Epoch:0071, train_loss=1.42155, train_acc=0.99494, val_loss=2.00939, val_acc=0.95438, time=1.02804
Early stopping...

Optimization Finished!

Test set results: loss= 1.79894, accuracy= 0.97076, time= 0.28898

Test Precision, Recall and F1-Score...
              precision    recall  f1-score   support

           0     0.8901    0.9310    0.9101        87
           1     0.9790    0.9917    0.9853      1083
           2     0.9839    0.9655    0.9746       696
           3     1.0000    1.0000    1.0000        10
           4     0.9136    0.9867    0.9487        75
           5     0.9440    0.9752    0.9593       121
           6     0.9615    0.6944    0.8065        36
           7     0.9342    0.8765    0.9045        81

    accuracy                         0.9708      2189
   macro avg     0.9508    0.9276    0.9361      2189
weighted avg     0.9710    0.9708    0.9704      2189


Macro average Test Precision, Recall and F1-Score...
(0.9507959294938808, 0.9276378009235443, 0.9361288129564148, None)

Micro average Test Precision, Recall and F1-Score...
(0.9707629054362723, 0.9707629054362723, 0.9707629054362723, None)

Embeddings:
Word_embeddings:7688
Train_doc_embeddings:5485
Test_doc_embeddings:2189
