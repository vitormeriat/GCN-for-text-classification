
==========: 65234215356600
Epoch:0001, train_loss=2.07039, train_acc=0.09885, val_loss=2.05846, val_acc=0.67883, time=1.37902
Epoch:0002, train_loss=1.89706, train_acc=0.66781, val_loss=2.04662, val_acc=0.72080, time=1.27101
Epoch:0003, train_loss=1.79427, train_acc=0.71866, val_loss=2.03956, val_acc=0.77555, time=1.09601
Epoch:0004, train_loss=1.73383, train_acc=0.75653, val_loss=2.03479, val_acc=0.79197, time=1.17600
Epoch:0005, train_loss=1.69322, train_acc=0.77699, val_loss=2.03107, val_acc=0.80657, time=1.17801
Epoch:0006, train_loss=1.66071, train_acc=0.78327, val_loss=2.02789, val_acc=0.81387, time=1.11301
Epoch:0007, train_loss=1.63193, train_acc=0.80231, val_loss=2.02521, val_acc=0.84672, time=1.22102
Epoch:0008, train_loss=1.60690, train_acc=0.83391, val_loss=2.02308, val_acc=0.87226, time=1.19298
Epoch:0009, train_loss=1.58641, train_acc=0.85740, val_loss=2.02141, val_acc=0.88869, time=1.19200
Epoch:0010, train_loss=1.56995, train_acc=0.87543, val_loss=2.02002, val_acc=0.90146, time=1.16101
Epoch:0011, train_loss=1.55612, train_acc=0.88839, val_loss=2.01882, val_acc=0.91058, time=1.28001
Epoch:0012, train_loss=1.54389, train_acc=0.89913, val_loss=2.01776, val_acc=0.92153, time=1.05301
Epoch:0013, train_loss=1.53290, train_acc=0.91108, val_loss=2.01683, val_acc=0.92701, time=1.17200
Epoch:0014, train_loss=1.52304, train_acc=0.91959, val_loss=2.01600, val_acc=0.92518, time=1.26801
Epoch:0015, train_loss=1.51409, train_acc=0.92384, val_loss=2.01525, val_acc=0.93066, time=1.33701
Epoch:0016, train_loss=1.50573, train_acc=0.93215, val_loss=2.01456, val_acc=0.92883, time=1.13100
Epoch:0017, train_loss=1.49782, train_acc=0.93559, val_loss=2.01393, val_acc=0.93248, time=1.16300
Epoch:0018, train_loss=1.49052, train_acc=0.94166, val_loss=2.01340, val_acc=0.93613, time=1.03201
Epoch:0019, train_loss=1.48415, train_acc=0.94713, val_loss=2.01298, val_acc=0.93796, time=1.15301
Epoch:0020, train_loss=1.47895, train_acc=0.95341, val_loss=2.01267, val_acc=0.94526, time=1.17201
Epoch:0021, train_loss=1.47489, train_acc=0.95625, val_loss=2.01244, val_acc=0.94708, time=1.07200
Epoch:0022, train_loss=1.47167, train_acc=0.95807, val_loss=2.01223, val_acc=0.95073, time=1.09001
Epoch:0023, train_loss=1.46877, train_acc=0.95908, val_loss=2.01200, val_acc=0.95073, time=1.11201
Epoch:0024, train_loss=1.46579, train_acc=0.96030, val_loss=2.01175, val_acc=0.95255, time=1.03700
Epoch:0025, train_loss=1.46255, train_acc=0.96293, val_loss=2.01147, val_acc=0.95255, time=1.08700
Epoch:0026, train_loss=1.45915, train_acc=0.96536, val_loss=2.01119, val_acc=0.95073, time=1.31402
Epoch:0027, train_loss=1.45585, train_acc=0.96901, val_loss=2.01093, val_acc=0.95438, time=1.33001
Epoch:0028, train_loss=1.45290, train_acc=0.97245, val_loss=2.01072, val_acc=0.95438, time=1.70903
Epoch:0029, train_loss=1.45047, train_acc=0.97306, val_loss=2.01056, val_acc=0.95620, time=1.20301
Epoch:0030, train_loss=1.44856, train_acc=0.97448, val_loss=2.01043, val_acc=0.95438, time=1.05501
Epoch:0031, train_loss=1.44705, train_acc=0.97509, val_loss=2.01031, val_acc=0.95438, time=1.18401
Epoch:0032, train_loss=1.44573, train_acc=0.97509, val_loss=2.01020, val_acc=0.95438, time=1.06399
Epoch:0033, train_loss=1.44444, train_acc=0.97529, val_loss=2.01008, val_acc=0.95255, time=1.06601
Epoch:0034, train_loss=1.44308, train_acc=0.97610, val_loss=2.00996, val_acc=0.95255, time=0.96400
Epoch:0035, train_loss=1.44167, train_acc=0.97711, val_loss=2.00985, val_acc=0.95255, time=1.19000
Epoch:0036, train_loss=1.44025, train_acc=0.97792, val_loss=2.00974, val_acc=0.95438, time=1.11800
Epoch:0037, train_loss=1.43893, train_acc=0.98015, val_loss=2.00966, val_acc=0.95803, time=1.16601
Epoch:0038, train_loss=1.43774, train_acc=0.98116, val_loss=2.00959, val_acc=0.95985, time=1.18500
Epoch:0039, train_loss=1.43672, train_acc=0.98299, val_loss=2.00955, val_acc=0.95985, time=1.20201
Epoch:0040, train_loss=1.43581, train_acc=0.98299, val_loss=2.00952, val_acc=0.95985, time=1.04101
Epoch:0041, train_loss=1.43496, train_acc=0.98299, val_loss=2.00949, val_acc=0.95803, time=1.08101
Epoch:0042, train_loss=1.43414, train_acc=0.98400, val_loss=2.00947, val_acc=0.95803, time=1.14501
Epoch:0043, train_loss=1.43332, train_acc=0.98420, val_loss=2.00944, val_acc=0.95985, time=1.22299
Epoch:0044, train_loss=1.43253, train_acc=0.98420, val_loss=2.00941, val_acc=0.96168, time=1.26300
Epoch:0045, train_loss=1.43179, train_acc=0.98461, val_loss=2.00939, val_acc=0.96350, time=1.04401
Epoch:0046, train_loss=1.43112, train_acc=0.98521, val_loss=2.00936, val_acc=0.96350, time=1.03501
Epoch:0047, train_loss=1.43053, train_acc=0.98643, val_loss=2.00934, val_acc=0.96168, time=1.03700
Epoch:0048, train_loss=1.42997, train_acc=0.98683, val_loss=2.00930, val_acc=0.95803, time=1.04301
Epoch:0049, train_loss=1.42942, train_acc=0.98643, val_loss=2.00927, val_acc=0.95985, time=1.18301
Epoch:0050, train_loss=1.42887, train_acc=0.98724, val_loss=2.00923, val_acc=0.95985, time=1.14701
Epoch:0051, train_loss=1.42833, train_acc=0.98805, val_loss=2.00919, val_acc=0.96168, time=1.01600
Epoch:0052, train_loss=1.42780, train_acc=0.98886, val_loss=2.00915, val_acc=0.96715, time=1.12902
Epoch:0053, train_loss=1.42731, train_acc=0.98926, val_loss=2.00913, val_acc=0.96715, time=1.09701
Epoch:0054, train_loss=1.42686, train_acc=0.99007, val_loss=2.00911, val_acc=0.96715, time=1.03500
Epoch:0055, train_loss=1.42644, train_acc=0.99007, val_loss=2.00910, val_acc=0.96533, time=1.20601
Epoch:0056, train_loss=1.42604, train_acc=0.99068, val_loss=2.00910, val_acc=0.96533, time=1.07900
Epoch:0057, train_loss=1.42563, train_acc=0.99129, val_loss=2.00910, val_acc=0.96533, time=1.00401
Epoch:0058, train_loss=1.42523, train_acc=0.99190, val_loss=2.00911, val_acc=0.96350, time=1.04300
Epoch:0059, train_loss=1.42483, train_acc=0.99190, val_loss=2.00911, val_acc=0.96350, time=1.14301
Epoch:0060, train_loss=1.42445, train_acc=0.99230, val_loss=2.00912, val_acc=0.96350, time=0.96201
Epoch:0061, train_loss=1.42410, train_acc=0.99251, val_loss=2.00913, val_acc=0.96350, time=1.10900
Early stopping...

Optimization Finished!

Test set results: loss= 1.79826, accuracy= 0.97305, time= 0.37800

Test Precision, Recall and F1-Score...
              precision    recall  f1-score   support

           0     0.8804    0.9310    0.9050        87
           1     0.9817    0.9917    0.9867      1083
           2     0.9840    0.9698    0.9768       696
           3     1.0000    1.0000    1.0000        10
           4     0.9250    0.9867    0.9548        75
           5     0.9593    0.9752    0.9672       121
           6     0.9655    0.7778    0.8615        36
           7     0.9333    0.8642    0.8974        81

    accuracy                         0.9730      2189
   macro avg     0.9537    0.9371    0.9437      2189
weighted avg     0.9733    0.9730    0.9728      2189


Macro average Test Precision, Recall and F1-Score...
(0.9536648037181924, 0.937050050817116, 0.9436972730324784, None)

Micro average Test Precision, Recall and F1-Score...
(0.9730470534490635, 0.9730470534490635, 0.9730470534490635, None)

Embeddings:
Word_embeddings:7688
Train_doc_embeddings:5485
Test_doc_embeddings:2189
