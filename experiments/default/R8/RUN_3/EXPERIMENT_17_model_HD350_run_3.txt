
==========: 64595223278600
Epoch:0001, train_loss=2.07062, train_acc=0.06846, val_loss=2.05953, val_acc=0.68796, time=1.25101
Epoch:0002, train_loss=1.90497, train_acc=0.69597, val_loss=2.04788, val_acc=0.72445, time=1.36501
Epoch:0003, train_loss=1.80315, train_acc=0.73061, val_loss=2.04060, val_acc=0.76825, time=1.17300
Epoch:0004, train_loss=1.73977, train_acc=0.76038, val_loss=2.03560, val_acc=0.80109, time=1.07201
Epoch:0005, train_loss=1.69621, train_acc=0.78064, val_loss=2.03170, val_acc=0.80657, time=1.13500
Epoch:0006, train_loss=1.66172, train_acc=0.78975, val_loss=2.02840, val_acc=0.81204, time=1.11502
Epoch:0007, train_loss=1.63165, train_acc=0.79806, val_loss=2.02555, val_acc=0.83029, time=1.28600
Epoch:0008, train_loss=1.60504, train_acc=0.82378, val_loss=2.02317, val_acc=0.86314, time=1.10701
Epoch:0009, train_loss=1.58250, train_acc=0.85173, val_loss=2.02128, val_acc=0.87956, time=1.11500
Epoch:0010, train_loss=1.56432, train_acc=0.87401, val_loss=2.01977, val_acc=0.89234, time=1.00400
Epoch:0011, train_loss=1.54967, train_acc=0.89407, val_loss=2.01852, val_acc=0.90693, time=1.14499
Epoch:0012, train_loss=1.53738, train_acc=0.90804, val_loss=2.01746, val_acc=0.91241, time=1.23703
Epoch:0013, train_loss=1.52664, train_acc=0.91979, val_loss=2.01654, val_acc=0.92701, time=1.14299
Epoch:0014, train_loss=1.51703, train_acc=0.92749, val_loss=2.01575, val_acc=0.93248, time=1.15899
Epoch:0015, train_loss=1.50838, train_acc=0.93741, val_loss=2.01508, val_acc=0.94161, time=1.00501
Epoch:0016, train_loss=1.50071, train_acc=0.94410, val_loss=2.01453, val_acc=0.94526, time=1.07201
Epoch:0017, train_loss=1.49412, train_acc=0.94956, val_loss=2.01408, val_acc=0.93613, time=0.99101
Epoch:0018, train_loss=1.48856, train_acc=0.94916, val_loss=2.01371, val_acc=0.92883, time=1.07701
Epoch:0019, train_loss=1.48369, train_acc=0.95078, val_loss=2.01334, val_acc=0.92701, time=1.10900
Epoch:0020, train_loss=1.47905, train_acc=0.95382, val_loss=2.01298, val_acc=0.92883, time=1.17501
Epoch:0021, train_loss=1.47445, train_acc=0.95645, val_loss=2.01260, val_acc=0.93796, time=1.20201
Epoch:0022, train_loss=1.46997, train_acc=0.95888, val_loss=2.01223, val_acc=0.93978, time=1.09901
Epoch:0023, train_loss=1.46577, train_acc=0.96030, val_loss=2.01188, val_acc=0.94526, time=1.25100
Epoch:0024, train_loss=1.46196, train_acc=0.96273, val_loss=2.01154, val_acc=0.94891, time=1.19101
Epoch:0025, train_loss=1.45858, train_acc=0.96496, val_loss=2.01123, val_acc=0.95073, time=1.10602
Epoch:0026, train_loss=1.45561, train_acc=0.96779, val_loss=2.01097, val_acc=0.95255, time=1.06598
Epoch:0027, train_loss=1.45299, train_acc=0.96941, val_loss=2.01074, val_acc=0.95803, time=1.25701
Epoch:0028, train_loss=1.45067, train_acc=0.97164, val_loss=2.01057, val_acc=0.95985, time=1.00201
Epoch:0029, train_loss=1.44859, train_acc=0.97266, val_loss=2.01043, val_acc=0.96168, time=1.08002
Epoch:0030, train_loss=1.44669, train_acc=0.97428, val_loss=2.01033, val_acc=0.96168, time=1.23102
Epoch:0031, train_loss=1.44496, train_acc=0.97650, val_loss=2.01025, val_acc=0.96168, time=1.04701
Epoch:0032, train_loss=1.44337, train_acc=0.97812, val_loss=2.01020, val_acc=0.95985, time=1.04701
Epoch:0033, train_loss=1.44191, train_acc=0.97995, val_loss=2.01014, val_acc=0.95438, time=1.03200
Epoch:0034, train_loss=1.44055, train_acc=0.97995, val_loss=2.01009, val_acc=0.95438, time=1.15103
Epoch:0035, train_loss=1.43924, train_acc=0.98096, val_loss=2.01003, val_acc=0.95438, time=1.08600
Epoch:0036, train_loss=1.43797, train_acc=0.98157, val_loss=2.00996, val_acc=0.95438, time=1.17000
Epoch:0037, train_loss=1.43676, train_acc=0.98218, val_loss=2.00988, val_acc=0.95255, time=1.24902
Epoch:0038, train_loss=1.43565, train_acc=0.98258, val_loss=2.00981, val_acc=0.95620, time=1.13400
Epoch:0039, train_loss=1.43466, train_acc=0.98319, val_loss=2.00974, val_acc=0.95620, time=1.06500
Epoch:0040, train_loss=1.43379, train_acc=0.98420, val_loss=2.00968, val_acc=0.95803, time=1.19601
Epoch:0041, train_loss=1.43301, train_acc=0.98420, val_loss=2.00962, val_acc=0.95620, time=1.02901
Epoch:0042, train_loss=1.43226, train_acc=0.98440, val_loss=2.00956, val_acc=0.95620, time=1.25601
Epoch:0043, train_loss=1.43151, train_acc=0.98521, val_loss=2.00951, val_acc=0.95620, time=1.18701
Epoch:0044, train_loss=1.43077, train_acc=0.98643, val_loss=2.00948, val_acc=0.95803, time=1.18101
Epoch:0045, train_loss=1.43006, train_acc=0.98704, val_loss=2.00945, val_acc=0.95803, time=1.21000
Epoch:0046, train_loss=1.42943, train_acc=0.98764, val_loss=2.00943, val_acc=0.95620, time=1.16000
Epoch:0047, train_loss=1.42885, train_acc=0.98825, val_loss=2.00942, val_acc=0.95438, time=1.20700
Epoch:0048, train_loss=1.42831, train_acc=0.98906, val_loss=2.00940, val_acc=0.95803, time=1.14201
Epoch:0049, train_loss=1.42780, train_acc=0.98886, val_loss=2.00939, val_acc=0.95803, time=1.28701
Epoch:0050, train_loss=1.42730, train_acc=0.98926, val_loss=2.00936, val_acc=0.95803, time=1.24000
Epoch:0051, train_loss=1.42681, train_acc=0.98906, val_loss=2.00934, val_acc=0.95985, time=1.09701
Epoch:0052, train_loss=1.42634, train_acc=0.98967, val_loss=2.00932, val_acc=0.96168, time=1.02899
Epoch:0053, train_loss=1.42590, train_acc=0.99068, val_loss=2.00931, val_acc=0.96350, time=1.11700
Epoch:0054, train_loss=1.42549, train_acc=0.99089, val_loss=2.00930, val_acc=0.96350, time=1.08401
Epoch:0055, train_loss=1.42508, train_acc=0.99089, val_loss=2.00930, val_acc=0.96350, time=1.18200
Epoch:0056, train_loss=1.42469, train_acc=0.99190, val_loss=2.00930, val_acc=0.96350, time=1.17901
Epoch:0057, train_loss=1.42432, train_acc=0.99210, val_loss=2.00930, val_acc=0.96350, time=1.21399
Epoch:0058, train_loss=1.42396, train_acc=0.99230, val_loss=2.00931, val_acc=0.96168, time=1.07702
Epoch:0059, train_loss=1.42362, train_acc=0.99291, val_loss=2.00933, val_acc=0.95985, time=1.16801
Early stopping...

Optimization Finished!

Test set results: loss= 1.79912, accuracy= 0.97031, time= 0.34900

Test Precision, Recall and F1-Score...
              precision    recall  f1-score   support

           0     0.8542    0.9425    0.8962        87
           1     0.9826    0.9917    0.9871      1083
           2     0.9826    0.9713    0.9769       696
           3     1.0000    1.0000    1.0000        10
           4     0.8916    0.9867    0.9367        75
           5     0.9508    0.9587    0.9547       121
           6     0.9615    0.6944    0.8065        36
           7     0.9437    0.8272    0.8816        81

    accuracy                         0.9703      2189
   macro avg     0.9459    0.9216    0.9300      2189
weighted avg     0.9709    0.9703    0.9699      2189


Macro average Test Precision, Recall and F1-Score...
(0.9458659785225627, 0.9215540181286852, 0.9299572200456405, None)

Micro average Test Precision, Recall and F1-Score...
(0.970306075833714, 0.970306075833714, 0.970306075833714, None)

Embeddings:
Word_embeddings:7688
Train_doc_embeddings:5485
Test_doc_embeddings:2189
