
==========: 63943607497500
Epoch:0001, train_loss=2.08533, train_acc=0.05449, val_loss=2.05970, val_acc=0.69708, time=1.13501
Epoch:0002, train_loss=1.91169, train_acc=0.68827, val_loss=2.04721, val_acc=0.76460, time=1.26602
Epoch:0003, train_loss=1.80325, train_acc=0.74316, val_loss=2.03979, val_acc=0.79380, time=1.02901
Epoch:0004, train_loss=1.73865, train_acc=0.77010, val_loss=2.03499, val_acc=0.80109, time=1.12500
Epoch:0005, train_loss=1.69630, train_acc=0.78023, val_loss=2.03129, val_acc=0.80657, time=1.10800
Epoch:0006, train_loss=1.66287, train_acc=0.78631, val_loss=2.02808, val_acc=0.81752, time=1.02202
Epoch:0007, train_loss=1.63308, train_acc=0.79927, val_loss=2.02521, val_acc=0.83029, time=1.03099
Epoch:0008, train_loss=1.60613, train_acc=0.82297, val_loss=2.02275, val_acc=0.85401, time=1.02100
Epoch:0009, train_loss=1.58278, train_acc=0.84525, val_loss=2.02076, val_acc=0.88321, time=1.10900
Epoch:0010, train_loss=1.56353, train_acc=0.87016, val_loss=2.01917, val_acc=0.89599, time=0.96700
Epoch:0011, train_loss=1.54786, train_acc=0.88860, val_loss=2.01788, val_acc=0.91606, time=1.15100
Epoch:0012, train_loss=1.53480, train_acc=0.90196, val_loss=2.01682, val_acc=0.92153, time=1.09301
Epoch:0013, train_loss=1.52354, train_acc=0.91513, val_loss=2.01593, val_acc=0.92701, time=1.37401
Epoch:0014, train_loss=1.51366, train_acc=0.92506, val_loss=2.01520, val_acc=0.93431, time=1.22100
Epoch:0015, train_loss=1.50512, train_acc=0.93842, val_loss=2.01462, val_acc=0.93796, time=1.25301
Epoch:0016, train_loss=1.49798, train_acc=0.94410, val_loss=2.01416, val_acc=0.94161, time=1.04198
Epoch:0017, train_loss=1.49212, train_acc=0.94653, val_loss=2.01379, val_acc=0.93978, time=1.38402
Epoch:0018, train_loss=1.48714, train_acc=0.94875, val_loss=2.01345, val_acc=0.93978, time=1.12701
Epoch:0019, train_loss=1.48252, train_acc=0.94977, val_loss=2.01311, val_acc=0.93796, time=1.02500
Epoch:0020, train_loss=1.47795, train_acc=0.95179, val_loss=2.01276, val_acc=0.93978, time=1.29302
Epoch:0021, train_loss=1.47346, train_acc=0.95645, val_loss=2.01243, val_acc=0.93978, time=1.01501
Epoch:0022, train_loss=1.46922, train_acc=0.95888, val_loss=2.01212, val_acc=0.94161, time=1.09401
Epoch:0023, train_loss=1.46537, train_acc=0.96111, val_loss=2.01182, val_acc=0.94708, time=1.04000
Epoch:0024, train_loss=1.46191, train_acc=0.96253, val_loss=2.01154, val_acc=0.94891, time=1.10700
Epoch:0025, train_loss=1.45876, train_acc=0.96476, val_loss=2.01126, val_acc=0.94891, time=1.11403
Epoch:0026, train_loss=1.45584, train_acc=0.96779, val_loss=2.01100, val_acc=0.95073, time=1.16801
Epoch:0027, train_loss=1.45311, train_acc=0.96941, val_loss=2.01076, val_acc=0.94891, time=1.13701
Epoch:0028, train_loss=1.45061, train_acc=0.97083, val_loss=2.01055, val_acc=0.95073, time=1.06798
Epoch:0029, train_loss=1.44837, train_acc=0.97266, val_loss=2.01038, val_acc=0.95255, time=1.10801
Epoch:0030, train_loss=1.44643, train_acc=0.97529, val_loss=2.01025, val_acc=0.95255, time=1.15401
Epoch:0031, train_loss=1.44476, train_acc=0.97691, val_loss=2.01015, val_acc=0.95438, time=1.07500
Epoch:0032, train_loss=1.44330, train_acc=0.97893, val_loss=2.01008, val_acc=0.95620, time=0.96203
Epoch:0033, train_loss=1.44198, train_acc=0.98096, val_loss=2.01001, val_acc=0.95620, time=1.20499
Epoch:0034, train_loss=1.44074, train_acc=0.98116, val_loss=2.00994, val_acc=0.95438, time=1.02802
Epoch:0035, train_loss=1.43955, train_acc=0.98218, val_loss=2.00988, val_acc=0.95620, time=1.10601
Epoch:0036, train_loss=1.43839, train_acc=0.98319, val_loss=2.00981, val_acc=0.95620, time=0.99101
Epoch:0037, train_loss=1.43727, train_acc=0.98400, val_loss=2.00974, val_acc=0.95620, time=1.16200
Epoch:0038, train_loss=1.43620, train_acc=0.98501, val_loss=2.00968, val_acc=0.95803, time=1.21002
Epoch:0039, train_loss=1.43519, train_acc=0.98461, val_loss=2.00961, val_acc=0.95803, time=1.04001
Epoch:0040, train_loss=1.43424, train_acc=0.98481, val_loss=2.00955, val_acc=0.95803, time=1.12201
Epoch:0041, train_loss=1.43335, train_acc=0.98461, val_loss=2.00949, val_acc=0.95985, time=1.17600
Epoch:0042, train_loss=1.43252, train_acc=0.98521, val_loss=2.00944, val_acc=0.96168, time=1.16701
Epoch:0043, train_loss=1.43175, train_acc=0.98582, val_loss=2.00940, val_acc=0.95803, time=1.37602
Epoch:0044, train_loss=1.43103, train_acc=0.98663, val_loss=2.00936, val_acc=0.95985, time=1.25701
Epoch:0045, train_loss=1.43036, train_acc=0.98724, val_loss=2.00933, val_acc=0.95985, time=1.12100
Epoch:0046, train_loss=1.42974, train_acc=0.98744, val_loss=2.00931, val_acc=0.96168, time=1.07202
Epoch:0047, train_loss=1.42916, train_acc=0.98764, val_loss=2.00930, val_acc=0.95985, time=1.19600
Epoch:0048, train_loss=1.42862, train_acc=0.98845, val_loss=2.00929, val_acc=0.95985, time=1.15202
Epoch:0049, train_loss=1.42811, train_acc=0.98866, val_loss=2.00928, val_acc=0.95985, time=0.99100
Epoch:0050, train_loss=1.42763, train_acc=0.98906, val_loss=2.00927, val_acc=0.95985, time=0.97701
Epoch:0051, train_loss=1.42717, train_acc=0.98947, val_loss=2.00927, val_acc=0.95985, time=1.06301
Epoch:0052, train_loss=1.42671, train_acc=0.98987, val_loss=2.00926, val_acc=0.95985, time=1.13002
Epoch:0053, train_loss=1.42626, train_acc=0.99068, val_loss=2.00925, val_acc=0.95985, time=1.21501
Epoch:0054, train_loss=1.42582, train_acc=0.99089, val_loss=2.00924, val_acc=0.95985, time=1.11401
Epoch:0055, train_loss=1.42538, train_acc=0.99190, val_loss=2.00923, val_acc=0.96168, time=1.36801
Epoch:0056, train_loss=1.42497, train_acc=0.99230, val_loss=2.00922, val_acc=0.96168, time=1.12000
Epoch:0057, train_loss=1.42457, train_acc=0.99230, val_loss=2.00921, val_acc=0.96168, time=1.04202
Epoch:0058, train_loss=1.42420, train_acc=0.99230, val_loss=2.00921, val_acc=0.96168, time=1.15701
Epoch:0059, train_loss=1.42385, train_acc=0.99291, val_loss=2.00920, val_acc=0.96168, time=1.23700
Epoch:0060, train_loss=1.42352, train_acc=0.99372, val_loss=2.00920, val_acc=0.96168, time=1.02102
Epoch:0061, train_loss=1.42320, train_acc=0.99392, val_loss=2.00919, val_acc=0.96168, time=1.03900
Epoch:0062, train_loss=1.42289, train_acc=0.99392, val_loss=2.00919, val_acc=0.96168, time=1.18102
Epoch:0063, train_loss=1.42260, train_acc=0.99453, val_loss=2.00918, val_acc=0.96350, time=1.04300
Epoch:0064, train_loss=1.42231, train_acc=0.99453, val_loss=2.00918, val_acc=0.96350, time=1.15401
Epoch:0065, train_loss=1.42203, train_acc=0.99453, val_loss=2.00918, val_acc=0.96350, time=1.11900
Epoch:0066, train_loss=1.42177, train_acc=0.99453, val_loss=2.00919, val_acc=0.96350, time=1.22402
Epoch:0067, train_loss=1.42151, train_acc=0.99473, val_loss=2.00919, val_acc=0.96350, time=1.15200
Epoch:0068, train_loss=1.42126, train_acc=0.99494, val_loss=2.00919, val_acc=0.96350, time=1.18602
Epoch:0069, train_loss=1.42102, train_acc=0.99514, val_loss=2.00919, val_acc=0.96350, time=1.00401
Early stopping...

Optimization Finished!

Test set results: loss= 1.79905, accuracy= 0.96985, time= 0.36400

Test Precision, Recall and F1-Score...
              precision    recall  f1-score   support

           0     0.8710    0.9310    0.9000        87
           1     0.9799    0.9917    0.9858      1083
           2     0.9839    0.9670    0.9754       696
           3     1.0000    1.0000    1.0000        10
           4     0.8916    0.9867    0.9367        75
           5     0.9440    0.9752    0.9593       121
           6     0.9615    0.6944    0.8065        36
           7     0.9444    0.8395    0.8889        81

    accuracy                         0.9698      2189
   macro avg     0.9470    0.9232    0.9316      2189
weighted avg     0.9703    0.9698    0.9694      2189


Macro average Test Precision, Recall and F1-Score...
(0.947045256116609, 0.923187768995064, 0.9315668206735441, None)

Micro average Test Precision, Recall and F1-Score...
(0.9698492462311558, 0.9698492462311558, 0.9698492462311558, None)

Embeddings:
Word_embeddings:7688
Train_doc_embeddings:5485
Test_doc_embeddings:2189
