
==========: 52951153153000
Epoch:0001, train_loss=2.02264, train_acc=0.45696, val_loss=2.04774, val_acc=0.78832, time=1.29101
Epoch:0002, train_loss=1.82327, train_acc=0.76484, val_loss=2.03475, val_acc=0.76642, time=1.09101
Epoch:0003, train_loss=1.68678, train_acc=0.77031, val_loss=2.02131, val_acc=0.87774, time=1.00401
Epoch:0004, train_loss=1.56760, train_acc=0.85477, val_loss=2.01844, val_acc=0.89599, time=1.03302
Epoch:0005, train_loss=1.54117, train_acc=0.88556, val_loss=2.01701, val_acc=0.91423, time=1.03500
Epoch:0006, train_loss=1.52547, train_acc=0.91108, val_loss=2.01568, val_acc=0.92701, time=1.05101
Epoch:0007, train_loss=1.50960, train_acc=0.92404, val_loss=2.01448, val_acc=0.93613, time=1.07100
Epoch:0008, train_loss=1.49468, train_acc=0.93903, val_loss=2.01337, val_acc=0.93613, time=1.10401
Epoch:0009, train_loss=1.48095, train_acc=0.94875, val_loss=2.01255, val_acc=0.94708, time=1.00801
Epoch:0010, train_loss=1.47024, train_acc=0.95321, val_loss=2.01203, val_acc=0.94161, time=1.08701
Epoch:0011, train_loss=1.46250, train_acc=0.95645, val_loss=2.01174, val_acc=0.94161, time=1.08900
Epoch:0012, train_loss=1.45710, train_acc=0.95969, val_loss=2.01150, val_acc=0.93796, time=1.28400
Epoch:0013, train_loss=1.45282, train_acc=0.96172, val_loss=2.01117, val_acc=0.94343, time=1.11801
Epoch:0014, train_loss=1.44846, train_acc=0.96273, val_loss=2.01072, val_acc=0.94343, time=1.07001
Epoch:0015, train_loss=1.44392, train_acc=0.96860, val_loss=2.01026, val_acc=0.94708, time=1.08901
Epoch:0016, train_loss=1.43977, train_acc=0.97286, val_loss=2.00987, val_acc=0.95073, time=1.10701
Epoch:0017, train_loss=1.43647, train_acc=0.97711, val_loss=2.00959, val_acc=0.95985, time=1.17601
Epoch:0018, train_loss=1.43396, train_acc=0.97914, val_loss=2.00941, val_acc=0.95803, time=1.21600
Epoch:0019, train_loss=1.43204, train_acc=0.98157, val_loss=2.00934, val_acc=0.95620, time=1.15402
Epoch:0020, train_loss=1.43051, train_acc=0.98299, val_loss=2.00933, val_acc=0.95620, time=1.00401
Epoch:0021, train_loss=1.42927, train_acc=0.98359, val_loss=2.00935, val_acc=0.95803, time=1.12000
Epoch:0022, train_loss=1.42819, train_acc=0.98461, val_loss=2.00937, val_acc=0.95803, time=1.26701
Epoch:0023, train_loss=1.42711, train_acc=0.98683, val_loss=2.00937, val_acc=0.95803, time=1.12800
Epoch:0024, train_loss=1.42600, train_acc=0.98704, val_loss=2.00937, val_acc=0.95985, time=1.15001
Epoch:0025, train_loss=1.42484, train_acc=0.98805, val_loss=2.00937, val_acc=0.95985, time=1.22001
Epoch:0026, train_loss=1.42363, train_acc=0.98906, val_loss=2.00938, val_acc=0.96168, time=1.19400
Epoch:0027, train_loss=1.42242, train_acc=0.99109, val_loss=2.00940, val_acc=0.96168, time=1.07900
Early stopping...

Optimization Finished!

Test set results: loss= 1.79992, accuracy= 0.97031, time= 0.37198

Test Precision, Recall and F1-Score...
              precision    recall  f1-score   support

           0     0.8764    0.8966    0.8864        87
           1     0.9862    0.9898    0.9880      1083
           2     0.9854    0.9727    0.9790       696
           3     0.8333    1.0000    0.9091        10
           4     0.8902    0.9733    0.9299        75
           5     0.9365    0.9752    0.9555       121
           6     0.9286    0.7222    0.8125        36
           7     0.8974    0.8642    0.8805        81

    accuracy                         0.9703      2189
   macro avg     0.9168    0.9243    0.9176      2189
weighted avg     0.9706    0.9703    0.9701      2189


Macro average Test Precision, Recall and F1-Score...
(0.9167676879863312, 0.9242569500221764, 0.9176136384868415, None)

Micro average Test Precision, Recall and F1-Score...
(0.970306075833714, 0.970306075833714, 0.970306075833714, None)

Embeddings:
Word_embeddings:7688
Train_doc_embeddings:5485
Test_doc_embeddings:2189
