
==========: 37413303736000
Epoch:0001, train_loss=2.06141, train_acc=0.41928, val_loss=2.05849, val_acc=0.52007, time=1.28401
Epoch:0002, train_loss=1.89616, train_acc=0.53454, val_loss=2.04562, val_acc=0.67701, time=1.22101
Epoch:0003, train_loss=1.78771, train_acc=0.67187, val_loss=2.03804, val_acc=0.76460, time=1.14302
Epoch:0004, train_loss=1.72423, train_acc=0.74033, val_loss=2.03346, val_acc=0.78832, time=1.18600
Epoch:0005, train_loss=1.68475, train_acc=0.76038, val_loss=2.03010, val_acc=0.79745, time=1.06000
Epoch:0006, train_loss=1.65381, train_acc=0.76889, val_loss=2.02724, val_acc=0.80657, time=1.16101
Epoch:0007, train_loss=1.62607, train_acc=0.78246, val_loss=2.02476, val_acc=0.83212, time=1.11700
Epoch:0008, train_loss=1.60131, train_acc=0.81082, val_loss=2.02271, val_acc=0.86496, time=1.11101
Epoch:0009, train_loss=1.58026, train_acc=0.85112, val_loss=2.02103, val_acc=0.88504, time=1.13300
Epoch:0010, train_loss=1.56299, train_acc=0.87563, val_loss=2.01965, val_acc=0.89781, time=1.04401
Epoch:0011, train_loss=1.54880, train_acc=0.89123, val_loss=2.01846, val_acc=0.90693, time=1.12500
Epoch:0012, train_loss=1.53674, train_acc=0.90784, val_loss=2.01742, val_acc=0.91971, time=1.21201
Epoch:0013, train_loss=1.52625, train_acc=0.92141, val_loss=2.01651, val_acc=0.92883, time=1.02699
Epoch:0014, train_loss=1.51708, train_acc=0.93032, val_loss=2.01572, val_acc=0.93431, time=1.14201
Epoch:0015, train_loss=1.50907, train_acc=0.93802, val_loss=2.01503, val_acc=0.94161, time=1.17000
Epoch:0016, train_loss=1.50199, train_acc=0.93964, val_loss=2.01441, val_acc=0.94161, time=1.18000
Epoch:0017, train_loss=1.49563, train_acc=0.94369, val_loss=2.01387, val_acc=0.93978, time=1.16501
Epoch:0018, train_loss=1.48982, train_acc=0.94956, val_loss=2.01337, val_acc=0.94708, time=1.11400
Epoch:0019, train_loss=1.48443, train_acc=0.95139, val_loss=2.01293, val_acc=0.94708, time=1.28300
Epoch:0020, train_loss=1.47940, train_acc=0.95240, val_loss=2.01254, val_acc=0.94891, time=1.30200
Epoch:0021, train_loss=1.47472, train_acc=0.95544, val_loss=2.01220, val_acc=0.95255, time=1.19000
Epoch:0022, train_loss=1.47044, train_acc=0.95989, val_loss=2.01190, val_acc=0.95073, time=1.05300
Epoch:0023, train_loss=1.46657, train_acc=0.96233, val_loss=2.01164, val_acc=0.94161, time=1.25300
Epoch:0024, train_loss=1.46307, train_acc=0.96374, val_loss=2.01140, val_acc=0.94708, time=1.28901
Epoch:0025, train_loss=1.45987, train_acc=0.96293, val_loss=2.01116, val_acc=0.94891, time=1.12801
Epoch:0026, train_loss=1.45690, train_acc=0.96476, val_loss=2.01094, val_acc=0.94891, time=1.21801
Epoch:0027, train_loss=1.45414, train_acc=0.96597, val_loss=2.01073, val_acc=0.94891, time=1.04501
Epoch:0028, train_loss=1.45157, train_acc=0.96840, val_loss=2.01053, val_acc=0.95073, time=1.00100
Epoch:0029, train_loss=1.44921, train_acc=0.97002, val_loss=2.01035, val_acc=0.95255, time=1.11802
Epoch:0030, train_loss=1.44707, train_acc=0.97205, val_loss=2.01020, val_acc=0.95620, time=0.97700
Epoch:0031, train_loss=1.44515, train_acc=0.97488, val_loss=2.01007, val_acc=0.95255, time=1.14402
Epoch:0032, train_loss=1.44346, train_acc=0.97792, val_loss=2.00997, val_acc=0.95255, time=1.08001
Epoch:0033, train_loss=1.44199, train_acc=0.97893, val_loss=2.00990, val_acc=0.95255, time=1.19701
Epoch:0034, train_loss=1.44070, train_acc=0.97995, val_loss=2.00984, val_acc=0.95438, time=1.08100
Epoch:0035, train_loss=1.43954, train_acc=0.98055, val_loss=2.00980, val_acc=0.95438, time=1.05501
Epoch:0036, train_loss=1.43845, train_acc=0.98137, val_loss=2.00976, val_acc=0.95438, time=1.18801
Epoch:0037, train_loss=1.43736, train_acc=0.98197, val_loss=2.00972, val_acc=0.95255, time=1.10300
Epoch:0038, train_loss=1.43628, train_acc=0.98278, val_loss=2.00967, val_acc=0.95073, time=1.21001
Epoch:0039, train_loss=1.43523, train_acc=0.98278, val_loss=2.00963, val_acc=0.94891, time=1.07901
Epoch:0040, train_loss=1.43424, train_acc=0.98319, val_loss=2.00958, val_acc=0.94891, time=1.01000
Epoch:0041, train_loss=1.43332, train_acc=0.98299, val_loss=2.00953, val_acc=0.94891, time=1.08701
Epoch:0042, train_loss=1.43246, train_acc=0.98420, val_loss=2.00947, val_acc=0.95073, time=1.12301
Epoch:0043, train_loss=1.43167, train_acc=0.98481, val_loss=2.00942, val_acc=0.95255, time=1.18700
Epoch:0044, train_loss=1.43093, train_acc=0.98582, val_loss=2.00937, val_acc=0.95073, time=1.02601
Epoch:0045, train_loss=1.43023, train_acc=0.98623, val_loss=2.00932, val_acc=0.95073, time=1.16000
Epoch:0046, train_loss=1.42959, train_acc=0.98663, val_loss=2.00929, val_acc=0.95803, time=1.12900
Epoch:0047, train_loss=1.42898, train_acc=0.98704, val_loss=2.00926, val_acc=0.95803, time=1.07201
Epoch:0048, train_loss=1.42840, train_acc=0.98825, val_loss=2.00925, val_acc=0.95803, time=1.10902
Epoch:0049, train_loss=1.42785, train_acc=0.98866, val_loss=2.00924, val_acc=0.95803, time=1.13900
Epoch:0050, train_loss=1.42733, train_acc=0.98906, val_loss=2.00924, val_acc=0.95803, time=1.11202
Epoch:0051, train_loss=1.42683, train_acc=0.98987, val_loss=2.00925, val_acc=0.95803, time=1.34200
Epoch:0052, train_loss=1.42636, train_acc=0.99007, val_loss=2.00925, val_acc=0.95803, time=1.17401
Epoch:0053, train_loss=1.42592, train_acc=0.99048, val_loss=2.00926, val_acc=0.95803, time=1.05900
Epoch:0054, train_loss=1.42549, train_acc=0.99089, val_loss=2.00926, val_acc=0.95620, time=1.07000
Epoch:0055, train_loss=1.42508, train_acc=0.99129, val_loss=2.00925, val_acc=0.95620, time=1.07802
Epoch:0056, train_loss=1.42467, train_acc=0.99149, val_loss=2.00924, val_acc=0.95620, time=1.11700
Epoch:0057, train_loss=1.42428, train_acc=0.99149, val_loss=2.00922, val_acc=0.95620, time=1.06099
Epoch:0058, train_loss=1.42390, train_acc=0.99210, val_loss=2.00921, val_acc=0.95985, time=1.24501
Epoch:0059, train_loss=1.42355, train_acc=0.99251, val_loss=2.00920, val_acc=0.95985, time=1.02800
Epoch:0060, train_loss=1.42321, train_acc=0.99271, val_loss=2.00919, val_acc=0.95985, time=1.38900
Epoch:0061, train_loss=1.42289, train_acc=0.99332, val_loss=2.00919, val_acc=0.95985, time=1.19502
Epoch:0062, train_loss=1.42258, train_acc=0.99352, val_loss=2.00919, val_acc=0.95985, time=1.11401
Epoch:0063, train_loss=1.42228, train_acc=0.99433, val_loss=2.00920, val_acc=0.95985, time=1.06701
Epoch:0064, train_loss=1.42199, train_acc=0.99453, val_loss=2.00921, val_acc=0.95985, time=1.17402
Epoch:0065, train_loss=1.42171, train_acc=0.99453, val_loss=2.00921, val_acc=0.95985, time=1.05800
Early stopping...

Optimization Finished!

Test set results: loss= 1.79853, accuracy= 0.97168, time= 0.40700

Test Precision, Recall and F1-Score...
              precision    recall  f1-score   support

           0     0.8696    0.9195    0.8939        87
           1     0.9835    0.9908    0.9871      1083
           2     0.9826    0.9727    0.9776       696
           3     1.0000    1.0000    1.0000        10
           4     0.9136    0.9867    0.9487        75
           5     0.9444    0.9835    0.9636       121
           6     1.0000    0.6944    0.8197        36
           7     0.9200    0.8519    0.8846        81

    accuracy                         0.9717      2189
   macro avg     0.9517    0.9249    0.9344      2189
weighted avg     0.9721    0.9717    0.9713      2189


Macro average Test Precision, Recall and F1-Score...
(0.9517093422395403, 0.9249302257889787, 0.934395101227489, None)

Micro average Test Precision, Recall and F1-Score...
(0.9716765646413887, 0.9716765646413887, 0.9716765646413887, None)

Embeddings:
Word_embeddings:7688
Train_doc_embeddings:5485
Test_doc_embeddings:2189
