
==========: 38197837243100
Epoch:0001, train_loss=2.07014, train_acc=0.35831, val_loss=2.04892, val_acc=0.75182, time=1.23802
Epoch:0002, train_loss=1.83109, train_acc=0.74154, val_loss=2.03349, val_acc=0.80292, time=1.28000
Epoch:0003, train_loss=1.69636, train_acc=0.77112, val_loss=2.02233, val_acc=0.87226, time=1.17201
Epoch:0004, train_loss=1.58276, train_acc=0.86470, val_loss=2.02027, val_acc=0.89964, time=1.13401
Epoch:0005, train_loss=1.55957, train_acc=0.88414, val_loss=2.01769, val_acc=0.91423, time=1.14501
Epoch:0006, train_loss=1.53135, train_acc=0.90521, val_loss=2.01561, val_acc=0.93613, time=1.08000
Epoch:0007, train_loss=1.50842, train_acc=0.93073, val_loss=2.01490, val_acc=0.93248, time=1.07800
Epoch:0008, train_loss=1.49986, train_acc=0.93194, val_loss=2.01407, val_acc=0.93066, time=1.08001
Epoch:0009, train_loss=1.48964, train_acc=0.93599, val_loss=2.01313, val_acc=0.93248, time=1.01701
Epoch:0010, train_loss=1.47821, train_acc=0.94835, val_loss=2.01252, val_acc=0.94526, time=1.02901
Epoch:0011, train_loss=1.47035, train_acc=0.95220, val_loss=2.01220, val_acc=0.95255, time=1.00601
Epoch:0012, train_loss=1.46540, train_acc=0.95260, val_loss=2.01192, val_acc=0.94891, time=1.10101
Epoch:0013, train_loss=1.46087, train_acc=0.95463, val_loss=2.01152, val_acc=0.95255, time=1.03701
Epoch:0014, train_loss=1.45547, train_acc=0.95888, val_loss=2.01097, val_acc=0.94891, time=1.12902
Epoch:0015, train_loss=1.44939, train_acc=0.96415, val_loss=2.01042, val_acc=0.94708, time=1.01300
Epoch:0016, train_loss=1.44384, train_acc=0.97002, val_loss=2.01003, val_acc=0.94526, time=1.14302
Epoch:0017, train_loss=1.43991, train_acc=0.97428, val_loss=2.00981, val_acc=0.95255, time=1.22400
Epoch:0018, train_loss=1.43760, train_acc=0.97367, val_loss=2.00969, val_acc=0.95255, time=1.20901
Epoch:0019, train_loss=1.43610, train_acc=0.97468, val_loss=2.00958, val_acc=0.95255, time=1.14801
Epoch:0020, train_loss=1.43455, train_acc=0.97610, val_loss=2.00946, val_acc=0.95255, time=1.15201
Epoch:0021, train_loss=1.43261, train_acc=0.97974, val_loss=2.00934, val_acc=0.95803, time=1.04898
Epoch:0022, train_loss=1.43065, train_acc=0.98278, val_loss=2.00930, val_acc=0.96533, time=1.12501
Epoch:0023, train_loss=1.42924, train_acc=0.98481, val_loss=2.00937, val_acc=0.96533, time=0.98500
Epoch:0024, train_loss=1.42852, train_acc=0.98542, val_loss=2.00948, val_acc=0.96715, time=1.01201
Early stopping...

Optimization Finished!

Test set results: loss= 1.80092, accuracy= 0.97031, time= 0.32300

Test Precision, Recall and F1-Score...
              precision    recall  f1-score   support

           0     0.9487    0.8506    0.8970        87
           1     0.9844    0.9917    0.9880      1083
           2     0.9868    0.9684    0.9775       696
           3     0.6667    1.0000    0.8000        10
           4     0.9012    0.9733    0.9359        75
           5     0.9213    0.9669    0.9435       121
           6     0.9615    0.6944    0.8065        36
           7     0.8750    0.9506    0.9112        81

    accuracy                         0.9703      2189
   macro avg     0.9057    0.9245    0.9075      2189
weighted avg     0.9715    0.9703    0.9701      2189


Macro average Test Precision, Recall and F1-Score...
(0.9057072866154403, 0.9244990598028282, 0.9074587695981551, None)

Micro average Test Precision, Recall and F1-Score...
(0.970306075833714, 0.970306075833714, 0.970306075833714, None)

Embeddings:
Word_embeddings:7688
Train_doc_embeddings:5485
Test_doc_embeddings:2189
