
==========: 254895949741700
Epoch:0001, train_loss=2.10722, train_acc=0.20478, val_loss=2.06172, val_acc=0.55839, time=1.25001
Epoch:0002, train_loss=1.93266, train_acc=0.52846, val_loss=2.04916, val_acc=0.74818, time=1.20100
Epoch:0003, train_loss=1.81899, train_acc=0.74073, val_loss=2.04142, val_acc=0.76460, time=1.27601
Epoch:0004, train_loss=1.74827, train_acc=0.75957, val_loss=2.03609, val_acc=0.78102, time=1.23601
Epoch:0005, train_loss=1.70008, train_acc=0.77557, val_loss=2.03175, val_acc=0.81022, time=1.22700
Epoch:0006, train_loss=1.66156, train_acc=0.79137, val_loss=2.02807, val_acc=0.82847, time=1.40300
Epoch:0007, train_loss=1.62895, train_acc=0.80778, val_loss=2.02503, val_acc=0.83942, time=1.15299
Epoch:0008, train_loss=1.60173, train_acc=0.83127, val_loss=2.02264, val_acc=0.87044, time=1.18600
Epoch:0009, train_loss=1.57961, train_acc=0.85031, val_loss=2.02081, val_acc=0.88869, time=1.20603
Epoch:0010, train_loss=1.56206, train_acc=0.86571, val_loss=2.01936, val_acc=0.89416, time=1.20901
Epoch:0011, train_loss=1.54777, train_acc=0.88191, val_loss=2.01815, val_acc=0.90876, time=1.20402
Epoch:0012, train_loss=1.53542, train_acc=0.89670, val_loss=2.01710, val_acc=0.91606, time=1.19301
Epoch:0013, train_loss=1.52441, train_acc=0.91209, val_loss=2.01620, val_acc=0.92336, time=1.17200
Epoch:0014, train_loss=1.51468, train_acc=0.92526, val_loss=2.01544, val_acc=0.93066, time=1.11501
Epoch:0015, train_loss=1.50630, train_acc=0.93539, val_loss=2.01481, val_acc=0.93431, time=1.15601
Epoch:0016, train_loss=1.49910, train_acc=0.94146, val_loss=2.01425, val_acc=0.93796, time=1.19301
Epoch:0017, train_loss=1.49274, train_acc=0.94511, val_loss=2.01375, val_acc=0.93613, time=1.28501
Epoch:0018, train_loss=1.48699, train_acc=0.94977, val_loss=2.01330, val_acc=0.94343, time=1.14501
Epoch:0019, train_loss=1.48172, train_acc=0.95321, val_loss=2.01289, val_acc=0.94343, time=1.25001
Epoch:0020, train_loss=1.47688, train_acc=0.95422, val_loss=2.01253, val_acc=0.94161, time=1.09202
Epoch:0021, train_loss=1.47239, train_acc=0.95584, val_loss=2.01221, val_acc=0.94343, time=1.22701
Epoch:0022, train_loss=1.46828, train_acc=0.95767, val_loss=2.01195, val_acc=0.94343, time=1.06300
Epoch:0023, train_loss=1.46459, train_acc=0.96111, val_loss=2.01173, val_acc=0.94526, time=1.26201
Epoch:0024, train_loss=1.46136, train_acc=0.96435, val_loss=2.01154, val_acc=0.94891, time=1.32900
Epoch:0025, train_loss=1.45850, train_acc=0.96557, val_loss=2.01136, val_acc=0.95073, time=1.25701
Epoch:0026, train_loss=1.45589, train_acc=0.96739, val_loss=2.01117, val_acc=0.95255, time=1.21601
Epoch:0027, train_loss=1.45341, train_acc=0.96901, val_loss=2.01097, val_acc=0.94708, time=1.20801
Epoch:0028, train_loss=1.45100, train_acc=0.97083, val_loss=2.01077, val_acc=0.95073, time=1.21700
Epoch:0029, train_loss=1.44871, train_acc=0.97266, val_loss=2.01058, val_acc=0.95073, time=1.21203
Epoch:0030, train_loss=1.44658, train_acc=0.97468, val_loss=2.01041, val_acc=0.95255, time=1.18899
Epoch:0031, train_loss=1.44468, train_acc=0.97731, val_loss=2.01026, val_acc=0.95438, time=1.21601
Epoch:0032, train_loss=1.44305, train_acc=0.97934, val_loss=2.01015, val_acc=0.95620, time=1.20801
Epoch:0033, train_loss=1.44167, train_acc=0.98076, val_loss=2.01007, val_acc=0.95803, time=1.18199
Epoch:0034, train_loss=1.44049, train_acc=0.98076, val_loss=2.01000, val_acc=0.95620, time=1.15601
Epoch:0035, train_loss=1.43945, train_acc=0.98177, val_loss=2.00994, val_acc=0.95438, time=1.16899
Epoch:0036, train_loss=1.43845, train_acc=0.98258, val_loss=2.00989, val_acc=0.95438, time=1.17401
Epoch:0037, train_loss=1.43744, train_acc=0.98299, val_loss=2.00983, val_acc=0.95438, time=1.25301
Epoch:0038, train_loss=1.43640, train_acc=0.98359, val_loss=2.00977, val_acc=0.95438, time=1.19601
Epoch:0039, train_loss=1.43535, train_acc=0.98380, val_loss=2.00972, val_acc=0.95620, time=1.24500
Epoch:0040, train_loss=1.43435, train_acc=0.98359, val_loss=2.00966, val_acc=0.95438, time=1.28199
Epoch:0041, train_loss=1.43341, train_acc=0.98461, val_loss=2.00962, val_acc=0.95438, time=1.28100
Epoch:0042, train_loss=1.43255, train_acc=0.98481, val_loss=2.00957, val_acc=0.95438, time=1.20701
Epoch:0043, train_loss=1.43176, train_acc=0.98481, val_loss=2.00953, val_acc=0.95438, time=1.20901
Epoch:0044, train_loss=1.43104, train_acc=0.98542, val_loss=2.00949, val_acc=0.95438, time=1.24702
Epoch:0045, train_loss=1.43038, train_acc=0.98582, val_loss=2.00946, val_acc=0.95438, time=1.21401
Epoch:0046, train_loss=1.42975, train_acc=0.98663, val_loss=2.00944, val_acc=0.95438, time=1.15300
Epoch:0047, train_loss=1.42917, train_acc=0.98704, val_loss=2.00942, val_acc=0.95438, time=1.17099
Epoch:0048, train_loss=1.42861, train_acc=0.98704, val_loss=2.00941, val_acc=0.95620, time=1.20699
Epoch:0049, train_loss=1.42809, train_acc=0.98764, val_loss=2.00941, val_acc=0.95620, time=1.15800
Epoch:0050, train_loss=1.42759, train_acc=0.98825, val_loss=2.00942, val_acc=0.95803, time=1.32901
Epoch:0051, train_loss=1.42711, train_acc=0.98845, val_loss=2.00942, val_acc=0.95620, time=1.19001
Epoch:0052, train_loss=1.42666, train_acc=0.98947, val_loss=2.00943, val_acc=0.95620, time=1.24001
Epoch:0053, train_loss=1.42623, train_acc=0.99007, val_loss=2.00943, val_acc=0.95620, time=1.24599
Epoch:0054, train_loss=1.42581, train_acc=0.99089, val_loss=2.00943, val_acc=0.95803, time=1.17401
Epoch:0055, train_loss=1.42540, train_acc=0.99109, val_loss=2.00942, val_acc=0.95803, time=1.28000
Epoch:0056, train_loss=1.42499, train_acc=0.99190, val_loss=2.00940, val_acc=0.95803, time=1.19902
Epoch:0057, train_loss=1.42459, train_acc=0.99230, val_loss=2.00938, val_acc=0.96168, time=1.14201
Epoch:0058, train_loss=1.42420, train_acc=0.99291, val_loss=2.00936, val_acc=0.96168, time=1.20900
Epoch:0059, train_loss=1.42384, train_acc=0.99332, val_loss=2.00934, val_acc=0.96350, time=1.19802
Epoch:0060, train_loss=1.42349, train_acc=0.99332, val_loss=2.00932, val_acc=0.96350, time=1.12799
Epoch:0061, train_loss=1.42316, train_acc=0.99392, val_loss=2.00931, val_acc=0.96350, time=1.30901
Epoch:0062, train_loss=1.42285, train_acc=0.99392, val_loss=2.00930, val_acc=0.96350, time=1.21000
Epoch:0063, train_loss=1.42256, train_acc=0.99433, val_loss=2.00931, val_acc=0.96350, time=1.14501
Epoch:0064, train_loss=1.42227, train_acc=0.99433, val_loss=2.00931, val_acc=0.96350, time=1.20600
Epoch:0065, train_loss=1.42199, train_acc=0.99453, val_loss=2.00932, val_acc=0.96350, time=1.17001
Epoch:0066, train_loss=1.42172, train_acc=0.99453, val_loss=2.00933, val_acc=0.96350, time=1.19701
Epoch:0067, train_loss=1.42146, train_acc=0.99453, val_loss=2.00934, val_acc=0.96350, time=1.17101
Early stopping...

Optimization Finished!

Test set results: loss= 1.79933, accuracy= 0.96985, time= 0.34499

Test Precision, Recall and F1-Score...
              precision    recall  f1-score   support

           0     0.8791    0.9195    0.8989        87
           1     0.9790    0.9917    0.9853      1083
           2     0.9839    0.9655    0.9746       696
           3     1.0000    1.0000    1.0000        10
           4     0.8706    0.9867    0.9250        75
           5     0.9520    0.9835    0.9675       121
           6     1.0000    0.6944    0.8197        36
           7     0.9452    0.8519    0.8961        81

    accuracy                         0.9698      2189
   macro avg     0.9512    0.9241    0.9334      2189
weighted avg     0.9706    0.9698    0.9695      2189


Macro average Test Precision, Recall and F1-Score...
(0.9512303631175472, 0.9241476574125022, 0.9333840621000125, None)

Micro average Test Precision, Recall and F1-Score...
(0.9698492462311558, 0.9698492462311558, 0.9698492462311558, None)

Embeddings:
Word_embeddings:7688
Train_doc_embeddings:5485
Test_doc_embeddings:2189
