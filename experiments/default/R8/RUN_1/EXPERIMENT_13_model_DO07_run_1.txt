
==========: 37576435685700
Epoch:0001, train_loss=2.11771, train_acc=0.04659, val_loss=2.06452, val_acc=0.54745, time=1.31202
Epoch:0002, train_loss=1.94399, train_acc=0.56816, val_loss=2.05101, val_acc=0.68978, time=1.35802
Epoch:0003, train_loss=1.82830, train_acc=0.68240, val_loss=2.04250, val_acc=0.76825, time=0.97101
Epoch:0004, train_loss=1.75655, train_acc=0.74316, val_loss=2.03738, val_acc=0.79197, time=1.03700
Epoch:0005, train_loss=1.71346, train_acc=0.76767, val_loss=2.03392, val_acc=0.79562, time=1.08801
Epoch:0006, train_loss=1.68338, train_acc=0.77962, val_loss=2.03106, val_acc=0.80474, time=1.13501
Epoch:0007, train_loss=1.65735, train_acc=0.78610, val_loss=2.02841, val_acc=0.81934, time=1.09803
Epoch:0008, train_loss=1.63240, train_acc=0.79583, val_loss=2.02591, val_acc=0.82664, time=1.08900
Epoch:0009, train_loss=1.60871, train_acc=0.81345, val_loss=2.02368, val_acc=0.84489, time=1.13500
Epoch:0010, train_loss=1.58736, train_acc=0.83816, val_loss=2.02177, val_acc=0.87226, time=1.02402
Epoch:0011, train_loss=1.56917, train_acc=0.86388, val_loss=2.02020, val_acc=0.89781, time=1.02401
Epoch:0012, train_loss=1.55413, train_acc=0.88536, val_loss=2.01891, val_acc=0.91058, time=1.03401
Epoch:0013, train_loss=1.54160, train_acc=0.90136, val_loss=2.01783, val_acc=0.91971, time=1.20900
Epoch:0014, train_loss=1.53087, train_acc=0.91513, val_loss=2.01689, val_acc=0.93066, time=1.19501
Epoch:0015, train_loss=1.52139, train_acc=0.92404, val_loss=2.01609, val_acc=0.93431, time=1.27002
Epoch:0016, train_loss=1.51283, train_acc=0.93215, val_loss=2.01538, val_acc=0.93978, time=1.06700
Epoch:0017, train_loss=1.50510, train_acc=0.94045, val_loss=2.01479, val_acc=0.94343, time=1.04001
Epoch:0018, train_loss=1.49824, train_acc=0.94693, val_loss=2.01431, val_acc=0.94343, time=1.19201
Epoch:0019, train_loss=1.49240, train_acc=0.94936, val_loss=2.01393, val_acc=0.94161, time=1.25401
Epoch:0020, train_loss=1.48753, train_acc=0.95118, val_loss=2.01361, val_acc=0.93613, time=1.09401
Epoch:0021, train_loss=1.48328, train_acc=0.95200, val_loss=2.01330, val_acc=0.93248, time=1.28601
Epoch:0022, train_loss=1.47915, train_acc=0.95321, val_loss=2.01296, val_acc=0.93431, time=1.09600
Epoch:0023, train_loss=1.47482, train_acc=0.95524, val_loss=2.01260, val_acc=0.94161, time=1.01501
Epoch:0024, train_loss=1.47033, train_acc=0.95787, val_loss=2.01223, val_acc=0.94343, time=1.10201
Epoch:0025, train_loss=1.46596, train_acc=0.95929, val_loss=2.01188, val_acc=0.94343, time=1.00900
Epoch:0026, train_loss=1.46201, train_acc=0.96172, val_loss=2.01156, val_acc=0.94526, time=1.01400
Epoch:0027, train_loss=1.45864, train_acc=0.96496, val_loss=2.01130, val_acc=0.95073, time=1.13401
Epoch:0028, train_loss=1.45582, train_acc=0.96678, val_loss=2.01107, val_acc=0.95255, time=1.07901
Epoch:0029, train_loss=1.45341, train_acc=0.96901, val_loss=2.01087, val_acc=0.95620, time=0.99601
Epoch:0030, train_loss=1.45128, train_acc=0.97144, val_loss=2.01071, val_acc=0.95620, time=0.99501
Epoch:0031, train_loss=1.44932, train_acc=0.97225, val_loss=2.01057, val_acc=0.95620, time=1.14501
Epoch:0032, train_loss=1.44749, train_acc=0.97569, val_loss=2.01045, val_acc=0.95620, time=1.20701
Epoch:0033, train_loss=1.44581, train_acc=0.97630, val_loss=2.01035, val_acc=0.95438, time=1.14099
Epoch:0034, train_loss=1.44427, train_acc=0.97711, val_loss=2.01027, val_acc=0.95620, time=1.03802
Epoch:0035, train_loss=1.44287, train_acc=0.97792, val_loss=2.01019, val_acc=0.95620, time=1.07800
Epoch:0036, train_loss=1.44157, train_acc=0.97853, val_loss=2.01011, val_acc=0.95620, time=1.04102
Epoch:0037, train_loss=1.44031, train_acc=0.97954, val_loss=2.01002, val_acc=0.95620, time=1.08801
Epoch:0038, train_loss=1.43907, train_acc=0.98055, val_loss=2.00992, val_acc=0.95803, time=0.96700
Epoch:0039, train_loss=1.43783, train_acc=0.98116, val_loss=2.00982, val_acc=0.95803, time=1.06401
Epoch:0040, train_loss=1.43665, train_acc=0.98238, val_loss=2.00973, val_acc=0.95620, time=1.09201
Epoch:0041, train_loss=1.43556, train_acc=0.98238, val_loss=2.00964, val_acc=0.95620, time=1.18700
Epoch:0042, train_loss=1.43461, train_acc=0.98299, val_loss=2.00958, val_acc=0.95438, time=1.04101
Epoch:0043, train_loss=1.43380, train_acc=0.98359, val_loss=2.00953, val_acc=0.95620, time=1.03301
Epoch:0044, train_loss=1.43310, train_acc=0.98481, val_loss=2.00949, val_acc=0.95438, time=1.11001
Epoch:0045, train_loss=1.43246, train_acc=0.98481, val_loss=2.00946, val_acc=0.95438, time=1.11301
Epoch:0046, train_loss=1.43182, train_acc=0.98501, val_loss=2.00944, val_acc=0.95438, time=1.28300
Epoch:0047, train_loss=1.43118, train_acc=0.98582, val_loss=2.00942, val_acc=0.95438, time=1.06201
Epoch:0048, train_loss=1.43054, train_acc=0.98602, val_loss=2.00940, val_acc=0.95803, time=0.97901
Epoch:0049, train_loss=1.42993, train_acc=0.98643, val_loss=2.00939, val_acc=0.95803, time=1.15201
Epoch:0050, train_loss=1.42937, train_acc=0.98764, val_loss=2.00938, val_acc=0.95803, time=1.04501
Epoch:0051, train_loss=1.42884, train_acc=0.98845, val_loss=2.00937, val_acc=0.95620, time=0.98300
Epoch:0052, train_loss=1.42833, train_acc=0.98926, val_loss=2.00935, val_acc=0.95620, time=1.07701
Epoch:0053, train_loss=1.42783, train_acc=0.98967, val_loss=2.00933, val_acc=0.95620, time=1.02601
Epoch:0054, train_loss=1.42734, train_acc=0.99028, val_loss=2.00930, val_acc=0.95620, time=1.15101
Epoch:0055, train_loss=1.42688, train_acc=0.99028, val_loss=2.00927, val_acc=0.95803, time=1.17601
Epoch:0056, train_loss=1.42644, train_acc=0.99028, val_loss=2.00925, val_acc=0.95985, time=1.11099
Epoch:0057, train_loss=1.42603, train_acc=0.99028, val_loss=2.00924, val_acc=0.95803, time=1.14201
Epoch:0058, train_loss=1.42564, train_acc=0.99068, val_loss=2.00923, val_acc=0.95985, time=1.33299
Epoch:0059, train_loss=1.42526, train_acc=0.99109, val_loss=2.00922, val_acc=0.96168, time=1.08501
Epoch:0060, train_loss=1.42489, train_acc=0.99149, val_loss=2.00922, val_acc=0.96168, time=1.04602
Epoch:0061, train_loss=1.42454, train_acc=0.99230, val_loss=2.00923, val_acc=0.96168, time=1.39401
Epoch:0062, train_loss=1.42420, train_acc=0.99230, val_loss=2.00924, val_acc=0.96168, time=1.27499
Epoch:0063, train_loss=1.42388, train_acc=0.99251, val_loss=2.00924, val_acc=0.95985, time=1.29802
Epoch:0064, train_loss=1.42357, train_acc=0.99332, val_loss=2.00925, val_acc=0.95985, time=1.24800
Early stopping...

Optimization Finished!

Test set results: loss= 1.79878, accuracy= 0.97031, time= 0.30602

Test Precision, Recall and F1-Score...
              precision    recall  f1-score   support

           0     0.8804    0.9310    0.9050        87
           1     0.9791    0.9926    0.9858      1083
           2     0.9825    0.9670    0.9747       696
           3     1.0000    1.0000    1.0000        10
           4     0.9012    0.9733    0.9359        75
           5     0.9440    0.9752    0.9593       121
           6     1.0000    0.6944    0.8197        36
           7     0.9452    0.8519    0.8961        81

    accuracy                         0.9703      2189
   macro avg     0.9541    0.9232    0.9346      2189
weighted avg     0.9708    0.9703    0.9699      2189


Macro average Test Precision, Recall and F1-Score...
(0.9540511756377401, 0.9231797323342111, 0.9345616715614753, None)

Micro average Test Precision, Recall and F1-Score...
(0.970306075833714, 0.970306075833714, 0.970306075833714, None)

Embeddings:
Word_embeddings:7688
Train_doc_embeddings:5485
Test_doc_embeddings:2189
