
==========: 37873386446200
Epoch:0001, train_loss=2.12916, train_acc=0.05671, val_loss=2.06318, val_acc=0.66788, time=1.37201
Epoch:0002, train_loss=1.94046, train_acc=0.66498, val_loss=2.04970, val_acc=0.72810, time=1.10801
Epoch:0003, train_loss=1.82237, train_acc=0.71825, val_loss=2.04165, val_acc=0.77737, time=1.20301
Epoch:0004, train_loss=1.75300, train_acc=0.75714, val_loss=2.03650, val_acc=0.79380, time=1.02201
Epoch:0005, train_loss=1.70932, train_acc=0.77577, val_loss=2.03279, val_acc=0.80109, time=1.19100
Epoch:0006, train_loss=1.67734, train_acc=0.78428, val_loss=2.02970, val_acc=0.80657, time=0.97099
Epoch:0007, train_loss=1.64950, train_acc=0.79380, val_loss=2.02691, val_acc=0.82482, time=0.95000
Epoch:0008, train_loss=1.62328, train_acc=0.80373, val_loss=2.02443, val_acc=0.83942, time=1.07100
Epoch:0009, train_loss=1.59915, train_acc=0.82155, val_loss=2.02233, val_acc=0.86679, time=1.03403
Epoch:0010, train_loss=1.57828, train_acc=0.85376, val_loss=2.02064, val_acc=0.89964, time=1.20399
Epoch:0011, train_loss=1.56117, train_acc=0.88232, val_loss=2.01931, val_acc=0.91058, time=0.96100
Epoch:0012, train_loss=1.54743, train_acc=0.89690, val_loss=2.01821, val_acc=0.91788, time=0.99000
Epoch:0013, train_loss=1.53611, train_acc=0.90824, val_loss=2.01727, val_acc=0.92518, time=1.10399
Epoch:0014, train_loss=1.52630, train_acc=0.91736, val_loss=2.01643, val_acc=0.93066, time=1.01801
Epoch:0015, train_loss=1.51739, train_acc=0.92566, val_loss=2.01566, val_acc=0.93248, time=1.15802
Epoch:0016, train_loss=1.50904, train_acc=0.93316, val_loss=2.01495, val_acc=0.93796, time=1.25400
Epoch:0017, train_loss=1.50123, train_acc=0.94004, val_loss=2.01432, val_acc=0.93613, time=1.20701
Epoch:0018, train_loss=1.49407, train_acc=0.94673, val_loss=2.01378, val_acc=0.93978, time=0.98699
Epoch:0019, train_loss=1.48777, train_acc=0.95260, val_loss=2.01335, val_acc=0.95073, time=1.14602
Epoch:0020, train_loss=1.48244, train_acc=0.95584, val_loss=2.01302, val_acc=0.94891, time=1.26501
Epoch:0021, train_loss=1.47804, train_acc=0.95645, val_loss=2.01275, val_acc=0.94526, time=0.96501
Epoch:0022, train_loss=1.47426, train_acc=0.95848, val_loss=2.01250, val_acc=0.94343, time=1.07401
Epoch:0023, train_loss=1.47068, train_acc=0.96050, val_loss=2.01225, val_acc=0.94343, time=1.01100
Epoch:0024, train_loss=1.46703, train_acc=0.96233, val_loss=2.01198, val_acc=0.94343, time=1.02502
Epoch:0025, train_loss=1.46332, train_acc=0.96293, val_loss=2.01170, val_acc=0.94343, time=1.02903
Epoch:0026, train_loss=1.45974, train_acc=0.96476, val_loss=2.01143, val_acc=0.95073, time=1.05901
Epoch:0027, train_loss=1.45650, train_acc=0.96860, val_loss=2.01120, val_acc=0.95255, time=1.09603
Epoch:0028, train_loss=1.45373, train_acc=0.97144, val_loss=2.01099, val_acc=0.95073, time=1.00300
Epoch:0029, train_loss=1.45142, train_acc=0.97387, val_loss=2.01080, val_acc=0.94891, time=1.09900
Epoch:0030, train_loss=1.44947, train_acc=0.97387, val_loss=2.01063, val_acc=0.94891, time=1.07102
Epoch:0031, train_loss=1.44774, train_acc=0.97387, val_loss=2.01048, val_acc=0.94891, time=1.14200
Epoch:0032, train_loss=1.44614, train_acc=0.97448, val_loss=2.01033, val_acc=0.95255, time=1.05801
Epoch:0033, train_loss=1.44460, train_acc=0.97590, val_loss=2.01019, val_acc=0.95438, time=1.08101
Epoch:0034, train_loss=1.44309, train_acc=0.97691, val_loss=2.01006, val_acc=0.95438, time=1.06701
Epoch:0035, train_loss=1.44160, train_acc=0.97853, val_loss=2.00995, val_acc=0.95803, time=1.01501
Epoch:0036, train_loss=1.44016, train_acc=0.97934, val_loss=2.00986, val_acc=0.95803, time=0.99501
Epoch:0037, train_loss=1.43881, train_acc=0.98015, val_loss=2.00979, val_acc=0.95985, time=1.17501
Epoch:0038, train_loss=1.43756, train_acc=0.98157, val_loss=2.00974, val_acc=0.95438, time=1.09200
Epoch:0039, train_loss=1.43644, train_acc=0.98258, val_loss=2.00970, val_acc=0.95620, time=1.03601
Epoch:0040, train_loss=1.43544, train_acc=0.98278, val_loss=2.00968, val_acc=0.95803, time=1.21402
Epoch:0041, train_loss=1.43452, train_acc=0.98339, val_loss=2.00965, val_acc=0.95620, time=1.18900
Epoch:0042, train_loss=1.43369, train_acc=0.98380, val_loss=2.00963, val_acc=0.95620, time=1.09603
Epoch:0043, train_loss=1.43291, train_acc=0.98501, val_loss=2.00960, val_acc=0.95803, time=1.11499
Epoch:0044, train_loss=1.43217, train_acc=0.98602, val_loss=2.00957, val_acc=0.95803, time=1.05401
Epoch:0045, train_loss=1.43145, train_acc=0.98602, val_loss=2.00953, val_acc=0.95803, time=0.95100
Epoch:0046, train_loss=1.43075, train_acc=0.98643, val_loss=2.00949, val_acc=0.95803, time=0.99300
Epoch:0047, train_loss=1.43008, train_acc=0.98764, val_loss=2.00945, val_acc=0.95803, time=1.03901
Epoch:0048, train_loss=1.42946, train_acc=0.98764, val_loss=2.00941, val_acc=0.95803, time=0.98001
Epoch:0049, train_loss=1.42887, train_acc=0.98825, val_loss=2.00938, val_acc=0.95803, time=1.10201
Epoch:0050, train_loss=1.42833, train_acc=0.98805, val_loss=2.00935, val_acc=0.95803, time=1.09001
Epoch:0051, train_loss=1.42781, train_acc=0.98866, val_loss=2.00933, val_acc=0.95803, time=0.98001
Epoch:0052, train_loss=1.42732, train_acc=0.98906, val_loss=2.00932, val_acc=0.96168, time=1.17099
Epoch:0053, train_loss=1.42684, train_acc=0.99048, val_loss=2.00931, val_acc=0.95803, time=1.05401
Epoch:0054, train_loss=1.42637, train_acc=0.99129, val_loss=2.00930, val_acc=0.95803, time=1.07103
Epoch:0055, train_loss=1.42592, train_acc=0.99170, val_loss=2.00930, val_acc=0.95803, time=1.14000
Epoch:0056, train_loss=1.42550, train_acc=0.99129, val_loss=2.00930, val_acc=0.95803, time=1.14801
Epoch:0057, train_loss=1.42509, train_acc=0.99170, val_loss=2.00929, val_acc=0.95803, time=1.14901
Epoch:0058, train_loss=1.42471, train_acc=0.99149, val_loss=2.00929, val_acc=0.95803, time=1.11901
Epoch:0059, train_loss=1.42433, train_acc=0.99190, val_loss=2.00928, val_acc=0.95803, time=1.06802
Epoch:0060, train_loss=1.42398, train_acc=0.99291, val_loss=2.00928, val_acc=0.95985, time=0.98300
Epoch:0061, train_loss=1.42363, train_acc=0.99332, val_loss=2.00927, val_acc=0.95985, time=1.14000
Epoch:0062, train_loss=1.42330, train_acc=0.99352, val_loss=2.00926, val_acc=0.95985, time=1.06100
Epoch:0063, train_loss=1.42299, train_acc=0.99372, val_loss=2.00926, val_acc=0.95985, time=1.09100
Epoch:0064, train_loss=1.42269, train_acc=0.99413, val_loss=2.00926, val_acc=0.95985, time=1.03401
Epoch:0065, train_loss=1.42240, train_acc=0.99453, val_loss=2.00926, val_acc=0.95985, time=1.09001
Epoch:0066, train_loss=1.42212, train_acc=0.99453, val_loss=2.00927, val_acc=0.95985, time=1.20601
Epoch:0067, train_loss=1.42186, train_acc=0.99453, val_loss=2.00927, val_acc=0.95985, time=1.05400
Epoch:0068, train_loss=1.42160, train_acc=0.99453, val_loss=2.00928, val_acc=0.95803, time=1.06001
Early stopping...

Optimization Finished!

Test set results: loss= 1.79847, accuracy= 0.97031, time= 0.34299

Test Precision, Recall and F1-Score...
              precision    recall  f1-score   support

           0     0.8804    0.9310    0.9050        87
           1     0.9799    0.9917    0.9858      1083
           2     0.9839    0.9684    0.9761       696
           3     1.0000    1.0000    1.0000        10
           4     0.9012    0.9733    0.9359        75
           5     0.9512    0.9669    0.9590       121
           6     0.9286    0.7222    0.8125        36
           7     0.9324    0.8519    0.8903        81

    accuracy                         0.9703      2189
   macro avg     0.9447    0.9257    0.9331      2189
weighted avg     0.9705    0.9703    0.9700      2189


Macro average Test Precision, Recall and F1-Score...
(0.94472016710595, 0.9256830742770725, 0.9330802382141021, None)

Micro average Test Precision, Recall and F1-Score...
(0.970306075833714, 0.970306075833714, 0.970306075833714, None)

Embeddings:
Word_embeddings:7688
Train_doc_embeddings:5485
Test_doc_embeddings:2189
