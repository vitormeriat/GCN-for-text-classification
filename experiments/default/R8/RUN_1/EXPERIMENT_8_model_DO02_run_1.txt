
==========: 37177505081700
Epoch:0001, train_loss=2.08812, train_acc=0.09216, val_loss=2.05967, val_acc=0.74270, time=1.40502
Epoch:0002, train_loss=1.90941, train_acc=0.70812, val_loss=2.04658, val_acc=0.78650, time=1.10399
Epoch:0003, train_loss=1.79639, train_acc=0.75795, val_loss=2.03888, val_acc=0.79745, time=1.18000
Epoch:0004, train_loss=1.72943, train_acc=0.77496, val_loss=2.03405, val_acc=0.80657, time=1.04801
Epoch:0005, train_loss=1.68665, train_acc=0.78509, val_loss=2.03044, val_acc=0.80839, time=1.03701
Epoch:0006, train_loss=1.65382, train_acc=0.79157, val_loss=2.02734, val_acc=0.81934, time=1.07301
Epoch:0007, train_loss=1.62519, train_acc=0.80494, val_loss=2.02463, val_acc=0.84489, time=1.03001
Epoch:0008, train_loss=1.59985, train_acc=0.83350, val_loss=2.02237, val_acc=0.87226, time=1.05801
Epoch:0009, train_loss=1.57855, train_acc=0.85619, val_loss=2.02056, val_acc=0.89051, time=1.19001
Epoch:0010, train_loss=1.56132, train_acc=0.86915, val_loss=2.01908, val_acc=0.90328, time=1.00600
Epoch:0011, train_loss=1.54708, train_acc=0.88171, val_loss=2.01784, val_acc=0.91241, time=1.11301
Epoch:0012, train_loss=1.53482, train_acc=0.89872, val_loss=2.01681, val_acc=0.92153, time=1.19700
Epoch:0013, train_loss=1.52422, train_acc=0.91189, val_loss=2.01598, val_acc=0.92883, time=0.98201
Epoch:0014, train_loss=1.51521, train_acc=0.92465, val_loss=2.01530, val_acc=0.93796, time=1.08201
Epoch:0015, train_loss=1.50751, train_acc=0.93356, val_loss=2.01472, val_acc=0.94343, time=1.23402
Epoch:0016, train_loss=1.50061, train_acc=0.93620, val_loss=2.01418, val_acc=0.93248, time=1.11000
Epoch:0017, train_loss=1.49413, train_acc=0.94065, val_loss=2.01369, val_acc=0.93796, time=1.07100
Epoch:0018, train_loss=1.48807, train_acc=0.94673, val_loss=2.01324, val_acc=0.93796, time=1.00201
Epoch:0019, train_loss=1.48264, train_acc=0.95362, val_loss=2.01286, val_acc=0.94161, time=1.07800
Epoch:0020, train_loss=1.47793, train_acc=0.95645, val_loss=2.01254, val_acc=0.94526, time=1.24700
Epoch:0021, train_loss=1.47379, train_acc=0.95746, val_loss=2.01225, val_acc=0.94526, time=1.05199
Epoch:0022, train_loss=1.46997, train_acc=0.95868, val_loss=2.01199, val_acc=0.94708, time=1.24401
Epoch:0023, train_loss=1.46635, train_acc=0.96091, val_loss=2.01175, val_acc=0.95255, time=1.21100
Epoch:0024, train_loss=1.46297, train_acc=0.96334, val_loss=2.01153, val_acc=0.95073, time=1.01400
Epoch:0025, train_loss=1.45987, train_acc=0.96638, val_loss=2.01134, val_acc=0.95255, time=1.06002
Epoch:0026, train_loss=1.45703, train_acc=0.96779, val_loss=2.01115, val_acc=0.94891, time=1.03800
Epoch:0027, train_loss=1.45442, train_acc=0.97022, val_loss=2.01097, val_acc=0.94891, time=1.01501
Epoch:0028, train_loss=1.45200, train_acc=0.97185, val_loss=2.01079, val_acc=0.94891, time=1.15501
Epoch:0029, train_loss=1.44978, train_acc=0.97164, val_loss=2.01063, val_acc=0.95255, time=1.03101
Epoch:0030, train_loss=1.44774, train_acc=0.97407, val_loss=2.01048, val_acc=0.95620, time=0.98100
Epoch:0031, train_loss=1.44587, train_acc=0.97549, val_loss=2.01035, val_acc=0.95438, time=1.12202
Epoch:0032, train_loss=1.44416, train_acc=0.97691, val_loss=2.01022, val_acc=0.95620, time=0.97400
Epoch:0033, train_loss=1.44262, train_acc=0.97974, val_loss=2.01011, val_acc=0.95620, time=1.03100
Epoch:0034, train_loss=1.44125, train_acc=0.98177, val_loss=2.01001, val_acc=0.95803, time=1.06700
Epoch:0035, train_loss=1.44004, train_acc=0.98319, val_loss=2.00992, val_acc=0.95803, time=1.14801
Epoch:0036, train_loss=1.43896, train_acc=0.98359, val_loss=2.00985, val_acc=0.95803, time=1.16301
Epoch:0037, train_loss=1.43796, train_acc=0.98359, val_loss=2.00977, val_acc=0.95803, time=1.13801
Epoch:0038, train_loss=1.43699, train_acc=0.98359, val_loss=2.00971, val_acc=0.95803, time=1.34901
Epoch:0039, train_loss=1.43602, train_acc=0.98380, val_loss=2.00964, val_acc=0.95620, time=1.01902
Epoch:0040, train_loss=1.43505, train_acc=0.98420, val_loss=2.00959, val_acc=0.95620, time=0.98001
Epoch:0041, train_loss=1.43411, train_acc=0.98440, val_loss=2.00954, val_acc=0.95803, time=1.04701
Epoch:0042, train_loss=1.43324, train_acc=0.98481, val_loss=2.00951, val_acc=0.95620, time=1.10700
Epoch:0043, train_loss=1.43246, train_acc=0.98562, val_loss=2.00948, val_acc=0.95438, time=1.08602
Epoch:0044, train_loss=1.43175, train_acc=0.98542, val_loss=2.00945, val_acc=0.95438, time=1.13700
Epoch:0045, train_loss=1.43108, train_acc=0.98562, val_loss=2.00942, val_acc=0.95438, time=1.03999
Epoch:0046, train_loss=1.43042, train_acc=0.98663, val_loss=2.00939, val_acc=0.95438, time=0.95901
Epoch:0047, train_loss=1.42976, train_acc=0.98724, val_loss=2.00936, val_acc=0.95438, time=0.98600
Epoch:0048, train_loss=1.42914, train_acc=0.98805, val_loss=2.00933, val_acc=0.95438, time=1.22901
Epoch:0049, train_loss=1.42856, train_acc=0.98845, val_loss=2.00930, val_acc=0.96168, time=1.40401
Epoch:0050, train_loss=1.42804, train_acc=0.98906, val_loss=2.00929, val_acc=0.95985, time=1.13801
Epoch:0051, train_loss=1.42757, train_acc=0.98906, val_loss=2.00927, val_acc=0.95985, time=1.20202
Epoch:0052, train_loss=1.42713, train_acc=0.98947, val_loss=2.00927, val_acc=0.95985, time=1.14499
Epoch:0053, train_loss=1.42670, train_acc=0.98987, val_loss=2.00926, val_acc=0.95985, time=1.12501
Epoch:0054, train_loss=1.42628, train_acc=0.99007, val_loss=2.00926, val_acc=0.95985, time=1.11100
Epoch:0055, train_loss=1.42586, train_acc=0.99048, val_loss=2.00927, val_acc=0.96168, time=1.01200
Epoch:0056, train_loss=1.42546, train_acc=0.99109, val_loss=2.00927, val_acc=0.95985, time=1.13600
Epoch:0057, train_loss=1.42507, train_acc=0.99149, val_loss=2.00927, val_acc=0.95803, time=1.13900
Epoch:0058, train_loss=1.42470, train_acc=0.99170, val_loss=2.00926, val_acc=0.95803, time=1.06601
Epoch:0059, train_loss=1.42434, train_acc=0.99210, val_loss=2.00926, val_acc=0.95803, time=1.02700
Epoch:0060, train_loss=1.42399, train_acc=0.99230, val_loss=2.00925, val_acc=0.95803, time=1.11501
Epoch:0061, train_loss=1.42364, train_acc=0.99271, val_loss=2.00924, val_acc=0.95803, time=1.09300
Epoch:0062, train_loss=1.42331, train_acc=0.99311, val_loss=2.00922, val_acc=0.95985, time=1.12202
Epoch:0063, train_loss=1.42299, train_acc=0.99332, val_loss=2.00921, val_acc=0.95985, time=1.10000
Epoch:0064, train_loss=1.42269, train_acc=0.99372, val_loss=2.00921, val_acc=0.95985, time=1.17800
Epoch:0065, train_loss=1.42240, train_acc=0.99413, val_loss=2.00920, val_acc=0.95985, time=1.17301
Epoch:0066, train_loss=1.42213, train_acc=0.99413, val_loss=2.00920, val_acc=0.95985, time=1.03101
Epoch:0067, train_loss=1.42186, train_acc=0.99453, val_loss=2.00921, val_acc=0.95985, time=1.13800
Epoch:0068, train_loss=1.42161, train_acc=0.99494, val_loss=2.00921, val_acc=0.95985, time=1.06002
Epoch:0069, train_loss=1.42136, train_acc=0.99494, val_loss=2.00922, val_acc=0.95985, time=1.11500
Epoch:0070, train_loss=1.42112, train_acc=0.99514, val_loss=2.00922, val_acc=0.95985, time=1.11800
Early stopping...

Optimization Finished!

Test set results: loss= 1.79873, accuracy= 0.97031, time= 0.31701

Test Precision, Recall and F1-Score...
              precision    recall  f1-score   support

           0     0.8617    0.9310    0.8950        87
           1     0.9808    0.9917    0.9862      1083
           2     0.9840    0.9698    0.9768       696
           3     1.0000    1.0000    1.0000        10
           4     0.9012    0.9733    0.9359        75
           5     0.9516    0.9752    0.9633       121
           6     0.9630    0.7222    0.8254        36
           7     0.9306    0.8272    0.8758        81

    accuracy                         0.9703      2189
   macro avg     0.9466    0.9238    0.9323      2189
weighted avg     0.9707    0.9703    0.9700      2189


Macro average Test Precision, Recall and F1-Score...
(0.9466068812113266, 0.9238093100763753, 0.9323094040575812, None)

Micro average Test Precision, Recall and F1-Score...
(0.970306075833714, 0.970306075833714, 0.970306075833714, None)

Embeddings:
Word_embeddings:7688
Train_doc_embeddings:5485
Test_doc_embeddings:2189
