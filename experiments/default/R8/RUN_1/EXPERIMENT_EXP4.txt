
==========: 256011204777100
Epoch:0001, train_loss=2.09756, train_acc=0.10047, val_loss=2.05942, val_acc=0.75182, time=1.37300
Epoch:0002, train_loss=1.90916, train_acc=0.70812, val_loss=2.04641, val_acc=0.77372, time=1.30600
Epoch:0003, train_loss=1.79531, train_acc=0.74701, val_loss=2.03900, val_acc=0.79745, time=1.37001
Epoch:0004, train_loss=1.73121, train_acc=0.77253, val_loss=2.03453, val_acc=0.81022, time=1.30300
Epoch:0005, train_loss=1.69272, train_acc=0.78428, val_loss=2.03129, val_acc=0.81569, time=1.18300
Epoch:0006, train_loss=1.66398, train_acc=0.78712, val_loss=2.02847, val_acc=0.81752, time=1.15401
Epoch:0007, train_loss=1.63782, train_acc=0.79340, val_loss=2.02584, val_acc=0.82299, time=1.26802
Epoch:0008, train_loss=1.61275, train_acc=0.80454, val_loss=2.02346, val_acc=0.84307, time=1.33401
Epoch:0009, train_loss=1.58970, train_acc=0.83289, val_loss=2.02142, val_acc=0.87956, time=1.15299
Epoch:0010, train_loss=1.56988, train_acc=0.86267, val_loss=2.01979, val_acc=0.89964, time=1.19900
Epoch:0011, train_loss=1.55370, train_acc=0.88151, val_loss=2.01851, val_acc=0.90876, time=1.17001
Epoch:0012, train_loss=1.54069, train_acc=0.89589, val_loss=2.01749, val_acc=0.91423, time=1.14699
Epoch:0013, train_loss=1.53004, train_acc=0.90723, val_loss=2.01665, val_acc=0.92701, time=1.21002
Epoch:0014, train_loss=1.52102, train_acc=0.92303, val_loss=2.01595, val_acc=0.93613, time=1.27700
Epoch:0015, train_loss=1.51315, train_acc=0.93417, val_loss=2.01534, val_acc=0.93796, time=1.22600
Epoch:0016, train_loss=1.50607, train_acc=0.94045, val_loss=2.01479, val_acc=0.94343, time=1.18400
Epoch:0017, train_loss=1.49952, train_acc=0.94410, val_loss=2.01428, val_acc=0.94891, time=1.21001
Epoch:0018, train_loss=1.49337, train_acc=0.95017, val_loss=2.01381, val_acc=0.94526, time=1.27904
Epoch:0019, train_loss=1.48767, train_acc=0.95422, val_loss=2.01338, val_acc=0.94708, time=1.27602
Epoch:0020, train_loss=1.48242, train_acc=0.95807, val_loss=2.01299, val_acc=0.94708, time=1.22700
Epoch:0021, train_loss=1.47756, train_acc=0.96010, val_loss=2.01261, val_acc=0.94891, time=1.22600
Epoch:0022, train_loss=1.47297, train_acc=0.96050, val_loss=2.01226, val_acc=0.94891, time=1.19402
Epoch:0023, train_loss=1.46864, train_acc=0.96212, val_loss=2.01193, val_acc=0.95255, time=1.16102
Epoch:0024, train_loss=1.46462, train_acc=0.96476, val_loss=2.01163, val_acc=0.95255, time=1.21701
Epoch:0025, train_loss=1.46095, train_acc=0.96719, val_loss=2.01136, val_acc=0.95073, time=1.22902
Epoch:0026, train_loss=1.45767, train_acc=0.96719, val_loss=2.01112, val_acc=0.94891, time=1.20702
Epoch:0027, train_loss=1.45478, train_acc=0.96901, val_loss=2.01091, val_acc=0.95255, time=1.31200
Epoch:0028, train_loss=1.45223, train_acc=0.96941, val_loss=2.01074, val_acc=0.95438, time=1.22102
Epoch:0029, train_loss=1.44998, train_acc=0.97063, val_loss=2.01059, val_acc=0.95438, time=1.26902
Epoch:0030, train_loss=1.44796, train_acc=0.97448, val_loss=2.01045, val_acc=0.95438, time=1.32603
Epoch:0031, train_loss=1.44613, train_acc=0.97691, val_loss=2.01033, val_acc=0.95438, time=1.20802
Epoch:0032, train_loss=1.44446, train_acc=0.97853, val_loss=2.01022, val_acc=0.95438, time=1.27702
Epoch:0033, train_loss=1.44294, train_acc=0.98096, val_loss=2.01011, val_acc=0.95803, time=1.24700
Epoch:0034, train_loss=1.44154, train_acc=0.98015, val_loss=2.01001, val_acc=0.95985, time=1.25002
Epoch:0035, train_loss=1.44023, train_acc=0.98137, val_loss=2.00992, val_acc=0.96168, time=1.29601
Epoch:0036, train_loss=1.43900, train_acc=0.98258, val_loss=2.00983, val_acc=0.95985, time=1.17102
Epoch:0037, train_loss=1.43781, train_acc=0.98299, val_loss=2.00975, val_acc=0.96350, time=1.18799
Epoch:0038, train_loss=1.43666, train_acc=0.98339, val_loss=2.00968, val_acc=0.96168, time=1.10802
Epoch:0039, train_loss=1.43557, train_acc=0.98461, val_loss=2.00962, val_acc=0.95985, time=1.22500
Epoch:0040, train_loss=1.43455, train_acc=0.98521, val_loss=2.00957, val_acc=0.95620, time=1.15702
Epoch:0041, train_loss=1.43363, train_acc=0.98582, val_loss=2.00953, val_acc=0.95620, time=1.22601
Epoch:0042, train_loss=1.43280, train_acc=0.98562, val_loss=2.00950, val_acc=0.95620, time=1.23601
Epoch:0043, train_loss=1.43204, train_acc=0.98663, val_loss=2.00947, val_acc=0.95620, time=1.14399
Epoch:0044, train_loss=1.43132, train_acc=0.98764, val_loss=2.00944, val_acc=0.95620, time=1.13699
Epoch:0045, train_loss=1.43062, train_acc=0.98825, val_loss=2.00940, val_acc=0.95620, time=1.20301
Epoch:0046, train_loss=1.42995, train_acc=0.98866, val_loss=2.00937, val_acc=0.95803, time=1.18701
Epoch:0047, train_loss=1.42930, train_acc=0.98866, val_loss=2.00934, val_acc=0.95985, time=1.16100
Epoch:0048, train_loss=1.42871, train_acc=0.98967, val_loss=2.00931, val_acc=0.95985, time=1.11500
Epoch:0049, train_loss=1.42815, train_acc=0.98987, val_loss=2.00929, val_acc=0.96168, time=1.18800
Epoch:0050, train_loss=1.42763, train_acc=0.99007, val_loss=2.00927, val_acc=0.96168, time=1.14402
Epoch:0051, train_loss=1.42714, train_acc=0.99028, val_loss=2.00926, val_acc=0.96350, time=1.22003
Epoch:0052, train_loss=1.42667, train_acc=0.99028, val_loss=2.00924, val_acc=0.96350, time=1.24302
Epoch:0053, train_loss=1.42622, train_acc=0.99028, val_loss=2.00924, val_acc=0.96350, time=1.07800
Epoch:0054, train_loss=1.42578, train_acc=0.99068, val_loss=2.00923, val_acc=0.96533, time=1.14201
Epoch:0055, train_loss=1.42536, train_acc=0.99089, val_loss=2.00923, val_acc=0.96533, time=1.24001
Epoch:0056, train_loss=1.42497, train_acc=0.99149, val_loss=2.00923, val_acc=0.96350, time=1.24801
Epoch:0057, train_loss=1.42458, train_acc=0.99149, val_loss=2.00923, val_acc=0.96350, time=1.23501
Epoch:0058, train_loss=1.42421, train_acc=0.99210, val_loss=2.00922, val_acc=0.96350, time=1.29199
Epoch:0059, train_loss=1.42385, train_acc=0.99230, val_loss=2.00922, val_acc=0.96350, time=1.21701
Epoch:0060, train_loss=1.42350, train_acc=0.99230, val_loss=2.00921, val_acc=0.96350, time=1.15901
Epoch:0061, train_loss=1.42317, train_acc=0.99291, val_loss=2.00921, val_acc=0.96350, time=1.29000
Epoch:0062, train_loss=1.42286, train_acc=0.99311, val_loss=2.00921, val_acc=0.96350, time=1.11101
Epoch:0063, train_loss=1.42256, train_acc=0.99392, val_loss=2.00920, val_acc=0.96350, time=1.18901
Epoch:0064, train_loss=1.42227, train_acc=0.99453, val_loss=2.00920, val_acc=0.96350, time=1.13101
Epoch:0065, train_loss=1.42199, train_acc=0.99453, val_loss=2.00920, val_acc=0.96533, time=1.20701
Epoch:0066, train_loss=1.42172, train_acc=0.99473, val_loss=2.00920, val_acc=0.96533, time=1.28901
Epoch:0067, train_loss=1.42146, train_acc=0.99473, val_loss=2.00921, val_acc=0.96533, time=1.22602
Epoch:0068, train_loss=1.42121, train_acc=0.99494, val_loss=2.00921, val_acc=0.96533, time=1.19400
Early stopping...

Optimization Finished!

Test set results: loss= 1.79855, accuracy= 0.97076, time= 0.37199

Test Precision, Recall and F1-Score...
              precision    recall  f1-score   support

           0     0.8696    0.9195    0.8939        87
           1     0.9799    0.9917    0.9858      1083
           2     0.9839    0.9670    0.9754       696
           3     1.0000    1.0000    1.0000        10
           4     0.8916    0.9867    0.9367        75
           5     0.9520    0.9835    0.9675       121
           6     1.0000    0.7222    0.8387        36
           7     0.9452    0.8519    0.8961        81

    accuracy                         0.9708      2189
   macro avg     0.9528    0.9278    0.9367      2189
weighted avg     0.9714    0.9708    0.9704      2189


Macro average Test Precision, Recall and F1-Score...
(0.9527727622322302, 0.9277994773358738, 0.9367490583779567, None)

Micro average Test Precision, Recall and F1-Score...
(0.9707629054362723, 0.9707629054362723, 0.9707629054362723, None)

Embeddings:
Word_embeddings:7688
Train_doc_embeddings:5485
Test_doc_embeddings:2189
