
==========: 50582155713000
Epoch:0001, train_loss=2.10033, train_acc=0.02674, val_loss=2.06089, val_acc=0.69161, time=1.41302
Epoch:0002, train_loss=1.92265, train_acc=0.66862, val_loss=2.04848, val_acc=0.77372, time=1.24200
Epoch:0003, train_loss=1.81459, train_acc=0.74296, val_loss=2.04115, val_acc=0.80474, time=1.26001
Epoch:0004, train_loss=1.74952, train_acc=0.76909, val_loss=2.03653, val_acc=0.80109, time=1.14901
Epoch:0005, train_loss=1.70744, train_acc=0.78084, val_loss=2.03294, val_acc=0.80292, time=1.36900
Epoch:0006, train_loss=1.67462, train_acc=0.78752, val_loss=2.02972, val_acc=0.81204, time=1.10103
Epoch:0007, train_loss=1.64513, train_acc=0.79461, val_loss=2.02676, val_acc=0.82664, time=1.08600
Epoch:0008, train_loss=1.61805, train_acc=0.81142, val_loss=2.02416, val_acc=0.84672, time=1.06601
Epoch:0009, train_loss=1.59403, train_acc=0.83249, val_loss=2.02201, val_acc=0.87226, time=1.39900
Epoch:0010, train_loss=1.57376, train_acc=0.85558, val_loss=2.02031, val_acc=0.89234, time=1.06201
Epoch:0011, train_loss=1.55724, train_acc=0.87948, val_loss=2.01897, val_acc=0.90511, time=1.15899
Epoch:0012, train_loss=1.54377, train_acc=0.89467, val_loss=2.01788, val_acc=0.91241, time=1.06601
Epoch:0013, train_loss=1.53248, train_acc=0.90460, val_loss=2.01697, val_acc=0.91788, time=1.24201
Epoch:0014, train_loss=1.52275, train_acc=0.92060, val_loss=2.01619, val_acc=0.93431, time=1.01001
Epoch:0015, train_loss=1.51424, train_acc=0.93275, val_loss=2.01552, val_acc=0.94161, time=1.21302
Epoch:0016, train_loss=1.50674, train_acc=0.94166, val_loss=2.01492, val_acc=0.94891, time=1.14301
Epoch:0017, train_loss=1.50003, train_acc=0.94410, val_loss=2.01437, val_acc=0.94526, time=1.12203
Epoch:0018, train_loss=1.49387, train_acc=0.94794, val_loss=2.01386, val_acc=0.94161, time=1.19200
Epoch:0019, train_loss=1.48807, train_acc=0.95341, val_loss=2.01337, val_acc=0.94526, time=1.14501
Epoch:0020, train_loss=1.48257, train_acc=0.95443, val_loss=2.01293, val_acc=0.94708, time=1.21300
Epoch:0021, train_loss=1.47749, train_acc=0.95625, val_loss=2.01256, val_acc=0.95255, time=1.03800
Epoch:0022, train_loss=1.47296, train_acc=0.95827, val_loss=2.01225, val_acc=0.95255, time=1.13201
Epoch:0023, train_loss=1.46902, train_acc=0.95888, val_loss=2.01199, val_acc=0.95073, time=1.01399
Epoch:0024, train_loss=1.46557, train_acc=0.95989, val_loss=2.01175, val_acc=0.95438, time=1.13301
Epoch:0025, train_loss=1.46243, train_acc=0.96334, val_loss=2.01152, val_acc=0.95620, time=1.34202
Epoch:0026, train_loss=1.45945, train_acc=0.96496, val_loss=2.01129, val_acc=0.95620, time=1.29601
Epoch:0027, train_loss=1.45659, train_acc=0.96698, val_loss=2.01106, val_acc=0.95803, time=1.14502
Epoch:0028, train_loss=1.45387, train_acc=0.96982, val_loss=2.01085, val_acc=0.95620, time=1.23999
Epoch:0029, train_loss=1.45135, train_acc=0.97144, val_loss=2.01067, val_acc=0.95803, time=1.01801
Epoch:0030, train_loss=1.44909, train_acc=0.97306, val_loss=2.01050, val_acc=0.95803, time=1.05701
Epoch:0031, train_loss=1.44710, train_acc=0.97529, val_loss=2.01037, val_acc=0.95985, time=1.07700
Epoch:0032, train_loss=1.44538, train_acc=0.97691, val_loss=2.01025, val_acc=0.96168, time=1.17201
Epoch:0033, train_loss=1.44387, train_acc=0.97873, val_loss=2.01015, val_acc=0.96350, time=1.09301
Epoch:0034, train_loss=1.44251, train_acc=0.97974, val_loss=2.01006, val_acc=0.95985, time=1.00501
Epoch:0035, train_loss=1.44125, train_acc=0.97974, val_loss=2.00997, val_acc=0.95803, time=1.09102
Epoch:0036, train_loss=1.44006, train_acc=0.98055, val_loss=2.00989, val_acc=0.95438, time=1.12099
Epoch:0037, train_loss=1.43889, train_acc=0.98137, val_loss=2.00982, val_acc=0.95255, time=1.19102
Epoch:0038, train_loss=1.43777, train_acc=0.98137, val_loss=2.00976, val_acc=0.95438, time=1.05600
Epoch:0039, train_loss=1.43669, train_acc=0.98258, val_loss=2.00970, val_acc=0.95438, time=0.98801
Epoch:0040, train_loss=1.43568, train_acc=0.98380, val_loss=2.00966, val_acc=0.95438, time=1.12700
Epoch:0041, train_loss=1.43473, train_acc=0.98339, val_loss=2.00962, val_acc=0.95438, time=1.03100
Epoch:0042, train_loss=1.43384, train_acc=0.98339, val_loss=2.00958, val_acc=0.95438, time=1.12702
Epoch:0043, train_loss=1.43300, train_acc=0.98440, val_loss=2.00955, val_acc=0.95620, time=1.20900
Epoch:0044, train_loss=1.43220, train_acc=0.98582, val_loss=2.00952, val_acc=0.95803, time=1.06802
Epoch:0045, train_loss=1.43144, train_acc=0.98623, val_loss=2.00949, val_acc=0.95620, time=1.11801
Epoch:0046, train_loss=1.43073, train_acc=0.98683, val_loss=2.00947, val_acc=0.95620, time=1.03200
Epoch:0047, train_loss=1.43008, train_acc=0.98724, val_loss=2.00945, val_acc=0.95620, time=1.10101
Epoch:0048, train_loss=1.42948, train_acc=0.98825, val_loss=2.00943, val_acc=0.95803, time=1.05901
Epoch:0049, train_loss=1.42893, train_acc=0.98886, val_loss=2.00942, val_acc=0.95620, time=1.15201
Epoch:0050, train_loss=1.42842, train_acc=0.98886, val_loss=2.00940, val_acc=0.95803, time=1.12001
Epoch:0051, train_loss=1.42792, train_acc=0.98926, val_loss=2.00939, val_acc=0.95803, time=1.22200
Epoch:0052, train_loss=1.42744, train_acc=0.98926, val_loss=2.00937, val_acc=0.95620, time=1.03502
Epoch:0053, train_loss=1.42696, train_acc=0.98987, val_loss=2.00936, val_acc=0.95803, time=1.15099
Epoch:0054, train_loss=1.42650, train_acc=0.99048, val_loss=2.00935, val_acc=0.95803, time=1.15202
Epoch:0055, train_loss=1.42605, train_acc=0.99028, val_loss=2.00933, val_acc=0.95985, time=1.13201
Epoch:0056, train_loss=1.42563, train_acc=0.99068, val_loss=2.00932, val_acc=0.95985, time=1.07699
Epoch:0057, train_loss=1.42522, train_acc=0.99109, val_loss=2.00931, val_acc=0.95985, time=1.04602
Epoch:0058, train_loss=1.42484, train_acc=0.99149, val_loss=2.00930, val_acc=0.96168, time=1.05700
Epoch:0059, train_loss=1.42447, train_acc=0.99170, val_loss=2.00929, val_acc=0.96168, time=1.08300
Epoch:0060, train_loss=1.42411, train_acc=0.99170, val_loss=2.00928, val_acc=0.96168, time=1.26301
Epoch:0061, train_loss=1.42377, train_acc=0.99230, val_loss=2.00927, val_acc=0.96168, time=1.08400
Epoch:0062, train_loss=1.42344, train_acc=0.99251, val_loss=2.00927, val_acc=0.96168, time=1.07300
Epoch:0063, train_loss=1.42313, train_acc=0.99311, val_loss=2.00927, val_acc=0.96168, time=1.02501
Epoch:0064, train_loss=1.42283, train_acc=0.99332, val_loss=2.00927, val_acc=0.96168, time=1.18101
Epoch:0065, train_loss=1.42254, train_acc=0.99352, val_loss=2.00927, val_acc=0.96168, time=1.09800
Epoch:0066, train_loss=1.42226, train_acc=0.99413, val_loss=2.00928, val_acc=0.96168, time=1.03301
Epoch:0067, train_loss=1.42199, train_acc=0.99453, val_loss=2.00928, val_acc=0.96168, time=1.14499
Epoch:0068, train_loss=1.42173, train_acc=0.99473, val_loss=2.00928, val_acc=0.96168, time=1.21600
Early stopping...

Optimization Finished!

Test set results: loss= 1.79870, accuracy= 0.96894, time= 0.32299

Test Precision, Recall and F1-Score...
              precision    recall  f1-score   support

           0     0.8667    0.8966    0.8814        87
           1     0.9808    0.9926    0.9867      1083
           2     0.9811    0.9684    0.9747       696
           3     1.0000    1.0000    1.0000        10
           4     0.8916    0.9867    0.9367        75
           5     0.9587    0.9587    0.9587       121
           6     0.9615    0.6944    0.8065        36
           7     0.9079    0.8519    0.8790        81

    accuracy                         0.9689      2189
   macro avg     0.9435    0.9186    0.9279      2189
weighted avg     0.9692    0.9689    0.9685      2189


Macro average Test Precision, Recall and F1-Score...
(0.9435325473915367, 0.9186495361719618, 0.927944852912215, None)

Micro average Test Precision, Recall and F1-Score...
(0.9689355870260393, 0.9689355870260393, 0.9689355870260393, None)

Embeddings:
Word_embeddings:7688
Train_doc_embeddings:5485
Test_doc_embeddings:2189
