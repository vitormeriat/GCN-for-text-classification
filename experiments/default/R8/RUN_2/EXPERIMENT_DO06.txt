
==========: 269829107107800
Epoch:0001, train_loss=2.17702, train_acc=0.03869, val_loss=2.06752, val_acc=0.56569, time=1.80902
Epoch:0002, train_loss=1.97298, train_acc=0.55641, val_loss=2.05215, val_acc=0.69161, time=1.61101
Epoch:0003, train_loss=1.84260, train_acc=0.69759, val_loss=2.04330, val_acc=0.77190, time=1.47200
Epoch:0004, train_loss=1.76871, train_acc=0.74276, val_loss=2.03814, val_acc=0.79745, time=1.35801
Epoch:0005, train_loss=1.72604, train_acc=0.76403, val_loss=2.03461, val_acc=0.80109, time=1.34202
Epoch:0006, train_loss=1.69618, train_acc=0.77496, val_loss=2.03164, val_acc=0.81204, time=1.30601
Epoch:0007, train_loss=1.66990, train_acc=0.78347, val_loss=2.02887, val_acc=0.81569, time=1.29800
Epoch:0008, train_loss=1.64462, train_acc=0.79542, val_loss=2.02630, val_acc=0.83394, time=1.33599
Epoch:0009, train_loss=1.62068, train_acc=0.81142, val_loss=2.02406, val_acc=0.85036, time=1.34400
Epoch:0010, train_loss=1.59945, train_acc=0.83978, val_loss=2.02222, val_acc=0.87956, time=1.35201
Epoch:0011, train_loss=1.58174, train_acc=0.85983, val_loss=2.02075, val_acc=0.89781, time=1.35301
Epoch:0012, train_loss=1.56729, train_acc=0.87766, val_loss=2.01953, val_acc=0.90328, time=1.32001
Epoch:0013, train_loss=1.55498, train_acc=0.88799, val_loss=2.01845, val_acc=0.91423, time=1.30101
Epoch:0014, train_loss=1.54374, train_acc=0.89872, val_loss=2.01747, val_acc=0.91423, time=1.45701
Epoch:0015, train_loss=1.53304, train_acc=0.90703, val_loss=2.01657, val_acc=0.92153, time=1.43400
Epoch:0016, train_loss=1.52282, train_acc=0.91695, val_loss=2.01578, val_acc=0.92701, time=1.34000
Epoch:0017, train_loss=1.51343, train_acc=0.92647, val_loss=2.01512, val_acc=0.93248, time=1.29800
Epoch:0018, train_loss=1.50523, train_acc=0.93356, val_loss=2.01461, val_acc=0.93248, time=1.41401
Epoch:0019, train_loss=1.49846, train_acc=0.94025, val_loss=2.01423, val_acc=0.94161, time=1.33902
Epoch:0020, train_loss=1.49300, train_acc=0.94653, val_loss=2.01395, val_acc=0.94343, time=1.34600
Epoch:0021, train_loss=1.48855, train_acc=0.94875, val_loss=2.01371, val_acc=0.94343, time=1.34001
Epoch:0022, train_loss=1.48468, train_acc=0.95260, val_loss=2.01348, val_acc=0.93796, time=1.38002
Epoch:0023, train_loss=1.48099, train_acc=0.95402, val_loss=2.01320, val_acc=0.93796, time=1.42101
Epoch:0024, train_loss=1.47719, train_acc=0.95463, val_loss=2.01288, val_acc=0.93796, time=1.46402
Epoch:0025, train_loss=1.47320, train_acc=0.95625, val_loss=2.01251, val_acc=0.94343, time=1.54799
Epoch:0026, train_loss=1.46913, train_acc=0.95726, val_loss=2.01214, val_acc=0.94526, time=1.51001
Epoch:0027, train_loss=1.46521, train_acc=0.96091, val_loss=2.01178, val_acc=0.94526, time=1.42402
Epoch:0028, train_loss=1.46163, train_acc=0.96476, val_loss=2.01145, val_acc=0.95255, time=1.47501
Epoch:0029, train_loss=1.45849, train_acc=0.96739, val_loss=2.01115, val_acc=0.95803, time=1.44502
Epoch:0030, train_loss=1.45578, train_acc=0.96982, val_loss=2.01088, val_acc=0.95985, time=1.48101
Epoch:0031, train_loss=1.45341, train_acc=0.97164, val_loss=2.01064, val_acc=0.95985, time=1.46201
Epoch:0032, train_loss=1.45131, train_acc=0.97407, val_loss=2.01045, val_acc=0.96168, time=1.51101
Epoch:0033, train_loss=1.44943, train_acc=0.97509, val_loss=2.01028, val_acc=0.96168, time=1.65201
Epoch:0034, train_loss=1.44773, train_acc=0.97590, val_loss=2.01015, val_acc=0.96168, time=1.50100
Epoch:0035, train_loss=1.44619, train_acc=0.97772, val_loss=2.01005, val_acc=0.96350, time=1.56601
Epoch:0036, train_loss=1.44477, train_acc=0.97792, val_loss=2.00997, val_acc=0.96350, time=1.30900
Epoch:0037, train_loss=1.44342, train_acc=0.97954, val_loss=2.00990, val_acc=0.96350, time=1.35701
Epoch:0038, train_loss=1.44212, train_acc=0.98015, val_loss=2.00984, val_acc=0.96168, time=1.32701
Epoch:0039, train_loss=1.44085, train_acc=0.98157, val_loss=2.00978, val_acc=0.96168, time=1.32401
Epoch:0040, train_loss=1.43962, train_acc=0.98218, val_loss=2.00973, val_acc=0.96168, time=1.43000
Epoch:0041, train_loss=1.43845, train_acc=0.98339, val_loss=2.00968, val_acc=0.96168, time=1.38901
Epoch:0042, train_loss=1.43737, train_acc=0.98339, val_loss=2.00964, val_acc=0.96168, time=1.31101
Epoch:0043, train_loss=1.43637, train_acc=0.98380, val_loss=2.00959, val_acc=0.95985, time=1.32401
Epoch:0044, train_loss=1.43546, train_acc=0.98400, val_loss=2.00954, val_acc=0.95985, time=1.33101
Epoch:0045, train_loss=1.43464, train_acc=0.98481, val_loss=2.00949, val_acc=0.95985, time=1.33000
Epoch:0046, train_loss=1.43390, train_acc=0.98440, val_loss=2.00944, val_acc=0.96168, time=1.37001
Epoch:0047, train_loss=1.43321, train_acc=0.98420, val_loss=2.00939, val_acc=0.96350, time=1.38202
Epoch:0048, train_loss=1.43256, train_acc=0.98420, val_loss=2.00934, val_acc=0.96350, time=1.39502
Epoch:0049, train_loss=1.43192, train_acc=0.98501, val_loss=2.00930, val_acc=0.96350, time=1.35300
Epoch:0050, train_loss=1.43130, train_acc=0.98602, val_loss=2.00925, val_acc=0.96350, time=1.31300
Epoch:0051, train_loss=1.43068, train_acc=0.98643, val_loss=2.00921, val_acc=0.96350, time=1.32401
Epoch:0052, train_loss=1.43008, train_acc=0.98744, val_loss=2.00918, val_acc=0.96350, time=1.35701
Epoch:0053, train_loss=1.42951, train_acc=0.98724, val_loss=2.00915, val_acc=0.96350, time=1.31700
Epoch:0054, train_loss=1.42898, train_acc=0.98825, val_loss=2.00913, val_acc=0.96168, time=1.39201
Epoch:0055, train_loss=1.42848, train_acc=0.98866, val_loss=2.00911, val_acc=0.96350, time=1.40201
Epoch:0056, train_loss=1.42802, train_acc=0.98866, val_loss=2.00910, val_acc=0.96350, time=1.40201
Epoch:0057, train_loss=1.42758, train_acc=0.98906, val_loss=2.00909, val_acc=0.96350, time=1.44501
Epoch:0058, train_loss=1.42715, train_acc=0.98906, val_loss=2.00908, val_acc=0.96350, time=1.36200
Epoch:0059, train_loss=1.42673, train_acc=0.98926, val_loss=2.00907, val_acc=0.96350, time=1.35600
Epoch:0060, train_loss=1.42633, train_acc=0.99007, val_loss=2.00906, val_acc=0.96350, time=1.32100
Epoch:0061, train_loss=1.42593, train_acc=0.99109, val_loss=2.00906, val_acc=0.96350, time=1.36300
Epoch:0062, train_loss=1.42554, train_acc=0.99109, val_loss=2.00905, val_acc=0.96350, time=1.30901
Epoch:0063, train_loss=1.42517, train_acc=0.99129, val_loss=2.00905, val_acc=0.96350, time=1.36700
Epoch:0064, train_loss=1.42482, train_acc=0.99170, val_loss=2.00906, val_acc=0.96350, time=1.33002
Epoch:0065, train_loss=1.42449, train_acc=0.99170, val_loss=2.00906, val_acc=0.96350, time=1.35299
Epoch:0066, train_loss=1.42417, train_acc=0.99170, val_loss=2.00907, val_acc=0.96350, time=1.34502
Early stopping...

Optimization Finished!

Test set results: loss= 1.79885, accuracy= 0.97122, time= 0.40700

Test Precision, Recall and F1-Score...
              precision    recall  f1-score   support

           0     0.8681    0.9080    0.8876        87
           1     0.9826    0.9926    0.9876      1083
           2     0.9811    0.9713    0.9762       696
           3     1.0000    1.0000    1.0000        10
           4     0.9012    0.9733    0.9359        75
           5     0.9590    0.9669    0.9630       121
           6     0.9643    0.7500    0.8437        36
           7     0.9324    0.8519    0.8903        81

    accuracy                         0.9712      2189
   macro avg     0.9486    0.9268    0.9355      2189
weighted avg     0.9714    0.9712    0.9710      2189


Macro average Test Precision, Recall and F1-Score...
(0.9486081990998783, 0.9267563488124734, 0.9355430406917677, None)

Micro average Test Precision, Recall and F1-Score...
(0.9712197350388305, 0.9712197350388305, 0.9712197350388305, None)

Embeddings:
Word_embeddings:7688
Train_doc_embeddings:5485
Test_doc_embeddings:2189
