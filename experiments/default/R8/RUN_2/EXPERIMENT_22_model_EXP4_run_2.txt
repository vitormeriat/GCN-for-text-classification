
==========: 51356085597100
Epoch:0001, train_loss=2.11561, train_acc=0.05226, val_loss=2.06241, val_acc=0.62774, time=1.19901
Epoch:0002, train_loss=1.92825, train_acc=0.62872, val_loss=2.04875, val_acc=0.74270, time=1.33801
Epoch:0003, train_loss=1.81278, train_acc=0.73081, val_loss=2.04089, val_acc=0.78102, time=1.11400
Epoch:0004, train_loss=1.74686, train_acc=0.76058, val_loss=2.03614, val_acc=0.79380, time=1.05100
Epoch:0005, train_loss=1.70611, train_acc=0.77557, val_loss=2.03267, val_acc=0.79745, time=1.26801
Epoch:0006, train_loss=1.67490, train_acc=0.78327, val_loss=2.02966, val_acc=0.80474, time=1.15400
Epoch:0007, train_loss=1.64692, train_acc=0.79198, val_loss=2.02685, val_acc=0.81569, time=1.17901
Epoch:0008, train_loss=1.62057, train_acc=0.80575, val_loss=2.02426, val_acc=0.83212, time=1.15103
Epoch:0009, train_loss=1.59632, train_acc=0.82884, val_loss=2.02201, val_acc=0.85584, time=0.99600
Epoch:0010, train_loss=1.57512, train_acc=0.85396, val_loss=2.02016, val_acc=0.88321, time=1.08601
Epoch:0011, train_loss=1.55748, train_acc=0.87462, val_loss=2.01869, val_acc=0.89599, time=1.01601
Epoch:0012, train_loss=1.54317, train_acc=0.89123, val_loss=2.01753, val_acc=0.91423, time=1.20400
Epoch:0013, train_loss=1.53144, train_acc=0.90440, val_loss=2.01660, val_acc=0.92336, time=1.11601
Epoch:0014, train_loss=1.52160, train_acc=0.91635, val_loss=2.01585, val_acc=0.93796, time=1.15801
Epoch:0015, train_loss=1.51320, train_acc=0.92931, val_loss=2.01523, val_acc=0.93978, time=1.08000
Epoch:0016, train_loss=1.50593, train_acc=0.93883, val_loss=2.01470, val_acc=0.93978, time=1.11601
Epoch:0017, train_loss=1.49950, train_acc=0.94248, val_loss=2.01423, val_acc=0.94161, time=1.10100
Epoch:0018, train_loss=1.49356, train_acc=0.94572, val_loss=2.01378, val_acc=0.93978, time=1.09200
Epoch:0019, train_loss=1.48789, train_acc=0.94936, val_loss=2.01335, val_acc=0.94343, time=1.06702
Epoch:0020, train_loss=1.48247, train_acc=0.95240, val_loss=2.01295, val_acc=0.94526, time=1.19100
Epoch:0021, train_loss=1.47746, train_acc=0.95686, val_loss=2.01258, val_acc=0.94708, time=1.09601
Epoch:0022, train_loss=1.47303, train_acc=0.96010, val_loss=2.01226, val_acc=0.95073, time=1.02901
Epoch:0023, train_loss=1.46915, train_acc=0.96030, val_loss=2.01197, val_acc=0.94891, time=0.97501
Epoch:0024, train_loss=1.46568, train_acc=0.96192, val_loss=2.01169, val_acc=0.94891, time=1.17501
Epoch:0025, train_loss=1.46242, train_acc=0.96374, val_loss=2.01142, val_acc=0.95438, time=1.05500
Epoch:0026, train_loss=1.45932, train_acc=0.96476, val_loss=2.01117, val_acc=0.95620, time=1.05302
Epoch:0027, train_loss=1.45638, train_acc=0.96658, val_loss=2.01094, val_acc=0.95803, time=1.07700
Epoch:0028, train_loss=1.45371, train_acc=0.96881, val_loss=2.01075, val_acc=0.96168, time=1.06501
Epoch:0029, train_loss=1.45135, train_acc=0.97185, val_loss=2.01059, val_acc=0.96168, time=1.28301
Epoch:0030, train_loss=1.44934, train_acc=0.97407, val_loss=2.01046, val_acc=0.95985, time=1.09902
Epoch:0031, train_loss=1.44763, train_acc=0.97509, val_loss=2.01035, val_acc=0.95620, time=1.27900
Epoch:0032, train_loss=1.44612, train_acc=0.97630, val_loss=2.01024, val_acc=0.95620, time=1.32401
Epoch:0033, train_loss=1.44471, train_acc=0.97549, val_loss=2.01013, val_acc=0.95803, time=1.01801
Epoch:0034, train_loss=1.44329, train_acc=0.97691, val_loss=2.01002, val_acc=0.95803, time=1.23701
Epoch:0035, train_loss=1.44185, train_acc=0.97792, val_loss=2.00990, val_acc=0.95620, time=1.02899
Epoch:0036, train_loss=1.44039, train_acc=0.97974, val_loss=2.00979, val_acc=0.95620, time=1.03402
Epoch:0037, train_loss=1.43899, train_acc=0.98055, val_loss=2.00969, val_acc=0.95803, time=1.25003
Epoch:0038, train_loss=1.43772, train_acc=0.98177, val_loss=2.00960, val_acc=0.95985, time=0.99700
Epoch:0039, train_loss=1.43660, train_acc=0.98319, val_loss=2.00954, val_acc=0.95985, time=1.07599
Epoch:0040, train_loss=1.43563, train_acc=0.98380, val_loss=2.00949, val_acc=0.96168, time=1.17999
Epoch:0041, train_loss=1.43479, train_acc=0.98400, val_loss=2.00946, val_acc=0.96168, time=1.01800
Epoch:0042, train_loss=1.43401, train_acc=0.98461, val_loss=2.00944, val_acc=0.96168, time=1.03201
Epoch:0043, train_loss=1.43326, train_acc=0.98501, val_loss=2.00942, val_acc=0.95985, time=1.06700
Epoch:0044, train_loss=1.43253, train_acc=0.98602, val_loss=2.00940, val_acc=0.95985, time=1.17001
Epoch:0045, train_loss=1.43183, train_acc=0.98643, val_loss=2.00939, val_acc=0.95985, time=1.14301
Epoch:0046, train_loss=1.43116, train_acc=0.98683, val_loss=2.00938, val_acc=0.95985, time=1.03200
Epoch:0047, train_loss=1.43055, train_acc=0.98764, val_loss=2.00937, val_acc=0.96168, time=1.15901
Epoch:0048, train_loss=1.42997, train_acc=0.98825, val_loss=2.00935, val_acc=0.95803, time=1.10902
Epoch:0049, train_loss=1.42940, train_acc=0.98906, val_loss=2.00933, val_acc=0.95803, time=0.94200
Epoch:0050, train_loss=1.42884, train_acc=0.98906, val_loss=2.00930, val_acc=0.95803, time=1.07001
Epoch:0051, train_loss=1.42829, train_acc=0.98886, val_loss=2.00927, val_acc=0.95985, time=0.95201
Epoch:0052, train_loss=1.42775, train_acc=0.98967, val_loss=2.00924, val_acc=0.96168, time=1.11900
Epoch:0053, train_loss=1.42725, train_acc=0.99007, val_loss=2.00921, val_acc=0.96168, time=1.05801
Epoch:0054, train_loss=1.42679, train_acc=0.99068, val_loss=2.00918, val_acc=0.96533, time=1.08001
Epoch:0055, train_loss=1.42637, train_acc=0.99109, val_loss=2.00917, val_acc=0.96533, time=1.10603
Epoch:0056, train_loss=1.42597, train_acc=0.99149, val_loss=2.00916, val_acc=0.96533, time=1.03001
Epoch:0057, train_loss=1.42558, train_acc=0.99109, val_loss=2.00915, val_acc=0.96533, time=1.15901
Epoch:0058, train_loss=1.42519, train_acc=0.99149, val_loss=2.00915, val_acc=0.96168, time=1.12701
Epoch:0059, train_loss=1.42482, train_acc=0.99170, val_loss=2.00916, val_acc=0.96168, time=1.10401
Epoch:0060, train_loss=1.42445, train_acc=0.99230, val_loss=2.00917, val_acc=0.96168, time=1.21601
Epoch:0061, train_loss=1.42411, train_acc=0.99251, val_loss=2.00918, val_acc=0.96168, time=1.13500
Epoch:0062, train_loss=1.42378, train_acc=0.99251, val_loss=2.00919, val_acc=0.96168, time=1.00800
Early stopping...

Optimization Finished!

Test set results: loss= 1.79884, accuracy= 0.96985, time= 0.32101

Test Precision, Recall and F1-Score...
              precision    recall  f1-score   support

           0     0.8587    0.9080    0.8827        87
           1     0.9808    0.9917    0.9862      1083
           2     0.9825    0.9698    0.9761       696
           3     1.0000    1.0000    1.0000        10
           4     0.9125    0.9733    0.9419        75
           5     0.9516    0.9752    0.9633       121
           6     0.9630    0.7222    0.8254        36
           7     0.9189    0.8395    0.8774        81

    accuracy                         0.9698      2189
   macro avg     0.9460    0.9225    0.9316      2189
weighted avg     0.9701    0.9698    0.9695      2189


Macro average Test Precision, Recall and F1-Score...
(0.9460056382726905, 0.9224789567345277, 0.9316329073031214, None)

Micro average Test Precision, Recall and F1-Score...
(0.9698492462311558, 0.9698492462311558, 0.9698492462311558, None)

Embeddings:
Word_embeddings:7688
Train_doc_embeddings:5485
Test_doc_embeddings:2189
