
==========: 269941572961500
Epoch:0001, train_loss=2.11306, train_acc=0.05590, val_loss=2.06252, val_acc=0.59124, time=1.28602
Epoch:0002, train_loss=1.92746, train_acc=0.60664, val_loss=2.04918, val_acc=0.74453, time=1.29401
Epoch:0003, train_loss=1.81415, train_acc=0.73749, val_loss=2.04152, val_acc=0.78467, time=1.30000
Epoch:0004, train_loss=1.74987, train_acc=0.76544, val_loss=2.03680, val_acc=0.79380, time=1.28401
Epoch:0005, train_loss=1.70947, train_acc=0.77456, val_loss=2.03326, val_acc=0.80474, time=1.23600
Epoch:0006, train_loss=1.67786, train_acc=0.78388, val_loss=2.03013, val_acc=0.80839, time=1.20302
Epoch:0007, train_loss=1.64913, train_acc=0.79178, val_loss=2.02720, val_acc=0.82482, time=1.25700
Epoch:0008, train_loss=1.62190, train_acc=0.80332, val_loss=2.02451, val_acc=0.84124, time=1.26499
Epoch:0009, train_loss=1.59679, train_acc=0.82520, val_loss=2.02219, val_acc=0.86679, time=1.32702
Epoch:0010, train_loss=1.57505, train_acc=0.85416, val_loss=2.02032, val_acc=0.89781, time=1.21400
Epoch:0011, train_loss=1.55738, train_acc=0.88394, val_loss=2.01888, val_acc=0.91788, time=1.29201
Epoch:0012, train_loss=1.54348, train_acc=0.90298, val_loss=2.01775, val_acc=0.92153, time=1.27600
Epoch:0013, train_loss=1.53232, train_acc=0.91452, val_loss=2.01684, val_acc=0.93613, time=1.22501
Epoch:0014, train_loss=1.52295, train_acc=0.92566, val_loss=2.01608, val_acc=0.93978, time=1.23101
Epoch:0015, train_loss=1.51476, train_acc=0.93316, val_loss=2.01543, val_acc=0.93978, time=1.26002
Epoch:0016, train_loss=1.50747, train_acc=0.94065, val_loss=2.01486, val_acc=0.93796, time=1.29402
Epoch:0017, train_loss=1.50087, train_acc=0.94308, val_loss=2.01436, val_acc=0.94343, time=1.28500
Epoch:0018, train_loss=1.49484, train_acc=0.94653, val_loss=2.01392, val_acc=0.94343, time=1.22401
Epoch:0019, train_loss=1.48933, train_acc=0.94977, val_loss=2.01353, val_acc=0.94708, time=1.19901
Epoch:0020, train_loss=1.48431, train_acc=0.95240, val_loss=2.01317, val_acc=0.95073, time=1.25200
Epoch:0021, train_loss=1.47971, train_acc=0.95483, val_loss=2.01283, val_acc=0.94708, time=1.24600
Epoch:0022, train_loss=1.47541, train_acc=0.95726, val_loss=2.01250, val_acc=0.94891, time=1.36700
Epoch:0023, train_loss=1.47133, train_acc=0.95908, val_loss=2.01218, val_acc=0.94891, time=1.23801
Epoch:0024, train_loss=1.46744, train_acc=0.96010, val_loss=2.01186, val_acc=0.94708, time=1.42501
Epoch:0025, train_loss=1.46378, train_acc=0.96152, val_loss=2.01156, val_acc=0.94891, time=1.26000
Epoch:0026, train_loss=1.46039, train_acc=0.96374, val_loss=2.01127, val_acc=0.95073, time=1.25601
Epoch:0027, train_loss=1.45734, train_acc=0.96516, val_loss=2.01102, val_acc=0.95073, time=1.21299
Epoch:0028, train_loss=1.45462, train_acc=0.96698, val_loss=2.01079, val_acc=0.95073, time=1.29800
Epoch:0029, train_loss=1.45222, train_acc=0.96860, val_loss=2.01060, val_acc=0.95438, time=1.31201
Epoch:0030, train_loss=1.45006, train_acc=0.97144, val_loss=2.01043, val_acc=0.95620, time=1.26702
Epoch:0031, train_loss=1.44809, train_acc=0.97347, val_loss=2.01029, val_acc=0.95803, time=1.24101
Epoch:0032, train_loss=1.44628, train_acc=0.97509, val_loss=2.01018, val_acc=0.95985, time=1.22901
Epoch:0033, train_loss=1.44460, train_acc=0.97650, val_loss=2.01009, val_acc=0.95985, time=1.25800
Epoch:0034, train_loss=1.44306, train_acc=0.97792, val_loss=2.01002, val_acc=0.96168, time=1.35601
Epoch:0035, train_loss=1.44165, train_acc=0.97914, val_loss=2.00996, val_acc=0.96350, time=1.22699
Epoch:0036, train_loss=1.44035, train_acc=0.97974, val_loss=2.00992, val_acc=0.96168, time=1.28201
Epoch:0037, train_loss=1.43916, train_acc=0.98157, val_loss=2.00989, val_acc=0.95985, time=1.37101
Epoch:0038, train_loss=1.43804, train_acc=0.98177, val_loss=2.00985, val_acc=0.95803, time=1.28701
Epoch:0039, train_loss=1.43698, train_acc=0.98319, val_loss=2.00981, val_acc=0.95803, time=1.36201
Epoch:0040, train_loss=1.43599, train_acc=0.98440, val_loss=2.00977, val_acc=0.95985, time=1.28303
Epoch:0041, train_loss=1.43505, train_acc=0.98461, val_loss=2.00973, val_acc=0.96168, time=1.24798
Epoch:0042, train_loss=1.43416, train_acc=0.98501, val_loss=2.00968, val_acc=0.96168, time=1.26000
Epoch:0043, train_loss=1.43330, train_acc=0.98562, val_loss=2.00962, val_acc=0.96168, time=1.27100
Epoch:0044, train_loss=1.43247, train_acc=0.98663, val_loss=2.00956, val_acc=0.95985, time=1.31201
Epoch:0045, train_loss=1.43168, train_acc=0.98724, val_loss=2.00951, val_acc=0.96350, time=1.29801
Epoch:0046, train_loss=1.43093, train_acc=0.98764, val_loss=2.00946, val_acc=0.96350, time=1.22400
Epoch:0047, train_loss=1.43024, train_acc=0.98886, val_loss=2.00941, val_acc=0.96350, time=1.26801
Epoch:0048, train_loss=1.42960, train_acc=0.98926, val_loss=2.00938, val_acc=0.95985, time=1.23201
Epoch:0049, train_loss=1.42901, train_acc=0.98967, val_loss=2.00935, val_acc=0.96168, time=1.20901
Epoch:0050, train_loss=1.42846, train_acc=0.98987, val_loss=2.00933, val_acc=0.96168, time=1.21201
Epoch:0051, train_loss=1.42794, train_acc=0.99007, val_loss=2.00931, val_acc=0.96168, time=1.20303
Epoch:0052, train_loss=1.42744, train_acc=0.99007, val_loss=2.00930, val_acc=0.96168, time=1.20799
Epoch:0053, train_loss=1.42696, train_acc=0.99048, val_loss=2.00929, val_acc=0.95985, time=1.20502
Epoch:0054, train_loss=1.42649, train_acc=0.99048, val_loss=2.00928, val_acc=0.95985, time=1.21200
Epoch:0055, train_loss=1.42605, train_acc=0.99089, val_loss=2.00927, val_acc=0.96168, time=1.27900
Epoch:0056, train_loss=1.42562, train_acc=0.99129, val_loss=2.00926, val_acc=0.96168, time=1.29801
Epoch:0057, train_loss=1.42520, train_acc=0.99149, val_loss=2.00926, val_acc=0.96168, time=1.26101
Epoch:0058, train_loss=1.42481, train_acc=0.99190, val_loss=2.00925, val_acc=0.96168, time=1.38202
Epoch:0059, train_loss=1.42443, train_acc=0.99230, val_loss=2.00925, val_acc=0.96168, time=1.27901
Epoch:0060, train_loss=1.42407, train_acc=0.99291, val_loss=2.00924, val_acc=0.96168, time=1.26500
Epoch:0061, train_loss=1.42373, train_acc=0.99392, val_loss=2.00924, val_acc=0.96168, time=1.30700
Epoch:0062, train_loss=1.42340, train_acc=0.99392, val_loss=2.00924, val_acc=0.96168, time=1.24999
Epoch:0063, train_loss=1.42308, train_acc=0.99413, val_loss=2.00924, val_acc=0.96168, time=1.23102
Epoch:0064, train_loss=1.42278, train_acc=0.99433, val_loss=2.00925, val_acc=0.95985, time=1.28701
Epoch:0065, train_loss=1.42249, train_acc=0.99453, val_loss=2.00925, val_acc=0.95985, time=1.17301
Epoch:0066, train_loss=1.42221, train_acc=0.99453, val_loss=2.00925, val_acc=0.96168, time=1.27701
Early stopping...

Optimization Finished!

Test set results: loss= 1.79863, accuracy= 0.97168, time= 0.50301

Test Precision, Recall and F1-Score...
              precision    recall  f1-score   support

           0     0.8791    0.9195    0.8989        87
           1     0.9835    0.9917    0.9876      1083
           2     0.9826    0.9741    0.9784       696
           3     1.0000    1.0000    1.0000        10
           4     0.9012    0.9733    0.9359        75
           5     0.9435    0.9669    0.9551       121
           6     0.9615    0.6944    0.8065        36
           7     0.9333    0.8642    0.8974        81

    accuracy                         0.9717      2189
   macro avg     0.9481    0.9230    0.9325      2189
weighted avg     0.9719    0.9717    0.9713      2189


Macro average Test Precision, Recall and F1-Score...
(0.9481126010199175, 0.9230356711267959, 0.9324630720998497, None)

Micro average Test Precision, Recall and F1-Score...
(0.9716765646413887, 0.9716765646413887, 0.9716765646413887, None)

Embeddings:
Word_embeddings:7688
Train_doc_embeddings:5485
Test_doc_embeddings:2189
