
==========: 50330691596800
Epoch:0001, train_loss=2.05088, train_acc=0.48694, val_loss=2.05757, val_acc=0.56569, time=1.30601
Epoch:0002, train_loss=1.88711, train_acc=0.57687, val_loss=2.04535, val_acc=0.72445, time=1.35901
Epoch:0003, train_loss=1.78413, train_acc=0.70650, val_loss=2.03817, val_acc=0.75912, time=1.32500
Epoch:0004, train_loss=1.72346, train_acc=0.75066, val_loss=2.03361, val_acc=0.79745, time=1.18401
Epoch:0005, train_loss=1.68347, train_acc=0.76909, val_loss=2.03007, val_acc=0.80109, time=1.30101
Epoch:0006, train_loss=1.65066, train_acc=0.78064, val_loss=2.02697, val_acc=0.81387, time=1.30801
Epoch:0007, train_loss=1.62097, train_acc=0.79887, val_loss=2.02427, val_acc=0.84489, time=1.04700
Epoch:0008, train_loss=1.59468, train_acc=0.82662, val_loss=2.02207, val_acc=0.87774, time=1.06200
Epoch:0009, train_loss=1.57305, train_acc=0.86105, val_loss=2.02036, val_acc=0.89416, time=1.13801
Epoch:0010, train_loss=1.55623, train_acc=0.88252, val_loss=2.01903, val_acc=0.90328, time=1.20501
Epoch:0011, train_loss=1.54307, train_acc=0.89731, val_loss=2.01795, val_acc=0.91423, time=1.12101
Epoch:0012, train_loss=1.53213, train_acc=0.91007, val_loss=2.01702, val_acc=0.92336, time=1.25701
Epoch:0013, train_loss=1.52251, train_acc=0.92181, val_loss=2.01622, val_acc=0.92883, time=1.22399
Epoch:0014, train_loss=1.51390, train_acc=0.93032, val_loss=2.01552, val_acc=0.93431, time=1.17301
Epoch:0015, train_loss=1.50618, train_acc=0.93620, val_loss=2.01491, val_acc=0.93613, time=1.20600
Epoch:0016, train_loss=1.49914, train_acc=0.93944, val_loss=2.01435, val_acc=0.93431, time=1.19299
Epoch:0017, train_loss=1.49261, train_acc=0.94531, val_loss=2.01385, val_acc=0.93066, time=1.08602
Epoch:0018, train_loss=1.48662, train_acc=0.94916, val_loss=2.01342, val_acc=0.93613, time=1.03701
Epoch:0019, train_loss=1.48124, train_acc=0.95321, val_loss=2.01304, val_acc=0.93796, time=1.20001
Epoch:0020, train_loss=1.47650, train_acc=0.95463, val_loss=2.01271, val_acc=0.93613, time=1.16401
Epoch:0021, train_loss=1.47228, train_acc=0.95483, val_loss=2.01241, val_acc=0.93978, time=0.95400
Epoch:0022, train_loss=1.46843, train_acc=0.95787, val_loss=2.01212, val_acc=0.94161, time=0.98802
Epoch:0023, train_loss=1.46480, train_acc=0.96050, val_loss=2.01182, val_acc=0.94526, time=0.97200
Epoch:0024, train_loss=1.46133, train_acc=0.96334, val_loss=2.01153, val_acc=0.94526, time=1.21101
Epoch:0025, train_loss=1.45801, train_acc=0.96516, val_loss=2.01125, val_acc=0.94526, time=1.20601
Epoch:0026, train_loss=1.45491, train_acc=0.96759, val_loss=2.01100, val_acc=0.94891, time=1.09201
Epoch:0027, train_loss=1.45210, train_acc=0.96921, val_loss=2.01077, val_acc=0.95438, time=1.14901
Epoch:0028, train_loss=1.44961, train_acc=0.97083, val_loss=2.01059, val_acc=0.95438, time=1.01600
Epoch:0029, train_loss=1.44740, train_acc=0.97225, val_loss=2.01044, val_acc=0.95255, time=1.13400
Epoch:0030, train_loss=1.44543, train_acc=0.97347, val_loss=2.01031, val_acc=0.95255, time=1.05001
Epoch:0031, train_loss=1.44367, train_acc=0.97691, val_loss=2.01022, val_acc=0.95255, time=0.96001
Epoch:0032, train_loss=1.44209, train_acc=0.97792, val_loss=2.01015, val_acc=0.95073, time=1.12900
Epoch:0033, train_loss=1.44069, train_acc=0.97873, val_loss=2.01009, val_acc=0.95073, time=1.15701
Epoch:0034, train_loss=1.43946, train_acc=0.97974, val_loss=2.01005, val_acc=0.95255, time=0.95001
Epoch:0035, train_loss=1.43836, train_acc=0.98015, val_loss=2.01002, val_acc=0.95255, time=1.18901
Epoch:0036, train_loss=1.43731, train_acc=0.98055, val_loss=2.00998, val_acc=0.95255, time=1.29602
Epoch:0037, train_loss=1.43628, train_acc=0.98177, val_loss=2.00993, val_acc=0.95073, time=1.04100
Epoch:0038, train_loss=1.43524, train_acc=0.98258, val_loss=2.00987, val_acc=0.95255, time=1.05501
Epoch:0039, train_loss=1.43422, train_acc=0.98299, val_loss=2.00981, val_acc=0.95620, time=0.96801
Epoch:0040, train_loss=1.43324, train_acc=0.98440, val_loss=2.00975, val_acc=0.95620, time=1.01100
Epoch:0041, train_loss=1.43233, train_acc=0.98461, val_loss=2.00968, val_acc=0.95438, time=1.11500
Epoch:0042, train_loss=1.43149, train_acc=0.98562, val_loss=2.00962, val_acc=0.95438, time=1.10999
Epoch:0043, train_loss=1.43071, train_acc=0.98623, val_loss=2.00957, val_acc=0.95620, time=1.04202
Epoch:0044, train_loss=1.42998, train_acc=0.98744, val_loss=2.00952, val_acc=0.95803, time=1.17101
Epoch:0045, train_loss=1.42931, train_acc=0.98805, val_loss=2.00948, val_acc=0.95803, time=1.09601
Epoch:0046, train_loss=1.42869, train_acc=0.98845, val_loss=2.00945, val_acc=0.95620, time=1.09300
Epoch:0047, train_loss=1.42813, train_acc=0.98967, val_loss=2.00942, val_acc=0.95620, time=1.16199
Epoch:0048, train_loss=1.42761, train_acc=0.99007, val_loss=2.00941, val_acc=0.95803, time=1.15101
Epoch:0049, train_loss=1.42711, train_acc=0.99068, val_loss=2.00940, val_acc=0.95803, time=0.99101
Epoch:0050, train_loss=1.42662, train_acc=0.99089, val_loss=2.00940, val_acc=0.95803, time=0.97001
Epoch:0051, train_loss=1.42615, train_acc=0.99089, val_loss=2.00940, val_acc=0.95985, time=0.93802
Epoch:0052, train_loss=1.42569, train_acc=0.99129, val_loss=2.00940, val_acc=0.95985, time=1.10900
Epoch:0053, train_loss=1.42525, train_acc=0.99170, val_loss=2.00940, val_acc=0.95985, time=1.18301
Epoch:0054, train_loss=1.42483, train_acc=0.99149, val_loss=2.00940, val_acc=0.95985, time=1.23499
Epoch:0055, train_loss=1.42442, train_acc=0.99210, val_loss=2.00940, val_acc=0.95985, time=1.09901
Epoch:0056, train_loss=1.42403, train_acc=0.99230, val_loss=2.00939, val_acc=0.95985, time=1.07100
Epoch:0057, train_loss=1.42366, train_acc=0.99311, val_loss=2.00939, val_acc=0.95985, time=1.00301
Epoch:0058, train_loss=1.42331, train_acc=0.99332, val_loss=2.00938, val_acc=0.96168, time=1.07101
Epoch:0059, train_loss=1.42297, train_acc=0.99372, val_loss=2.00937, val_acc=0.96168, time=0.96201
Epoch:0060, train_loss=1.42265, train_acc=0.99372, val_loss=2.00937, val_acc=0.96168, time=0.98500
Epoch:0061, train_loss=1.42234, train_acc=0.99433, val_loss=2.00937, val_acc=0.96168, time=1.06900
Epoch:0062, train_loss=1.42204, train_acc=0.99433, val_loss=2.00937, val_acc=0.96168, time=1.02900
Epoch:0063, train_loss=1.42175, train_acc=0.99453, val_loss=2.00937, val_acc=0.96168, time=1.06999
Epoch:0064, train_loss=1.42148, train_acc=0.99494, val_loss=2.00938, val_acc=0.96168, time=1.09701
Epoch:0065, train_loss=1.42121, train_acc=0.99494, val_loss=2.00938, val_acc=0.96168, time=1.07801
Early stopping...

Optimization Finished!

Test set results: loss= 1.79937, accuracy= 0.96848, time= 0.30200

Test Precision, Recall and F1-Score...
              precision    recall  f1-score   support

           0     0.8696    0.9195    0.8939        87
           1     0.9790    0.9917    0.9853      1083
           2     0.9824    0.9641    0.9732       696
           3     0.9091    1.0000    0.9524        10
           4     0.8706    0.9867    0.9250        75
           5     0.9516    0.9752    0.9633       121
           6     1.0000    0.6944    0.8197        36
           7     0.9444    0.8395    0.8889        81

    accuracy                         0.9685      2189
   macro avg     0.9383    0.9214    0.9252      2189
weighted avg     0.9693    0.9685    0.9681      2189


Macro average Test Precision, Recall and F1-Score...
(0.9383457364595711, 0.9213917919835698, 0.9251940113846616, None)

Micro average Test Precision, Recall and F1-Score...
(0.968478757423481, 0.968478757423481, 0.968478757423481, None)

Embeddings:
Word_embeddings:7688
Train_doc_embeddings:5485
Test_doc_embeddings:2189
