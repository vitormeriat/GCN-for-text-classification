
==========: 51234202491500
Epoch:0001, train_loss=2.06308, train_acc=0.16366, val_loss=2.04638, val_acc=0.75365, time=1.26301
Epoch:0002, train_loss=1.79987, train_acc=0.76038, val_loss=2.03554, val_acc=0.77737, time=1.17599
Epoch:0003, train_loss=1.72185, train_acc=0.74620, val_loss=2.02242, val_acc=0.83577, time=1.20601
Epoch:0004, train_loss=1.57804, train_acc=0.83654, val_loss=2.02081, val_acc=0.88504, time=1.29001
Epoch:0005, train_loss=1.55436, train_acc=0.87584, val_loss=2.01912, val_acc=0.89964, time=1.06299
Epoch:0006, train_loss=1.53832, train_acc=0.90541, val_loss=2.01681, val_acc=0.92883, time=1.22101
Epoch:0007, train_loss=1.51851, train_acc=0.92627, val_loss=2.01485, val_acc=0.94343, time=1.17602
Epoch:0008, train_loss=1.50122, train_acc=0.93883, val_loss=2.01365, val_acc=0.94161, time=1.23000
Epoch:0009, train_loss=1.48991, train_acc=0.94410, val_loss=2.01303, val_acc=0.94708, time=1.17601
Epoch:0010, train_loss=1.48307, train_acc=0.94491, val_loss=2.01260, val_acc=0.94891, time=1.12002
Epoch:0011, train_loss=1.47652, train_acc=0.94754, val_loss=2.01212, val_acc=0.94526, time=1.06401
Epoch:0012, train_loss=1.46805, train_acc=0.95301, val_loss=2.01172, val_acc=0.94891, time=1.26701
Epoch:0013, train_loss=1.45989, train_acc=0.95665, val_loss=2.01157, val_acc=0.95073, time=1.09001
Epoch:0014, train_loss=1.45436, train_acc=0.95989, val_loss=2.01153, val_acc=0.94343, time=1.17301
Epoch:0015, train_loss=1.45085, train_acc=0.96273, val_loss=2.01133, val_acc=0.93796, time=1.19300
Epoch:0016, train_loss=1.44735, train_acc=0.96395, val_loss=2.01089, val_acc=0.93796, time=1.12401
Epoch:0017, train_loss=1.44319, train_acc=0.96678, val_loss=2.01033, val_acc=0.94708, time=1.15502
Epoch:0018, train_loss=1.43900, train_acc=0.97245, val_loss=2.00981, val_acc=0.94891, time=1.16201
Epoch:0019, train_loss=1.43558, train_acc=0.97549, val_loss=2.00942, val_acc=0.95255, time=1.07800
Epoch:0020, train_loss=1.43332, train_acc=0.97995, val_loss=2.00920, val_acc=0.95985, time=1.07302
Epoch:0021, train_loss=1.43216, train_acc=0.98076, val_loss=2.00910, val_acc=0.96168, time=1.10900
Epoch:0022, train_loss=1.43154, train_acc=0.98076, val_loss=2.00907, val_acc=0.96350, time=1.17401
Epoch:0023, train_loss=1.43069, train_acc=0.98157, val_loss=2.00907, val_acc=0.96168, time=1.27101
Epoch:0024, train_loss=1.42934, train_acc=0.98380, val_loss=2.00910, val_acc=0.96350, time=0.99900
Epoch:0025, train_loss=1.42778, train_acc=0.98542, val_loss=2.00918, val_acc=0.96350, time=1.17701
Early stopping...

Optimization Finished!

Test set results: loss= 1.80100, accuracy= 0.96985, time= 0.35201

Test Precision, Recall and F1-Score...
              precision    recall  f1-score   support

           0     0.8681    0.9080    0.8876        87
           1     0.9871    0.9898    0.9885      1083
           2     0.9840    0.9727    0.9783       696
           3     0.8333    1.0000    0.9091        10
           4     0.9012    0.9733    0.9359        75
           5     0.9023    0.9917    0.9449       121
           6     1.0000    0.6111    0.7586        36
           7     0.9211    0.8642    0.8917        81

    accuracy                         0.9698      2189
   macro avg     0.9246    0.9139    0.9118      2189
weighted avg     0.9708    0.9698    0.9693      2189


Macro average Test Precision, Recall and F1-Score...
(0.9246410404458809, 0.9138709584449622, 0.9118310962024935, None)

Micro average Test Precision, Recall and F1-Score...
(0.9698492462311558, 0.9698492462311558, 0.9698492462311558, None)

Embeddings:
Word_embeddings:7688
Train_doc_embeddings:5485
Test_doc_embeddings:2189
