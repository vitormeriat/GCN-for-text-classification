
==========: 270045216895000
Epoch:0001, train_loss=2.12286, train_acc=0.04132, val_loss=2.06354, val_acc=0.55109, time=1.19501
Epoch:0002, train_loss=1.93884, train_acc=0.56391, val_loss=2.04988, val_acc=0.63321, time=1.30002
Epoch:0003, train_loss=1.82184, train_acc=0.64128, val_loss=2.04162, val_acc=0.72628, time=1.27501
Epoch:0004, train_loss=1.75199, train_acc=0.72109, val_loss=2.03646, val_acc=0.77372, time=1.22001
Epoch:0005, train_loss=1.70918, train_acc=0.76200, val_loss=2.03279, val_acc=0.79380, time=1.22500
Epoch:0006, train_loss=1.67847, train_acc=0.77841, val_loss=2.02972, val_acc=0.81204, time=1.32500
Epoch:0007, train_loss=1.65169, train_acc=0.78833, val_loss=2.02698, val_acc=0.82664, time=1.23802
Epoch:0008, train_loss=1.62641, train_acc=0.80312, val_loss=2.02459, val_acc=0.84307, time=1.27500
Epoch:0009, train_loss=1.60329, train_acc=0.82641, val_loss=2.02265, val_acc=0.87226, time=1.22001
Epoch:0010, train_loss=1.58360, train_acc=0.85821, val_loss=2.02110, val_acc=0.88869, time=1.27201
Epoch:0011, train_loss=1.56738, train_acc=0.88211, val_loss=2.01982, val_acc=0.89964, time=1.21401
Epoch:0012, train_loss=1.55351, train_acc=0.89569, val_loss=2.01867, val_acc=0.90693, time=1.25400
Epoch:0013, train_loss=1.54089, train_acc=0.90440, val_loss=2.01761, val_acc=0.91971, time=1.22902
Epoch:0014, train_loss=1.52907, train_acc=0.91270, val_loss=2.01665, val_acc=0.92701, time=1.25001
Epoch:0015, train_loss=1.51826, train_acc=0.92344, val_loss=2.01583, val_acc=0.93431, time=1.24101
Epoch:0016, train_loss=1.50894, train_acc=0.93356, val_loss=2.01520, val_acc=0.93431, time=1.22901
Epoch:0017, train_loss=1.50142, train_acc=0.93984, val_loss=2.01472, val_acc=0.93248, time=1.22701
Epoch:0018, train_loss=1.49558, train_acc=0.94268, val_loss=2.01434, val_acc=0.93066, time=1.28701
Epoch:0019, train_loss=1.49080, train_acc=0.94450, val_loss=2.01399, val_acc=0.92883, time=1.26801
Epoch:0020, train_loss=1.48635, train_acc=0.94551, val_loss=2.01361, val_acc=0.93248, time=1.35301
Epoch:0021, train_loss=1.48182, train_acc=0.94855, val_loss=2.01321, val_acc=0.93431, time=1.30701
Epoch:0022, train_loss=1.47718, train_acc=0.95321, val_loss=2.01280, val_acc=0.93796, time=1.24601
Epoch:0023, train_loss=1.47261, train_acc=0.95584, val_loss=2.01242, val_acc=0.93978, time=1.17300
Epoch:0024, train_loss=1.46834, train_acc=0.95908, val_loss=2.01207, val_acc=0.94526, time=1.33101
Epoch:0025, train_loss=1.46450, train_acc=0.96111, val_loss=2.01177, val_acc=0.94526, time=1.24102
Epoch:0026, train_loss=1.46115, train_acc=0.96476, val_loss=2.01149, val_acc=0.94526, time=1.25901
Epoch:0027, train_loss=1.45823, train_acc=0.96719, val_loss=2.01125, val_acc=0.94891, time=1.24801
Epoch:0028, train_loss=1.45568, train_acc=0.96800, val_loss=2.01104, val_acc=0.95255, time=1.27401
Epoch:0029, train_loss=1.45340, train_acc=0.97144, val_loss=2.01085, val_acc=0.95255, time=1.27801
Epoch:0030, train_loss=1.45131, train_acc=0.97205, val_loss=2.01069, val_acc=0.95438, time=1.27702
Epoch:0031, train_loss=1.44935, train_acc=0.97367, val_loss=2.01054, val_acc=0.95620, time=1.21101
Epoch:0032, train_loss=1.44750, train_acc=0.97488, val_loss=2.01042, val_acc=0.95620, time=1.30499
Epoch:0033, train_loss=1.44575, train_acc=0.97610, val_loss=2.01032, val_acc=0.95620, time=1.23002
Epoch:0034, train_loss=1.44410, train_acc=0.97792, val_loss=2.01023, val_acc=0.95620, time=1.27600
Epoch:0035, train_loss=1.44257, train_acc=0.97873, val_loss=2.01017, val_acc=0.95438, time=1.37500
Epoch:0036, train_loss=1.44116, train_acc=0.97934, val_loss=2.01011, val_acc=0.95255, time=1.27802
Epoch:0037, train_loss=1.43987, train_acc=0.98116, val_loss=2.01007, val_acc=0.95255, time=1.27600
Epoch:0038, train_loss=1.43870, train_acc=0.98197, val_loss=2.01003, val_acc=0.95255, time=1.23202
Epoch:0039, train_loss=1.43764, train_acc=0.98238, val_loss=2.00999, val_acc=0.95073, time=1.24900
Epoch:0040, train_loss=1.43667, train_acc=0.98258, val_loss=2.00996, val_acc=0.94891, time=1.25101
Epoch:0041, train_loss=1.43577, train_acc=0.98359, val_loss=2.00991, val_acc=0.94891, time=1.31301
Epoch:0042, train_loss=1.43490, train_acc=0.98440, val_loss=2.00986, val_acc=0.95255, time=1.16700
Epoch:0043, train_loss=1.43404, train_acc=0.98501, val_loss=2.00980, val_acc=0.95620, time=1.17300
Epoch:0044, train_loss=1.43321, train_acc=0.98461, val_loss=2.00974, val_acc=0.95620, time=1.16500
Epoch:0045, train_loss=1.43242, train_acc=0.98521, val_loss=2.00967, val_acc=0.95620, time=1.19801
Epoch:0046, train_loss=1.43168, train_acc=0.98501, val_loss=2.00961, val_acc=0.95803, time=1.21502
Epoch:0047, train_loss=1.43101, train_acc=0.98582, val_loss=2.00956, val_acc=0.96168, time=1.19402
Epoch:0048, train_loss=1.43039, train_acc=0.98623, val_loss=2.00951, val_acc=0.95803, time=1.20200
Epoch:0049, train_loss=1.42981, train_acc=0.98663, val_loss=2.00947, val_acc=0.95803, time=1.18501
Epoch:0050, train_loss=1.42926, train_acc=0.98825, val_loss=2.00944, val_acc=0.95803, time=1.26700
Epoch:0051, train_loss=1.42874, train_acc=0.98825, val_loss=2.00941, val_acc=0.95803, time=1.22601
Epoch:0052, train_loss=1.42824, train_acc=0.98926, val_loss=2.00940, val_acc=0.95985, time=1.18100
Epoch:0053, train_loss=1.42776, train_acc=0.98947, val_loss=2.00939, val_acc=0.95985, time=1.31601
Epoch:0054, train_loss=1.42730, train_acc=0.98967, val_loss=2.00939, val_acc=0.95985, time=1.20999
Epoch:0055, train_loss=1.42686, train_acc=0.99028, val_loss=2.00939, val_acc=0.95985, time=1.12601
Epoch:0056, train_loss=1.42643, train_acc=0.99068, val_loss=2.00940, val_acc=0.95985, time=1.24001
Epoch:0057, train_loss=1.42601, train_acc=0.99109, val_loss=2.00942, val_acc=0.95985, time=1.25200
Epoch:0058, train_loss=1.42561, train_acc=0.99170, val_loss=2.00943, val_acc=0.95985, time=1.21201
Early stopping...

Optimization Finished!

Test set results: loss= 1.79844, accuracy= 0.97122, time= 0.38998

Test Precision, Recall and F1-Score...
              precision    recall  f1-score   support

           0     0.8778    0.9080    0.8927        87
           1     0.9817    0.9908    0.9862      1083
           2     0.9840    0.9713    0.9776       696
           3     1.0000    1.0000    1.0000        10
           4     0.9125    0.9733    0.9419        75
           5     0.9593    0.9752    0.9672       121
           6     0.9310    0.7500    0.8308        36
           7     0.9091    0.8642    0.8861        81

    accuracy                         0.9712      2189
   macro avg     0.9444    0.9291    0.9353      2189
weighted avg     0.9713    0.9712    0.9710      2189


Macro average Test Precision, Recall and F1-Score...
(0.9444303570781869, 0.9291017762817151, 0.9353059176898153, None)

Micro average Test Precision, Recall and F1-Score...
(0.9712197350388305, 0.9712197350388305, 0.9712197350388305, None)

Embeddings:
Word_embeddings:7688
Train_doc_embeddings:5485
Test_doc_embeddings:2189
