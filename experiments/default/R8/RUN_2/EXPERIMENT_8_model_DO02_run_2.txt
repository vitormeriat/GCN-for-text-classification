
==========: 50186102851700
Epoch:0001, train_loss=2.10261, train_acc=0.08082, val_loss=2.06194, val_acc=0.69708, time=1.26101
Epoch:0002, train_loss=1.92773, train_acc=0.67409, val_loss=2.04896, val_acc=0.74453, time=1.12201
Epoch:0003, train_loss=1.81360, train_acc=0.74944, val_loss=2.04108, val_acc=0.76277, time=1.13202
Epoch:0004, train_loss=1.74441, train_acc=0.76686, val_loss=2.03593, val_acc=0.78650, time=1.29501
Epoch:0005, train_loss=1.69979, train_acc=0.77740, val_loss=2.03202, val_acc=0.79745, time=1.34201
Epoch:0006, train_loss=1.66616, train_acc=0.78590, val_loss=2.02871, val_acc=0.80474, time=0.99601
Epoch:0007, train_loss=1.63734, train_acc=0.79421, val_loss=2.02583, val_acc=0.82299, time=0.99201
Epoch:0008, train_loss=1.61137, train_acc=0.81304, val_loss=2.02338, val_acc=0.85219, time=1.20200
Epoch:0009, train_loss=1.58841, train_acc=0.84485, val_loss=2.02143, val_acc=0.87956, time=1.09200
Epoch:0010, train_loss=1.56942, train_acc=0.86956, val_loss=2.01995, val_acc=0.89964, time=1.11002
Epoch:0011, train_loss=1.55442, train_acc=0.88799, val_loss=2.01880, val_acc=0.90511, time=1.04900
Epoch:0012, train_loss=1.54245, train_acc=0.89812, val_loss=2.01787, val_acc=0.91423, time=1.02501
Epoch:0013, train_loss=1.53237, train_acc=0.91169, val_loss=2.01705, val_acc=0.91788, time=1.01901
Epoch:0014, train_loss=1.52337, train_acc=0.92060, val_loss=2.01628, val_acc=0.92518, time=1.13701
Epoch:0015, train_loss=1.51493, train_acc=0.92526, val_loss=2.01555, val_acc=0.93066, time=1.12001
Epoch:0016, train_loss=1.50680, train_acc=0.93174, val_loss=2.01485, val_acc=0.93613, time=1.07500
Epoch:0017, train_loss=1.49898, train_acc=0.93680, val_loss=2.01421, val_acc=0.93978, time=1.20099
Epoch:0018, train_loss=1.49174, train_acc=0.94227, val_loss=2.01366, val_acc=0.94161, time=0.97701
Epoch:0019, train_loss=1.48548, train_acc=0.95017, val_loss=2.01323, val_acc=0.94161, time=1.01403
Epoch:0020, train_loss=1.48047, train_acc=0.95422, val_loss=2.01291, val_acc=0.94526, time=1.18900
Epoch:0021, train_loss=1.47666, train_acc=0.95524, val_loss=2.01266, val_acc=0.94891, time=1.02500
Epoch:0022, train_loss=1.47356, train_acc=0.95787, val_loss=2.01242, val_acc=0.94708, time=1.04000
Epoch:0023, train_loss=1.47053, train_acc=0.95746, val_loss=2.01214, val_acc=0.94708, time=1.07501
Epoch:0024, train_loss=1.46713, train_acc=0.95929, val_loss=2.01183, val_acc=0.94708, time=0.96900
Epoch:0025, train_loss=1.46337, train_acc=0.96233, val_loss=2.01151, val_acc=0.95255, time=1.04299
Epoch:0026, train_loss=1.45955, train_acc=0.96536, val_loss=2.01122, val_acc=0.95255, time=1.17000
Epoch:0027, train_loss=1.45604, train_acc=0.96678, val_loss=2.01098, val_acc=0.94526, time=1.25201
Epoch:0028, train_loss=1.45307, train_acc=0.97144, val_loss=2.01079, val_acc=0.94526, time=1.16401
Epoch:0029, train_loss=1.45070, train_acc=0.97205, val_loss=2.01066, val_acc=0.94891, time=1.02701
Epoch:0030, train_loss=1.44882, train_acc=0.97306, val_loss=2.01055, val_acc=0.94891, time=1.08100
Epoch:0031, train_loss=1.44726, train_acc=0.97326, val_loss=2.01046, val_acc=0.94891, time=1.24601
Epoch:0032, train_loss=1.44584, train_acc=0.97367, val_loss=2.01036, val_acc=0.94891, time=1.14201
Epoch:0033, train_loss=1.44445, train_acc=0.97468, val_loss=2.01026, val_acc=0.94891, time=1.09500
Epoch:0034, train_loss=1.44303, train_acc=0.97630, val_loss=2.01016, val_acc=0.94891, time=1.22701
Epoch:0035, train_loss=1.44161, train_acc=0.97792, val_loss=2.01005, val_acc=0.95255, time=1.04301
Epoch:0036, train_loss=1.44024, train_acc=0.97893, val_loss=2.00996, val_acc=0.95255, time=1.05901
Epoch:0037, train_loss=1.43896, train_acc=0.98116, val_loss=2.00988, val_acc=0.95255, time=1.15801
Epoch:0038, train_loss=1.43780, train_acc=0.98299, val_loss=2.00982, val_acc=0.95803, time=0.98001
Epoch:0039, train_loss=1.43676, train_acc=0.98299, val_loss=2.00977, val_acc=0.95803, time=1.09800
Epoch:0040, train_loss=1.43581, train_acc=0.98278, val_loss=2.00973, val_acc=0.95803, time=1.25399
Epoch:0041, train_loss=1.43490, train_acc=0.98339, val_loss=2.00969, val_acc=0.95620, time=1.13401
Epoch:0042, train_loss=1.43401, train_acc=0.98440, val_loss=2.00966, val_acc=0.95438, time=1.02000
Epoch:0043, train_loss=1.43314, train_acc=0.98542, val_loss=2.00963, val_acc=0.95438, time=0.98100
Epoch:0044, train_loss=1.43232, train_acc=0.98501, val_loss=2.00961, val_acc=0.95438, time=1.18502
Epoch:0045, train_loss=1.43157, train_acc=0.98623, val_loss=2.00958, val_acc=0.95438, time=1.08100
Epoch:0046, train_loss=1.43089, train_acc=0.98683, val_loss=2.00956, val_acc=0.95438, time=1.14401
Epoch:0047, train_loss=1.43026, train_acc=0.98704, val_loss=2.00953, val_acc=0.95438, time=1.10601
Epoch:0048, train_loss=1.42966, train_acc=0.98785, val_loss=2.00949, val_acc=0.95438, time=1.09900
Epoch:0049, train_loss=1.42907, train_acc=0.98866, val_loss=2.00945, val_acc=0.95620, time=0.97101
Epoch:0050, train_loss=1.42851, train_acc=0.98947, val_loss=2.00941, val_acc=0.95985, time=1.06300
Epoch:0051, train_loss=1.42797, train_acc=0.99028, val_loss=2.00937, val_acc=0.96168, time=1.20001
Epoch:0052, train_loss=1.42748, train_acc=0.99007, val_loss=2.00933, val_acc=0.96168, time=1.41700
Epoch:0053, train_loss=1.42701, train_acc=0.99007, val_loss=2.00930, val_acc=0.96168, time=1.12599
Epoch:0054, train_loss=1.42656, train_acc=0.99007, val_loss=2.00928, val_acc=0.96168, time=1.10301
Epoch:0055, train_loss=1.42612, train_acc=0.99109, val_loss=2.00927, val_acc=0.96168, time=1.04800
Epoch:0056, train_loss=1.42569, train_acc=0.99129, val_loss=2.00927, val_acc=0.96168, time=1.03300
Epoch:0057, train_loss=1.42526, train_acc=0.99170, val_loss=2.00927, val_acc=0.96168, time=1.05501
Epoch:0058, train_loss=1.42486, train_acc=0.99170, val_loss=2.00928, val_acc=0.95985, time=1.23301
Epoch:0059, train_loss=1.42448, train_acc=0.99210, val_loss=2.00930, val_acc=0.96168, time=1.40401
Epoch:0060, train_loss=1.42413, train_acc=0.99230, val_loss=2.00931, val_acc=0.96168, time=1.34201
Early stopping...

Optimization Finished!

Test set results: loss= 1.79873, accuracy= 0.97031, time= 0.33499

Test Precision, Recall and F1-Score...
              precision    recall  f1-score   support

           0     0.8526    0.9310    0.8901        87
           1     0.9826    0.9908    0.9867      1083
           2     0.9812    0.9741    0.9776       696
           3     1.0000    1.0000    1.0000        10
           4     0.9114    0.9600    0.9351        75
           5     0.9440    0.9752    0.9593       121
           6     1.0000    0.6944    0.8197        36
           7     0.9306    0.8272    0.8758        81

    accuracy                         0.9703      2189
   macro avg     0.9503    0.9191    0.9305      2189
weighted avg     0.9709    0.9703    0.9699      2189


Macro average Test Precision, Recall and F1-Score...
(0.9502958697661652, 0.919093794161664, 0.9305412266762159, None)

Micro average Test Precision, Recall and F1-Score...
(0.970306075833714, 0.970306075833714, 0.970306075833714, None)

Embeddings:
Word_embeddings:7688
Train_doc_embeddings:5485
Test_doc_embeddings:2189
