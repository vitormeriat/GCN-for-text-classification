
==========: 7271501148574377998
Epoch:0001, train_loss=1.09054, train_acc=0.39494, val_loss=1.09190, val_acc=0.53261, time=2.05597
Epoch:0002, train_loss=1.04350, train_acc=0.52488, val_loss=1.08578, val_acc=0.59855, time=2.10428
Epoch:0003, train_loss=0.98732, train_acc=0.58388, val_loss=1.07966, val_acc=0.61522, time=2.14626
Epoch:0004, train_loss=0.93126, train_acc=0.60787, val_loss=1.07586, val_acc=0.72609, time=2.08970
Epoch:0005, train_loss=0.89636, train_acc=0.71824, val_loss=1.07330, val_acc=0.73333, time=2.21360
Epoch:0006, train_loss=0.87216, train_acc=0.74054, val_loss=1.07054, val_acc=0.74493, time=2.21186
Epoch:0007, train_loss=0.84622, train_acc=0.74159, val_loss=1.06715, val_acc=0.75942, time=2.31745
Epoch:0008, train_loss=0.81630, train_acc=0.75777, val_loss=1.06425, val_acc=0.76522, time=2.15683
Epoch:0009, train_loss=0.79184, train_acc=0.76453, val_loss=1.06234, val_acc=0.77029, time=2.32777
Epoch:0010, train_loss=0.77505, train_acc=0.77322, val_loss=1.06090, val_acc=0.78623, time=2.12123
Epoch:0011, train_loss=0.76124, train_acc=0.78136, val_loss=1.05925, val_acc=0.79783, time=2.23836
Epoch:0012, train_loss=0.74663, train_acc=0.79399, val_loss=1.05792, val_acc=0.79710, time=2.14377
Epoch:0013, train_loss=0.73538, train_acc=0.80221, val_loss=1.05737, val_acc=0.80145, time=2.14004
Epoch:0014, train_loss=0.72973, train_acc=0.80132, val_loss=1.05705, val_acc=0.79493, time=2.22368
Epoch:0015, train_loss=0.72613, train_acc=0.80390, val_loss=1.05632, val_acc=0.79565, time=2.08120
Epoch:0016, train_loss=0.72029, train_acc=0.80438, val_loss=1.05585, val_acc=0.79638, time=2.46743
Epoch:0017, train_loss=0.71571, train_acc=0.80679, val_loss=1.05577, val_acc=0.80290, time=2.17020
Epoch:0018, train_loss=0.71356, train_acc=0.80848, val_loss=1.05540, val_acc=0.80870, time=2.38558
Epoch:0019, train_loss=0.71030, train_acc=0.80969, val_loss=1.05485, val_acc=0.80652, time=2.31358
Epoch:0020, train_loss=0.70533, train_acc=0.81436, val_loss=1.05451, val_acc=0.80580, time=2.13609
Epoch:0021, train_loss=0.70094, train_acc=0.81750, val_loss=1.05425, val_acc=0.81014, time=2.31501
Epoch:0022, train_loss=0.69814, train_acc=0.81959, val_loss=1.05387, val_acc=0.81159, time=2.28861
Epoch:0023, train_loss=0.69483, train_acc=0.82289, val_loss=1.05352, val_acc=0.81957, time=2.29833
Epoch:0024, train_loss=0.69061, train_acc=0.82555, val_loss=1.05325, val_acc=0.82464, time=2.31901
Epoch:0025, train_loss=0.68757, train_acc=0.82748, val_loss=1.05298, val_acc=0.82536, time=2.14673
Epoch:0026, train_loss=0.68538, train_acc=0.82958, val_loss=1.05275, val_acc=0.82754, time=2.15660
Epoch:0027, train_loss=0.68253, train_acc=0.83207, val_loss=1.05251, val_acc=0.83333, time=2.15981
Epoch:0028, train_loss=0.67930, train_acc=0.83296, val_loss=1.05223, val_acc=0.82826, time=2.17870
Epoch:0029, train_loss=0.67664, train_acc=0.83328, val_loss=1.05206, val_acc=0.83333, time=2.09874
Epoch:0030, train_loss=0.67436, train_acc=0.83529, val_loss=1.05186, val_acc=0.83261, time=2.14401
Epoch:0031, train_loss=0.67157, train_acc=0.83739, val_loss=1.05152, val_acc=0.83696, time=2.07066
Epoch:0032, train_loss=0.66868, train_acc=0.84052, val_loss=1.05131, val_acc=0.83768, time=2.29050
Epoch:0033, train_loss=0.66653, train_acc=0.84254, val_loss=1.05120, val_acc=0.83696, time=2.15757
Epoch:0034, train_loss=0.66467, train_acc=0.84270, val_loss=1.05099, val_acc=0.83841, time=2.35392
Epoch:0035, train_loss=0.66254, train_acc=0.84310, val_loss=1.05087, val_acc=0.84275, time=2.40895
Epoch:0036, train_loss=0.66067, train_acc=0.84527, val_loss=1.05083, val_acc=0.84275, time=2.31850
Epoch:0037, train_loss=0.65929, train_acc=0.84777, val_loss=1.05068, val_acc=0.84565, time=2.32413
Epoch:0038, train_loss=0.65784, train_acc=0.84785, val_loss=1.05055, val_acc=0.84855, time=2.05758
Epoch:0039, train_loss=0.65619, train_acc=0.84849, val_loss=1.05043, val_acc=0.84710, time=2.18514
Epoch:0040, train_loss=0.65477, train_acc=0.84938, val_loss=1.05029, val_acc=0.84638, time=2.11894
Epoch:0041, train_loss=0.65359, train_acc=0.85010, val_loss=1.05023, val_acc=0.84855, time=2.21386
Epoch:0042, train_loss=0.65228, train_acc=0.85196, val_loss=1.05013, val_acc=0.84638, time=2.29940
Epoch:0043, train_loss=0.65101, train_acc=0.85228, val_loss=1.05007, val_acc=0.84638, time=2.23738
Epoch:0044, train_loss=0.65003, train_acc=0.85405, val_loss=1.05006, val_acc=0.84855, time=2.16617
Epoch:0045, train_loss=0.64912, train_acc=0.85445, val_loss=1.04992, val_acc=0.84710, time=2.53472
Epoch:0046, train_loss=0.64810, train_acc=0.85590, val_loss=1.04986, val_acc=0.84928, time=2.26775
Epoch:0047, train_loss=0.64714, train_acc=0.85654, val_loss=1.04977, val_acc=0.85000, time=2.32750
Epoch:0048, train_loss=0.64630, train_acc=0.85840, val_loss=1.04969, val_acc=0.85000, time=2.29196
Epoch:0049, train_loss=0.64536, train_acc=0.85985, val_loss=1.04966, val_acc=0.84710, time=2.47357
Epoch:0050, train_loss=0.64440, train_acc=0.85920, val_loss=1.04958, val_acc=0.84710, time=2.47886
Epoch:0051, train_loss=0.64358, train_acc=0.86017, val_loss=1.04957, val_acc=0.85000, time=2.25510
Epoch:0052, train_loss=0.64278, train_acc=0.86057, val_loss=1.04943, val_acc=0.84783, time=2.16375
Epoch:0053, train_loss=0.64194, train_acc=0.86081, val_loss=1.04943, val_acc=0.84855, time=2.11489
Epoch:0054, train_loss=0.64119, train_acc=0.86210, val_loss=1.04929, val_acc=0.84783, time=2.35926
Epoch:0055, train_loss=0.64050, train_acc=0.86266, val_loss=1.04935, val_acc=0.84783, time=2.37327
Epoch:0056, train_loss=0.63979, train_acc=0.86299, val_loss=1.04918, val_acc=0.84855, time=2.23311
Epoch:0057, train_loss=0.63916, train_acc=0.86266, val_loss=1.04939, val_acc=0.84710, time=2.26618
Epoch:0058, train_loss=0.63874, train_acc=0.86379, val_loss=1.04909, val_acc=0.85435, time=2.29742
Epoch:0059, train_loss=0.63861, train_acc=0.86226, val_loss=1.04954, val_acc=0.84565, time=2.09802
Early stopping...

Optimization Finished!

Test set results: loss= 0.89087, accuracy= 0.85072, time= 0.71488

Test Precision, Recall and F1-Score...
              precision    recall  f1-score   support

           0     0.8682    0.8447    0.8563      2356
           1     0.8503    0.8411    0.8457      1202
           2     0.8344    0.8617    0.8478      2357

    accuracy                         0.8507      5915
   macro avg     0.8510    0.8491    0.8499      5915
weighted avg     0.8511    0.8507    0.8508      5915


Macro average Test Precision, Recall and F1-Score...
(0.850986878630045, 0.8491462364553467, 0.8499310795445205, None)

Micro average Test Precision, Recall and F1-Score...
(0.8507185122569738, 0.8507185122569738, 0.8507185122569738, None)

Embeddings:
Word_embeddings:452
Train_doc_embeddings:13802
Test_doc_embeddings:5915
