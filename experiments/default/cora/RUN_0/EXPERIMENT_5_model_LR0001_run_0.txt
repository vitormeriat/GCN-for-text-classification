
==========: 293643636348000
Epoch:0001, train_loss=1.99599, train_acc=0.07440, val_loss=1.95213, val_acc=0.06878, time=0.13001
Epoch:0002, train_loss=1.98954, train_acc=0.07499, val_loss=1.95136, val_acc=0.06878, time=0.12300
Epoch:0003, train_loss=1.98331, train_acc=0.07791, val_loss=1.95062, val_acc=0.06878, time=0.11900
Epoch:0004, train_loss=1.97729, train_acc=0.08026, val_loss=1.94990, val_acc=0.07407, time=0.11500
Epoch:0005, train_loss=1.97149, train_acc=0.08436, val_loss=1.94921, val_acc=0.07937, time=0.12301
Epoch:0006, train_loss=1.96591, train_acc=0.08612, val_loss=1.94855, val_acc=0.08466, time=0.12501
Epoch:0007, train_loss=1.96057, train_acc=0.09139, val_loss=1.94791, val_acc=0.08995, time=0.13100
Epoch:0008, train_loss=1.95545, train_acc=0.10135, val_loss=1.94730, val_acc=0.10053, time=0.11601
Epoch:0009, train_loss=1.95058, train_acc=0.11424, val_loss=1.94672, val_acc=0.10053, time=0.13000
Epoch:0010, train_loss=1.94594, train_acc=0.13533, val_loss=1.94616, val_acc=0.11640, time=0.12001
Epoch:0011, train_loss=1.94154, train_acc=0.15056, val_loss=1.94564, val_acc=0.15344, time=0.12801
Epoch:0012, train_loss=1.93737, train_acc=0.18395, val_loss=1.94514, val_acc=0.18519, time=0.12500
Epoch:0013, train_loss=1.93345, train_acc=0.21090, val_loss=1.94467, val_acc=0.22751, time=0.08701
Epoch:0014, train_loss=1.92976, train_acc=0.23199, val_loss=1.94423, val_acc=0.25926, time=0.11600
Epoch:0015, train_loss=1.92631, train_acc=0.25776, val_loss=1.94381, val_acc=0.30159, time=0.11700
Epoch:0016, train_loss=1.92307, train_acc=0.27182, val_loss=1.94342, val_acc=0.31217, time=0.12899
Epoch:0017, train_loss=1.92006, train_acc=0.28120, val_loss=1.94306, val_acc=0.32804, time=0.11602
Epoch:0018, train_loss=1.91725, train_acc=0.29115, val_loss=1.94273, val_acc=0.33333, time=0.12799
Epoch:0019, train_loss=1.91465, train_acc=0.29174, val_loss=1.94242, val_acc=0.33862, time=0.12201
Epoch:0020, train_loss=1.91224, train_acc=0.29701, val_loss=1.94213, val_acc=0.33862, time=0.13099
Epoch:0021, train_loss=1.91002, train_acc=0.29818, val_loss=1.94187, val_acc=0.33333, time=0.11701
Epoch:0022, train_loss=1.90796, train_acc=0.30228, val_loss=1.94163, val_acc=0.33333, time=0.11800
Epoch:0023, train_loss=1.90606, train_acc=0.30287, val_loss=1.94141, val_acc=0.33333, time=0.13201
Epoch:0024, train_loss=1.90431, train_acc=0.30287, val_loss=1.94121, val_acc=0.33333, time=0.12799
Epoch:0025, train_loss=1.90269, train_acc=0.30521, val_loss=1.94103, val_acc=0.33333, time=0.12801
Epoch:0026, train_loss=1.90118, train_acc=0.30404, val_loss=1.94087, val_acc=0.33333, time=0.08404
Epoch:0027, train_loss=1.89979, train_acc=0.30639, val_loss=1.94072, val_acc=0.33862, time=0.09899
Epoch:0028, train_loss=1.89849, train_acc=0.30697, val_loss=1.94059, val_acc=0.33862, time=0.10602
Epoch:0029, train_loss=1.89728, train_acc=0.30814, val_loss=1.94046, val_acc=0.33862, time=0.09700
Epoch:0030, train_loss=1.89614, train_acc=0.30931, val_loss=1.94036, val_acc=0.33862, time=0.11799
Epoch:0031, train_loss=1.89507, train_acc=0.30990, val_loss=1.94026, val_acc=0.33862, time=0.12400
Epoch:0032, train_loss=1.89405, train_acc=0.30990, val_loss=1.94017, val_acc=0.33862, time=0.12697
Epoch:0033, train_loss=1.89309, train_acc=0.30990, val_loss=1.94009, val_acc=0.33862, time=0.09100
Epoch:0034, train_loss=1.89217, train_acc=0.31049, val_loss=1.94002, val_acc=0.33862, time=0.08901
Epoch:0035, train_loss=1.89128, train_acc=0.31049, val_loss=1.93996, val_acc=0.33862, time=0.11300
Epoch:0036, train_loss=1.89043, train_acc=0.31049, val_loss=1.93990, val_acc=0.33862, time=0.11602
Epoch:0037, train_loss=1.88961, train_acc=0.31049, val_loss=1.93986, val_acc=0.33862, time=0.12901
Epoch:0038, train_loss=1.88881, train_acc=0.31049, val_loss=1.93981, val_acc=0.33862, time=0.09500
Epoch:0039, train_loss=1.88804, train_acc=0.31049, val_loss=1.93978, val_acc=0.33862, time=0.11700
Epoch:0040, train_loss=1.88728, train_acc=0.31049, val_loss=1.93974, val_acc=0.33862, time=0.10101
Epoch:0041, train_loss=1.88654, train_acc=0.31049, val_loss=1.93972, val_acc=0.33862, time=0.08703
Epoch:0042, train_loss=1.88581, train_acc=0.31049, val_loss=1.93969, val_acc=0.33862, time=0.14600
Epoch:0043, train_loss=1.88510, train_acc=0.31049, val_loss=1.93967, val_acc=0.33862, time=0.10000
Epoch:0044, train_loss=1.88440, train_acc=0.30990, val_loss=1.93965, val_acc=0.33862, time=0.12900
Epoch:0045, train_loss=1.88370, train_acc=0.31049, val_loss=1.93964, val_acc=0.33862, time=0.08901
Epoch:0046, train_loss=1.88302, train_acc=0.31107, val_loss=1.93963, val_acc=0.33862, time=0.09901
Epoch:0047, train_loss=1.88234, train_acc=0.31166, val_loss=1.93962, val_acc=0.33862, time=0.12999
Epoch:0048, train_loss=1.88166, train_acc=0.31283, val_loss=1.93961, val_acc=0.33862, time=0.08802
Epoch:0049, train_loss=1.88099, train_acc=0.31342, val_loss=1.93960, val_acc=0.33862, time=0.11800
Epoch:0050, train_loss=1.88032, train_acc=0.31459, val_loss=1.93960, val_acc=0.33862, time=0.09601
Epoch:0051, train_loss=1.87966, train_acc=0.31459, val_loss=1.93959, val_acc=0.33862, time=0.12200
Epoch:0052, train_loss=1.87899, train_acc=0.31459, val_loss=1.93959, val_acc=0.33862, time=0.10201
Epoch:0053, train_loss=1.87833, train_acc=0.31459, val_loss=1.93958, val_acc=0.33862, time=0.08602
Epoch:0054, train_loss=1.87766, train_acc=0.31459, val_loss=1.93958, val_acc=0.33862, time=0.08900
Epoch:0055, train_loss=1.87700, train_acc=0.31459, val_loss=1.93958, val_acc=0.33862, time=0.09601
Epoch:0056, train_loss=1.87633, train_acc=0.31459, val_loss=1.93958, val_acc=0.33862, time=0.10500
Epoch:0057, train_loss=1.87567, train_acc=0.31517, val_loss=1.93958, val_acc=0.33862, time=0.08501
Epoch:0058, train_loss=1.87500, train_acc=0.31576, val_loss=1.93957, val_acc=0.33862, time=0.11300
Epoch:0059, train_loss=1.87434, train_acc=0.31576, val_loss=1.93957, val_acc=0.33862, time=0.10800
Epoch:0060, train_loss=1.87367, train_acc=0.31576, val_loss=1.93957, val_acc=0.33862, time=0.08599
Epoch:0061, train_loss=1.87301, train_acc=0.31576, val_loss=1.93957, val_acc=0.33862, time=0.11499
Epoch:0062, train_loss=1.87235, train_acc=0.31634, val_loss=1.93958, val_acc=0.33862, time=0.12800
Epoch:0063, train_loss=1.87168, train_acc=0.31634, val_loss=1.93958, val_acc=0.33862, time=0.08601
Early stopping...

Optimization Finished!

Test set results: loss= 1.92679, accuracy= 0.28695, time= 0.02500

Test Precision, Recall and F1-Score...
              precision    recall  f1-score   support

           0     0.0000    0.0000    0.0000       140
           1     0.0000    0.0000    0.0000        45
           2     0.1875    0.0248    0.0438       121
           3     0.0000    0.0000    0.0000        92
           4     0.2500    0.0086    0.0167       116
           5     0.0000    0.0000    0.0000        65
           6     0.2910    0.9828    0.4490       233

    accuracy                         0.2869       812
   macro avg     0.1041    0.1452    0.0728       812
weighted avg     0.1471    0.2869    0.1378       812


Macro average Test Precision, Recall and F1-Score...
(0.10406834271192593, 0.14517809944438223, 0.07278312784968002, None)

Micro average Test Precision, Recall and F1-Score...
(0.2869458128078818, 0.2869458128078818, 0.2869458128078818, None)

Embeddings:
Word_embeddings:1343
Train_doc_embeddings:1896
Test_doc_embeddings:812
