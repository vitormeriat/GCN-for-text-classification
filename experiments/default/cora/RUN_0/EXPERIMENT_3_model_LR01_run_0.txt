
==========: 293627796263400
Epoch:0001, train_loss=1.98395, train_acc=0.11131, val_loss=2.01498, val_acc=0.34392, time=0.10200
Epoch:0002, train_loss=2.69594, train_acc=0.30463, val_loss=1.96175, val_acc=0.15873, time=0.11801
Epoch:0003, train_loss=2.13133, train_acc=0.15173, val_loss=1.95412, val_acc=0.14286, time=0.13402
Epoch:0004, train_loss=2.00576, train_acc=0.17047, val_loss=1.94883, val_acc=0.13757, time=0.13000
Epoch:0005, train_loss=1.94125, train_acc=0.18629, val_loss=1.94592, val_acc=0.14286, time=0.12800
Epoch:0006, train_loss=1.91135, train_acc=0.23609, val_loss=1.94478, val_acc=0.14286, time=0.13401
Epoch:0007, train_loss=1.90081, train_acc=0.27065, val_loss=1.94457, val_acc=0.20106, time=0.11001
Epoch:0008, train_loss=1.89800, train_acc=0.32689, val_loss=1.94462, val_acc=0.24868, time=0.13003
Epoch:0009, train_loss=1.89655, train_acc=0.35735, val_loss=1.94465, val_acc=0.26455, time=0.13001
Epoch:0010, train_loss=1.89390, train_acc=0.35852, val_loss=1.94463, val_acc=0.29630, time=0.12399
Epoch:0011, train_loss=1.88952, train_acc=0.36497, val_loss=1.94455, val_acc=0.30159, time=0.11601
Epoch:0012, train_loss=1.88329, train_acc=0.36965, val_loss=1.94441, val_acc=0.30159, time=0.10900
Epoch:0013, train_loss=1.87521, train_acc=0.37610, val_loss=1.94426, val_acc=0.29630, time=0.13001
Epoch:0014, train_loss=1.86536, train_acc=0.38723, val_loss=1.94420, val_acc=0.30688, time=0.12600
Epoch:0015, train_loss=1.85488, train_acc=0.40129, val_loss=1.94436, val_acc=0.32275, time=0.12799
Epoch:0016, train_loss=1.84528, train_acc=0.41066, val_loss=1.94432, val_acc=0.32804, time=0.12201
Epoch:0017, train_loss=1.83470, train_acc=0.42296, val_loss=1.94389, val_acc=0.31217, time=0.12002
Epoch:0018, train_loss=1.82190, train_acc=0.42004, val_loss=1.94330, val_acc=0.32275, time=0.11899
Epoch:0019, train_loss=1.80833, train_acc=0.41769, val_loss=1.94272, val_acc=0.32275, time=0.12601
Epoch:0020, train_loss=1.79535, train_acc=0.40832, val_loss=1.94229, val_acc=0.33333, time=0.11899
Epoch:0021, train_loss=1.78302, train_acc=0.40715, val_loss=1.94203, val_acc=0.33333, time=0.12701
Epoch:0022, train_loss=1.77053, train_acc=0.41301, val_loss=1.94189, val_acc=0.33333, time=0.13199
Epoch:0023, train_loss=1.75681, train_acc=0.42414, val_loss=1.94195, val_acc=0.32804, time=0.12100
Epoch:0024, train_loss=1.74148, train_acc=0.43761, val_loss=1.94239, val_acc=0.32804, time=0.13002
Epoch:0025, train_loss=1.72633, train_acc=0.45987, val_loss=1.94338, val_acc=0.33333, time=0.11901
Early stopping...

Optimization Finished!

Test set results: loss= 1.95025, accuracy= 0.25246, time= 0.02400

Test Precision, Recall and F1-Score...
              precision    recall  f1-score   support

           0     0.1800    0.0643    0.0947       140
           1     0.0000    0.0000    0.0000        45
           2     0.1290    0.0661    0.0874       121
           3     0.0952    0.0217    0.0354        92
           4     0.1724    0.0431    0.0690       116
           5     0.1667    0.0154    0.0282        65
           6     0.2799    0.7725    0.4110       233

    accuracy                         0.2525       812
   macro avg     0.1462    0.1405    0.1037       812
weighted avg     0.1794    0.2525    0.1634       812


Macro average Test Precision, Recall and F1-Score...
(0.14618408638208466, 0.14045154281450212, 0.1036657430883293, None)

Micro average Test Precision, Recall and F1-Score...
(0.2524630541871921, 0.2524630541871921, 0.2524630541871921, None)

Embeddings:
Word_embeddings:1343
Train_doc_embeddings:1896
Test_doc_embeddings:812
