
==========: 294354105330800
Epoch:0001, train_loss=2.02305, train_acc=0.09666, val_loss=1.94563, val_acc=0.10053, time=0.11099
Epoch:0002, train_loss=1.95484, train_acc=0.12771, val_loss=1.94077, val_acc=0.33333, time=0.13100
Epoch:0003, train_loss=1.91452, train_acc=0.28530, val_loss=1.93877, val_acc=0.33862, time=0.11798
Epoch:0004, train_loss=1.89950, train_acc=0.30111, val_loss=1.93868, val_acc=0.34392, time=0.12700
Epoch:0005, train_loss=1.90005, train_acc=0.30697, val_loss=1.93914, val_acc=0.34392, time=0.13100
Epoch:0006, train_loss=1.90274, train_acc=0.30873, val_loss=1.93940, val_acc=0.34392, time=0.10799
Epoch:0007, train_loss=1.90147, train_acc=0.30873, val_loss=1.93944, val_acc=0.34392, time=0.11202
Epoch:0008, train_loss=1.89680, train_acc=0.30931, val_loss=1.93939, val_acc=0.33862, time=0.13099
Epoch:0009, train_loss=1.89065, train_acc=0.31283, val_loss=1.93932, val_acc=0.33333, time=0.13100
Epoch:0010, train_loss=1.88410, train_acc=0.31400, val_loss=1.93923, val_acc=0.32804, time=0.12401
Epoch:0011, train_loss=1.87747, train_acc=0.31693, val_loss=1.93912, val_acc=0.33333, time=0.11799
Epoch:0012, train_loss=1.87084, train_acc=0.32455, val_loss=1.93899, val_acc=0.33333, time=0.13001
Epoch:0013, train_loss=1.86433, train_acc=0.32923, val_loss=1.93888, val_acc=0.33333, time=0.12200
Epoch:0014, train_loss=1.85812, train_acc=0.33158, val_loss=1.93879, val_acc=0.32804, time=0.13102
Epoch:0015, train_loss=1.85232, train_acc=0.33099, val_loss=1.93874, val_acc=0.32275, time=0.13201
Epoch:0016, train_loss=1.84695, train_acc=0.33040, val_loss=1.93875, val_acc=0.32275, time=0.12601
Epoch:0017, train_loss=1.84192, train_acc=0.32748, val_loss=1.93878, val_acc=0.32275, time=0.12900
Epoch:0018, train_loss=1.83712, train_acc=0.32865, val_loss=1.93885, val_acc=0.32804, time=0.12701
Epoch:0019, train_loss=1.83241, train_acc=0.33275, val_loss=1.93894, val_acc=0.32804, time=0.12798
Epoch:0020, train_loss=1.82771, train_acc=0.33685, val_loss=1.93905, val_acc=0.32804, time=0.13200
Early stopping...

Optimization Finished!

Test set results: loss= 1.93110, accuracy= 0.28079, time= 0.02900

Test Precision, Recall and F1-Score...
              precision    recall  f1-score   support

           0     0.0000    0.0000    0.0000       140
           1     0.0000    0.0000    0.0000        45
           2     0.2059    0.0579    0.0903       121
           3     0.0000    0.0000    0.0000        92
           4     0.0000    0.0000    0.0000       116
           5     0.0000    0.0000    0.0000        65
           6     0.2881    0.9485    0.4420       233

    accuracy                         0.2808       812
   macro avg     0.0706    0.1438    0.0760       812
weighted avg     0.1134    0.2808    0.1403       812


Macro average Test Precision, Recall and F1-Score...
(0.07057399230878793, 0.14376415624952493, 0.07604608294930874, None)

Micro average Test Precision, Recall and F1-Score...
(0.28078817733990147, 0.28078817733990147, 0.28078817733990147, None)

Embeddings:
Word_embeddings:1343
Train_doc_embeddings:1896
Test_doc_embeddings:812
