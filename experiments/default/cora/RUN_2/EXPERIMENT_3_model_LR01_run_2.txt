
==========: 294346268745100
Epoch:0001, train_loss=1.98720, train_acc=0.10779, val_loss=1.99989, val_acc=0.34392, time=0.11999
Epoch:0002, train_loss=2.48421, train_acc=0.30521, val_loss=1.97758, val_acc=0.13757, time=0.12400
Epoch:0003, train_loss=2.21448, train_acc=0.16462, val_loss=1.94745, val_acc=0.14815, time=0.11301
Epoch:0004, train_loss=1.93011, train_acc=0.19449, val_loss=1.94574, val_acc=0.12698, time=0.12000
Epoch:0005, train_loss=1.91677, train_acc=0.18805, val_loss=1.94558, val_acc=0.13228, time=0.11601
Epoch:0006, train_loss=1.91706, train_acc=0.25132, val_loss=1.94447, val_acc=0.22751, time=0.11700
Epoch:0007, train_loss=1.90669, train_acc=0.34271, val_loss=1.94351, val_acc=0.23810, time=0.12400
Epoch:0008, train_loss=1.89602, train_acc=0.39016, val_loss=1.94297, val_acc=0.26984, time=0.12198
Epoch:0009, train_loss=1.88745, train_acc=0.40129, val_loss=1.94271, val_acc=0.31217, time=0.11899
Epoch:0010, train_loss=1.88034, train_acc=0.39660, val_loss=1.94254, val_acc=0.31217, time=0.12699
Epoch:0011, train_loss=1.87341, train_acc=0.39426, val_loss=1.94238, val_acc=0.31746, time=0.11001
Epoch:0012, train_loss=1.86566, train_acc=0.39426, val_loss=1.94217, val_acc=0.32804, time=0.12700
Epoch:0013, train_loss=1.85655, train_acc=0.39602, val_loss=1.94185, val_acc=0.32804, time=0.12700
Epoch:0014, train_loss=1.84580, train_acc=0.39895, val_loss=1.94141, val_acc=0.33862, time=0.12999
Epoch:0015, train_loss=1.83333, train_acc=0.39660, val_loss=1.94086, val_acc=0.33862, time=0.12900
Epoch:0016, train_loss=1.81965, train_acc=0.38899, val_loss=1.94036, val_acc=0.33862, time=0.12700
Epoch:0017, train_loss=1.80540, train_acc=0.38489, val_loss=1.94001, val_acc=0.34392, time=0.12800
Epoch:0018, train_loss=1.79112, train_acc=0.38079, val_loss=1.93999, val_acc=0.33862, time=0.12201
Epoch:0019, train_loss=1.77647, train_acc=0.37786, val_loss=1.94049, val_acc=0.33862, time=0.11601
Epoch:0020, train_loss=1.76107, train_acc=0.39016, val_loss=1.94170, val_acc=0.31746, time=0.11801
Early stopping...

Optimization Finished!

Test set results: loss= 1.94629, accuracy= 0.27340, time= 0.03900

Test Precision, Recall and F1-Score...
              precision    recall  f1-score   support

           0     0.1333    0.0143    0.0258       140
           1     0.0000    0.0000    0.0000        45
           2     0.0938    0.0248    0.0392       121
           3     0.0000    0.0000    0.0000        92
           4     0.2258    0.0603    0.0952       116
           5     0.0000    0.0000    0.0000        65
           6     0.2921    0.9013    0.4412       233

    accuracy                         0.2734       812
   macro avg     0.1064    0.1430    0.0859       812
weighted avg     0.1530    0.2734    0.1505       812


Macro average Test Precision, Recall and F1-Score...
(0.10642315823094459, 0.1429587834213917, 0.08591952910196336, None)

Micro average Test Precision, Recall and F1-Score...
(0.2733990147783251, 0.2733990147783251, 0.2733990147783251, None)

Embeddings:
Word_embeddings:1343
Train_doc_embeddings:1896
Test_doc_embeddings:812
