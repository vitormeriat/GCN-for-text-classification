
==========: 10951904901880159086
Epoch:0001, train_loss=1.93838, train_acc=0.16227, val_loss=1.93715, val_acc=0.31746, time=0.09334
Epoch:0002, train_loss=1.87233, train_acc=0.30639, val_loss=1.93431, val_acc=0.32275, time=0.09618
Epoch:0003, train_loss=1.83954, train_acc=0.31400, val_loss=1.93040, val_acc=0.33333, time=0.09764
Epoch:0004, train_loss=1.79702, train_acc=0.33626, val_loss=1.92653, val_acc=0.40212, time=0.09786
Epoch:0005, train_loss=1.75529, train_acc=0.43234, val_loss=1.92316, val_acc=0.53439, time=0.09680
Epoch:0006, train_loss=1.71812, train_acc=0.56649, val_loss=1.91999, val_acc=0.60847, time=0.09611
Epoch:0007, train_loss=1.68292, train_acc=0.65905, val_loss=1.91692, val_acc=0.65079, time=0.09833
Epoch:0008, train_loss=1.64861, train_acc=0.69830, val_loss=1.91400, val_acc=0.68783, time=0.09720
Epoch:0009, train_loss=1.61568, train_acc=0.72876, val_loss=1.91124, val_acc=0.70899, time=0.09579
Epoch:0010, train_loss=1.58419, train_acc=0.74634, val_loss=1.90861, val_acc=0.70370, time=0.09746
Epoch:0011, train_loss=1.55380, train_acc=0.75806, val_loss=1.90605, val_acc=0.70370, time=0.09682
Epoch:0012, train_loss=1.52445, train_acc=0.77797, val_loss=1.90366, val_acc=0.71958, time=0.09734
Epoch:0013, train_loss=1.49696, train_acc=0.79906, val_loss=1.90157, val_acc=0.74074, time=0.09625
Epoch:0014, train_loss=1.47239, train_acc=0.81136, val_loss=1.89981, val_acc=0.74603, time=0.09738
Epoch:0015, train_loss=1.45062, train_acc=0.81898, val_loss=1.89825, val_acc=0.74074, time=0.09713
Epoch:0016, train_loss=1.43035, train_acc=0.82601, val_loss=1.89684, val_acc=0.74603, time=0.10143
Epoch:0017, train_loss=1.41120, train_acc=0.83128, val_loss=1.89567, val_acc=0.75132, time=0.10530
Epoch:0018, train_loss=1.39405, train_acc=0.83656, val_loss=1.89475, val_acc=0.76190, time=0.10852
Epoch:0019, train_loss=1.37933, train_acc=0.84124, val_loss=1.89396, val_acc=0.77249, time=0.09980
Epoch:0020, train_loss=1.36614, train_acc=0.84241, val_loss=1.89319, val_acc=0.76720, time=0.09852
Epoch:0021, train_loss=1.35365, train_acc=0.84944, val_loss=1.89251, val_acc=0.74603, time=0.09750
Epoch:0022, train_loss=1.34203, train_acc=0.85530, val_loss=1.89200, val_acc=0.75661, time=0.10060
Epoch:0023, train_loss=1.33155, train_acc=0.86175, val_loss=1.89163, val_acc=0.76190, time=0.10642
Epoch:0024, train_loss=1.32187, train_acc=0.86702, val_loss=1.89137, val_acc=0.76190, time=0.09907
Epoch:0025, train_loss=1.31267, train_acc=0.87346, val_loss=1.89119, val_acc=0.75132, time=0.09652
Epoch:0026, train_loss=1.30401, train_acc=0.87581, val_loss=1.89102, val_acc=0.75132, time=0.09672
Epoch:0027, train_loss=1.29578, train_acc=0.88049, val_loss=1.89081, val_acc=0.75132, time=0.09784
Epoch:0028, train_loss=1.28780, train_acc=0.88752, val_loss=1.89062, val_acc=0.76190, time=0.10235
Epoch:0029, train_loss=1.28023, train_acc=0.89104, val_loss=1.89049, val_acc=0.76190, time=0.10407
Epoch:0030, train_loss=1.27316, train_acc=0.89865, val_loss=1.89043, val_acc=0.75661, time=0.10667
Epoch:0031, train_loss=1.26633, train_acc=0.90275, val_loss=1.89041, val_acc=0.75661, time=0.10606
Epoch:0032, train_loss=1.25958, train_acc=0.90803, val_loss=1.89040, val_acc=0.75661, time=0.10796
Epoch:0033, train_loss=1.25316, train_acc=0.91330, val_loss=1.89039, val_acc=0.75132, time=0.10737
Epoch:0034, train_loss=1.24720, train_acc=0.92091, val_loss=1.89034, val_acc=0.75661, time=0.10433
Epoch:0035, train_loss=1.24147, train_acc=0.92560, val_loss=1.89029, val_acc=0.75661, time=0.10335
Epoch:0036, train_loss=1.23589, train_acc=0.92853, val_loss=1.89029, val_acc=0.75661, time=0.13630
Epoch:0037, train_loss=1.23055, train_acc=0.93087, val_loss=1.89038, val_acc=0.75661, time=0.13298
Epoch:0038, train_loss=1.22552, train_acc=0.93497, val_loss=1.89052, val_acc=0.75132, time=0.10920
Early stopping...

Optimization Finished!

Test set results: loss= 1.70314, accuracy= 0.76108, time= 0.03683

Test Precision, Recall and F1-Score...
              precision    recall  f1-score   support

           0     0.5714    0.5333    0.5517        45
           1     0.7787    0.7851    0.7819       121
           2     0.7611    0.8069    0.7833       233
           3     0.8369    0.8429    0.8399       140
           4     0.6881    0.6466    0.6667       116
           5     0.8627    0.6769    0.7586        65
           6     0.7400    0.8043    0.7708        92

    accuracy                         0.7611       812
   macro avg     0.7484    0.7280    0.7361       812
weighted avg     0.7616    0.7611    0.7600       812


Macro average Test Precision, Recall and F1-Score...
(0.7484212320594777, 0.7280005747243271, 0.7361326880400453, None)

Micro average Test Precision, Recall and F1-Score...
(0.7610837438423645, 0.7610837438423645, 0.7610837438423645, None)

Embeddings:
Word_embeddings:1343
Train_doc_embeddings:1896
Test_doc_embeddings:812
