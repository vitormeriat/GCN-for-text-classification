
==========: 293883157908300
Epoch:0001, train_loss=2.02319, train_acc=0.07440, val_loss=1.94177, val_acc=0.31746, time=0.10202
Epoch:0002, train_loss=1.91864, train_acc=0.27182, val_loss=1.93866, val_acc=0.34392, time=0.12199
Epoch:0003, train_loss=1.90435, train_acc=0.30463, val_loss=1.93973, val_acc=0.34392, time=0.12400
Epoch:0004, train_loss=1.91647, train_acc=0.30404, val_loss=1.93973, val_acc=0.34392, time=0.12901
Epoch:0005, train_loss=1.90996, train_acc=0.30521, val_loss=1.93937, val_acc=0.33862, time=0.11401
Epoch:0006, train_loss=1.89606, train_acc=0.31283, val_loss=1.93928, val_acc=0.33333, time=0.11499
Epoch:0007, train_loss=1.88326, train_acc=0.32220, val_loss=1.93939, val_acc=0.32275, time=0.12600
Epoch:0008, train_loss=1.87216, train_acc=0.34915, val_loss=1.93957, val_acc=0.31217, time=0.13100
Epoch:0009, train_loss=1.86237, train_acc=0.35442, val_loss=1.93976, val_acc=0.32275, time=0.12400
Epoch:0010, train_loss=1.85355, train_acc=0.35091, val_loss=1.93993, val_acc=0.33862, time=0.11702
Epoch:0011, train_loss=1.84531, train_acc=0.34446, val_loss=1.94006, val_acc=0.33862, time=0.11699
Epoch:0012, train_loss=1.83724, train_acc=0.33919, val_loss=1.94014, val_acc=0.33862, time=0.09401
Epoch:0013, train_loss=1.82918, train_acc=0.33919, val_loss=1.94019, val_acc=0.32275, time=0.12901
Epoch:0014, train_loss=1.82111, train_acc=0.34212, val_loss=1.94024, val_acc=0.32804, time=0.10000
Epoch:0015, train_loss=1.81310, train_acc=0.35325, val_loss=1.94031, val_acc=0.31746, time=0.14400
Epoch:0016, train_loss=1.80524, train_acc=0.37083, val_loss=1.94042, val_acc=0.30688, time=0.11400
Epoch:0017, train_loss=1.79749, train_acc=0.38196, val_loss=1.94054, val_acc=0.29630, time=0.12299
Epoch:0018, train_loss=1.78977, train_acc=0.40539, val_loss=1.94063, val_acc=0.29630, time=0.11100
Epoch:0019, train_loss=1.78181, train_acc=0.43292, val_loss=1.94068, val_acc=0.28042, time=0.11299
Epoch:0020, train_loss=1.77340, train_acc=0.45577, val_loss=1.94069, val_acc=0.29101, time=0.13601
Epoch:0021, train_loss=1.76450, train_acc=0.47100, val_loss=1.94073, val_acc=0.28571, time=0.12699
Epoch:0022, train_loss=1.75527, train_acc=0.47803, val_loss=1.94088, val_acc=0.28042, time=0.10701
Epoch:0023, train_loss=1.74593, train_acc=0.47920, val_loss=1.94120, val_acc=0.28042, time=0.10900
Epoch:0024, train_loss=1.73677, train_acc=0.48565, val_loss=1.94171, val_acc=0.26984, time=0.12900
Epoch:0025, train_loss=1.72807, train_acc=0.49561, val_loss=1.94238, val_acc=0.26455, time=0.13300
Epoch:0026, train_loss=1.71992, train_acc=0.50615, val_loss=1.94306, val_acc=0.24868, time=0.12501
Epoch:0027, train_loss=1.71186, train_acc=0.52138, val_loss=1.94359, val_acc=0.23280, time=0.12400
Epoch:0028, train_loss=1.70313, train_acc=0.53427, val_loss=1.94391, val_acc=0.23280, time=0.12500
Epoch:0029, train_loss=1.69355, train_acc=0.55360, val_loss=1.94413, val_acc=0.22222, time=0.11001
Epoch:0030, train_loss=1.68374, train_acc=0.56122, val_loss=1.94444, val_acc=0.21693, time=0.08601
Epoch:0031, train_loss=1.67442, train_acc=0.56415, val_loss=1.94495, val_acc=0.22751, time=0.10401
Epoch:0032, train_loss=1.66572, train_acc=0.57469, val_loss=1.94567, val_acc=0.23280, time=0.11599
Epoch:0033, train_loss=1.65725, train_acc=0.58992, val_loss=1.94646, val_acc=0.22222, time=0.11901
Epoch:0034, train_loss=1.64840, train_acc=0.59637, val_loss=1.94723, val_acc=0.21164, time=0.11801
Epoch:0035, train_loss=1.63914, train_acc=0.61219, val_loss=1.94797, val_acc=0.21164, time=0.08501
Epoch:0036, train_loss=1.62992, train_acc=0.62683, val_loss=1.94869, val_acc=0.21164, time=0.10401
Epoch:0037, train_loss=1.62101, train_acc=0.63679, val_loss=1.94941, val_acc=0.21164, time=0.12402
Epoch:0038, train_loss=1.61215, train_acc=0.65085, val_loss=1.95009, val_acc=0.19577, time=0.12800
Epoch:0039, train_loss=1.60319, train_acc=0.66081, val_loss=1.95074, val_acc=0.20635, time=0.08401
Epoch:0040, train_loss=1.59422, train_acc=0.66081, val_loss=1.95140, val_acc=0.20635, time=0.08402
Epoch:0041, train_loss=1.58548, train_acc=0.66432, val_loss=1.95216, val_acc=0.20106, time=0.08502
Epoch:0042, train_loss=1.57701, train_acc=0.66257, val_loss=1.95304, val_acc=0.19577, time=0.11002
Epoch:0043, train_loss=1.56829, train_acc=0.66901, val_loss=1.95405, val_acc=0.19577, time=0.12999
Epoch:0044, train_loss=1.55930, train_acc=0.68307, val_loss=1.95517, val_acc=0.18519, time=0.12601
Epoch:0045, train_loss=1.55082, train_acc=0.69244, val_loss=1.95625, val_acc=0.17989, time=0.08799
Epoch:0046, train_loss=1.54270, train_acc=0.70240, val_loss=1.95717, val_acc=0.17989, time=0.12000
Epoch:0047, train_loss=1.53432, train_acc=0.71529, val_loss=1.95803, val_acc=0.18519, time=0.08303
Epoch:0048, train_loss=1.52580, train_acc=0.72349, val_loss=1.95893, val_acc=0.18519, time=0.10096
Epoch:0049, train_loss=1.51755, train_acc=0.72291, val_loss=1.95988, val_acc=0.19577, time=0.12500
Epoch:0050, train_loss=1.50959, train_acc=0.72525, val_loss=1.96085, val_acc=0.20106, time=0.12900
Epoch:0051, train_loss=1.50155, train_acc=0.73228, val_loss=1.96195, val_acc=0.20106, time=0.11701
Epoch:0052, train_loss=1.49357, train_acc=0.74517, val_loss=1.96321, val_acc=0.19577, time=0.10400
Epoch:0053, train_loss=1.48577, train_acc=0.75806, val_loss=1.96446, val_acc=0.20635, time=0.08402
Epoch:0054, train_loss=1.47804, train_acc=0.76098, val_loss=1.96551, val_acc=0.21164, time=0.08300
Epoch:0055, train_loss=1.47036, train_acc=0.77036, val_loss=1.96646, val_acc=0.20635, time=0.11401
Epoch:0056, train_loss=1.46290, train_acc=0.77797, val_loss=1.96753, val_acc=0.20635, time=0.10099
Epoch:0057, train_loss=1.45551, train_acc=0.78500, val_loss=1.96879, val_acc=0.20635, time=0.08801
Epoch:0058, train_loss=1.44807, train_acc=0.79145, val_loss=1.97008, val_acc=0.20635, time=0.08303
Epoch:0059, train_loss=1.44088, train_acc=0.80023, val_loss=1.97126, val_acc=0.19048, time=0.09802
Epoch:0060, train_loss=1.43385, train_acc=0.81195, val_loss=1.97237, val_acc=0.20106, time=0.12600
Epoch:0061, train_loss=1.42679, train_acc=0.82015, val_loss=1.97355, val_acc=0.20106, time=0.11700
Epoch:0062, train_loss=1.41986, train_acc=0.82484, val_loss=1.97479, val_acc=0.20106, time=0.08500
Epoch:0063, train_loss=1.41312, train_acc=0.83187, val_loss=1.97604, val_acc=0.20635, time=0.10699
Epoch:0064, train_loss=1.40643, train_acc=0.84476, val_loss=1.97736, val_acc=0.20106, time=0.08700
Epoch:0065, train_loss=1.39985, train_acc=0.84769, val_loss=1.97868, val_acc=0.20635, time=0.10801
Epoch:0066, train_loss=1.39339, train_acc=0.85706, val_loss=1.97990, val_acc=0.20635, time=0.10201
Epoch:0067, train_loss=1.38702, train_acc=0.85999, val_loss=1.98111, val_acc=0.20635, time=0.10998
Epoch:0068, train_loss=1.38080, train_acc=0.86702, val_loss=1.98243, val_acc=0.20635, time=0.11200
Epoch:0069, train_loss=1.37467, train_acc=0.87053, val_loss=1.98385, val_acc=0.21164, time=0.12100
Epoch:0070, train_loss=1.36863, train_acc=0.88284, val_loss=1.98526, val_acc=0.20635, time=0.08401
Epoch:0071, train_loss=1.36275, train_acc=0.88987, val_loss=1.98664, val_acc=0.20106, time=0.08502
Epoch:0072, train_loss=1.35696, train_acc=0.89748, val_loss=1.98799, val_acc=0.20106, time=0.08500
Epoch:0073, train_loss=1.35125, train_acc=0.90100, val_loss=1.98932, val_acc=0.20106, time=0.08401
Epoch:0074, train_loss=1.34570, train_acc=0.90334, val_loss=1.99073, val_acc=0.19577, time=0.08300
Epoch:0075, train_loss=1.34024, train_acc=0.90978, val_loss=1.99228, val_acc=0.19577, time=0.10900
Epoch:0076, train_loss=1.33489, train_acc=0.91330, val_loss=1.99378, val_acc=0.19048, time=0.11400
Epoch:0077, train_loss=1.32966, train_acc=0.91857, val_loss=1.99515, val_acc=0.19048, time=0.11100
Epoch:0078, train_loss=1.32453, train_acc=0.92443, val_loss=1.99659, val_acc=0.19577, time=0.09400
Epoch:0079, train_loss=1.31951, train_acc=0.92853, val_loss=1.99814, val_acc=0.19577, time=0.08400
Epoch:0080, train_loss=1.31460, train_acc=0.93146, val_loss=1.99966, val_acc=0.20106, time=0.08401
Epoch:0081, train_loss=1.30980, train_acc=0.93439, val_loss=2.00119, val_acc=0.19048, time=0.10499
Epoch:0082, train_loss=1.30512, train_acc=0.93673, val_loss=2.00274, val_acc=0.19048, time=0.08401
Epoch:0083, train_loss=1.30052, train_acc=0.94083, val_loss=2.00425, val_acc=0.19577, time=0.13001
Epoch:0084, train_loss=1.29605, train_acc=0.94142, val_loss=2.00578, val_acc=0.20106, time=0.11899
Epoch:0085, train_loss=1.29167, train_acc=0.94318, val_loss=2.00743, val_acc=0.19577, time=0.12003
Epoch:0086, train_loss=1.28740, train_acc=0.94669, val_loss=2.00902, val_acc=0.19577, time=0.11000
Epoch:0087, train_loss=1.28323, train_acc=0.95021, val_loss=2.01057, val_acc=0.19577, time=0.08501
Epoch:0088, train_loss=1.27916, train_acc=0.95079, val_loss=2.01217, val_acc=0.19577, time=0.08501
Epoch:0089, train_loss=1.27519, train_acc=0.95489, val_loss=2.01376, val_acc=0.19577, time=0.12999
Epoch:0090, train_loss=1.27132, train_acc=0.95958, val_loss=2.01541, val_acc=0.19577, time=0.09200
Epoch:0091, train_loss=1.26755, train_acc=0.96075, val_loss=2.01708, val_acc=0.19577, time=0.11700
Epoch:0092, train_loss=1.26388, train_acc=0.96485, val_loss=2.01865, val_acc=0.20106, time=0.12800
Epoch:0093, train_loss=1.26029, train_acc=0.96602, val_loss=2.02026, val_acc=0.20106, time=0.08301
Epoch:0094, train_loss=1.25681, train_acc=0.96954, val_loss=2.02194, val_acc=0.20106, time=0.08401
Epoch:0095, train_loss=1.25341, train_acc=0.97422, val_loss=2.02357, val_acc=0.20106, time=0.12200
Epoch:0096, train_loss=1.25011, train_acc=0.97598, val_loss=2.02522, val_acc=0.19577, time=0.12699
Epoch:0097, train_loss=1.24689, train_acc=0.97657, val_loss=2.02687, val_acc=0.20106, time=0.08501
Epoch:0098, train_loss=1.24376, train_acc=0.97715, val_loss=2.02848, val_acc=0.20106, time=0.08302
Epoch:0099, train_loss=1.24072, train_acc=0.98008, val_loss=2.03015, val_acc=0.20106, time=0.08202
Epoch:0100, train_loss=1.23775, train_acc=0.98184, val_loss=2.03184, val_acc=0.20106, time=0.08403
Epoch:0101, train_loss=1.23488, train_acc=0.98301, val_loss=2.03348, val_acc=0.20106, time=0.08402
Epoch:0102, train_loss=1.23208, train_acc=0.98360, val_loss=2.03512, val_acc=0.20106, time=0.08301
Early stopping...

Optimization Finished!

Test set results: loss= 2.35414, accuracy= 0.18227, time= 0.02401

Test Precision, Recall and F1-Score...
              precision    recall  f1-score   support

           0     0.1712    0.1357    0.1514       140
           1     0.0750    0.0667    0.0706        45
           2     0.1628    0.2314    0.1911       121
           3     0.1341    0.1196    0.1264        92
           4     0.1250    0.0776    0.0957       116
           5     0.0909    0.0769    0.0833        65
           6     0.2607    0.3133    0.2846       233

    accuracy                         0.1823       812
   macro avg     0.1457    0.1459    0.1433       812
weighted avg     0.1731    0.1823    0.1748       812


Macro average Test Precision, Recall and F1-Score...
(0.14567594099034015, 0.14588073332851634, 0.14331773187507088, None)

Micro average Test Precision, Recall and F1-Score...
(0.18226600985221675, 0.18226600985221675, 0.18226600985221675, None)

Embeddings:
Word_embeddings:1343
Train_doc_embeddings:1896
Test_doc_embeddings:812
