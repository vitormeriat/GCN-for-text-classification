
==================== Torch Seed: 23835951998400
Epoch:0001, train_loss=5.51739, train_acc=0.05067, val_loss=3.25295, val_acc=0.04598, time=7.91407
Epoch:0002, train_loss=4.99409, train_acc=0.07385, val_loss=3.23392, val_acc=0.04863, time=8.38808
Epoch:0003, train_loss=4.57192, train_acc=0.10233, val_loss=3.22103, val_acc=0.04951, time=8.17008
Epoch:0004, train_loss=4.23592, train_acc=0.13434, val_loss=3.21258, val_acc=0.05747, time=7.93809
Epoch:0005, train_loss=3.96187, train_acc=0.17883, val_loss=3.20742, val_acc=0.05482, time=8.07106
Epoch:0006, train_loss=3.73702, train_acc=0.22832, val_loss=3.20459, val_acc=0.05305, time=8.47807
Epoch:0007, train_loss=3.54977, train_acc=0.28096, val_loss=3.20300, val_acc=0.05217, time=8.08909
Epoch:0008, train_loss=3.38567, train_acc=0.33890, val_loss=3.20180, val_acc=0.05217, time=8.16907
Epoch:0009, train_loss=3.23665, train_acc=0.40558, val_loss=3.20065, val_acc=0.04863, time=8.08906
Epoch:0010, train_loss=3.10211, train_acc=0.47786, val_loss=3.19950, val_acc=0.04863, time=8.17908
Epoch:0011, train_loss=2.98436, train_acc=0.55691, val_loss=3.19840, val_acc=0.05128, time=8.36306
Epoch:0012, train_loss=2.88385, train_acc=0.62388, val_loss=3.19740, val_acc=0.05217, time=8.42506
Epoch:0013, train_loss=2.79999, train_acc=0.68791, val_loss=3.19653, val_acc=0.05570, time=8.24107
Epoch:0014, train_loss=2.73156, train_acc=0.74654, val_loss=3.19577, val_acc=0.05570, time=8.41409
Epoch:0015, train_loss=2.67689, train_acc=0.79711, val_loss=3.19508, val_acc=0.05659, time=8.17808
Epoch:0016, train_loss=2.63381, train_acc=0.84160, val_loss=3.19444, val_acc=0.05393, time=8.43407
Epoch:0017, train_loss=2.60013, train_acc=0.87764, val_loss=3.19384, val_acc=0.05305, time=8.23507
Epoch:0018, train_loss=2.57408, train_acc=0.90631, val_loss=3.19330, val_acc=0.05482, time=8.65907
Epoch:0019, train_loss=2.55428, train_acc=0.92998, val_loss=3.19282, val_acc=0.05659, time=8.54709
Epoch:0020, train_loss=2.53954, train_acc=0.94805, val_loss=3.19241, val_acc=0.05482, time=7.59209
Epoch:0021, train_loss=2.52867, train_acc=0.96288, val_loss=3.19207, val_acc=0.05393, time=7.50006
Epoch:0022, train_loss=2.52070, train_acc=0.97299, val_loss=3.19179, val_acc=0.05305, time=7.25407
Epoch:0023, train_loss=2.51493, train_acc=0.98115, val_loss=3.19156, val_acc=0.05128, time=7.60009
Epoch:0024, train_loss=2.51079, train_acc=0.98704, val_loss=3.19138, val_acc=0.04951, time=7.84408
Epoch:0025, train_loss=2.50783, train_acc=0.99146, val_loss=3.19123, val_acc=0.04863, time=7.47206
Epoch:0026, train_loss=2.50572, train_acc=0.99411, val_loss=3.19112, val_acc=0.04775, time=7.73108
Epoch:0027, train_loss=2.50421, train_acc=0.99578, val_loss=3.19102, val_acc=0.04775, time=7.60806
Epoch:0028, train_loss=2.50313, train_acc=0.99686, val_loss=3.19095, val_acc=0.04863, time=8.01308
Epoch:0029, train_loss=2.50234, train_acc=0.99715, val_loss=3.19090, val_acc=0.04863, time=7.26707
Epoch:0030, train_loss=2.50177, train_acc=0.99843, val_loss=3.19085, val_acc=0.04863, time=7.98306
Epoch:0031, train_loss=2.50136, train_acc=0.99921, val_loss=3.19082, val_acc=0.04863, time=7.86008
Epoch:0032, train_loss=2.50108, train_acc=0.99951, val_loss=3.19080, val_acc=0.04775, time=8.41108
Epoch:0033, train_loss=2.50090, train_acc=0.99961, val_loss=3.19078, val_acc=0.04863, time=7.62307
Epoch:0034, train_loss=2.50078, train_acc=0.99980, val_loss=3.19077, val_acc=0.04863, time=8.29107
Epoch:0035, train_loss=2.50070, train_acc=0.99980, val_loss=3.19076, val_acc=0.04775, time=7.73806
Epoch:0036, train_loss=2.50065, train_acc=0.99980, val_loss=3.19076, val_acc=0.04775, time=7.73307
Epoch:0037, train_loss=2.50061, train_acc=1.00000, val_loss=3.19076, val_acc=0.04775, time=7.84207
Epoch:0038, train_loss=2.50059, train_acc=1.00000, val_loss=3.19076, val_acc=0.04863, time=7.14908
Epoch:0039, train_loss=2.50058, train_acc=1.00000, val_loss=3.19076, val_acc=0.04863, time=7.44806
Epoch:0040, train_loss=2.50057, train_acc=1.00000, val_loss=3.19077, val_acc=0.04775, time=7.54208
Epoch:0041, train_loss=2.50057, train_acc=1.00000, val_loss=3.19077, val_acc=0.04686, time=8.17206
Epoch:0042, train_loss=2.50056, train_acc=1.00000, val_loss=3.19077, val_acc=0.04686, time=7.48008
Early stopping...

Optimization Finished!

Test set results: loss= 4.25359, accuracy= 0.05165, time= 2.28203

Test Precision, Recall and F1-Score...
              precision    recall  f1-score   support

           0     0.0659    0.0609    0.0633       394
           1     0.0403    0.0305    0.0347       394
           2     0.0559    0.0653    0.0603       398
           3     0.0667    0.0859    0.0751       396
           4     0.0642    0.0376    0.0474       319
           5     0.0448    0.0304    0.0362       395
           6     0.0594    0.0495    0.0540       364
           7     0.0324    0.0558    0.0410       251
           8     0.0419    0.0407    0.0413       393
           9     0.0373    0.0387    0.0380       310
          10     0.0663    0.0925    0.0773       389
          11     0.0430    0.0399    0.0414       376
          12     0.0483    0.0402    0.0439       398
          13     0.0515    0.0752    0.0612       399
          14     0.0772    0.0536    0.0633       392
          15     0.0542    0.0597    0.0569       385
          16     0.0468    0.0680    0.0554       397
          17     0.0485    0.0253    0.0332       396
          18     0.0458    0.0308    0.0368       390
          19     0.0418    0.0480    0.0447       396

    accuracy                         0.0516      7532
   macro avg     0.0516    0.0514    0.0503      7532
weighted avg     0.0520    0.0516    0.0506      7532


Macro average Test Precision, Recall and F1-Score...
(0.051610354246846867, 0.05141767817928655, 0.05025556041609667, None)

Micro average Test Precision, Recall and F1-Score...
(0.05164630908125332, 0.05164630908125332, 0.05164630908125332, None)

Embeddings:
Word_embeddings:42757
Train_doc_embeddings:11314
Test_doc_embeddings:7532
