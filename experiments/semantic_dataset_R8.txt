
==================== Torch Seed: 2078625157500
Epoch:0001, train_loss=2.14939, train_acc=0.18392, val_loss=2.05885, val_acc=0.61679, time=0.66402
Epoch:0002, train_loss=1.89772, train_acc=0.58517, val_loss=2.04479, val_acc=0.72263, time=0.65800
Epoch:0003, train_loss=1.75870, train_acc=0.70610, val_loss=2.03817, val_acc=0.72993, time=0.85601
Epoch:0004, train_loss=1.68853, train_acc=0.75613, val_loss=2.03377, val_acc=0.76277, time=0.60401
Epoch:0005, train_loss=1.64288, train_acc=0.78043, val_loss=2.02963, val_acc=0.78285, time=0.58700
Epoch:0006, train_loss=1.60207, train_acc=0.80717, val_loss=2.02579, val_acc=0.81022, time=0.54601
Epoch:0007, train_loss=1.56477, train_acc=0.84322, val_loss=2.02264, val_acc=0.85219, time=0.53201
Epoch:0008, train_loss=1.53396, train_acc=0.88576, val_loss=2.02028, val_acc=0.88321, time=0.75700
Epoch:0009, train_loss=1.51076, train_acc=0.91918, val_loss=2.01858, val_acc=0.89416, time=0.73400
Epoch:0010, train_loss=1.49399, train_acc=0.93620, val_loss=2.01736, val_acc=0.91241, time=0.73104
Epoch:0011, train_loss=1.48194, train_acc=0.94997, val_loss=2.01646, val_acc=0.91606, time=0.76101
Epoch:0012, train_loss=1.47316, train_acc=0.95868, val_loss=2.01576, val_acc=0.91788, time=0.70701
Epoch:0013, train_loss=1.46655, train_acc=0.96536, val_loss=2.01519, val_acc=0.92153, time=0.61301
Epoch:0014, train_loss=1.46132, train_acc=0.96982, val_loss=2.01467, val_acc=0.92153, time=0.63900
Epoch:0015, train_loss=1.45692, train_acc=0.97286, val_loss=2.01419, val_acc=0.92518, time=0.67701
Epoch:0016, train_loss=1.45302, train_acc=0.97448, val_loss=2.01373, val_acc=0.93066, time=0.66701
Epoch:0017, train_loss=1.44944, train_acc=0.97529, val_loss=2.01329, val_acc=0.93248, time=0.72002
Epoch:0018, train_loss=1.44613, train_acc=0.97812, val_loss=2.01287, val_acc=0.93796, time=0.71600
Epoch:0019, train_loss=1.44309, train_acc=0.97974, val_loss=2.01250, val_acc=0.93796, time=0.64801
Epoch:0020, train_loss=1.44036, train_acc=0.98137, val_loss=2.01217, val_acc=0.93796, time=0.52703
Epoch:0021, train_loss=1.43793, train_acc=0.98299, val_loss=2.01189, val_acc=0.94161, time=0.70400
Epoch:0022, train_loss=1.43581, train_acc=0.98380, val_loss=2.01166, val_acc=0.94161, time=0.71402
Epoch:0023, train_loss=1.43398, train_acc=0.98582, val_loss=2.01147, val_acc=0.94161, time=0.63701
Epoch:0024, train_loss=1.43241, train_acc=0.98724, val_loss=2.01131, val_acc=0.93978, time=0.65303
Epoch:0025, train_loss=1.43105, train_acc=0.98744, val_loss=2.01119, val_acc=0.93978, time=0.56201
Epoch:0026, train_loss=1.42987, train_acc=0.98764, val_loss=2.01109, val_acc=0.93796, time=0.52100
Epoch:0027, train_loss=1.42881, train_acc=0.98805, val_loss=2.01100, val_acc=0.93796, time=0.63401
Epoch:0028, train_loss=1.42784, train_acc=0.98866, val_loss=2.01094, val_acc=0.93613, time=0.75402
Epoch:0029, train_loss=1.42694, train_acc=0.98926, val_loss=2.01088, val_acc=0.93613, time=0.62201
Epoch:0030, train_loss=1.42609, train_acc=0.98967, val_loss=2.01083, val_acc=0.93796, time=0.77200
Epoch:0031, train_loss=1.42530, train_acc=0.98987, val_loss=2.01079, val_acc=0.93978, time=0.72201
Epoch:0032, train_loss=1.42455, train_acc=0.99089, val_loss=2.01075, val_acc=0.93978, time=0.69602
Epoch:0033, train_loss=1.42384, train_acc=0.99149, val_loss=2.01072, val_acc=0.94161, time=0.66202
Epoch:0034, train_loss=1.42319, train_acc=0.99170, val_loss=2.01068, val_acc=0.94161, time=0.77200
Epoch:0035, train_loss=1.42258, train_acc=0.99190, val_loss=2.01066, val_acc=0.94161, time=0.77701
Epoch:0036, train_loss=1.42201, train_acc=0.99251, val_loss=2.01063, val_acc=0.94526, time=0.72801
Epoch:0037, train_loss=1.42148, train_acc=0.99372, val_loss=2.01061, val_acc=0.94708, time=0.77401
Epoch:0038, train_loss=1.42100, train_acc=0.99413, val_loss=2.01058, val_acc=0.94708, time=0.73502
Epoch:0039, train_loss=1.42056, train_acc=0.99473, val_loss=2.01056, val_acc=0.94891, time=0.73501
Epoch:0040, train_loss=1.42015, train_acc=0.99534, val_loss=2.01054, val_acc=0.94891, time=0.73601
Epoch:0041, train_loss=1.41978, train_acc=0.99514, val_loss=2.01052, val_acc=0.94891, time=0.69000
Epoch:0042, train_loss=1.41943, train_acc=0.99514, val_loss=2.01050, val_acc=0.94708, time=0.67102
Epoch:0043, train_loss=1.41911, train_acc=0.99575, val_loss=2.01048, val_acc=0.94526, time=0.55103
Epoch:0044, train_loss=1.41881, train_acc=0.99575, val_loss=2.01047, val_acc=0.94526, time=0.59701
Epoch:0045, train_loss=1.41853, train_acc=0.99615, val_loss=2.01045, val_acc=0.94526, time=0.65601
Epoch:0046, train_loss=1.41827, train_acc=0.99615, val_loss=2.01044, val_acc=0.94526, time=0.52702
Epoch:0047, train_loss=1.41802, train_acc=0.99656, val_loss=2.01043, val_acc=0.94526, time=0.57701
Epoch:0048, train_loss=1.41779, train_acc=0.99696, val_loss=2.01042, val_acc=0.94526, time=0.63401
Epoch:0049, train_loss=1.41757, train_acc=0.99716, val_loss=2.01041, val_acc=0.94526, time=0.66400
Epoch:0050, train_loss=1.41737, train_acc=0.99737, val_loss=2.01039, val_acc=0.94526, time=0.76302
Epoch:0051, train_loss=1.41717, train_acc=0.99737, val_loss=2.01038, val_acc=0.94526, time=0.67500
Epoch:0052, train_loss=1.41699, train_acc=0.99757, val_loss=2.01037, val_acc=0.94526, time=0.65802
Epoch:0053, train_loss=1.41682, train_acc=0.99777, val_loss=2.01036, val_acc=0.94526, time=0.72200
Epoch:0054, train_loss=1.41666, train_acc=0.99797, val_loss=2.01035, val_acc=0.94526, time=0.69502
Epoch:0055, train_loss=1.41651, train_acc=0.99777, val_loss=2.01034, val_acc=0.94526, time=0.80700
Epoch:0056, train_loss=1.41637, train_acc=0.99777, val_loss=2.01033, val_acc=0.94526, time=0.71899
Epoch:0057, train_loss=1.41623, train_acc=0.99797, val_loss=2.01032, val_acc=0.94526, time=0.81501
Epoch:0058, train_loss=1.41611, train_acc=0.99797, val_loss=2.01032, val_acc=0.94526, time=0.79302
Epoch:0059, train_loss=1.41599, train_acc=0.99797, val_loss=2.01032, val_acc=0.94526, time=0.82101
Epoch:0060, train_loss=1.41588, train_acc=0.99797, val_loss=2.01031, val_acc=0.94526, time=0.72502
Epoch:0061, train_loss=1.41577, train_acc=0.99797, val_loss=2.01031, val_acc=0.94161, time=0.68400
Epoch:0062, train_loss=1.41567, train_acc=0.99797, val_loss=2.01031, val_acc=0.94161, time=0.65900
Epoch:0063, train_loss=1.41557, train_acc=0.99797, val_loss=2.01031, val_acc=0.94161, time=0.83401
Epoch:0064, train_loss=1.41548, train_acc=0.99818, val_loss=2.01031, val_acc=0.94161, time=0.78903
Epoch:0065, train_loss=1.41539, train_acc=0.99838, val_loss=2.01031, val_acc=0.94161, time=0.69200
Epoch:0066, train_loss=1.41531, train_acc=0.99838, val_loss=2.01031, val_acc=0.94161, time=0.82201
Epoch:0067, train_loss=1.41523, train_acc=0.99838, val_loss=2.01032, val_acc=0.94161, time=0.75104
Early stopping...

Optimization Finished!

Test set results: loss= 1.80485, accuracy= 0.95295, time= 0.27702

Test Precision, Recall and F1-Score...
              precision    recall  f1-score   support

           0     0.9776    0.9425    0.9598       696
           1     0.9683    0.9861    0.9771      1083
           2     0.8642    0.9333    0.8974        75
           3     0.9008    0.9752    0.9365       121
           4     0.8841    0.7531    0.8133        81
           5     0.8229    0.9080    0.8634        87
           6     0.7692    1.0000    0.8696        10
           7     0.9600    0.6667    0.7869        36

    accuracy                         0.9529      2189
   macro avg     0.8934    0.8956    0.8880      2189
weighted avg     0.9540    0.9529    0.9524      2189


Macro average Test Precision, Recall and F1-Score...
(0.8933849951362203, 0.8956271660568184, 0.8880010865480293, None)

Micro average Test Precision, Recall and F1-Score...
(0.9529465509365007, 0.9529465509365007, 0.9529465509365007, None)

Embeddings:
Word_embeddings:7688
Train_doc_embeddings:5485
Test_doc_embeddings:2189

Elapsed time is 47.930556 seconds.
