
==================== Torch Seed: 29405825432000
Epoch:0001, train_loss=2.25952, train_acc=0.15131, val_loss=2.06713, val_acc=0.48723, time=1.01901
Epoch:0002, train_loss=1.97736, train_acc=0.50152, val_loss=2.04824, val_acc=0.66606, time=0.88501
Epoch:0003, train_loss=1.79476, train_acc=0.69536, val_loss=2.03955, val_acc=0.70073, time=0.80900
Epoch:0004, train_loss=1.70247, train_acc=0.72331, val_loss=2.03636, val_acc=0.71168, time=0.91101
Epoch:0005, train_loss=1.66239, train_acc=0.73405, val_loss=2.03310, val_acc=0.73358, time=0.86301
Epoch:0006, train_loss=1.62705, train_acc=0.77172, val_loss=2.02940, val_acc=0.78467, time=0.81701
Epoch:0007, train_loss=1.59249, train_acc=0.82864, val_loss=2.02621, val_acc=0.81934, time=0.81500
Epoch:0008, train_loss=1.56515, train_acc=0.86692, val_loss=2.02377, val_acc=0.85036, time=0.81901
Epoch:0009, train_loss=1.54514, train_acc=0.88779, val_loss=2.02182, val_acc=0.86131, time=0.81901
Epoch:0010, train_loss=1.52915, train_acc=0.89690, val_loss=2.02010, val_acc=0.87226, time=0.81601
Epoch:0011, train_loss=1.51449, train_acc=0.91007, val_loss=2.01846, val_acc=0.88139, time=0.83401
Epoch:0012, train_loss=1.50018, train_acc=0.92445, val_loss=2.01693, val_acc=0.90146, time=0.83400
Epoch:0013, train_loss=1.48653, train_acc=0.93761, val_loss=2.01560, val_acc=0.90876, time=0.87701
Epoch:0014, train_loss=1.47452, train_acc=0.94997, val_loss=2.01456, val_acc=0.91423, time=0.84300
Epoch:0015, train_loss=1.46499, train_acc=0.95989, val_loss=2.01386, val_acc=0.92153, time=0.82808
Epoch:0016, train_loss=1.45812, train_acc=0.96658, val_loss=2.01348, val_acc=0.92518, time=0.80912
Epoch:0017, train_loss=1.45353, train_acc=0.97164, val_loss=2.01333, val_acc=0.92701, time=0.95913
Epoch:0018, train_loss=1.45056, train_acc=0.97407, val_loss=2.01330, val_acc=0.93248, time=0.84411
Epoch:0019, train_loss=1.44852, train_acc=0.97428, val_loss=2.01331, val_acc=0.93248, time=0.84411
Epoch:0020, train_loss=1.44684, train_acc=0.97468, val_loss=2.01328, val_acc=0.93613, time=0.85010
Epoch:0021, train_loss=1.44516, train_acc=0.97731, val_loss=2.01319, val_acc=0.93431, time=0.84409
Epoch:0022, train_loss=1.44331, train_acc=0.97873, val_loss=2.01303, val_acc=0.93431, time=0.86009
Epoch:0023, train_loss=1.44127, train_acc=0.98076, val_loss=2.01282, val_acc=0.93613, time=0.86208
Epoch:0024, train_loss=1.43916, train_acc=0.98420, val_loss=2.01258, val_acc=0.93796, time=0.85708
Epoch:0025, train_loss=1.43708, train_acc=0.98420, val_loss=2.01234, val_acc=0.93796, time=0.84608
Epoch:0026, train_loss=1.43514, train_acc=0.98542, val_loss=2.01211, val_acc=0.93978, time=0.85007
Epoch:0027, train_loss=1.43341, train_acc=0.98683, val_loss=2.01190, val_acc=0.93978, time=0.86608
Epoch:0028, train_loss=1.43189, train_acc=0.98724, val_loss=2.01172, val_acc=0.93978, time=0.83906
Epoch:0029, train_loss=1.43057, train_acc=0.98866, val_loss=2.01157, val_acc=0.93978, time=0.86306
Epoch:0030, train_loss=1.42944, train_acc=0.98987, val_loss=2.01146, val_acc=0.93796, time=0.83505
Epoch:0031, train_loss=1.42846, train_acc=0.99028, val_loss=2.01137, val_acc=0.93978, time=0.85906
Epoch:0032, train_loss=1.42759, train_acc=0.99048, val_loss=2.01131, val_acc=0.94161, time=0.85005
Epoch:0033, train_loss=1.42681, train_acc=0.99109, val_loss=2.01127, val_acc=0.94161, time=0.84606
Epoch:0034, train_loss=1.42610, train_acc=0.99170, val_loss=2.01124, val_acc=0.94343, time=0.85605
Epoch:0035, train_loss=1.42544, train_acc=0.99251, val_loss=2.01123, val_acc=0.94343, time=0.84905
Epoch:0036, train_loss=1.42482, train_acc=0.99271, val_loss=2.01123, val_acc=0.94343, time=0.85504
Epoch:0037, train_loss=1.42423, train_acc=0.99291, val_loss=2.01123, val_acc=0.94343, time=0.85104
Epoch:0038, train_loss=1.42368, train_acc=0.99291, val_loss=2.01124, val_acc=0.94343, time=0.85104
Epoch:0039, train_loss=1.42316, train_acc=0.99332, val_loss=2.01125, val_acc=0.94343, time=0.85404
Epoch:0040, train_loss=1.42267, train_acc=0.99352, val_loss=2.01127, val_acc=0.94343, time=0.84404
Epoch:0041, train_loss=1.42221, train_acc=0.99392, val_loss=2.01129, val_acc=0.94343, time=0.84904
Early stopping...

Optimization Finished!

Test set results: loss= 1.80594, accuracy= 0.95295, time= 0.28001

Test Precision, Recall and F1-Score...
              precision    recall  f1-score   support

           0     0.9750    0.9511    0.9629       696
           1     0.9717    0.9843    0.9780      1083
           2     0.8140    0.9333    0.8696        75
           3     0.8824    0.9917    0.9339       121
           4     0.8242    0.8621    0.8427        87
           5     0.9219    0.7284    0.8138        81
           6     1.0000    0.6667    0.8000        36
           7     0.8333    1.0000    0.9091        10

    accuracy                         0.9529      2189
   macro avg     0.9028    0.8897    0.8887      2189
weighted avg     0.9545    0.9529    0.9523      2189


Macro average Test Precision, Recall and F1-Score...
(0.9027993600413056, 0.8897064815177851, 0.8887360926883789, None)

Micro average Test Precision, Recall and F1-Score...
(0.9529465509365007, 0.9529465509365007, 0.9529465509365007, None)

Embeddings:
Word_embeddings:7688
Train_doc_embeddings:5485
Test_doc_embeddings:2189

Elapsed time is 36.929060 seconds.
