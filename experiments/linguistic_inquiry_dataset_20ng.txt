
==================== Torch Seed: 148795326001200
Epoch:0001, train_loss=3.06528, train_acc=0.05244, val_loss=2.99552, val_acc=0.14766, time=7.26887
Epoch:0002, train_loss=2.98585, train_acc=0.15987, val_loss=2.98883, val_acc=0.34129, time=6.75089
Epoch:0003, train_loss=2.92235, train_acc=0.35127, val_loss=2.98335, val_acc=0.51636, time=6.69890
Epoch:0004, train_loss=2.87032, train_acc=0.54728, val_loss=2.97871, val_acc=0.62511, time=6.88988
Epoch:0005, train_loss=2.82632, train_acc=0.66513, val_loss=2.97459, val_acc=0.69938, time=6.61289
Epoch:0006, train_loss=2.78725, train_acc=0.73358, val_loss=2.97080, val_acc=0.73917, time=6.94389
Epoch:0007, train_loss=2.75124, train_acc=0.78857, val_loss=2.96732, val_acc=0.77984, time=7.18589
Epoch:0008, train_loss=2.71797, train_acc=0.83531, val_loss=2.96418, val_acc=0.81963, time=7.21089
Epoch:0009, train_loss=2.68789, train_acc=0.87243, val_loss=2.96142, val_acc=0.84439, time=7.37588
Epoch:0010, train_loss=2.66146, train_acc=0.90219, val_loss=2.95905, val_acc=0.86914, time=8.26192
Epoch:0011, train_loss=2.63889, train_acc=0.92487, val_loss=2.95706, val_acc=0.87622, time=7.75886
Epoch:0012, train_loss=2.62001, train_acc=0.93754, val_loss=2.95541, val_acc=0.88683, time=7.06589
Epoch:0013, train_loss=2.60441, train_acc=0.94432, val_loss=2.95405, val_acc=0.89567, time=7.64476
Epoch:0014, train_loss=2.59157, train_acc=0.95001, val_loss=2.95292, val_acc=0.89567, time=7.80243
Epoch:0015, train_loss=2.58095, train_acc=0.95375, val_loss=2.95199, val_acc=0.89920, time=7.15189
Epoch:0016, train_loss=2.57205, train_acc=0.95728, val_loss=2.95119, val_acc=0.90451, time=7.16289
Epoch:0017, train_loss=2.56443, train_acc=0.95993, val_loss=2.95049, val_acc=0.90363, time=7.09288
Epoch:0018, train_loss=2.55779, train_acc=0.96160, val_loss=2.94989, val_acc=0.90628, time=6.69190
Epoch:0019, train_loss=2.55198, train_acc=0.96435, val_loss=2.94938, val_acc=0.90893, time=6.63690
Epoch:0020, train_loss=2.54691, train_acc=0.96681, val_loss=2.94895, val_acc=0.91158, time=7.03990
Epoch:0021, train_loss=2.54252, train_acc=0.96907, val_loss=2.94859, val_acc=0.91247, time=6.72890
Epoch:0022, train_loss=2.53873, train_acc=0.97142, val_loss=2.94829, val_acc=0.91335, time=6.69189
Epoch:0023, train_loss=2.53546, train_acc=0.97339, val_loss=2.94805, val_acc=0.91424, time=7.06089
Epoch:0024, train_loss=2.53261, train_acc=0.97594, val_loss=2.94785, val_acc=0.91424, time=7.08589
Epoch:0025, train_loss=2.53011, train_acc=0.97751, val_loss=2.94768, val_acc=0.91866, time=7.52888
Epoch:0026, train_loss=2.52790, train_acc=0.97938, val_loss=2.94754, val_acc=0.92131, time=7.65806
Epoch:0027, train_loss=2.52592, train_acc=0.98095, val_loss=2.94741, val_acc=0.92308, time=7.93488
Epoch:0028, train_loss=2.52415, train_acc=0.98203, val_loss=2.94730, val_acc=0.92308, time=7.61088
Epoch:0029, train_loss=2.52255, train_acc=0.98360, val_loss=2.94720, val_acc=0.92396, time=7.13488
Epoch:0030, train_loss=2.52110, train_acc=0.98448, val_loss=2.94712, val_acc=0.92661, time=6.88192
Epoch:0031, train_loss=2.51977, train_acc=0.98517, val_loss=2.94704, val_acc=0.92573, time=7.33487
Epoch:0032, train_loss=2.51855, train_acc=0.98655, val_loss=2.94697, val_acc=0.92573, time=6.90290
Epoch:0033, train_loss=2.51743, train_acc=0.98733, val_loss=2.94690, val_acc=0.92485, time=6.93189
Epoch:0034, train_loss=2.51640, train_acc=0.98841, val_loss=2.94685, val_acc=0.92485, time=7.20088
Epoch:0035, train_loss=2.51547, train_acc=0.98930, val_loss=2.94680, val_acc=0.92573, time=6.85191
Epoch:0036, train_loss=2.51461, train_acc=0.98998, val_loss=2.94676, val_acc=0.92573, time=6.83689
Epoch:0037, train_loss=2.51383, train_acc=0.99106, val_loss=2.94673, val_acc=0.92750, time=6.91188
Epoch:0038, train_loss=2.51311, train_acc=0.99146, val_loss=2.94671, val_acc=0.92750, time=6.72391
Epoch:0039, train_loss=2.51245, train_acc=0.99185, val_loss=2.94669, val_acc=0.92750, time=7.00990
Epoch:0040, train_loss=2.51183, train_acc=0.99234, val_loss=2.94667, val_acc=0.92750, time=6.95987
Epoch:0041, train_loss=2.51126, train_acc=0.99313, val_loss=2.94666, val_acc=0.92838, time=7.10690
Epoch:0042, train_loss=2.51073, train_acc=0.99352, val_loss=2.94665, val_acc=0.92838, time=6.77789
Epoch:0043, train_loss=2.51023, train_acc=0.99421, val_loss=2.94664, val_acc=0.92661, time=7.09391
Epoch:0044, train_loss=2.50977, train_acc=0.99470, val_loss=2.94663, val_acc=0.92661, time=7.20889
Epoch:0045, train_loss=2.50934, train_acc=0.99529, val_loss=2.94662, val_acc=0.92661, time=6.93689
Epoch:0046, train_loss=2.50893, train_acc=0.99529, val_loss=2.94662, val_acc=0.92661, time=6.90390
Epoch:0047, train_loss=2.50855, train_acc=0.99588, val_loss=2.94661, val_acc=0.92661, time=6.79690
Epoch:0048, train_loss=2.50820, train_acc=0.99637, val_loss=2.94660, val_acc=0.92838, time=7.04190
Epoch:0049, train_loss=2.50787, train_acc=0.99676, val_loss=2.94659, val_acc=0.92838, time=7.03090
Epoch:0050, train_loss=2.50756, train_acc=0.99696, val_loss=2.94659, val_acc=0.92838, time=7.04688
Epoch:0051, train_loss=2.50728, train_acc=0.99715, val_loss=2.94659, val_acc=0.92838, time=6.92190
Epoch:0052, train_loss=2.50701, train_acc=0.99725, val_loss=2.94658, val_acc=0.92750, time=6.87489
Epoch:0053, train_loss=2.50676, train_acc=0.99745, val_loss=2.94658, val_acc=0.92750, time=6.72591
Epoch:0054, train_loss=2.50652, train_acc=0.99735, val_loss=2.94659, val_acc=0.92661, time=7.23389
Epoch:0055, train_loss=2.50630, train_acc=0.99745, val_loss=2.94659, val_acc=0.92661, time=7.01591
Epoch:0056, train_loss=2.50609, train_acc=0.99794, val_loss=2.94659, val_acc=0.92661, time=6.74190
Epoch:0057, train_loss=2.50590, train_acc=0.99813, val_loss=2.94659, val_acc=0.92573, time=6.77087
Early stopping...

Optimization Finished!

Test set results: loss= 2.70158, accuracy= 0.84241, time= 2.23896

Test Precision, Recall and F1-Score...
              precision    recall  f1-score   support

           0     0.9208    0.8813    0.9006       396
           1     0.8559    0.6516    0.7399       310
           2     0.8211    0.7671    0.7932       395
           3     0.8208    0.8208    0.8208       385
           4     0.6703    0.7781    0.7202       392
           5     0.9747    0.9674    0.9711       399
           6     0.7769    0.7710    0.7739       393
           7     0.7146    0.7789    0.7454       389
           8     0.7415    0.7872    0.7637       390
           9     0.9837    0.9654    0.9745       376
          10     0.9526    0.9598    0.9562       398
          11     0.7658    0.8984    0.8268       364
          12     0.7877    0.6497    0.7121       394
          13     0.8807    0.9271    0.9033       398
          14     0.8624    0.8056    0.8331       319
          15     0.8668    0.8712    0.8690       396
          16     0.8940    0.9369    0.9149       396
          17     0.7297    0.6454    0.6850       251
          18     0.9310    0.9521    0.9415       397
          19     0.8936    0.9162    0.9048       394

    accuracy                         0.8424      7532
   macro avg     0.8422    0.8366    0.8375      7532
weighted avg     0.8445    0.8424    0.8417      7532


Macro average Test Precision, Recall and F1-Score...
(0.842247214498727, 0.8365675161565147, 0.8374969082252111, None)

Micro average Test Precision, Recall and F1-Score...
(0.8424057355284121, 0.8424057355284121, 0.8424057355284121, None)

Embeddings:
Word_embeddings:42757
Train_doc_embeddings:11314
Test_doc_embeddings:7532

Elapsed time is 414.674059 seconds.
