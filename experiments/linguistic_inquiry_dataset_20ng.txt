
==================== Torch Seed: 47377674134000
Epoch:0001, train_loss=3.06946, train_acc=0.04409, val_loss=2.99473, val_acc=0.14589, time=9.01286
Epoch:0002, train_loss=2.98871, train_acc=0.14848, val_loss=2.98848, val_acc=0.33952, time=7.54888
Epoch:0003, train_loss=2.92620, train_acc=0.34725, val_loss=2.98335, val_acc=0.48895, time=6.89288
Epoch:0004, train_loss=2.87389, train_acc=0.53216, val_loss=2.97900, val_acc=0.58267, time=7.03387
Epoch:0005, train_loss=2.82923, train_acc=0.64519, val_loss=2.97506, val_acc=0.64191, time=7.04988
Epoch:0006, train_loss=2.78948, train_acc=0.71845, val_loss=2.97132, val_acc=0.70203, time=6.77490
Epoch:0007, train_loss=2.75260, train_acc=0.78140, val_loss=2.96776, val_acc=0.75243, time=6.79889
Epoch:0008, train_loss=2.71824, train_acc=0.83757, val_loss=2.96451, val_acc=0.79134, time=6.63889
Epoch:0009, train_loss=2.68723, train_acc=0.87911, val_loss=2.96167, val_acc=0.82582, time=6.92489
Epoch:0010, train_loss=2.66036, train_acc=0.90455, val_loss=2.95927, val_acc=0.84527, time=7.84187
Epoch:0011, train_loss=2.63780, train_acc=0.92242, val_loss=2.95728, val_acc=0.85234, time=7.07188
Epoch:0012, train_loss=2.61915, train_acc=0.93234, val_loss=2.95564, val_acc=0.86207, time=7.22888
Epoch:0013, train_loss=2.60374, train_acc=0.94118, val_loss=2.95429, val_acc=0.87179, time=7.01490
Epoch:0014, train_loss=2.59092, train_acc=0.94717, val_loss=2.95315, val_acc=0.87622, time=7.26887
Epoch:0015, train_loss=2.58016, train_acc=0.95139, val_loss=2.95220, val_acc=0.88152, time=7.71788
Epoch:0016, train_loss=2.57106, train_acc=0.95492, val_loss=2.95141, val_acc=0.88594, time=7.43890
Epoch:0017, train_loss=2.56331, train_acc=0.95885, val_loss=2.95074, val_acc=0.88771, time=7.66486
Epoch:0018, train_loss=2.55668, train_acc=0.96131, val_loss=2.95018, val_acc=0.88771, time=7.49489
Epoch:0019, train_loss=2.55098, train_acc=0.96366, val_loss=2.94972, val_acc=0.89125, time=7.38488
Epoch:0020, train_loss=2.54605, train_acc=0.96691, val_loss=2.94933, val_acc=0.89302, time=7.10191
Epoch:0021, train_loss=2.54176, train_acc=0.96916, val_loss=2.94900, val_acc=0.89302, time=7.15788
Epoch:0022, train_loss=2.53800, train_acc=0.97142, val_loss=2.94873, val_acc=0.89302, time=7.01689
Epoch:0023, train_loss=2.53471, train_acc=0.97388, val_loss=2.94850, val_acc=0.89302, time=7.43090
Epoch:0024, train_loss=2.53182, train_acc=0.97565, val_loss=2.94831, val_acc=0.89478, time=7.71090
Epoch:0025, train_loss=2.52928, train_acc=0.97751, val_loss=2.94815, val_acc=0.89478, time=7.37887
Epoch:0026, train_loss=2.52705, train_acc=0.97967, val_loss=2.94803, val_acc=0.89744, time=7.14289
Epoch:0027, train_loss=2.52509, train_acc=0.98085, val_loss=2.94793, val_acc=0.90009, time=7.01391
Epoch:0028, train_loss=2.52335, train_acc=0.98154, val_loss=2.94785, val_acc=0.90009, time=6.91591
Epoch:0029, train_loss=2.52180, train_acc=0.98291, val_loss=2.94778, val_acc=0.89832, time=7.02690
Epoch:0030, train_loss=2.52039, train_acc=0.98439, val_loss=2.94772, val_acc=0.89920, time=6.82390
Epoch:0031, train_loss=2.51911, train_acc=0.98507, val_loss=2.94767, val_acc=0.90009, time=6.76690
Epoch:0032, train_loss=2.51794, train_acc=0.98615, val_loss=2.94763, val_acc=0.90186, time=6.92589
Epoch:0033, train_loss=2.51686, train_acc=0.98694, val_loss=2.94759, val_acc=0.90363, time=6.93892
Epoch:0034, train_loss=2.51587, train_acc=0.98782, val_loss=2.94756, val_acc=0.90186, time=6.74690
Epoch:0035, train_loss=2.51496, train_acc=0.98851, val_loss=2.94753, val_acc=0.90363, time=7.57989
Epoch:0036, train_loss=2.51413, train_acc=0.98959, val_loss=2.94750, val_acc=0.90539, time=7.00790
Epoch:0037, train_loss=2.51337, train_acc=0.99008, val_loss=2.94748, val_acc=0.90539, time=6.94890
Epoch:0038, train_loss=2.51266, train_acc=0.99087, val_loss=2.94747, val_acc=0.90539, time=6.60793
Epoch:0039, train_loss=2.51200, train_acc=0.99155, val_loss=2.94745, val_acc=0.90451, time=6.74689
Epoch:0040, train_loss=2.51139, train_acc=0.99273, val_loss=2.94744, val_acc=0.90628, time=6.68391
Epoch:0041, train_loss=2.51083, train_acc=0.99362, val_loss=2.94744, val_acc=0.90893, time=7.00990
Epoch:0042, train_loss=2.51031, train_acc=0.99391, val_loss=2.94744, val_acc=0.90893, time=6.71192
Epoch:0043, train_loss=2.50983, train_acc=0.99470, val_loss=2.94744, val_acc=0.91070, time=6.77890
Epoch:0044, train_loss=2.50939, train_acc=0.99480, val_loss=2.94744, val_acc=0.91070, time=7.02392
Epoch:0045, train_loss=2.50898, train_acc=0.99548, val_loss=2.94744, val_acc=0.91158, time=6.62992
Epoch:0046, train_loss=2.50860, train_acc=0.99607, val_loss=2.94745, val_acc=0.91424, time=6.71789
Epoch:0047, train_loss=2.50825, train_acc=0.99666, val_loss=2.94745, val_acc=0.91512, time=6.86191
Early stopping...

Optimization Finished!

Test set results: loss= 2.70093, accuracy= 0.83909, time= 2.18099

Test Precision, Recall and F1-Score...
              precision    recall  f1-score   support

           0     0.7888    0.7468    0.7672       395
           1     0.7055    0.7635    0.7333       389
           2     0.8768    0.9296    0.9024       398
           3     0.8867    0.9137    0.9000       394
           4     0.7848    0.7608    0.7726       393
           5     0.6614    0.7526    0.7041       392
           6     0.9169    0.8914    0.9040       396
           7     0.9240    0.9496    0.9366       397
           8     0.8969    0.9444    0.9200       396
           9     0.8838    0.8838    0.8838       396
          10     0.9415    0.9698    0.9554       398
          11     0.9755    0.9548    0.9651       376
          12     0.8306    0.7896    0.8096       385
          13     0.8379    0.6839    0.7531       310
          14     0.8651    0.7837    0.8224       319
          15     0.7623    0.6675    0.7118       394
          16     0.7222    0.6733    0.6969       251
          17     0.9672    0.9599    0.9635       399
          18     0.7494    0.7897    0.7690       390
          19     0.7822    0.8681    0.8229       364

    accuracy                         0.8391      7532
   macro avg     0.8380    0.8338    0.8347      7532
weighted avg     0.8402    0.8391    0.8385      7532


Macro average Test Precision, Recall and F1-Score...
(0.8379716378104595, 0.8338390692687412, 0.8346946499858152, None)

Micro average Test Precision, Recall and F1-Score...
(0.8390865639936272, 0.8390865639936272, 0.8390865639936272, None)

Embeddings:
Word_embeddings:42757
Train_doc_embeddings:11314
Test_doc_embeddings:7532
