
==================== Torch Seed: 47969927199400
Epoch:0001, train_loss=3.08763, train_acc=0.05627, val_loss=2.99586, val_acc=0.14589, time=10.30110
Epoch:0002, train_loss=3.00040, train_acc=0.14131, val_loss=2.98960, val_acc=0.31477, time=8.33807
Epoch:0003, train_loss=2.93905, train_acc=0.31140, val_loss=2.98443, val_acc=0.43678, time=8.42108
Epoch:0004, train_loss=2.88789, train_acc=0.44820, val_loss=2.97993, val_acc=0.57206, time=8.05707
Epoch:0005, train_loss=2.84284, train_acc=0.59884, val_loss=2.97602, val_acc=0.65959, time=8.24908
Epoch:0006, train_loss=2.80330, train_acc=0.69793, val_loss=2.97251, val_acc=0.70292, time=8.08507
Epoch:0007, train_loss=2.76793, train_acc=0.75881, val_loss=2.96921, val_acc=0.74536, time=8.19107
Epoch:0008, train_loss=2.73515, train_acc=0.80497, val_loss=2.96609, val_acc=0.78338, time=7.99206
Epoch:0009, train_loss=2.70453, train_acc=0.84847, val_loss=2.96322, val_acc=0.81786, time=8.12108
Epoch:0010, train_loss=2.67670, train_acc=0.88942, val_loss=2.96070, val_acc=0.84085, time=8.30207
Epoch:0011, train_loss=2.65245, train_acc=0.91515, val_loss=2.95857, val_acc=0.85234, time=8.13706
Epoch:0012, train_loss=2.63202, train_acc=0.92959, val_loss=2.95680, val_acc=0.86737, time=8.30406
Epoch:0013, train_loss=2.61509, train_acc=0.94049, val_loss=2.95534, val_acc=0.87356, time=8.41207
Epoch:0014, train_loss=2.60105, train_acc=0.94677, val_loss=2.95412, val_acc=0.87798, time=8.48508
Epoch:0015, train_loss=2.58929, train_acc=0.95208, val_loss=2.95310, val_acc=0.87798, time=8.44007
Epoch:0016, train_loss=2.57934, train_acc=0.95512, val_loss=2.95224, val_acc=0.87975, time=8.03207
Epoch:0017, train_loss=2.57084, train_acc=0.95875, val_loss=2.95150, val_acc=0.88064, time=8.43207
Epoch:0018, train_loss=2.56353, train_acc=0.96052, val_loss=2.95087, val_acc=0.88506, time=8.23208
Epoch:0019, train_loss=2.55722, train_acc=0.96288, val_loss=2.95033, val_acc=0.89036, time=8.49307
Epoch:0020, train_loss=2.55176, train_acc=0.96533, val_loss=2.94986, val_acc=0.89125, time=8.31807
Epoch:0021, train_loss=2.54702, train_acc=0.96789, val_loss=2.94946, val_acc=0.89655, time=8.25906
Epoch:0022, train_loss=2.54291, train_acc=0.96985, val_loss=2.94912, val_acc=0.89920, time=8.22507
Epoch:0023, train_loss=2.53934, train_acc=0.97152, val_loss=2.94882, val_acc=0.90186, time=8.05506
Epoch:0024, train_loss=2.53621, train_acc=0.97329, val_loss=2.94857, val_acc=0.90274, time=8.25507
Epoch:0025, train_loss=2.53346, train_acc=0.97545, val_loss=2.94835, val_acc=0.90274, time=8.23707
Epoch:0026, train_loss=2.53103, train_acc=0.97712, val_loss=2.94815, val_acc=0.90363, time=8.12307
Epoch:0027, train_loss=2.52885, train_acc=0.97840, val_loss=2.94798, val_acc=0.90716, time=8.11107
Epoch:0028, train_loss=2.52689, train_acc=0.98036, val_loss=2.94783, val_acc=0.90893, time=8.40506
Epoch:0029, train_loss=2.52512, train_acc=0.98183, val_loss=2.94769, val_acc=0.91070, time=8.24308
Epoch:0030, train_loss=2.52352, train_acc=0.98272, val_loss=2.94757, val_acc=0.91070, time=8.46908
Epoch:0031, train_loss=2.52207, train_acc=0.98389, val_loss=2.94746, val_acc=0.91070, time=8.48507
Epoch:0032, train_loss=2.52075, train_acc=0.98458, val_loss=2.94737, val_acc=0.91335, time=8.23408
Epoch:0033, train_loss=2.51955, train_acc=0.98517, val_loss=2.94729, val_acc=0.91424, time=8.24408
Epoch:0034, train_loss=2.51846, train_acc=0.98596, val_loss=2.94722, val_acc=0.91512, time=8.12307
Epoch:0035, train_loss=2.51745, train_acc=0.98694, val_loss=2.94716, val_acc=0.91512, time=8.06109
Epoch:0036, train_loss=2.51653, train_acc=0.98792, val_loss=2.94710, val_acc=0.91512, time=8.21906
Epoch:0037, train_loss=2.51567, train_acc=0.98920, val_loss=2.94705, val_acc=0.91600, time=8.11608
Epoch:0038, train_loss=2.51488, train_acc=0.98979, val_loss=2.94701, val_acc=0.91689, time=8.05206
Epoch:0039, train_loss=2.51414, train_acc=0.99028, val_loss=2.94697, val_acc=0.91689, time=8.12609
Epoch:0040, train_loss=2.51346, train_acc=0.99106, val_loss=2.94693, val_acc=0.91689, time=8.41006
Epoch:0041, train_loss=2.51282, train_acc=0.99155, val_loss=2.94690, val_acc=0.91777, time=8.28308
Epoch:0042, train_loss=2.51223, train_acc=0.99214, val_loss=2.94688, val_acc=0.91866, time=8.23607
Epoch:0043, train_loss=2.51168, train_acc=0.99293, val_loss=2.94685, val_acc=0.91866, time=8.54906
Epoch:0044, train_loss=2.51116, train_acc=0.99303, val_loss=2.94683, val_acc=0.91954, time=7.99208
Epoch:0045, train_loss=2.51068, train_acc=0.99352, val_loss=2.94681, val_acc=0.91866, time=8.14306
Epoch:0046, train_loss=2.51023, train_acc=0.99421, val_loss=2.94679, val_acc=0.91954, time=8.27408
Epoch:0047, train_loss=2.50981, train_acc=0.99450, val_loss=2.94677, val_acc=0.91954, time=8.24107
Epoch:0048, train_loss=2.50942, train_acc=0.99470, val_loss=2.94675, val_acc=0.91954, time=8.45807
Epoch:0049, train_loss=2.50905, train_acc=0.99509, val_loss=2.94674, val_acc=0.91954, time=8.15109
Epoch:0050, train_loss=2.50870, train_acc=0.99558, val_loss=2.94672, val_acc=0.91954, time=8.33306
Epoch:0051, train_loss=2.50838, train_acc=0.99568, val_loss=2.94671, val_acc=0.91954, time=8.36407
Epoch:0052, train_loss=2.50808, train_acc=0.99617, val_loss=2.94670, val_acc=0.91777, time=8.07506
Epoch:0053, train_loss=2.50779, train_acc=0.99666, val_loss=2.94669, val_acc=0.91689, time=8.10308
Epoch:0054, train_loss=2.50752, train_acc=0.99686, val_loss=2.94668, val_acc=0.91689, time=8.12707
Epoch:0055, train_loss=2.50727, train_acc=0.99696, val_loss=2.94667, val_acc=0.91689, time=8.23408
Epoch:0056, train_loss=2.50703, train_acc=0.99696, val_loss=2.94667, val_acc=0.91600, time=8.25108
Epoch:0057, train_loss=2.50680, train_acc=0.99715, val_loss=2.94666, val_acc=0.91600, time=8.04307
Epoch:0058, train_loss=2.50658, train_acc=0.99735, val_loss=2.94666, val_acc=0.91600, time=8.30206
Epoch:0059, train_loss=2.50638, train_acc=0.99764, val_loss=2.94666, val_acc=0.91600, time=7.95906
Epoch:0060, train_loss=2.50619, train_acc=0.99794, val_loss=2.94666, val_acc=0.91600, time=8.16709
Epoch:0061, train_loss=2.50600, train_acc=0.99804, val_loss=2.94665, val_acc=0.91600, time=8.65998
Epoch:0062, train_loss=2.50583, train_acc=0.99804, val_loss=2.94665, val_acc=0.91600, time=8.34807
Epoch:0063, train_loss=2.50567, train_acc=0.99804, val_loss=2.94665, val_acc=0.91600, time=8.24008
Epoch:0064, train_loss=2.50552, train_acc=0.99823, val_loss=2.94665, val_acc=0.91600, time=8.10407
Epoch:0065, train_loss=2.50537, train_acc=0.99833, val_loss=2.94664, val_acc=0.91689, time=8.19407
Epoch:0066, train_loss=2.50523, train_acc=0.99863, val_loss=2.94664, val_acc=0.91689, time=8.26208
Epoch:0067, train_loss=2.50510, train_acc=0.99872, val_loss=2.94664, val_acc=0.91689, time=8.06806
Epoch:0068, train_loss=2.50497, train_acc=0.99882, val_loss=2.94664, val_acc=0.91689, time=8.35907
Epoch:0069, train_loss=2.50485, train_acc=0.99882, val_loss=2.94664, val_acc=0.91600, time=8.04308
Epoch:0070, train_loss=2.50474, train_acc=0.99882, val_loss=2.94664, val_acc=0.91600, time=8.08406
Epoch:0071, train_loss=2.50463, train_acc=0.99882, val_loss=2.94665, val_acc=0.91512, time=8.32707
Early stopping...

Optimization Finished!

Test set results: loss= 2.70063, accuracy= 0.84254, time= 2.50803

Test Precision, Recall and F1-Score...
              precision    recall  f1-score   support

           0     0.7153    0.7815    0.7469       389
           1     0.9350    0.9421    0.9385       397
           2     0.8729    0.9322    0.9016       398
           3     0.8655    0.8985    0.8817       394
           4     0.9622    0.9574    0.9598       399
           5     0.9570    0.9497    0.9533       398
           6     0.7458    0.7897    0.7671       390
           7     0.6746    0.7934    0.7292       392
           8     0.8854    0.7994    0.8402       319
           9     0.8471    0.6613    0.7428       310
          10     0.8068    0.7863    0.7964       393
          11     0.9062    0.9268    0.9164       396
          12     0.7842    0.8984    0.8374       364
          13     0.8825    0.8535    0.8678       396
          14     0.7265    0.6773    0.7010       251
          15     0.8056    0.8286    0.8169       385
          16     0.8175    0.7823    0.7995       395
          17     0.9121    0.8914    0.9017       396
          18     0.9727    0.9468    0.9596       376
          19     0.7805    0.6497    0.7091       394

    accuracy                         0.8425      7532
   macro avg     0.8428    0.8373    0.8383      7532
weighted avg     0.8449    0.8425    0.8421      7532


Macro average Test Precision, Recall and F1-Score...
(0.842765453344781, 0.8373067397200515, 0.8383413249160195, None)

Micro average Test Precision, Recall and F1-Score...
(0.8425385023898035, 0.8425385023898035, 0.8425385023898035, None)

Embeddings:
Word_embeddings:42757
Train_doc_embeddings:11314
Test_doc_embeddings:7532
