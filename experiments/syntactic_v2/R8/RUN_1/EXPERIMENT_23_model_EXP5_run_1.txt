
==========: 212140632538900
Epoch:0001, train_loss=2.54268, train_acc=0.02978, val_loss=2.08916, val_acc=0.27007, time=1.05802
Epoch:0002, train_loss=2.15888, train_acc=0.27041, val_loss=2.06019, val_acc=0.59307, time=1.15000
Epoch:0003, train_loss=1.89434, train_acc=0.59611, val_loss=2.04476, val_acc=0.70438, time=1.07000
Epoch:0004, train_loss=1.75580, train_acc=0.71805, val_loss=2.03831, val_acc=0.73540, time=1.06699
Epoch:0005, train_loss=1.69742, train_acc=0.75370, val_loss=2.03550, val_acc=0.76095, time=1.24002
Epoch:0006, train_loss=1.67017, train_acc=0.76423, val_loss=2.03326, val_acc=0.78285, time=1.13601
Epoch:0007, train_loss=1.64665, train_acc=0.78590, val_loss=2.03078, val_acc=0.81204, time=0.96200
Epoch:0008, train_loss=1.62026, train_acc=0.81325, val_loss=2.02818, val_acc=0.83577, time=1.04901
Epoch:0009, train_loss=1.59267, train_acc=0.83978, val_loss=2.02570, val_acc=0.86131, time=1.07800
Epoch:0010, train_loss=1.56660, train_acc=0.86247, val_loss=2.02352, val_acc=0.88504, time=1.04701
Epoch:0011, train_loss=1.54384, train_acc=0.88455, val_loss=2.02166, val_acc=0.89051, time=1.04100
Epoch:0012, train_loss=1.52501, train_acc=0.90115, val_loss=2.02008, val_acc=0.89416, time=1.08001
Epoch:0013, train_loss=1.50974, train_acc=0.91979, val_loss=2.01872, val_acc=0.90511, time=1.06301
Epoch:0014, train_loss=1.49728, train_acc=0.93073, val_loss=2.01753, val_acc=0.91423, time=1.16802
Epoch:0015, train_loss=1.48687, train_acc=0.94025, val_loss=2.01645, val_acc=0.91423, time=1.11601
Epoch:0016, train_loss=1.47800, train_acc=0.94855, val_loss=2.01547, val_acc=0.92701, time=1.02901
Epoch:0017, train_loss=1.47042, train_acc=0.95483, val_loss=2.01460, val_acc=0.93796, time=1.10100
Epoch:0018, train_loss=1.46400, train_acc=0.96152, val_loss=2.01384, val_acc=0.93978, time=1.12202
Epoch:0019, train_loss=1.45863, train_acc=0.96516, val_loss=2.01319, val_acc=0.94343, time=1.04102
Epoch:0020, train_loss=1.45417, train_acc=0.96860, val_loss=2.01265, val_acc=0.94891, time=1.32599
Epoch:0021, train_loss=1.45047, train_acc=0.97245, val_loss=2.01221, val_acc=0.95073, time=1.06602
Epoch:0022, train_loss=1.44733, train_acc=0.97468, val_loss=2.01186, val_acc=0.95255, time=1.12698
Epoch:0023, train_loss=1.44464, train_acc=0.97731, val_loss=2.01158, val_acc=0.95620, time=1.08500
Epoch:0024, train_loss=1.44230, train_acc=0.97914, val_loss=2.01136, val_acc=0.95620, time=1.10801
Epoch:0025, train_loss=1.44025, train_acc=0.97954, val_loss=2.01120, val_acc=0.95985, time=1.06801
Epoch:0026, train_loss=1.43845, train_acc=0.98197, val_loss=2.01108, val_acc=0.95803, time=1.09301
Epoch:0027, train_loss=1.43689, train_acc=0.98319, val_loss=2.01099, val_acc=0.95620, time=1.06500
Epoch:0028, train_loss=1.43556, train_acc=0.98501, val_loss=2.01092, val_acc=0.95620, time=1.01002
Epoch:0029, train_loss=1.43442, train_acc=0.98683, val_loss=2.01088, val_acc=0.95255, time=1.15100
Epoch:0030, train_loss=1.43344, train_acc=0.98643, val_loss=2.01085, val_acc=0.95255, time=1.09800
Epoch:0031, train_loss=1.43256, train_acc=0.98764, val_loss=2.01083, val_acc=0.95438, time=1.10301
Epoch:0032, train_loss=1.43174, train_acc=0.98744, val_loss=2.01081, val_acc=0.95803, time=1.12299
Epoch:0033, train_loss=1.43092, train_acc=0.98744, val_loss=2.01080, val_acc=0.95985, time=0.98602
Epoch:0034, train_loss=1.43007, train_acc=0.98785, val_loss=2.01078, val_acc=0.95985, time=1.12301
Epoch:0035, train_loss=1.42919, train_acc=0.98805, val_loss=2.01075, val_acc=0.96168, time=1.08001
Epoch:0036, train_loss=1.42828, train_acc=0.98825, val_loss=2.01073, val_acc=0.96168, time=1.09600
Epoch:0037, train_loss=1.42737, train_acc=0.98845, val_loss=2.01070, val_acc=0.96168, time=1.18300
Epoch:0038, train_loss=1.42649, train_acc=0.98987, val_loss=2.01068, val_acc=0.96168, time=1.13201
Epoch:0039, train_loss=1.42567, train_acc=0.99109, val_loss=2.01067, val_acc=0.96168, time=1.17200
Epoch:0040, train_loss=1.42493, train_acc=0.99251, val_loss=2.01065, val_acc=0.95985, time=1.04502
Epoch:0041, train_loss=1.42429, train_acc=0.99332, val_loss=2.01065, val_acc=0.95985, time=1.25001
Epoch:0042, train_loss=1.42374, train_acc=0.99311, val_loss=2.01065, val_acc=0.96168, time=1.07998
Epoch:0043, train_loss=1.42326, train_acc=0.99332, val_loss=2.01065, val_acc=0.96350, time=0.98802
Epoch:0044, train_loss=1.42285, train_acc=0.99372, val_loss=2.01065, val_acc=0.96533, time=0.98500
Epoch:0045, train_loss=1.42246, train_acc=0.99392, val_loss=2.01065, val_acc=0.96350, time=1.04001
Epoch:0046, train_loss=1.42210, train_acc=0.99392, val_loss=2.01065, val_acc=0.96533, time=1.15500
Epoch:0047, train_loss=1.42176, train_acc=0.99392, val_loss=2.01065, val_acc=0.96533, time=0.95402
Epoch:0048, train_loss=1.42142, train_acc=0.99413, val_loss=2.01064, val_acc=0.96533, time=1.00200
Epoch:0049, train_loss=1.42108, train_acc=0.99433, val_loss=2.01064, val_acc=0.96350, time=1.00301
Epoch:0050, train_loss=1.42075, train_acc=0.99473, val_loss=2.01063, val_acc=0.96350, time=1.22701
Epoch:0051, train_loss=1.42043, train_acc=0.99473, val_loss=2.01062, val_acc=0.96350, time=1.00000
Epoch:0052, train_loss=1.42013, train_acc=0.99473, val_loss=2.01061, val_acc=0.96350, time=1.13001
Epoch:0053, train_loss=1.41984, train_acc=0.99554, val_loss=2.01060, val_acc=0.96350, time=1.17901
Epoch:0054, train_loss=1.41957, train_acc=0.99575, val_loss=2.01059, val_acc=0.96350, time=0.97401
Epoch:0055, train_loss=1.41932, train_acc=0.99595, val_loss=2.01058, val_acc=0.96533, time=0.96599
Epoch:0056, train_loss=1.41910, train_acc=0.99595, val_loss=2.01057, val_acc=0.96350, time=1.13601
Epoch:0057, train_loss=1.41889, train_acc=0.99615, val_loss=2.01056, val_acc=0.96350, time=0.99202
Epoch:0058, train_loss=1.41869, train_acc=0.99635, val_loss=2.01055, val_acc=0.96533, time=1.05601
Epoch:0059, train_loss=1.41851, train_acc=0.99656, val_loss=2.01054, val_acc=0.96533, time=1.33499
Epoch:0060, train_loss=1.41833, train_acc=0.99635, val_loss=2.01053, val_acc=0.96533, time=1.11603
Epoch:0061, train_loss=1.41816, train_acc=0.99635, val_loss=2.01052, val_acc=0.96533, time=1.07602
Epoch:0062, train_loss=1.41799, train_acc=0.99656, val_loss=2.01051, val_acc=0.96533, time=1.01000
Epoch:0063, train_loss=1.41783, train_acc=0.99656, val_loss=2.01050, val_acc=0.96533, time=0.99599
Epoch:0064, train_loss=1.41767, train_acc=0.99696, val_loss=2.01049, val_acc=0.96533, time=1.07401
Epoch:0065, train_loss=1.41751, train_acc=0.99716, val_loss=2.01048, val_acc=0.96533, time=1.10002
Epoch:0066, train_loss=1.41736, train_acc=0.99737, val_loss=2.01047, val_acc=0.96533, time=0.95101
Epoch:0067, train_loss=1.41722, train_acc=0.99737, val_loss=2.01047, val_acc=0.96533, time=1.03301
Epoch:0068, train_loss=1.41708, train_acc=0.99777, val_loss=2.01046, val_acc=0.96533, time=1.03501
Epoch:0069, train_loss=1.41696, train_acc=0.99777, val_loss=2.01046, val_acc=0.96350, time=1.17900
Epoch:0070, train_loss=1.41684, train_acc=0.99777, val_loss=2.01046, val_acc=0.96168, time=1.12702
Epoch:0071, train_loss=1.41673, train_acc=0.99797, val_loss=2.01046, val_acc=0.96168, time=1.18001
Epoch:0072, train_loss=1.41662, train_acc=0.99797, val_loss=2.01046, val_acc=0.96168, time=1.16801
Epoch:0073, train_loss=1.41652, train_acc=0.99797, val_loss=2.01046, val_acc=0.96168, time=1.06300
Epoch:0074, train_loss=1.41642, train_acc=0.99797, val_loss=2.01046, val_acc=0.96168, time=0.99901
Epoch:0075, train_loss=1.41633, train_acc=0.99838, val_loss=2.01046, val_acc=0.96350, time=0.99799
Epoch:0076, train_loss=1.41623, train_acc=0.99838, val_loss=2.01047, val_acc=0.96350, time=1.05902
Early stopping...

Optimization Finished!

Test set results: loss= 1.80479, accuracy= 0.95386, time= 0.29200

Test Precision, Recall and F1-Score...
              precision    recall  f1-score   support

           0     0.9665    0.9871    0.9767      1083
           1     0.9777    0.9440    0.9605       696
           2     0.8806    0.9752    0.9255       121
           3     0.8636    0.8736    0.8686        87
           4     0.8765    0.9467    0.9103        75
           5     0.8649    0.7901    0.8258        81
           6     0.9583    0.6389    0.7667        36
           7     1.0000    1.0000    1.0000        10

    accuracy                         0.9539      2189
   macro avg     0.9235    0.8944    0.9043      2189
weighted avg     0.9544    0.9539    0.9532      2189


Macro average Test Precision, Recall and F1-Score...
(0.9235249337725977, 0.8944359131337263, 0.9042523949056054, None)

Micro average Test Precision, Recall and F1-Score...
(0.9538602101416171, 0.9538602101416171, 0.9538602101416171, None)

Embeddings:
Word_embeddings:7688
Train_doc_embeddings:5485
Test_doc_embeddings:2189
