
==========: 211117454978700
Epoch:0001, train_loss=2.14635, train_acc=0.21876, val_loss=2.05824, val_acc=0.60401, time=1.25801
Epoch:0002, train_loss=1.91094, train_acc=0.57039, val_loss=2.04489, val_acc=0.71168, time=1.23297
Epoch:0003, train_loss=1.77155, train_acc=0.69172, val_loss=2.03789, val_acc=0.74088, time=1.30402
Epoch:0004, train_loss=1.69601, train_acc=0.74114, val_loss=2.03243, val_acc=0.77007, time=1.12700
Epoch:0005, train_loss=1.64017, train_acc=0.78286, val_loss=2.02772, val_acc=0.81387, time=1.20001
Epoch:0006, train_loss=1.59367, train_acc=0.83107, val_loss=2.02430, val_acc=0.86496, time=1.16001
Epoch:0007, train_loss=1.55954, train_acc=0.87401, val_loss=2.02211, val_acc=0.88321, time=1.11901
Epoch:0008, train_loss=1.53671, train_acc=0.89812, val_loss=2.02047, val_acc=0.89416, time=1.10300
Epoch:0009, train_loss=1.51946, train_acc=0.91108, val_loss=2.01901, val_acc=0.90328, time=0.96201
Epoch:0010, train_loss=1.50424, train_acc=0.92303, val_loss=2.01765, val_acc=0.91423, time=0.95402
Epoch:0011, train_loss=1.49061, train_acc=0.93883, val_loss=2.01648, val_acc=0.91058, time=1.06401
Epoch:0012, train_loss=1.47922, train_acc=0.95240, val_loss=2.01555, val_acc=0.91606, time=1.01700
Epoch:0013, train_loss=1.47038, train_acc=0.95888, val_loss=2.01483, val_acc=0.92153, time=1.02401
Epoch:0014, train_loss=1.46369, train_acc=0.96374, val_loss=2.01426, val_acc=0.92701, time=1.07001
Epoch:0015, train_loss=1.45840, train_acc=0.96759, val_loss=2.01378, val_acc=0.92883, time=0.98300
Epoch:0016, train_loss=1.45387, train_acc=0.97063, val_loss=2.01333, val_acc=0.93248, time=1.33800
Epoch:0017, train_loss=1.44978, train_acc=0.97488, val_loss=2.01293, val_acc=0.93613, time=1.12201
Epoch:0018, train_loss=1.44613, train_acc=0.97772, val_loss=2.01257, val_acc=0.94161, time=0.97700
Epoch:0019, train_loss=1.44306, train_acc=0.97954, val_loss=2.01227, val_acc=0.94343, time=1.01701
Epoch:0020, train_loss=1.44060, train_acc=0.98157, val_loss=2.01204, val_acc=0.94343, time=1.02800
Epoch:0021, train_loss=1.43865, train_acc=0.98218, val_loss=2.01184, val_acc=0.94526, time=1.03202
Epoch:0022, train_loss=1.43701, train_acc=0.98359, val_loss=2.01168, val_acc=0.94891, time=1.13202
Epoch:0023, train_loss=1.43548, train_acc=0.98359, val_loss=2.01154, val_acc=0.95438, time=1.07701
Epoch:0024, train_loss=1.43398, train_acc=0.98420, val_loss=2.01140, val_acc=0.95073, time=1.04001
Epoch:0025, train_loss=1.43247, train_acc=0.98582, val_loss=2.01128, val_acc=0.95073, time=1.10101
Epoch:0026, train_loss=1.43101, train_acc=0.98704, val_loss=2.01117, val_acc=0.95073, time=1.04201
Epoch:0027, train_loss=1.42963, train_acc=0.98785, val_loss=2.01108, val_acc=0.94708, time=1.13000
Epoch:0028, train_loss=1.42841, train_acc=0.98845, val_loss=2.01100, val_acc=0.94891, time=1.11801
Epoch:0029, train_loss=1.42734, train_acc=0.98967, val_loss=2.01094, val_acc=0.95073, time=1.02500
Epoch:0030, train_loss=1.42644, train_acc=0.99068, val_loss=2.01089, val_acc=0.95255, time=1.19902
Epoch:0031, train_loss=1.42566, train_acc=0.99170, val_loss=2.01086, val_acc=0.95073, time=1.01800
Epoch:0032, train_loss=1.42497, train_acc=0.99210, val_loss=2.01083, val_acc=0.95255, time=1.10300
Epoch:0033, train_loss=1.42433, train_acc=0.99251, val_loss=2.01080, val_acc=0.95255, time=1.01399
Epoch:0034, train_loss=1.42371, train_acc=0.99251, val_loss=2.01078, val_acc=0.95255, time=1.04301
Epoch:0035, train_loss=1.42310, train_acc=0.99271, val_loss=2.01076, val_acc=0.95255, time=1.25601
Epoch:0036, train_loss=1.42249, train_acc=0.99291, val_loss=2.01075, val_acc=0.95073, time=1.00101
Epoch:0037, train_loss=1.42191, train_acc=0.99311, val_loss=2.01074, val_acc=0.95255, time=1.02800
Epoch:0038, train_loss=1.42136, train_acc=0.99352, val_loss=2.01073, val_acc=0.95255, time=1.17801
Epoch:0039, train_loss=1.42087, train_acc=0.99473, val_loss=2.01073, val_acc=0.95438, time=1.18602
Epoch:0040, train_loss=1.42043, train_acc=0.99534, val_loss=2.01073, val_acc=0.95438, time=1.16201
Epoch:0041, train_loss=1.42004, train_acc=0.99575, val_loss=2.01074, val_acc=0.95620, time=1.00400
Epoch:0042, train_loss=1.41970, train_acc=0.99554, val_loss=2.01074, val_acc=0.95620, time=0.99001
Epoch:0043, train_loss=1.41940, train_acc=0.99575, val_loss=2.01075, val_acc=0.95438, time=1.15199
Epoch:0044, train_loss=1.41910, train_acc=0.99615, val_loss=2.01075, val_acc=0.95438, time=1.14201
Early stopping...

Optimization Finished!

Test set results: loss= 1.80337, accuracy= 0.96071, time= 0.40001

Test Precision, Recall and F1-Score...
              precision    recall  f1-score   support

           0     0.9692    0.9889    0.9790      1083
           1     0.9778    0.9497    0.9636       696
           2     0.8939    0.9752    0.9328       121
           3     0.9176    0.8966    0.9070        87
           4     0.8974    0.9333    0.9150        75
           5     0.9103    0.8765    0.8931        81
           6     1.0000    0.6667    0.8000        36
           7     0.9091    1.0000    0.9524        10

    accuracy                         0.9607      2189
   macro avg     0.9344    0.9109    0.9179      2189
weighted avg     0.9613    0.9607    0.9602      2189


Macro average Test Precision, Recall and F1-Score...
(0.9344263862080604, 0.9108667321066138, 0.9178514433422664, None)

Micro average Test Precision, Recall and F1-Score...
(0.9607126541799909, 0.9607126541799909, 0.9607126541799909, None)

Embeddings:
Word_embeddings:7688
Train_doc_embeddings:5485
Test_doc_embeddings:2189
