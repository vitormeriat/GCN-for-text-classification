
==========: 211333866187900
Epoch:0001, train_loss=2.12035, train_acc=0.09398, val_loss=2.05855, val_acc=0.60219, time=1.03702
Epoch:0002, train_loss=1.88307, train_acc=0.59510, val_loss=2.04646, val_acc=0.71168, time=1.17400
Epoch:0003, train_loss=1.76529, train_acc=0.70164, val_loss=2.03977, val_acc=0.74635, time=1.10002
Epoch:0004, train_loss=1.69742, train_acc=0.75228, val_loss=2.03470, val_acc=0.77007, time=1.06600
Epoch:0005, train_loss=1.64647, train_acc=0.78489, val_loss=2.03026, val_acc=0.80474, time=1.09501
Epoch:0006, train_loss=1.60262, train_acc=0.82743, val_loss=2.02643, val_acc=0.83942, time=1.06501
Epoch:0007, train_loss=1.56528, train_acc=0.86429, val_loss=2.02337, val_acc=0.86496, time=1.33401
Epoch:0008, train_loss=1.53575, train_acc=0.90095, val_loss=2.02108, val_acc=0.88504, time=1.24101
Epoch:0009, train_loss=1.51374, train_acc=0.92607, val_loss=2.01939, val_acc=0.89964, time=1.00100
Epoch:0010, train_loss=1.49744, train_acc=0.94126, val_loss=2.01811, val_acc=0.91058, time=1.07302
Epoch:0011, train_loss=1.48510, train_acc=0.95321, val_loss=2.01710, val_acc=0.92153, time=1.11203
Epoch:0012, train_loss=1.47544, train_acc=0.95989, val_loss=2.01627, val_acc=0.92883, time=1.05300
Epoch:0013, train_loss=1.46758, train_acc=0.96496, val_loss=2.01556, val_acc=0.93248, time=1.15501
Epoch:0014, train_loss=1.46099, train_acc=0.97083, val_loss=2.01495, val_acc=0.93613, time=1.00701
Epoch:0015, train_loss=1.45547, train_acc=0.97367, val_loss=2.01444, val_acc=0.93613, time=1.10100
Epoch:0016, train_loss=1.45091, train_acc=0.97691, val_loss=2.01402, val_acc=0.93248, time=0.95502
Epoch:0017, train_loss=1.44721, train_acc=0.97853, val_loss=2.01368, val_acc=0.93431, time=1.05100
Epoch:0018, train_loss=1.44423, train_acc=0.98015, val_loss=2.01341, val_acc=0.93613, time=1.02401
Epoch:0019, train_loss=1.44179, train_acc=0.98157, val_loss=2.01319, val_acc=0.93796, time=1.12401
Epoch:0020, train_loss=1.43970, train_acc=0.98278, val_loss=2.01298, val_acc=0.93978, time=1.10001
Epoch:0021, train_loss=1.43780, train_acc=0.98420, val_loss=2.01280, val_acc=0.93978, time=0.98901
Epoch:0022, train_loss=1.43601, train_acc=0.98521, val_loss=2.01262, val_acc=0.94526, time=1.15101
Epoch:0023, train_loss=1.43429, train_acc=0.98582, val_loss=2.01245, val_acc=0.94526, time=1.12101
Epoch:0024, train_loss=1.43264, train_acc=0.98683, val_loss=2.01229, val_acc=0.94891, time=1.12702
Epoch:0025, train_loss=1.43109, train_acc=0.98805, val_loss=2.01214, val_acc=0.94891, time=1.01700
Epoch:0026, train_loss=1.42966, train_acc=0.98866, val_loss=2.01202, val_acc=0.94891, time=1.10202
Epoch:0027, train_loss=1.42837, train_acc=0.98987, val_loss=2.01191, val_acc=0.94891, time=1.22000
Epoch:0028, train_loss=1.42721, train_acc=0.99149, val_loss=2.01182, val_acc=0.94526, time=1.04199
Epoch:0029, train_loss=1.42618, train_acc=0.99271, val_loss=2.01175, val_acc=0.94891, time=1.20600
Epoch:0030, train_loss=1.42526, train_acc=0.99271, val_loss=2.01170, val_acc=0.95073, time=1.12802
Epoch:0031, train_loss=1.42443, train_acc=0.99352, val_loss=2.01166, val_acc=0.94708, time=1.03000
Epoch:0032, train_loss=1.42370, train_acc=0.99392, val_loss=2.01163, val_acc=0.94708, time=1.08201
Epoch:0033, train_loss=1.42305, train_acc=0.99372, val_loss=2.01162, val_acc=0.94526, time=1.15100
Epoch:0034, train_loss=1.42248, train_acc=0.99433, val_loss=2.01161, val_acc=0.94526, time=1.09702
Epoch:0035, train_loss=1.42197, train_acc=0.99433, val_loss=2.01161, val_acc=0.94526, time=1.19200
Epoch:0036, train_loss=1.42150, train_acc=0.99473, val_loss=2.01162, val_acc=0.94526, time=1.14402
Epoch:0037, train_loss=1.42107, train_acc=0.99514, val_loss=2.01162, val_acc=0.94526, time=1.05700
Epoch:0038, train_loss=1.42067, train_acc=0.99514, val_loss=2.01163, val_acc=0.94161, time=1.07501
Epoch:0039, train_loss=1.42028, train_acc=0.99554, val_loss=2.01164, val_acc=0.94161, time=1.00900
Epoch:0040, train_loss=1.41991, train_acc=0.99554, val_loss=2.01165, val_acc=0.94161, time=1.27603
Early stopping...

Optimization Finished!

Test set results: loss= 1.80272, accuracy= 0.95706, time= 0.38498

Test Precision, Recall and F1-Score...
              precision    recall  f1-score   support

           0     0.9709    0.9871    0.9789      1083
           1     0.9750    0.9526    0.9637       696
           2     0.9350    0.9504    0.9426       121
           3     0.8387    0.8966    0.8667        87
           4     0.8571    0.9600    0.9057        75
           5     0.8800    0.8148    0.8462        81
           6     1.0000    0.6389    0.7797        36
           7     0.9000    0.9000    0.9000        10

    accuracy                         0.9571      2189
   macro avg     0.9196    0.8875    0.8979      2189
weighted avg     0.9579    0.9571    0.9565      2189


Macro average Test Precision, Recall and F1-Score...
(0.9195934246656942, 0.8875409754250476, 0.8979206721979038, None)

Micro average Test Precision, Recall and F1-Score...
(0.9570580173595249, 0.9570580173595249, 0.9570580173595249, None)

Embeddings:
Word_embeddings:7688
Train_doc_embeddings:5485
Test_doc_embeddings:2189
