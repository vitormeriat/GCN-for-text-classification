
==========: 200081654490100
Epoch:0001, train_loss=2.09757, train_acc=0.30261, val_loss=2.05594, val_acc=0.63686, time=1.25101
Epoch:0002, train_loss=1.87526, train_acc=0.61839, val_loss=2.04394, val_acc=0.70073, time=1.16501
Epoch:0003, train_loss=1.75214, train_acc=0.71076, val_loss=2.03681, val_acc=0.72628, time=1.08101
Epoch:0004, train_loss=1.68029, train_acc=0.74985, val_loss=2.03109, val_acc=0.78102, time=1.00001
Epoch:0005, train_loss=1.62534, train_acc=0.79400, val_loss=2.02649, val_acc=0.82664, time=1.01200
Epoch:0006, train_loss=1.58191, train_acc=0.83857, val_loss=2.02309, val_acc=0.86131, time=1.02501
Epoch:0007, train_loss=1.54934, train_acc=0.88070, val_loss=2.02058, val_acc=0.89599, time=1.20800
Epoch:0008, train_loss=1.52474, train_acc=0.91047, val_loss=2.01866, val_acc=0.91058, time=1.04301
Epoch:0009, train_loss=1.50577, train_acc=0.92931, val_loss=2.01721, val_acc=0.91423, time=1.23901
Epoch:0010, train_loss=1.49127, train_acc=0.94410, val_loss=2.01612, val_acc=0.92883, time=1.07502
Epoch:0011, train_loss=1.48023, train_acc=0.95341, val_loss=2.01530, val_acc=0.93978, time=0.95401
Epoch:0012, train_loss=1.47172, train_acc=0.95888, val_loss=2.01469, val_acc=0.94526, time=1.03301
Epoch:0013, train_loss=1.46507, train_acc=0.96455, val_loss=2.01422, val_acc=0.94526, time=1.06400
Epoch:0014, train_loss=1.45980, train_acc=0.96860, val_loss=2.01382, val_acc=0.94526, time=1.17701
Epoch:0015, train_loss=1.45553, train_acc=0.97083, val_loss=2.01346, val_acc=0.94708, time=1.22101
Epoch:0016, train_loss=1.45180, train_acc=0.97225, val_loss=2.01309, val_acc=0.94708, time=1.00602
Epoch:0017, train_loss=1.44826, train_acc=0.97488, val_loss=2.01272, val_acc=0.94526, time=1.16101
Epoch:0018, train_loss=1.44478, train_acc=0.97772, val_loss=2.01236, val_acc=0.94526, time=1.16900
Epoch:0019, train_loss=1.44148, train_acc=0.98116, val_loss=2.01203, val_acc=0.94708, time=1.08500
Epoch:0020, train_loss=1.43856, train_acc=0.98359, val_loss=2.01175, val_acc=0.95073, time=0.96000
Epoch:0021, train_loss=1.43614, train_acc=0.98521, val_loss=2.01154, val_acc=0.95255, time=1.06001
Epoch:0022, train_loss=1.43421, train_acc=0.98623, val_loss=2.01137, val_acc=0.95620, time=1.26002
Epoch:0023, train_loss=1.43267, train_acc=0.98704, val_loss=2.01124, val_acc=0.95620, time=1.07900
Epoch:0024, train_loss=1.43138, train_acc=0.98744, val_loss=2.01114, val_acc=0.95438, time=1.16200
Epoch:0025, train_loss=1.43021, train_acc=0.98785, val_loss=2.01105, val_acc=0.95438, time=1.08901
Epoch:0026, train_loss=1.42910, train_acc=0.98825, val_loss=2.01098, val_acc=0.95803, time=0.94701
Epoch:0027, train_loss=1.42802, train_acc=0.98947, val_loss=2.01092, val_acc=0.95803, time=1.11501
Epoch:0028, train_loss=1.42699, train_acc=0.98967, val_loss=2.01088, val_acc=0.95803, time=1.05901
Epoch:0029, train_loss=1.42603, train_acc=0.99068, val_loss=2.01085, val_acc=0.95438, time=0.97700
Epoch:0030, train_loss=1.42516, train_acc=0.99170, val_loss=2.01083, val_acc=0.95985, time=1.18203
Epoch:0031, train_loss=1.42439, train_acc=0.99190, val_loss=2.01082, val_acc=0.95985, time=0.98101
Epoch:0032, train_loss=1.42370, train_acc=0.99271, val_loss=2.01081, val_acc=0.95985, time=1.05701
Epoch:0033, train_loss=1.42307, train_acc=0.99352, val_loss=2.01080, val_acc=0.95803, time=1.12900
Epoch:0034, train_loss=1.42248, train_acc=0.99352, val_loss=2.01079, val_acc=0.95803, time=1.01801
Epoch:0035, train_loss=1.42191, train_acc=0.99433, val_loss=2.01077, val_acc=0.95620, time=1.06400
Epoch:0036, train_loss=1.42136, train_acc=0.99514, val_loss=2.01076, val_acc=0.95620, time=1.46401
Epoch:0037, train_loss=1.42084, train_acc=0.99534, val_loss=2.01074, val_acc=0.95620, time=1.01102
Epoch:0038, train_loss=1.42036, train_acc=0.99575, val_loss=2.01073, val_acc=0.95985, time=1.13800
Epoch:0039, train_loss=1.41992, train_acc=0.99575, val_loss=2.01072, val_acc=0.95985, time=1.08502
Epoch:0040, train_loss=1.41952, train_acc=0.99595, val_loss=2.01071, val_acc=0.95803, time=0.98400
Epoch:0041, train_loss=1.41917, train_acc=0.99656, val_loss=2.01071, val_acc=0.95620, time=0.97501
Epoch:0042, train_loss=1.41884, train_acc=0.99656, val_loss=2.01071, val_acc=0.95803, time=1.09700
Epoch:0043, train_loss=1.41854, train_acc=0.99676, val_loss=2.01071, val_acc=0.95620, time=1.18002
Epoch:0044, train_loss=1.41826, train_acc=0.99676, val_loss=2.01071, val_acc=0.95438, time=1.06401
Epoch:0045, train_loss=1.41799, train_acc=0.99676, val_loss=2.01071, val_acc=0.95620, time=1.08600
Epoch:0046, train_loss=1.41774, train_acc=0.99676, val_loss=2.01071, val_acc=0.95803, time=1.10502
Epoch:0047, train_loss=1.41750, train_acc=0.99696, val_loss=2.01071, val_acc=0.96168, time=1.02499
Epoch:0048, train_loss=1.41728, train_acc=0.99716, val_loss=2.01071, val_acc=0.96168, time=0.98800
Epoch:0049, train_loss=1.41708, train_acc=0.99737, val_loss=2.01071, val_acc=0.96168, time=1.07801
Early stopping...

Optimization Finished!

Test set results: loss= 1.80448, accuracy= 0.95295, time= 0.29100

Test Precision, Recall and F1-Score...
              precision    recall  f1-score   support

           0     0.9631    0.9880    0.9754      1083
           1     0.9819    0.9368    0.9588       696
           2     0.8931    0.9669    0.9286       121
           3     0.8736    0.8736    0.8736        87
           4     0.8554    0.9467    0.8987        75
           5     0.8684    0.8148    0.8408        81
           6     0.9231    0.6667    0.7742        36
           7     0.9091    1.0000    0.9524        10

    accuracy                         0.9529      2189
   macro avg     0.9085    0.8992    0.9003      2189
weighted avg     0.9536    0.9529    0.9525      2189


Macro average Test Precision, Recall and F1-Score...
(0.9084659476754884, 0.8991789288813186, 0.9003023257255547, None)

Micro average Test Precision, Recall and F1-Score...
(0.9529465509365007, 0.9529465509365007, 0.9529465509365007, None)

Embeddings:
Word_embeddings:7688
Train_doc_embeddings:5485
Test_doc_embeddings:2189
