
==========: 200029304349000
Epoch:0001, train_loss=2.13849, train_acc=0.22524, val_loss=2.05960, val_acc=0.58577, time=1.14601
Epoch:0002, train_loss=1.90216, train_acc=0.58619, val_loss=2.04361, val_acc=0.67883, time=1.07399
Epoch:0003, train_loss=1.76458, train_acc=0.68361, val_loss=2.03613, val_acc=0.71350, time=1.06801
Epoch:0004, train_loss=1.69413, train_acc=0.70934, val_loss=2.03093, val_acc=0.76460, time=1.22400
Epoch:0005, train_loss=1.64039, train_acc=0.76767, val_loss=2.02679, val_acc=0.82664, time=1.05601
Epoch:0006, train_loss=1.59618, train_acc=0.82277, val_loss=2.02373, val_acc=0.85036, time=1.15801
Epoch:0007, train_loss=1.56240, train_acc=0.85902, val_loss=2.02127, val_acc=0.86861, time=1.15902
Epoch:0008, train_loss=1.53519, train_acc=0.88799, val_loss=2.01918, val_acc=0.88686, time=1.01600
Epoch:0009, train_loss=1.51279, train_acc=0.91392, val_loss=2.01743, val_acc=0.89964, time=1.03800
Epoch:0010, train_loss=1.49523, train_acc=0.93761, val_loss=2.01606, val_acc=0.91971, time=1.02302
Epoch:0011, train_loss=1.48225, train_acc=0.95058, val_loss=2.01500, val_acc=0.93978, time=1.02901
Epoch:0012, train_loss=1.47287, train_acc=0.96111, val_loss=2.01420, val_acc=0.95073, time=0.95501
Epoch:0013, train_loss=1.46608, train_acc=0.96698, val_loss=2.01360, val_acc=0.94891, time=1.03901
Epoch:0014, train_loss=1.46104, train_acc=0.97063, val_loss=2.01313, val_acc=0.95073, time=1.05300
Epoch:0015, train_loss=1.45709, train_acc=0.97387, val_loss=2.01272, val_acc=0.94708, time=1.11301
Epoch:0016, train_loss=1.45360, train_acc=0.97468, val_loss=2.01233, val_acc=0.95255, time=1.20001
Epoch:0017, train_loss=1.45016, train_acc=0.97569, val_loss=2.01195, val_acc=0.95803, time=1.15100
Epoch:0018, train_loss=1.44663, train_acc=0.97772, val_loss=2.01158, val_acc=0.95985, time=1.08002
Epoch:0019, train_loss=1.44317, train_acc=0.98116, val_loss=2.01124, val_acc=0.95985, time=1.10300
Epoch:0020, train_loss=1.43998, train_acc=0.98238, val_loss=2.01096, val_acc=0.95985, time=1.09002
Epoch:0021, train_loss=1.43723, train_acc=0.98501, val_loss=2.01072, val_acc=0.95985, time=1.06600
Epoch:0022, train_loss=1.43496, train_acc=0.98643, val_loss=2.01054, val_acc=0.95803, time=0.98700
Epoch:0023, train_loss=1.43313, train_acc=0.98683, val_loss=2.01040, val_acc=0.95803, time=1.17101
Epoch:0024, train_loss=1.43166, train_acc=0.98764, val_loss=2.01028, val_acc=0.95803, time=1.02102
Epoch:0025, train_loss=1.43042, train_acc=0.98764, val_loss=2.01018, val_acc=0.95803, time=1.12602
Epoch:0026, train_loss=1.42933, train_acc=0.98805, val_loss=2.01009, val_acc=0.95803, time=1.01201
Epoch:0027, train_loss=1.42833, train_acc=0.98825, val_loss=2.01001, val_acc=0.95985, time=0.98600
Epoch:0028, train_loss=1.42740, train_acc=0.98886, val_loss=2.00994, val_acc=0.95985, time=0.96204
Epoch:0029, train_loss=1.42651, train_acc=0.98926, val_loss=2.00987, val_acc=0.95803, time=1.23601
Epoch:0030, train_loss=1.42568, train_acc=0.99048, val_loss=2.00980, val_acc=0.96168, time=1.03501
Epoch:0031, train_loss=1.42492, train_acc=0.99089, val_loss=2.00975, val_acc=0.96533, time=1.12203
Epoch:0032, train_loss=1.42423, train_acc=0.99210, val_loss=2.00970, val_acc=0.96533, time=0.98001
Epoch:0033, train_loss=1.42360, train_acc=0.99210, val_loss=2.00966, val_acc=0.96533, time=1.14602
Epoch:0034, train_loss=1.42302, train_acc=0.99291, val_loss=2.00962, val_acc=0.96350, time=1.14500
Epoch:0035, train_loss=1.42248, train_acc=0.99311, val_loss=2.00959, val_acc=0.96168, time=1.04301
Epoch:0036, train_loss=1.42196, train_acc=0.99352, val_loss=2.00957, val_acc=0.96350, time=0.99601
Epoch:0037, train_loss=1.42146, train_acc=0.99392, val_loss=2.00955, val_acc=0.96533, time=1.04101
Epoch:0038, train_loss=1.42098, train_acc=0.99514, val_loss=2.00953, val_acc=0.96715, time=1.04300
Epoch:0039, train_loss=1.42052, train_acc=0.99534, val_loss=2.00952, val_acc=0.96715, time=1.07401
Epoch:0040, train_loss=1.42008, train_acc=0.99575, val_loss=2.00951, val_acc=0.96715, time=1.03401
Epoch:0041, train_loss=1.41968, train_acc=0.99595, val_loss=2.00951, val_acc=0.96533, time=1.03100
Epoch:0042, train_loss=1.41932, train_acc=0.99595, val_loss=2.00951, val_acc=0.96533, time=1.05200
Epoch:0043, train_loss=1.41899, train_acc=0.99615, val_loss=2.00952, val_acc=0.96715, time=1.07102
Epoch:0044, train_loss=1.41868, train_acc=0.99635, val_loss=2.00953, val_acc=0.96715, time=0.97201
Epoch:0045, train_loss=1.41840, train_acc=0.99635, val_loss=2.00954, val_acc=0.96715, time=1.05700
Early stopping...

Optimization Finished!

Test set results: loss= 1.80447, accuracy= 0.95797, time= 0.30901

Test Precision, Recall and F1-Score...
              precision    recall  f1-score   support

           0     0.9709    0.9871    0.9789      1083
           1     0.9793    0.9526    0.9658       696
           2     0.9015    0.9835    0.9407       121
           3     0.8511    0.9195    0.8840        87
           4     0.8554    0.9467    0.8987        75
           5     0.9155    0.8025    0.8553        81
           6     1.0000    0.5556    0.7143        36
           7     0.9091    1.0000    0.9524        10

    accuracy                         0.9580      2189
   macro avg     0.9229    0.8934    0.8988      2189
weighted avg     0.9592    0.9580    0.9571      2189


Macro average Test Precision, Recall and F1-Score...
(0.9228550724767952, 0.8934202268385206, 0.8987574355129271, None)

Micro average Test Precision, Recall and F1-Score...
(0.9579716765646414, 0.9579716765646414, 0.9579716765646414, None)

Embeddings:
Word_embeddings:7688
Train_doc_embeddings:5485
Test_doc_embeddings:2189
