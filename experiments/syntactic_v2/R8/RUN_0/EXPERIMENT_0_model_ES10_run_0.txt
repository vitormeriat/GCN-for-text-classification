
==========: 189347047463100
Epoch:0001, train_loss=2.18341, train_acc=0.07312, val_loss=2.05986, val_acc=0.60219, time=1.24802
Epoch:0002, train_loss=1.92278, train_acc=0.56715, val_loss=2.04606, val_acc=0.71168, time=1.24801
Epoch:0003, train_loss=1.78568, train_acc=0.68301, val_loss=2.03852, val_acc=0.73723, time=1.25799
Epoch:0004, train_loss=1.70632, train_acc=0.73871, val_loss=2.03311, val_acc=0.77372, time=1.27501
Epoch:0005, train_loss=1.64955, train_acc=0.78509, val_loss=2.02868, val_acc=0.82482, time=1.18401
Epoch:0006, train_loss=1.60381, train_acc=0.83330, val_loss=2.02530, val_acc=0.84489, time=1.20601
Epoch:0007, train_loss=1.56913, train_acc=0.87604, val_loss=2.02279, val_acc=0.87226, time=1.24300
Epoch:0008, train_loss=1.54373, train_acc=0.89974, val_loss=2.02084, val_acc=0.90146, time=1.28801
Epoch:0009, train_loss=1.52412, train_acc=0.91918, val_loss=2.01921, val_acc=0.90146, time=1.22401
Epoch:0010, train_loss=1.50808, train_acc=0.93458, val_loss=2.01781, val_acc=0.91606, time=1.17101
Epoch:0011, train_loss=1.49476, train_acc=0.94491, val_loss=2.01666, val_acc=0.91971, time=1.21900
Epoch:0012, train_loss=1.48391, train_acc=0.95362, val_loss=2.01573, val_acc=0.93431, time=1.11601
Epoch:0013, train_loss=1.47521, train_acc=0.95868, val_loss=2.01497, val_acc=0.93796, time=1.25501
Epoch:0014, train_loss=1.46813, train_acc=0.96293, val_loss=2.01432, val_acc=0.93613, time=1.30000
Epoch:0015, train_loss=1.46210, train_acc=0.96779, val_loss=2.01373, val_acc=0.93978, time=1.25601
Epoch:0016, train_loss=1.45674, train_acc=0.97063, val_loss=2.01318, val_acc=0.93978, time=1.31901
Epoch:0017, train_loss=1.45192, train_acc=0.97387, val_loss=2.01267, val_acc=0.94526, time=1.29402
Epoch:0018, train_loss=1.44762, train_acc=0.97590, val_loss=2.01221, val_acc=0.94708, time=1.37603
Epoch:0019, train_loss=1.44388, train_acc=0.97752, val_loss=2.01182, val_acc=0.95073, time=1.28201
Epoch:0020, train_loss=1.44072, train_acc=0.97853, val_loss=2.01149, val_acc=0.94891, time=1.37400
Epoch:0021, train_loss=1.43809, train_acc=0.98076, val_loss=2.01122, val_acc=0.95073, time=1.22902
Epoch:0022, train_loss=1.43592, train_acc=0.98278, val_loss=2.01100, val_acc=0.94891, time=1.30901
Epoch:0023, train_loss=1.43414, train_acc=0.98420, val_loss=2.01084, val_acc=0.94891, time=1.33301
Epoch:0024, train_loss=1.43266, train_acc=0.98420, val_loss=2.01071, val_acc=0.94891, time=1.22901
Epoch:0025, train_loss=1.43141, train_acc=0.98562, val_loss=2.01061, val_acc=0.95073, time=1.34401
Epoch:0026, train_loss=1.43032, train_acc=0.98663, val_loss=2.01053, val_acc=0.95073, time=1.31201
Epoch:0027, train_loss=1.42935, train_acc=0.98785, val_loss=2.01047, val_acc=0.95073, time=1.26901
Epoch:0028, train_loss=1.42846, train_acc=0.98805, val_loss=2.01042, val_acc=0.95073, time=1.19500
Epoch:0029, train_loss=1.42762, train_acc=0.98866, val_loss=2.01038, val_acc=0.95073, time=1.28900
Epoch:0030, train_loss=1.42682, train_acc=0.98866, val_loss=2.01035, val_acc=0.95255, time=1.20701
Epoch:0031, train_loss=1.42607, train_acc=0.98967, val_loss=2.01032, val_acc=0.94891, time=1.15700
Epoch:0032, train_loss=1.42535, train_acc=0.99068, val_loss=2.01030, val_acc=0.95073, time=1.25000
Epoch:0033, train_loss=1.42467, train_acc=0.99170, val_loss=2.01028, val_acc=0.95255, time=1.15301
Epoch:0034, train_loss=1.42402, train_acc=0.99190, val_loss=2.01027, val_acc=0.95620, time=1.11999
Epoch:0035, train_loss=1.42341, train_acc=0.99230, val_loss=2.01025, val_acc=0.95620, time=1.30399
Epoch:0036, train_loss=1.42283, train_acc=0.99372, val_loss=2.01024, val_acc=0.95620, time=1.46201
Epoch:0037, train_loss=1.42228, train_acc=0.99453, val_loss=2.01022, val_acc=0.95620, time=1.39402
Epoch:0038, train_loss=1.42176, train_acc=0.99473, val_loss=2.01021, val_acc=0.95620, time=1.28501
Epoch:0039, train_loss=1.42127, train_acc=0.99473, val_loss=2.01021, val_acc=0.95620, time=1.16301
Epoch:0040, train_loss=1.42081, train_acc=0.99494, val_loss=2.01020, val_acc=0.95620, time=1.30901
Epoch:0041, train_loss=1.42039, train_acc=0.99595, val_loss=2.01020, val_acc=0.95620, time=1.18000
Epoch:0042, train_loss=1.42000, train_acc=0.99595, val_loss=2.01020, val_acc=0.95620, time=1.21401
Epoch:0043, train_loss=1.41965, train_acc=0.99615, val_loss=2.01020, val_acc=0.95255, time=1.29601
Epoch:0044, train_loss=1.41933, train_acc=0.99635, val_loss=2.01021, val_acc=0.95255, time=1.24201
Epoch:0045, train_loss=1.41903, train_acc=0.99656, val_loss=2.01022, val_acc=0.95438, time=1.27501
Early stopping...

Optimization Finished!

Test set results: loss= 1.80290, accuracy= 0.95477, time= 0.39199

Test Precision, Recall and F1-Score...
              precision    recall  f1-score   support

           0     0.9648    0.9871    0.9758      1083
           1     0.9791    0.9425    0.9605       696
           2     0.9187    0.9339    0.9262       121
           3     0.8539    0.8736    0.8636        87
           4     0.8780    0.9600    0.9172        75
           5     0.8889    0.7901    0.8366        81
           6     0.8824    0.8333    0.8571        36
           7     0.9091    1.0000    0.9524        10

    accuracy                         0.9548      2189
   macro avg     0.9094    0.9151    0.9112      2189
weighted avg     0.9550    0.9548    0.9545      2189


Macro average Test Precision, Recall and F1-Score...
(0.9093649015701085, 0.9150632483986008, 0.9111833867956571, None)

Micro average Test Precision, Recall and F1-Score...
(0.9547738693467337, 0.9547738693467337, 0.9547738693467337, None)

Embeddings:
Word_embeddings:7688
Train_doc_embeddings:5485
Test_doc_embeddings:2189
