
==========: 201057621219700
Epoch:0001, train_loss=2.41745, train_acc=0.09641, val_loss=2.07943, val_acc=0.51095, time=1.39801
Epoch:0002, train_loss=2.11747, train_acc=0.49686, val_loss=2.05793, val_acc=0.57847, time=1.12300
Epoch:0003, train_loss=1.90049, train_acc=0.54871, val_loss=2.04308, val_acc=0.67336, time=1.06301
Epoch:0004, train_loss=1.74527, train_acc=0.67511, val_loss=2.03595, val_acc=0.73723, time=1.06801
Epoch:0005, train_loss=1.66925, train_acc=0.74843, val_loss=2.03351, val_acc=0.74088, time=1.30900
Epoch:0006, train_loss=1.63941, train_acc=0.76808, val_loss=2.03130, val_acc=0.75730, time=1.14101
Epoch:0007, train_loss=1.61421, train_acc=0.78935, val_loss=2.02822, val_acc=0.77920, time=1.14700
Epoch:0008, train_loss=1.58337, train_acc=0.81710, val_loss=2.02493, val_acc=0.81934, time=1.13301
Epoch:0009, train_loss=1.55190, train_acc=0.85518, val_loss=2.02226, val_acc=0.86314, time=1.10000
Epoch:0010, train_loss=1.52647, train_acc=0.89872, val_loss=2.02044, val_acc=0.89781, time=1.18800
Epoch:0011, train_loss=1.50883, train_acc=0.92526, val_loss=2.01927, val_acc=0.91241, time=1.15800
Epoch:0012, train_loss=1.49698, train_acc=0.94288, val_loss=2.01850, val_acc=0.91606, time=1.01499
Epoch:0013, train_loss=1.48865, train_acc=0.94835, val_loss=2.01794, val_acc=0.91606, time=1.26999
Epoch:0014, train_loss=1.48233, train_acc=0.95098, val_loss=2.01748, val_acc=0.91606, time=1.11100
Epoch:0015, train_loss=1.47700, train_acc=0.95362, val_loss=2.01701, val_acc=0.91971, time=1.15500
Epoch:0016, train_loss=1.47200, train_acc=0.95564, val_loss=2.01652, val_acc=0.92153, time=1.13901
Epoch:0017, train_loss=1.46707, train_acc=0.95949, val_loss=2.01599, val_acc=0.92336, time=1.11502
Epoch:0018, train_loss=1.46217, train_acc=0.96233, val_loss=2.01546, val_acc=0.92883, time=1.25401
Epoch:0019, train_loss=1.45747, train_acc=0.96678, val_loss=2.01495, val_acc=0.92883, time=1.13401
Epoch:0020, train_loss=1.45316, train_acc=0.97083, val_loss=2.01449, val_acc=0.93431, time=1.10800
Epoch:0021, train_loss=1.44936, train_acc=0.97387, val_loss=2.01412, val_acc=0.94161, time=1.09299
Epoch:0022, train_loss=1.44614, train_acc=0.97630, val_loss=2.01382, val_acc=0.94161, time=1.13700
Epoch:0023, train_loss=1.44347, train_acc=0.97731, val_loss=2.01358, val_acc=0.94161, time=1.45601
Epoch:0024, train_loss=1.44127, train_acc=0.97812, val_loss=2.01338, val_acc=0.94526, time=1.29202
Epoch:0025, train_loss=1.43945, train_acc=0.97934, val_loss=2.01323, val_acc=0.94526, time=1.22500
Epoch:0026, train_loss=1.43789, train_acc=0.98096, val_loss=2.01309, val_acc=0.94708, time=1.21301
Epoch:0027, train_loss=1.43649, train_acc=0.98177, val_loss=2.01296, val_acc=0.94708, time=1.11800
Epoch:0028, train_loss=1.43518, train_acc=0.98238, val_loss=2.01284, val_acc=0.94891, time=1.01201
Epoch:0029, train_loss=1.43392, train_acc=0.98299, val_loss=2.01271, val_acc=0.94708, time=1.03102
Epoch:0030, train_loss=1.43269, train_acc=0.98440, val_loss=2.01258, val_acc=0.94708, time=1.14499
Epoch:0031, train_loss=1.43148, train_acc=0.98582, val_loss=2.01246, val_acc=0.94708, time=1.21600
Epoch:0032, train_loss=1.43033, train_acc=0.98744, val_loss=2.01233, val_acc=0.94708, time=1.02700
Epoch:0033, train_loss=1.42924, train_acc=0.98906, val_loss=2.01221, val_acc=0.94708, time=1.19400
Epoch:0034, train_loss=1.42824, train_acc=0.98947, val_loss=2.01210, val_acc=0.94891, time=1.12402
Epoch:0035, train_loss=1.42733, train_acc=0.98926, val_loss=2.01200, val_acc=0.94891, time=1.23399
Epoch:0036, train_loss=1.42651, train_acc=0.99048, val_loss=2.01190, val_acc=0.94891, time=1.14201
Epoch:0037, train_loss=1.42577, train_acc=0.99109, val_loss=2.01182, val_acc=0.94891, time=1.07700
Epoch:0038, train_loss=1.42510, train_acc=0.99170, val_loss=2.01174, val_acc=0.95073, time=1.26300
Epoch:0039, train_loss=1.42449, train_acc=0.99251, val_loss=2.01167, val_acc=0.95073, time=1.10600
Epoch:0040, train_loss=1.42392, train_acc=0.99251, val_loss=2.01160, val_acc=0.95255, time=1.08600
Epoch:0041, train_loss=1.42338, train_acc=0.99291, val_loss=2.01154, val_acc=0.95255, time=1.11402
Epoch:0042, train_loss=1.42287, train_acc=0.99372, val_loss=2.01148, val_acc=0.95073, time=1.11900
Epoch:0043, train_loss=1.42239, train_acc=0.99413, val_loss=2.01143, val_acc=0.95073, time=1.14601
Epoch:0044, train_loss=1.42194, train_acc=0.99473, val_loss=2.01137, val_acc=0.95255, time=1.19101
Epoch:0045, train_loss=1.42152, train_acc=0.99473, val_loss=2.01133, val_acc=0.95438, time=1.20599
Epoch:0046, train_loss=1.42112, train_acc=0.99554, val_loss=2.01128, val_acc=0.95255, time=1.18402
Epoch:0047, train_loss=1.42075, train_acc=0.99554, val_loss=2.01124, val_acc=0.95255, time=1.17500
Epoch:0048, train_loss=1.42041, train_acc=0.99595, val_loss=2.01121, val_acc=0.95255, time=1.05301
Epoch:0049, train_loss=1.42009, train_acc=0.99615, val_loss=2.01118, val_acc=0.95255, time=1.01000
Epoch:0050, train_loss=1.41980, train_acc=0.99615, val_loss=2.01115, val_acc=0.95438, time=1.05801
Epoch:0051, train_loss=1.41952, train_acc=0.99656, val_loss=2.01112, val_acc=0.95438, time=1.01601
Epoch:0052, train_loss=1.41927, train_acc=0.99656, val_loss=2.01110, val_acc=0.95438, time=1.03600
Epoch:0053, train_loss=1.41903, train_acc=0.99676, val_loss=2.01108, val_acc=0.95620, time=1.15800
Epoch:0054, train_loss=1.41881, train_acc=0.99696, val_loss=2.01107, val_acc=0.95620, time=1.12301
Epoch:0055, train_loss=1.41860, train_acc=0.99737, val_loss=2.01105, val_acc=0.95620, time=1.01800
Epoch:0056, train_loss=1.41840, train_acc=0.99737, val_loss=2.01104, val_acc=0.95438, time=1.10601
Epoch:0057, train_loss=1.41821, train_acc=0.99696, val_loss=2.01102, val_acc=0.95438, time=1.09400
Epoch:0058, train_loss=1.41803, train_acc=0.99696, val_loss=2.01101, val_acc=0.95438, time=1.13201
Epoch:0059, train_loss=1.41785, train_acc=0.99696, val_loss=2.01100, val_acc=0.95438, time=1.22401
Epoch:0060, train_loss=1.41768, train_acc=0.99696, val_loss=2.01099, val_acc=0.95438, time=1.07700
Epoch:0061, train_loss=1.41752, train_acc=0.99716, val_loss=2.01098, val_acc=0.95803, time=1.23501
Epoch:0062, train_loss=1.41736, train_acc=0.99737, val_loss=2.01097, val_acc=0.95803, time=1.16001
Epoch:0063, train_loss=1.41721, train_acc=0.99737, val_loss=2.01096, val_acc=0.95620, time=1.13301
Epoch:0064, train_loss=1.41706, train_acc=0.99716, val_loss=2.01095, val_acc=0.95620, time=1.12400
Epoch:0065, train_loss=1.41693, train_acc=0.99757, val_loss=2.01095, val_acc=0.95620, time=1.10701
Epoch:0066, train_loss=1.41679, train_acc=0.99757, val_loss=2.01094, val_acc=0.95620, time=1.16701
Epoch:0067, train_loss=1.41667, train_acc=0.99757, val_loss=2.01093, val_acc=0.95620, time=1.18600
Epoch:0068, train_loss=1.41655, train_acc=0.99757, val_loss=2.01093, val_acc=0.95620, time=1.10601
Epoch:0069, train_loss=1.41643, train_acc=0.99757, val_loss=2.01092, val_acc=0.95620, time=0.99601
Epoch:0070, train_loss=1.41632, train_acc=0.99757, val_loss=2.01092, val_acc=0.95620, time=0.98401
Epoch:0071, train_loss=1.41621, train_acc=0.99757, val_loss=2.01092, val_acc=0.95620, time=1.10501
Epoch:0072, train_loss=1.41611, train_acc=0.99777, val_loss=2.01091, val_acc=0.95620, time=1.05700
Epoch:0073, train_loss=1.41601, train_acc=0.99777, val_loss=2.01091, val_acc=0.95620, time=1.04301
Epoch:0074, train_loss=1.41592, train_acc=0.99777, val_loss=2.01091, val_acc=0.95620, time=1.08600
Epoch:0075, train_loss=1.41583, train_acc=0.99797, val_loss=2.01091, val_acc=0.95620, time=1.07001
Epoch:0076, train_loss=1.41574, train_acc=0.99797, val_loss=2.01091, val_acc=0.95620, time=0.98900
Epoch:0077, train_loss=1.41566, train_acc=0.99797, val_loss=2.01091, val_acc=0.95620, time=1.12600
Epoch:0078, train_loss=1.41558, train_acc=0.99757, val_loss=2.01091, val_acc=0.95620, time=1.21700
Epoch:0079, train_loss=1.41550, train_acc=0.99757, val_loss=2.01091, val_acc=0.95620, time=1.03700
Epoch:0080, train_loss=1.41542, train_acc=0.99757, val_loss=2.01091, val_acc=0.95620, time=1.13902
Epoch:0081, train_loss=1.41535, train_acc=0.99757, val_loss=2.01091, val_acc=0.95620, time=1.09599
Epoch:0082, train_loss=1.41528, train_acc=0.99757, val_loss=2.01091, val_acc=0.95620, time=1.04201
Early stopping...

Optimization Finished!

Test set results: loss= 1.80493, accuracy= 0.95066, time= 0.40200

Test Precision, Recall and F1-Score...
              precision    recall  f1-score   support

           0     0.9604    0.9861    0.9731      1083
           1     0.9746    0.9368    0.9553       696
           2     0.9048    0.9421    0.9231       121
           3     0.8824    0.8621    0.8721        87
           4     0.8554    0.9467    0.8987        75
           5     0.8701    0.8272    0.8481        81
           6     0.9231    0.6667    0.7742        36
           7     0.9091    1.0000    0.9524        10

    accuracy                         0.9507      2189
   macro avg     0.9100    0.8960    0.8996      2189
weighted avg     0.9510    0.9507    0.9501      2189


Macro average Test Precision, Recall and F1-Score...
(0.909981853546728, 0.8959553433364064, 0.8996264967974377, None)

Micro average Test Precision, Recall and F1-Score...
(0.9506624029237094, 0.9506624029237094, 0.9506624029237094, None)

Embeddings:
Word_embeddings:7688
Train_doc_embeddings:5485
Test_doc_embeddings:2189
