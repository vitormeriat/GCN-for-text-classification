
==========: 200759052756000
Epoch:0001, train_loss=2.64979, train_acc=0.03626, val_loss=2.09663, val_acc=0.34854, time=1.15502
Epoch:0002, train_loss=2.27021, train_acc=0.34616, val_loss=2.06704, val_acc=0.56022, time=1.09000
Epoch:0003, train_loss=1.99586, train_acc=0.52826, val_loss=2.04884, val_acc=0.65876, time=1.10301
Epoch:0004, train_loss=1.82099, train_acc=0.60340, val_loss=2.03882, val_acc=0.74088, time=1.07201
Epoch:0005, train_loss=1.71489, train_acc=0.70670, val_loss=2.03455, val_acc=0.75547, time=1.12201
Epoch:0006, train_loss=1.66226, train_acc=0.76119, val_loss=2.03344, val_acc=0.75000, time=1.07301
Epoch:0007, train_loss=1.64342, train_acc=0.77983, val_loss=2.03267, val_acc=0.75730, time=1.05601
Epoch:0008, train_loss=1.63101, train_acc=0.78813, val_loss=2.03101, val_acc=0.77737, time=1.06800
Epoch:0009, train_loss=1.61217, train_acc=0.80109, val_loss=2.02857, val_acc=0.81022, time=1.00900
Epoch:0010, train_loss=1.58730, train_acc=0.82702, val_loss=2.02598, val_acc=0.83759, time=1.07702
Epoch:0011, train_loss=1.56174, train_acc=0.85761, val_loss=2.02378, val_acc=0.86131, time=1.17201
Epoch:0012, train_loss=1.54010, train_acc=0.88171, val_loss=2.02214, val_acc=0.87956, time=1.15200
Epoch:0013, train_loss=1.52367, train_acc=0.89893, val_loss=2.02091, val_acc=0.88869, time=1.10902
Epoch:0014, train_loss=1.51111, train_acc=0.91797, val_loss=2.01990, val_acc=0.89781, time=1.04399
Epoch:0015, train_loss=1.50093, train_acc=0.92789, val_loss=2.01902, val_acc=0.90146, time=1.02402
Epoch:0016, train_loss=1.49228, train_acc=0.93640, val_loss=2.01823, val_acc=0.90693, time=1.01001
Epoch:0017, train_loss=1.48479, train_acc=0.94430, val_loss=2.01751, val_acc=0.90876, time=1.01700
Epoch:0018, train_loss=1.47827, train_acc=0.94977, val_loss=2.01687, val_acc=0.91241, time=1.10701
Epoch:0019, train_loss=1.47255, train_acc=0.95301, val_loss=2.01630, val_acc=0.91606, time=0.97800
Epoch:0020, train_loss=1.46748, train_acc=0.95605, val_loss=2.01579, val_acc=0.91971, time=1.18002
Epoch:0021, train_loss=1.46289, train_acc=0.96070, val_loss=2.01532, val_acc=0.92336, time=1.03000
Epoch:0022, train_loss=1.45866, train_acc=0.96435, val_loss=2.01491, val_acc=0.92701, time=1.08402
Epoch:0023, train_loss=1.45474, train_acc=0.96800, val_loss=2.01453, val_acc=0.92883, time=1.07102
Epoch:0024, train_loss=1.45111, train_acc=0.97144, val_loss=2.01420, val_acc=0.93248, time=1.08600
Epoch:0025, train_loss=1.44778, train_acc=0.97367, val_loss=2.01392, val_acc=0.93431, time=1.06499
Epoch:0026, train_loss=1.44479, train_acc=0.97468, val_loss=2.01368, val_acc=0.93796, time=1.12501
Epoch:0027, train_loss=1.44215, train_acc=0.97711, val_loss=2.01347, val_acc=0.93978, time=1.07300
Epoch:0028, train_loss=1.43991, train_acc=0.97995, val_loss=2.01330, val_acc=0.94161, time=1.23300
Epoch:0029, train_loss=1.43806, train_acc=0.98137, val_loss=2.01317, val_acc=0.93978, time=1.10602
Epoch:0030, train_loss=1.43657, train_acc=0.98339, val_loss=2.01305, val_acc=0.93978, time=1.05300
Epoch:0031, train_loss=1.43538, train_acc=0.98420, val_loss=2.01296, val_acc=0.94343, time=1.20801
Epoch:0032, train_loss=1.43439, train_acc=0.98440, val_loss=2.01287, val_acc=0.94343, time=1.00500
Epoch:0033, train_loss=1.43351, train_acc=0.98501, val_loss=2.01279, val_acc=0.94526, time=0.96600
Epoch:0034, train_loss=1.43268, train_acc=0.98562, val_loss=2.01270, val_acc=0.94526, time=1.15600
Epoch:0035, train_loss=1.43185, train_acc=0.98521, val_loss=2.01261, val_acc=0.94343, time=1.12503
Epoch:0036, train_loss=1.43099, train_acc=0.98683, val_loss=2.01251, val_acc=0.94526, time=1.08301
Epoch:0037, train_loss=1.43011, train_acc=0.98704, val_loss=2.01240, val_acc=0.94708, time=1.15000
Epoch:0038, train_loss=1.42922, train_acc=0.98866, val_loss=2.01230, val_acc=0.94708, time=1.12600
Epoch:0039, train_loss=1.42837, train_acc=0.98967, val_loss=2.01220, val_acc=0.94891, time=1.12601
Epoch:0040, train_loss=1.42758, train_acc=0.99129, val_loss=2.01212, val_acc=0.94708, time=1.06100
Epoch:0041, train_loss=1.42687, train_acc=0.99210, val_loss=2.01204, val_acc=0.94526, time=1.18301
Epoch:0042, train_loss=1.42627, train_acc=0.99251, val_loss=2.01198, val_acc=0.94343, time=1.20201
Epoch:0043, train_loss=1.42575, train_acc=0.99251, val_loss=2.01192, val_acc=0.94708, time=1.10101
Epoch:0044, train_loss=1.42530, train_acc=0.99291, val_loss=2.01188, val_acc=0.94708, time=1.13800
Epoch:0045, train_loss=1.42490, train_acc=0.99352, val_loss=2.01184, val_acc=0.94891, time=1.05001
Epoch:0046, train_loss=1.42452, train_acc=0.99372, val_loss=2.01181, val_acc=0.94891, time=1.06901
Epoch:0047, train_loss=1.42414, train_acc=0.99392, val_loss=2.01178, val_acc=0.94891, time=1.11601
Epoch:0048, train_loss=1.42376, train_acc=0.99413, val_loss=2.01175, val_acc=0.94891, time=1.08201
Epoch:0049, train_loss=1.42337, train_acc=0.99433, val_loss=2.01172, val_acc=0.94891, time=1.08901
Epoch:0050, train_loss=1.42297, train_acc=0.99473, val_loss=2.01170, val_acc=0.95255, time=1.07700
Epoch:0051, train_loss=1.42256, train_acc=0.99534, val_loss=2.01167, val_acc=0.95255, time=1.12002
Epoch:0052, train_loss=1.42215, train_acc=0.99514, val_loss=2.01165, val_acc=0.95438, time=1.00501
Epoch:0053, train_loss=1.42176, train_acc=0.99534, val_loss=2.01164, val_acc=0.95438, time=1.15400
Epoch:0054, train_loss=1.42139, train_acc=0.99575, val_loss=2.01163, val_acc=0.95438, time=1.09001
Epoch:0055, train_loss=1.42104, train_acc=0.99595, val_loss=2.01162, val_acc=0.95620, time=1.05200
Epoch:0056, train_loss=1.42073, train_acc=0.99575, val_loss=2.01162, val_acc=0.95438, time=1.18500
Epoch:0057, train_loss=1.42045, train_acc=0.99656, val_loss=2.01162, val_acc=0.95438, time=1.21201
Epoch:0058, train_loss=1.42020, train_acc=0.99676, val_loss=2.01162, val_acc=0.95438, time=1.03800
Epoch:0059, train_loss=1.41998, train_acc=0.99676, val_loss=2.01162, val_acc=0.95438, time=1.16700
Epoch:0060, train_loss=1.41978, train_acc=0.99635, val_loss=2.01163, val_acc=0.95438, time=0.98502
Epoch:0061, train_loss=1.41960, train_acc=0.99635, val_loss=2.01163, val_acc=0.95438, time=1.07000
Epoch:0062, train_loss=1.41942, train_acc=0.99635, val_loss=2.01163, val_acc=0.95438, time=1.10501
Early stopping...

Optimization Finished!

Test set results: loss= 1.80462, accuracy= 0.95751, time= 0.31001

Test Precision, Recall and F1-Score...
              precision    recall  f1-score   support

           0     0.9718    0.9880    0.9799      1083
           1     0.9792    0.9468    0.9627       696
           2     0.9084    0.9835    0.9444       121
           3     0.8916    0.8506    0.8706        87
           4     0.8353    0.9467    0.8875        75
           5     0.8625    0.8519    0.8571        81
           6     1.0000    0.6667    0.8000        36
           7     0.8333    1.0000    0.9091        10

    accuracy                         0.9575      2189
   macro avg     0.9103    0.9043    0.9014      2189
weighted avg     0.9586    0.9575    0.9571      2189


Macro average Test Precision, Recall and F1-Score...
(0.9102665079467807, 0.9042582949030827, 0.9014208070174882, None)

Micro average Test Precision, Recall and F1-Score...
(0.9575148469620831, 0.9575148469620831, 0.9575148469620831, None)

Embeddings:
Word_embeddings:7688
Train_doc_embeddings:5485
Test_doc_embeddings:2189
