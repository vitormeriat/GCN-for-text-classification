
==========: 107127686711300
Epoch:0001, train_loss=2.21000, train_acc=0.06299, val_loss=2.06043, val_acc=0.59854, time=1.39209
Epoch:0002, train_loss=1.92892, train_acc=0.56269, val_loss=2.04429, val_acc=0.71350, time=1.33407
Epoch:0003, train_loss=1.78640, train_acc=0.69192, val_loss=2.03633, val_acc=0.75182, time=1.37408
Epoch:0004, train_loss=1.71501, train_acc=0.73871, val_loss=2.03157, val_acc=0.77920, time=1.33407
Epoch:0005, train_loss=1.67067, train_acc=0.76990, val_loss=2.02775, val_acc=0.80292, time=1.44208
Epoch:0006, train_loss=1.63325, train_acc=0.80170, val_loss=2.02429, val_acc=0.82847, time=1.38007
Epoch:0007, train_loss=1.59824, train_acc=0.83593, val_loss=2.02133, val_acc=0.86314, time=1.41108
Epoch:0008, train_loss=1.56724, train_acc=0.86996, val_loss=2.01901, val_acc=0.89964, time=1.43908
Epoch:0009, train_loss=1.54208, train_acc=0.89710, val_loss=2.01733, val_acc=0.91971, time=1.42708
Epoch:0010, train_loss=1.52274, train_acc=0.91392, val_loss=2.01612, val_acc=0.93248, time=1.39808
Epoch:0011, train_loss=1.50763, train_acc=0.92911, val_loss=2.01519, val_acc=0.93248, time=1.44608
Epoch:0012, train_loss=1.49502, train_acc=0.93863, val_loss=2.01443, val_acc=0.93613, time=1.41106
Epoch:0013, train_loss=1.48387, train_acc=0.94612, val_loss=2.01379, val_acc=0.93613, time=1.39508
Epoch:0014, train_loss=1.47393, train_acc=0.95341, val_loss=2.01326, val_acc=0.93978, time=1.39408
Epoch:0015, train_loss=1.46538, train_acc=0.96131, val_loss=2.01285, val_acc=0.94161, time=1.34007
Epoch:0016, train_loss=1.45844, train_acc=0.96941, val_loss=2.01255, val_acc=0.94526, time=1.39408
Epoch:0017, train_loss=1.45317, train_acc=0.97509, val_loss=2.01234, val_acc=0.94708, time=1.36708
Epoch:0018, train_loss=1.44933, train_acc=0.97731, val_loss=2.01219, val_acc=0.94708, time=1.58210
Epoch:0019, train_loss=1.44655, train_acc=0.98035, val_loss=2.01207, val_acc=0.94891, time=1.45210
Epoch:0020, train_loss=1.44447, train_acc=0.98258, val_loss=2.01195, val_acc=0.94708, time=1.34107
Epoch:0021, train_loss=1.44278, train_acc=0.98157, val_loss=2.01182, val_acc=0.94526, time=1.40609
Epoch:0022, train_loss=1.44124, train_acc=0.98197, val_loss=2.01167, val_acc=0.94526, time=1.43208
Epoch:0023, train_loss=1.43968, train_acc=0.98177, val_loss=2.01149, val_acc=0.94526, time=1.39008
Epoch:0024, train_loss=1.43803, train_acc=0.98177, val_loss=2.01128, val_acc=0.94526, time=1.38008
Epoch:0025, train_loss=1.43628, train_acc=0.98278, val_loss=2.01105, val_acc=0.95255, time=1.34207
Epoch:0026, train_loss=1.43448, train_acc=0.98461, val_loss=2.01083, val_acc=0.95255, time=1.31506
Epoch:0027, train_loss=1.43273, train_acc=0.98562, val_loss=2.01062, val_acc=0.95255, time=1.32707
Epoch:0028, train_loss=1.43112, train_acc=0.98724, val_loss=2.01043, val_acc=0.95438, time=1.42909
Epoch:0029, train_loss=1.42969, train_acc=0.98845, val_loss=2.01028, val_acc=0.95803, time=1.42709
Epoch:0030, train_loss=1.42847, train_acc=0.98987, val_loss=2.01015, val_acc=0.95803, time=1.38809
Epoch:0031, train_loss=1.42743, train_acc=0.99028, val_loss=2.01005, val_acc=0.96168, time=1.38008
Epoch:0032, train_loss=1.42652, train_acc=0.99028, val_loss=2.00998, val_acc=0.96168, time=1.35909
Epoch:0033, train_loss=1.42571, train_acc=0.99129, val_loss=2.00992, val_acc=0.96350, time=1.38208
Epoch:0034, train_loss=1.42497, train_acc=0.99149, val_loss=2.00989, val_acc=0.96168, time=1.38406
Epoch:0035, train_loss=1.42427, train_acc=0.99291, val_loss=2.00986, val_acc=0.96168, time=1.39808
Epoch:0036, train_loss=1.42362, train_acc=0.99311, val_loss=2.00986, val_acc=0.95985, time=1.36607
Epoch:0037, train_loss=1.42301, train_acc=0.99311, val_loss=2.00986, val_acc=0.95803, time=1.39309
Epoch:0038, train_loss=1.42245, train_acc=0.99311, val_loss=2.00988, val_acc=0.95985, time=1.46308
Epoch:0039, train_loss=1.42195, train_acc=0.99332, val_loss=2.00991, val_acc=0.96350, time=1.31908
Epoch:0040, train_loss=1.42150, train_acc=0.99332, val_loss=2.00994, val_acc=0.96168, time=1.47808
Early stopping...

Optimization Finished!

Test set results: loss= 1.80493, accuracy= 0.95523, time= 0.44603

Test Precision, Recall and F1-Score...
              precision    recall  f1-score   support

           0     0.9710    0.9898    0.9803      1083
           1     0.9821    0.9483    0.9649       696
           2     0.8721    0.8621    0.8671        87
           3     0.8731    0.9669    0.9176       121
           4     0.9524    0.5556    0.7018        36
           5     0.8000    0.9600    0.8727        75
           6     0.9041    0.8148    0.8571        81
           7     1.0000    0.9000    0.9474        10

    accuracy                         0.9552      2189
   macro avg     0.9194    0.8747    0.8886      2189
weighted avg     0.9567    0.9552    0.9545      2189


Macro average Test Precision, Recall and F1-Score...
(0.9193594053665689, 0.8746875469176375, 0.8886178328235946, None)

Micro average Test Precision, Recall and F1-Score...
(0.9552306989492919, 0.9552306989492919, 0.9552306989492919, None)

Embeddings:
Word_embeddings:7688
Train_doc_embeddings:5485
Test_doc_embeddings:2189
