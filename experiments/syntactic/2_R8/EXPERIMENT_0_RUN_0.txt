
==========: 107019737464200
Epoch:0001, train_loss=2.28665, train_acc=0.05388, val_loss=2.06515, val_acc=0.51642, time=1.45207
Epoch:0002, train_loss=1.97765, train_acc=0.48613, val_loss=2.04716, val_acc=0.68431, time=1.41908
Epoch:0003, train_loss=1.81128, train_acc=0.64047, val_loss=2.03881, val_acc=0.75000, time=1.41707
Epoch:0004, train_loss=1.73180, train_acc=0.72311, val_loss=2.03428, val_acc=0.77920, time=1.45809
Epoch:0005, train_loss=1.68618, train_acc=0.76747, val_loss=2.03087, val_acc=0.79380, time=1.41108
Epoch:0006, train_loss=1.65002, train_acc=0.79036, val_loss=2.02754, val_acc=0.81022, time=1.39008
Epoch:0007, train_loss=1.61483, train_acc=0.81426, val_loss=2.02426, val_acc=0.82482, time=1.39509
Epoch:0008, train_loss=1.58079, train_acc=0.84140, val_loss=2.02135, val_acc=0.85219, time=1.38707
Epoch:0009, train_loss=1.55109, train_acc=0.87725, val_loss=2.01903, val_acc=0.87956, time=1.36708
Epoch:0010, train_loss=1.52799, train_acc=0.90338, val_loss=2.01734, val_acc=0.89416, time=1.36707
Epoch:0011, train_loss=1.51111, train_acc=0.92627, val_loss=2.01611, val_acc=0.91058, time=1.40508
Epoch:0012, train_loss=1.49864, train_acc=0.93883, val_loss=2.01518, val_acc=0.92153, time=1.35907
Epoch:0013, train_loss=1.48884, train_acc=0.94855, val_loss=2.01444, val_acc=0.92701, time=1.47209
Epoch:0014, train_loss=1.48068, train_acc=0.95787, val_loss=2.01383, val_acc=0.93431, time=1.44310
Epoch:0015, train_loss=1.47358, train_acc=0.96395, val_loss=2.01331, val_acc=0.94161, time=1.43208
Epoch:0016, train_loss=1.46720, train_acc=0.96719, val_loss=2.01285, val_acc=0.94526, time=1.41109
Epoch:0017, train_loss=1.46139, train_acc=0.97063, val_loss=2.01247, val_acc=0.94343, time=1.40009
Epoch:0018, train_loss=1.45614, train_acc=0.97387, val_loss=2.01215, val_acc=0.94891, time=1.45009
Epoch:0019, train_loss=1.45153, train_acc=0.97529, val_loss=2.01189, val_acc=0.94891, time=1.42207
Epoch:0020, train_loss=1.44765, train_acc=0.97873, val_loss=2.01170, val_acc=0.95073, time=1.42308
Epoch:0021, train_loss=1.44446, train_acc=0.98116, val_loss=2.01156, val_acc=0.95255, time=1.44208
Epoch:0022, train_loss=1.44190, train_acc=0.98157, val_loss=2.01146, val_acc=0.95438, time=1.39008
Epoch:0023, train_loss=1.43984, train_acc=0.98319, val_loss=2.01138, val_acc=0.95073, time=1.41407
Epoch:0024, train_loss=1.43813, train_acc=0.98440, val_loss=2.01131, val_acc=0.95073, time=1.40509
Epoch:0025, train_loss=1.43664, train_acc=0.98420, val_loss=2.01124, val_acc=0.95073, time=1.40408
Epoch:0026, train_loss=1.43527, train_acc=0.98461, val_loss=2.01116, val_acc=0.95073, time=1.34908
Epoch:0027, train_loss=1.43398, train_acc=0.98481, val_loss=2.01107, val_acc=0.95073, time=1.41708
Epoch:0028, train_loss=1.43272, train_acc=0.98582, val_loss=2.01097, val_acc=0.95073, time=1.40209
Epoch:0029, train_loss=1.43149, train_acc=0.98704, val_loss=2.01087, val_acc=0.95255, time=1.41707
Epoch:0030, train_loss=1.43030, train_acc=0.98724, val_loss=2.01076, val_acc=0.95620, time=1.39407
Epoch:0031, train_loss=1.42917, train_acc=0.98845, val_loss=2.01066, val_acc=0.95620, time=1.39008
Epoch:0032, train_loss=1.42810, train_acc=0.98947, val_loss=2.01056, val_acc=0.95620, time=1.40309
Epoch:0033, train_loss=1.42711, train_acc=0.99007, val_loss=2.01047, val_acc=0.95620, time=1.38508
Epoch:0034, train_loss=1.42620, train_acc=0.99048, val_loss=2.01039, val_acc=0.95803, time=1.37308
Epoch:0035, train_loss=1.42536, train_acc=0.99149, val_loss=2.01033, val_acc=0.95985, time=1.33807
Epoch:0036, train_loss=1.42461, train_acc=0.99149, val_loss=2.01027, val_acc=0.96168, time=1.34907
Epoch:0037, train_loss=1.42393, train_acc=0.99251, val_loss=2.01023, val_acc=0.95985, time=1.27007
Epoch:0038, train_loss=1.42333, train_acc=0.99291, val_loss=2.01019, val_acc=0.95985, time=1.36009
Epoch:0039, train_loss=1.42278, train_acc=0.99352, val_loss=2.01016, val_acc=0.95985, time=1.36708
Epoch:0040, train_loss=1.42229, train_acc=0.99372, val_loss=2.01014, val_acc=0.95985, time=1.37707
Epoch:0041, train_loss=1.42185, train_acc=0.99392, val_loss=2.01013, val_acc=0.95803, time=1.36309
Epoch:0042, train_loss=1.42146, train_acc=0.99392, val_loss=2.01012, val_acc=0.95803, time=1.36408
Epoch:0043, train_loss=1.42109, train_acc=0.99392, val_loss=2.01011, val_acc=0.95985, time=1.40008
Epoch:0044, train_loss=1.42075, train_acc=0.99473, val_loss=2.01010, val_acc=0.95985, time=1.37908
Epoch:0045, train_loss=1.42043, train_acc=0.99554, val_loss=2.01009, val_acc=0.96168, time=1.47710
Epoch:0046, train_loss=1.42013, train_acc=0.99575, val_loss=2.01007, val_acc=0.96350, time=1.48008
Epoch:0047, train_loss=1.41983, train_acc=0.99635, val_loss=2.01006, val_acc=0.96350, time=1.40907
Epoch:0048, train_loss=1.41956, train_acc=0.99635, val_loss=2.01005, val_acc=0.96350, time=1.38508
Epoch:0049, train_loss=1.41929, train_acc=0.99676, val_loss=2.01004, val_acc=0.96350, time=1.36407
Epoch:0050, train_loss=1.41903, train_acc=0.99676, val_loss=2.01003, val_acc=0.96350, time=1.41507
Epoch:0051, train_loss=1.41879, train_acc=0.99696, val_loss=2.01002, val_acc=0.96350, time=1.35107
Epoch:0052, train_loss=1.41855, train_acc=0.99696, val_loss=2.01001, val_acc=0.96350, time=1.42308
Epoch:0053, train_loss=1.41833, train_acc=0.99696, val_loss=2.01001, val_acc=0.96350, time=1.39308
Epoch:0054, train_loss=1.41811, train_acc=0.99716, val_loss=2.01000, val_acc=0.96350, time=1.38908
Epoch:0055, train_loss=1.41791, train_acc=0.99716, val_loss=2.01000, val_acc=0.96350, time=1.41008
Epoch:0056, train_loss=1.41772, train_acc=0.99757, val_loss=2.01000, val_acc=0.96350, time=1.40206
Epoch:0057, train_loss=1.41754, train_acc=0.99757, val_loss=2.01000, val_acc=0.96350, time=1.37708
Epoch:0058, train_loss=1.41736, train_acc=0.99777, val_loss=2.01000, val_acc=0.96350, time=1.39907
Epoch:0059, train_loss=1.41720, train_acc=0.99797, val_loss=2.01000, val_acc=0.96168, time=1.36506
Epoch:0060, train_loss=1.41705, train_acc=0.99797, val_loss=2.01000, val_acc=0.96168, time=1.39908
Epoch:0061, train_loss=1.41691, train_acc=0.99797, val_loss=2.01001, val_acc=0.96168, time=1.36908
Early stopping...

Optimization Finished!

Test set results: loss= 1.80499, accuracy= 0.95249, time= 0.43703

Test Precision, Recall and F1-Score...
              precision    recall  f1-score   support

           0     0.9657    0.9871    0.9763      1083
           1     0.9820    0.9382    0.9596       696
           2     0.8229    0.9080    0.8634        87
           3     0.9062    0.9587    0.9317       121
           4     0.8846    0.6389    0.7419        36
           5     0.8642    0.9333    0.8974        75
           6     0.9028    0.8025    0.8497        81
           7     0.7143    1.0000    0.8333        10

    accuracy                         0.9525      2189
   macro avg     0.8803    0.8958    0.8817      2189
weighted avg     0.9536    0.9525    0.9521      2189


Macro average Test Precision, Recall and F1-Score...
(0.8803338689363774, 0.8958382946641119, 0.8816671310786945, None)

Micro average Test Precision, Recall and F1-Score...
(0.9524897213339424, 0.9524897213339424, 0.9524897213339424, None)

Embeddings:
Word_embeddings:7688
Train_doc_embeddings:5485
Test_doc_embeddings:2189
