
==========: 267514003656000
Epoch:0001, train_loss=2.27409, train_acc=0.02390, val_loss=2.07261, val_acc=0.37591, time=1.48501
Epoch:0002, train_loss=1.99290, train_acc=0.41543, val_loss=2.06359, val_acc=0.47628, time=1.70401
Epoch:0003, train_loss=1.89612, train_acc=0.50881, val_loss=2.06290, val_acc=0.48175, time=1.64400
Epoch:0004, train_loss=1.87207, train_acc=0.51874, val_loss=2.06415, val_acc=0.47080, time=1.48201
Epoch:0005, train_loss=1.86694, train_acc=0.53028, val_loss=2.06548, val_acc=0.46168, time=1.48200
Epoch:0006, train_loss=1.86414, train_acc=0.54000, val_loss=2.06567, val_acc=0.47080, time=1.31100
Epoch:0007, train_loss=1.85225, train_acc=0.55682, val_loss=2.06525, val_acc=0.47993, time=1.30502
Epoch:0008, train_loss=1.83632, train_acc=0.56937, val_loss=2.06479, val_acc=0.47810, time=1.28300
Epoch:0009, train_loss=1.82150, train_acc=0.57525, val_loss=2.06431, val_acc=0.47810, time=1.33601
Epoch:0010, train_loss=1.80790, train_acc=0.58436, val_loss=2.06388, val_acc=0.46898, time=1.42200
Epoch:0011, train_loss=1.79554, train_acc=0.59915, val_loss=2.06353, val_acc=0.45620, time=1.19601
Epoch:0012, train_loss=1.78426, train_acc=0.61171, val_loss=2.06327, val_acc=0.45073, time=1.41900
Epoch:0013, train_loss=1.77395, train_acc=0.62670, val_loss=2.06318, val_acc=0.45438, time=1.37901
Epoch:0014, train_loss=1.76486, train_acc=0.63662, val_loss=2.06332, val_acc=0.44708, time=1.31201
Epoch:0015, train_loss=1.75718, train_acc=0.64412, val_loss=2.06367, val_acc=0.44708, time=1.34401
Epoch:0016, train_loss=1.75056, train_acc=0.64493, val_loss=2.06416, val_acc=0.44526, time=1.41301
Epoch:0017, train_loss=1.74441, train_acc=0.64918, val_loss=2.06472, val_acc=0.44343, time=1.43201
Epoch:0018, train_loss=1.73825, train_acc=0.65505, val_loss=2.06531, val_acc=0.44526, time=1.36499
Epoch:0019, train_loss=1.73197, train_acc=0.66255, val_loss=2.06592, val_acc=0.44343, time=1.29902
Epoch:0020, train_loss=1.72574, train_acc=0.66680, val_loss=2.06654, val_acc=0.44161, time=1.30001
Epoch:0021, train_loss=1.71971, train_acc=0.67430, val_loss=2.06714, val_acc=0.44343, time=1.29701
Epoch:0022, train_loss=1.71380, train_acc=0.68382, val_loss=2.06771, val_acc=0.44526, time=1.40801
Epoch:0023, train_loss=1.70788, train_acc=0.69172, val_loss=2.06822, val_acc=0.44343, time=1.35400
Epoch:0024, train_loss=1.70207, train_acc=0.69658, val_loss=2.06871, val_acc=0.44343, time=1.50203
Epoch:0025, train_loss=1.69664, train_acc=0.69982, val_loss=2.06920, val_acc=0.44526, time=1.42800
Epoch:0026, train_loss=1.69175, train_acc=0.70569, val_loss=2.06968, val_acc=0.43796, time=1.43901
Epoch:0027, train_loss=1.68737, train_acc=0.70954, val_loss=2.07016, val_acc=0.43431, time=1.37502
Epoch:0028, train_loss=1.68332, train_acc=0.71521, val_loss=2.07064, val_acc=0.43431, time=1.53601
Epoch:0029, train_loss=1.67939, train_acc=0.72028, val_loss=2.07109, val_acc=0.43431, time=1.36600
Epoch:0030, train_loss=1.67539, train_acc=0.72655, val_loss=2.07152, val_acc=0.43613, time=1.28801
Epoch:0031, train_loss=1.67122, train_acc=0.73324, val_loss=2.07194, val_acc=0.42883, time=1.37501
Epoch:0032, train_loss=1.66696, train_acc=0.73709, val_loss=2.07238, val_acc=0.42883, time=1.30103
Epoch:0033, train_loss=1.66282, train_acc=0.74276, val_loss=2.07285, val_acc=0.42883, time=1.30500
Epoch:0034, train_loss=1.65897, train_acc=0.74418, val_loss=2.07334, val_acc=0.42883, time=1.30800
Epoch:0035, train_loss=1.65543, train_acc=0.74924, val_loss=2.07384, val_acc=0.42883, time=1.23502
Epoch:0036, train_loss=1.65210, train_acc=0.75309, val_loss=2.07434, val_acc=0.42701, time=1.43901
Epoch:0037, train_loss=1.64889, train_acc=0.75755, val_loss=2.07482, val_acc=0.42701, time=1.31000
Epoch:0038, train_loss=1.64571, train_acc=0.76079, val_loss=2.07528, val_acc=0.42701, time=1.31600
Epoch:0039, train_loss=1.64250, train_acc=0.76625, val_loss=2.07574, val_acc=0.42701, time=1.36502
Epoch:0040, train_loss=1.63926, train_acc=0.77010, val_loss=2.07619, val_acc=0.42336, time=1.29800
Epoch:0041, train_loss=1.63605, train_acc=0.77476, val_loss=2.07665, val_acc=0.42518, time=1.22401
Epoch:0042, train_loss=1.63294, train_acc=0.77780, val_loss=2.07713, val_acc=0.42518, time=1.39301
Epoch:0043, train_loss=1.62997, train_acc=0.78104, val_loss=2.07763, val_acc=0.42701, time=1.46102
Epoch:0044, train_loss=1.62713, train_acc=0.78448, val_loss=2.07817, val_acc=0.42701, time=1.41501
Epoch:0045, train_loss=1.62438, train_acc=0.78793, val_loss=2.07872, val_acc=0.42883, time=1.31001
Epoch:0046, train_loss=1.62168, train_acc=0.79056, val_loss=2.07928, val_acc=0.43066, time=1.37700
Epoch:0047, train_loss=1.61900, train_acc=0.79502, val_loss=2.07986, val_acc=0.43066, time=1.28700
Epoch:0048, train_loss=1.61633, train_acc=0.79583, val_loss=2.08043, val_acc=0.42701, time=1.34502
Epoch:0049, train_loss=1.61370, train_acc=0.79785, val_loss=2.08101, val_acc=0.42883, time=1.32900
Epoch:0050, train_loss=1.61114, train_acc=0.79947, val_loss=2.08158, val_acc=0.42701, time=1.31800
Epoch:0051, train_loss=1.60866, train_acc=0.80231, val_loss=2.08213, val_acc=0.43066, time=1.21001
Epoch:0052, train_loss=1.60623, train_acc=0.80555, val_loss=2.08265, val_acc=0.42883, time=1.34901
Epoch:0053, train_loss=1.60383, train_acc=0.80677, val_loss=2.08316, val_acc=0.42883, time=1.32700
Epoch:0054, train_loss=1.60147, train_acc=0.81061, val_loss=2.08365, val_acc=0.42701, time=1.22301
Epoch:0055, train_loss=1.59912, train_acc=0.81345, val_loss=2.08414, val_acc=0.42883, time=1.27000
Epoch:0056, train_loss=1.59681, train_acc=0.81710, val_loss=2.08463, val_acc=0.42518, time=1.26601
Epoch:0057, train_loss=1.59456, train_acc=0.81973, val_loss=2.08514, val_acc=0.42518, time=1.19100
Epoch:0058, train_loss=1.59237, train_acc=0.82337, val_loss=2.08566, val_acc=0.42153, time=1.33500
Epoch:0059, train_loss=1.59023, train_acc=0.82702, val_loss=2.08622, val_acc=0.41971, time=1.34401
Epoch:0060, train_loss=1.58812, train_acc=0.82925, val_loss=2.08680, val_acc=0.41788, time=1.35301
Epoch:0061, train_loss=1.58602, train_acc=0.83188, val_loss=2.08740, val_acc=0.41971, time=1.31300
Epoch:0062, train_loss=1.58394, train_acc=0.83451, val_loss=2.08802, val_acc=0.41971, time=1.26201
Epoch:0063, train_loss=1.58189, train_acc=0.83553, val_loss=2.08865, val_acc=0.41971, time=1.23701
Epoch:0064, train_loss=1.57989, train_acc=0.83796, val_loss=2.08928, val_acc=0.41971, time=1.22001
Epoch:0065, train_loss=1.57794, train_acc=0.83897, val_loss=2.08991, val_acc=0.42336, time=1.32901
Epoch:0066, train_loss=1.57603, train_acc=0.84019, val_loss=2.09052, val_acc=0.42336, time=1.26000
Epoch:0067, train_loss=1.57414, train_acc=0.84363, val_loss=2.09111, val_acc=0.42701, time=1.33101
Epoch:0068, train_loss=1.57228, train_acc=0.84667, val_loss=2.09169, val_acc=0.42701, time=1.37401
Epoch:0069, train_loss=1.57043, train_acc=0.84768, val_loss=2.09226, val_acc=0.42336, time=1.44401
Epoch:0070, train_loss=1.56861, train_acc=0.85031, val_loss=2.09283, val_acc=0.42153, time=1.32102
Epoch:0071, train_loss=1.56684, train_acc=0.85234, val_loss=2.09342, val_acc=0.42153, time=1.32001
Epoch:0072, train_loss=1.56510, train_acc=0.85538, val_loss=2.09402, val_acc=0.41971, time=1.43700
Epoch:0073, train_loss=1.56339, train_acc=0.85740, val_loss=2.09465, val_acc=0.41423, time=1.34001
Epoch:0074, train_loss=1.56170, train_acc=0.85963, val_loss=2.09529, val_acc=0.41423, time=1.25402
Epoch:0075, train_loss=1.56004, train_acc=0.86125, val_loss=2.09595, val_acc=0.41241, time=1.30300
Epoch:0076, train_loss=1.55841, train_acc=0.86247, val_loss=2.09661, val_acc=0.41241, time=1.26800
Epoch:0077, train_loss=1.55680, train_acc=0.86449, val_loss=2.09727, val_acc=0.41241, time=1.30301
Epoch:0078, train_loss=1.55522, train_acc=0.86551, val_loss=2.09792, val_acc=0.41241, time=1.32302
Epoch:0079, train_loss=1.55367, train_acc=0.86794, val_loss=2.09857, val_acc=0.41241, time=1.44802
Epoch:0080, train_loss=1.55215, train_acc=0.86854, val_loss=2.09920, val_acc=0.41606, time=1.31500
Epoch:0081, train_loss=1.55065, train_acc=0.87037, val_loss=2.09984, val_acc=0.41606, time=1.27901
Epoch:0082, train_loss=1.54917, train_acc=0.87300, val_loss=2.10048, val_acc=0.41423, time=1.39801
Epoch:0083, train_loss=1.54773, train_acc=0.87381, val_loss=2.10112, val_acc=0.41241, time=1.38502
Epoch:0084, train_loss=1.54630, train_acc=0.87442, val_loss=2.10177, val_acc=0.41241, time=1.35601
Epoch:0085, train_loss=1.54490, train_acc=0.87563, val_loss=2.10243, val_acc=0.41241, time=1.18701
Epoch:0086, train_loss=1.54352, train_acc=0.87705, val_loss=2.10309, val_acc=0.41241, time=1.32501
Epoch:0087, train_loss=1.54216, train_acc=0.87847, val_loss=2.10375, val_acc=0.41241, time=1.26900
Epoch:0088, train_loss=1.54083, train_acc=0.88090, val_loss=2.10441, val_acc=0.41241, time=1.31401
Epoch:0089, train_loss=1.53952, train_acc=0.88232, val_loss=2.10508, val_acc=0.41241, time=1.20501
Epoch:0090, train_loss=1.53823, train_acc=0.88414, val_loss=2.10574, val_acc=0.41241, time=1.19300
Epoch:0091, train_loss=1.53696, train_acc=0.88515, val_loss=2.10640, val_acc=0.41241, time=1.25401
Epoch:0092, train_loss=1.53571, train_acc=0.88718, val_loss=2.10707, val_acc=0.41423, time=1.35602
Epoch:0093, train_loss=1.53448, train_acc=0.88920, val_loss=2.10774, val_acc=0.41241, time=1.30802
Epoch:0094, train_loss=1.53327, train_acc=0.89022, val_loss=2.10841, val_acc=0.41423, time=1.36500
Epoch:0095, train_loss=1.53208, train_acc=0.89123, val_loss=2.10909, val_acc=0.41606, time=1.33701
Epoch:0096, train_loss=1.53090, train_acc=0.89305, val_loss=2.10977, val_acc=0.41606, time=1.30000
Epoch:0097, train_loss=1.52975, train_acc=0.89386, val_loss=2.11044, val_acc=0.41606, time=1.25301
Epoch:0098, train_loss=1.52862, train_acc=0.89488, val_loss=2.11111, val_acc=0.41606, time=1.30201
Epoch:0099, train_loss=1.52750, train_acc=0.89629, val_loss=2.11177, val_acc=0.41606, time=1.29901
Epoch:0100, train_loss=1.52640, train_acc=0.89710, val_loss=2.11244, val_acc=0.41606, time=1.26601
Epoch:0101, train_loss=1.52532, train_acc=0.89913, val_loss=2.11310, val_acc=0.41606, time=1.40001
Epoch:0102, train_loss=1.52426, train_acc=0.90034, val_loss=2.11378, val_acc=0.41606, time=1.38601
Early stopping...

Optimization Finished!

Test set results: loss= 2.20172, accuracy= 0.38693, time= 0.36501

Test Precision, Recall and F1-Score...
              precision    recall  f1-score   support

           0     0.3134    0.3233    0.3182       696
           1     0.4916    0.5679    0.5270      1083
           2     0.0455    0.0230    0.0305        87
           3     0.0392    0.0165    0.0233       121
           4     0.0000    0.0000    0.0000        75
           5     0.0000    0.0000    0.0000        10
           6     0.0465    0.0247    0.0323        81
           7     0.0500    0.0278    0.0357        36

    accuracy                         0.3869      2189
   macro avg     0.1233    0.1229    0.1209      2189
weighted avg     0.3494    0.3869    0.3662      2189


Macro average Test Precision, Recall and F1-Score...
(0.12326988097524173, 0.12289118315618452, 0.12087511419649966, None)

Micro average Test Precision, Recall and F1-Score...
(0.3869346733668342, 0.3869346733668342, 0.3869346733668342, None)

Embeddings:
Word_embeddings:7688
Train_doc_embeddings:5485
Test_doc_embeddings:2189
