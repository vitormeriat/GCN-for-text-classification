
==========: 272355064984400
Epoch:0001, train_loss=2.42814, train_acc=0.03362, val_loss=2.08140, val_acc=0.28285, time=1.29901
Epoch:0002, train_loss=2.08923, train_acc=0.27041, val_loss=2.06389, val_acc=0.47080, time=1.26500
Epoch:0003, train_loss=1.92264, train_acc=0.48410, val_loss=2.05994, val_acc=0.49088, time=1.29201
Epoch:0004, train_loss=1.88521, train_acc=0.51853, val_loss=2.05991, val_acc=0.47993, time=1.20799
Epoch:0005, train_loss=1.88322, train_acc=0.52988, val_loss=2.06109, val_acc=0.45255, time=1.17002
Epoch:0006, train_loss=1.88950, train_acc=0.52947, val_loss=2.06202, val_acc=0.43431, time=1.24400
Epoch:0007, train_loss=1.88899, train_acc=0.53575, val_loss=2.06203, val_acc=0.44891, time=1.17401
Epoch:0008, train_loss=1.87656, train_acc=0.56391, val_loss=2.06177, val_acc=0.46715, time=1.20501
Epoch:0009, train_loss=1.85976, train_acc=0.57849, val_loss=2.06159, val_acc=0.47263, time=1.30100
Epoch:0010, train_loss=1.84348, train_acc=0.57768, val_loss=2.06145, val_acc=0.47263, time=1.20401
Epoch:0011, train_loss=1.82843, train_acc=0.58193, val_loss=2.06149, val_acc=0.46898, time=1.23801
Epoch:0012, train_loss=1.81641, train_acc=0.59409, val_loss=2.06195, val_acc=0.45985, time=1.25200
Epoch:0013, train_loss=1.80935, train_acc=0.60523, val_loss=2.06261, val_acc=0.45620, time=1.20300
Epoch:0014, train_loss=1.80466, train_acc=0.61839, val_loss=2.06299, val_acc=0.45073, time=1.29301
Epoch:0015, train_loss=1.79727, train_acc=0.63054, val_loss=2.06292, val_acc=0.44526, time=1.21702
Epoch:0016, train_loss=1.78566, train_acc=0.63824, val_loss=2.06264, val_acc=0.45073, time=1.25502
Epoch:0017, train_loss=1.77213, train_acc=0.64391, val_loss=2.06243, val_acc=0.45620, time=1.17801
Epoch:0018, train_loss=1.75960, train_acc=0.64351, val_loss=2.06242, val_acc=0.45985, time=1.35101
Epoch:0019, train_loss=1.74946, train_acc=0.64351, val_loss=2.06256, val_acc=0.46168, time=1.28601
Epoch:0020, train_loss=1.74149, train_acc=0.64472, val_loss=2.06277, val_acc=0.46350, time=1.16301
Epoch:0021, train_loss=1.73485, train_acc=0.64594, val_loss=2.06300, val_acc=0.45985, time=1.21800
Epoch:0022, train_loss=1.72892, train_acc=0.65222, val_loss=2.06325, val_acc=0.44891, time=1.30401
Epoch:0023, train_loss=1.72349, train_acc=0.65890, val_loss=2.06352, val_acc=0.44343, time=1.57301
Epoch:0024, train_loss=1.71840, train_acc=0.66457, val_loss=2.06384, val_acc=0.43978, time=1.31901
Epoch:0025, train_loss=1.71346, train_acc=0.66964, val_loss=2.06417, val_acc=0.43431, time=1.33902
Epoch:0026, train_loss=1.70850, train_acc=0.67531, val_loss=2.06451, val_acc=0.43248, time=1.21401
Epoch:0027, train_loss=1.70351, train_acc=0.68098, val_loss=2.06487, val_acc=0.43248, time=1.32001
Epoch:0028, train_loss=1.69861, train_acc=0.68361, val_loss=2.06525, val_acc=0.43796, time=1.30401
Epoch:0029, train_loss=1.69394, train_acc=0.69030, val_loss=2.06563, val_acc=0.43796, time=1.21400
Epoch:0030, train_loss=1.68950, train_acc=0.69496, val_loss=2.06599, val_acc=0.43613, time=1.33002
Epoch:0031, train_loss=1.68523, train_acc=0.69658, val_loss=2.06632, val_acc=0.43978, time=1.23801
Epoch:0032, train_loss=1.68106, train_acc=0.70063, val_loss=2.06661, val_acc=0.43613, time=1.26200
Epoch:0033, train_loss=1.67695, train_acc=0.70630, val_loss=2.06688, val_acc=0.43248, time=1.30302
Epoch:0034, train_loss=1.67294, train_acc=0.71562, val_loss=2.06716, val_acc=0.43431, time=1.31099
Epoch:0035, train_loss=1.66909, train_acc=0.72392, val_loss=2.06745, val_acc=0.43431, time=1.24802
Epoch:0036, train_loss=1.66544, train_acc=0.73061, val_loss=2.06777, val_acc=0.43431, time=1.23001
Epoch:0037, train_loss=1.66201, train_acc=0.73688, val_loss=2.06811, val_acc=0.43066, time=1.31601
Epoch:0038, train_loss=1.65876, train_acc=0.73911, val_loss=2.06849, val_acc=0.43248, time=1.16501
Epoch:0039, train_loss=1.65570, train_acc=0.74013, val_loss=2.06889, val_acc=0.43431, time=1.22201
Epoch:0040, train_loss=1.65280, train_acc=0.74215, val_loss=2.06931, val_acc=0.43431, time=1.23900
Epoch:0041, train_loss=1.65002, train_acc=0.74377, val_loss=2.06973, val_acc=0.43431, time=1.26601
Epoch:0042, train_loss=1.64730, train_acc=0.74742, val_loss=2.07014, val_acc=0.43248, time=1.25600
Epoch:0043, train_loss=1.64456, train_acc=0.74965, val_loss=2.07053, val_acc=0.43613, time=1.21701
Epoch:0044, train_loss=1.64177, train_acc=0.75451, val_loss=2.07092, val_acc=0.43066, time=1.26601
Epoch:0045, train_loss=1.63894, train_acc=0.75896, val_loss=2.07131, val_acc=0.43248, time=1.18901
Epoch:0046, train_loss=1.63610, train_acc=0.76484, val_loss=2.07172, val_acc=0.43066, time=1.27302
Epoch:0047, train_loss=1.63331, train_acc=0.76848, val_loss=2.07214, val_acc=0.42701, time=1.24001
Epoch:0048, train_loss=1.63056, train_acc=0.77314, val_loss=2.07259, val_acc=0.42518, time=1.26799
Epoch:0049, train_loss=1.62790, train_acc=0.77800, val_loss=2.07306, val_acc=0.42883, time=1.33401
Epoch:0050, train_loss=1.62532, train_acc=0.78064, val_loss=2.07353, val_acc=0.42883, time=1.28300
Epoch:0051, train_loss=1.62284, train_acc=0.78307, val_loss=2.07400, val_acc=0.42518, time=1.28700
Epoch:0052, train_loss=1.62042, train_acc=0.78671, val_loss=2.07447, val_acc=0.42701, time=1.26801
Epoch:0053, train_loss=1.61805, train_acc=0.78955, val_loss=2.07493, val_acc=0.42336, time=1.51502
Epoch:0054, train_loss=1.61570, train_acc=0.79238, val_loss=2.07538, val_acc=0.42336, time=1.31102
Epoch:0055, train_loss=1.61338, train_acc=0.79522, val_loss=2.07583, val_acc=0.42701, time=1.31899
Epoch:0056, train_loss=1.61111, train_acc=0.79704, val_loss=2.07629, val_acc=0.42883, time=1.24601
Epoch:0057, train_loss=1.60887, train_acc=0.80150, val_loss=2.07676, val_acc=0.42883, time=1.25500
Epoch:0058, train_loss=1.60667, train_acc=0.80211, val_loss=2.07724, val_acc=0.42701, time=1.28902
Epoch:0059, train_loss=1.60450, train_acc=0.80494, val_loss=2.07773, val_acc=0.42701, time=1.22700
Epoch:0060, train_loss=1.60237, train_acc=0.80636, val_loss=2.07822, val_acc=0.42701, time=1.29600
Epoch:0061, train_loss=1.60027, train_acc=0.80818, val_loss=2.07871, val_acc=0.42883, time=1.24402
Epoch:0062, train_loss=1.59819, train_acc=0.80980, val_loss=2.07919, val_acc=0.42701, time=1.27601
Epoch:0063, train_loss=1.59615, train_acc=0.81284, val_loss=2.07968, val_acc=0.42701, time=1.29903
Epoch:0064, train_loss=1.59414, train_acc=0.81547, val_loss=2.08017, val_acc=0.42701, time=1.23300
Epoch:0065, train_loss=1.59218, train_acc=0.81953, val_loss=2.08067, val_acc=0.42701, time=1.27801
Epoch:0066, train_loss=1.59026, train_acc=0.82094, val_loss=2.08117, val_acc=0.42336, time=1.27700
Epoch:0067, train_loss=1.58837, train_acc=0.82297, val_loss=2.08168, val_acc=0.42336, time=1.23201
Epoch:0068, train_loss=1.58651, train_acc=0.82581, val_loss=2.08218, val_acc=0.42153, time=1.26202
Epoch:0069, train_loss=1.58467, train_acc=0.82803, val_loss=2.08268, val_acc=0.41788, time=1.16300
Epoch:0070, train_loss=1.58284, train_acc=0.83127, val_loss=2.08317, val_acc=0.41423, time=1.37102
Epoch:0071, train_loss=1.58104, train_acc=0.83370, val_loss=2.08367, val_acc=0.41423, time=1.27901
Epoch:0072, train_loss=1.57926, train_acc=0.83593, val_loss=2.08417, val_acc=0.41423, time=1.24501
Epoch:0073, train_loss=1.57751, train_acc=0.83796, val_loss=2.08469, val_acc=0.41423, time=1.24201
Epoch:0074, train_loss=1.57578, train_acc=0.83938, val_loss=2.08520, val_acc=0.41241, time=1.28500
Epoch:0075, train_loss=1.57409, train_acc=0.84059, val_loss=2.08571, val_acc=0.41241, time=1.24602
Epoch:0076, train_loss=1.57242, train_acc=0.84302, val_loss=2.08621, val_acc=0.41423, time=1.28901
Epoch:0077, train_loss=1.57078, train_acc=0.84505, val_loss=2.08670, val_acc=0.41423, time=1.24201
Epoch:0078, train_loss=1.56916, train_acc=0.84748, val_loss=2.08720, val_acc=0.41423, time=1.19601
Epoch:0079, train_loss=1.56756, train_acc=0.84930, val_loss=2.08770, val_acc=0.41241, time=1.24002
Epoch:0080, train_loss=1.56599, train_acc=0.85072, val_loss=2.08821, val_acc=0.41241, time=1.25401
Epoch:0081, train_loss=1.56443, train_acc=0.85112, val_loss=2.08872, val_acc=0.40693, time=1.23100
Epoch:0082, train_loss=1.56290, train_acc=0.85254, val_loss=2.08922, val_acc=0.41241, time=1.31301
Epoch:0083, train_loss=1.56139, train_acc=0.85355, val_loss=2.08971, val_acc=0.41058, time=1.26801
Epoch:0084, train_loss=1.55991, train_acc=0.85761, val_loss=2.09021, val_acc=0.40876, time=1.22900
Epoch:0085, train_loss=1.55844, train_acc=0.85983, val_loss=2.09072, val_acc=0.40876, time=1.27501
Epoch:0086, train_loss=1.55700, train_acc=0.86206, val_loss=2.09123, val_acc=0.40693, time=1.25701
Epoch:0087, train_loss=1.55558, train_acc=0.86429, val_loss=2.09174, val_acc=0.40693, time=1.18701
Epoch:0088, train_loss=1.55418, train_acc=0.86470, val_loss=2.09225, val_acc=0.40693, time=1.28800
Epoch:0089, train_loss=1.55280, train_acc=0.86672, val_loss=2.09275, val_acc=0.40693, time=1.26801
Epoch:0090, train_loss=1.55144, train_acc=0.86854, val_loss=2.09326, val_acc=0.40511, time=1.17500
Epoch:0091, train_loss=1.55010, train_acc=0.87057, val_loss=2.09378, val_acc=0.40328, time=1.25601
Epoch:0092, train_loss=1.54877, train_acc=0.87178, val_loss=2.09430, val_acc=0.40146, time=1.23704
Epoch:0093, train_loss=1.54747, train_acc=0.87300, val_loss=2.09482, val_acc=0.39964, time=1.24199
Epoch:0094, train_loss=1.54619, train_acc=0.87422, val_loss=2.09533, val_acc=0.40146, time=1.27901
Epoch:0095, train_loss=1.54492, train_acc=0.87503, val_loss=2.09585, val_acc=0.39781, time=1.33501
Epoch:0096, train_loss=1.54368, train_acc=0.87604, val_loss=2.09637, val_acc=0.39781, time=1.19200
Epoch:0097, train_loss=1.54245, train_acc=0.87685, val_loss=2.09689, val_acc=0.39781, time=1.26802
Epoch:0098, train_loss=1.54124, train_acc=0.87867, val_loss=2.09741, val_acc=0.39964, time=1.24400
Epoch:0099, train_loss=1.54005, train_acc=0.87908, val_loss=2.09793, val_acc=0.39781, time=1.21900
Epoch:0100, train_loss=1.53887, train_acc=0.87989, val_loss=2.09846, val_acc=0.39964, time=1.19901
Epoch:0101, train_loss=1.53772, train_acc=0.88151, val_loss=2.09898, val_acc=0.39964, time=1.22700
Epoch:0102, train_loss=1.53658, train_acc=0.88313, val_loss=2.09951, val_acc=0.39781, time=1.22900
Early stopping...

Optimization Finished!

Test set results: loss= 2.17624, accuracy= 0.40018, time= 0.39101

Test Precision, Recall and F1-Score...
              precision    recall  f1-score   support

           0     0.0612    0.0345    0.0441        87
           1     0.5030    0.6140    0.5530      1083
           2     0.3168    0.2931    0.3045       696
           3     0.0000    0.0000    0.0000        10
           4     0.0238    0.0133    0.0171        75
           5     0.0294    0.0165    0.0212       121
           6     0.0000    0.0000    0.0000        36
           7     0.0238    0.0123    0.0163        81

    accuracy                         0.4002      2189
   macro avg     0.1198    0.1230    0.1195      2189
weighted avg     0.3553    0.4002    0.3745      2189


Macro average Test Precision, Recall and F1-Score...
(0.11975640088305267, 0.12297865407267046, 0.1195160016091674, None)

Micro average Test Precision, Recall and F1-Score...
(0.4001827318410233, 0.4001827318410233, 0.4001827318410233, None)

Embeddings:
Word_embeddings:7688
Train_doc_embeddings:5485
Test_doc_embeddings:2189
