
==========: 282496506986600
Epoch:0001, train_loss=2.41885, train_acc=0.04152, val_loss=2.08198, val_acc=0.26642, time=1.27401
Epoch:0002, train_loss=2.10916, train_acc=0.25947, val_loss=2.06356, val_acc=0.41971, time=1.24200
Epoch:0003, train_loss=1.93107, train_acc=0.42455, val_loss=2.05937, val_acc=0.48175, time=1.24403
Epoch:0004, train_loss=1.88137, train_acc=0.50719, val_loss=2.06237, val_acc=0.49453, time=1.40200
Epoch:0005, train_loss=1.89916, train_acc=0.52603, val_loss=2.06417, val_acc=0.49453, time=1.26099
Epoch:0006, train_loss=1.90870, train_acc=0.53575, val_loss=2.06386, val_acc=0.49635, time=1.30301
Epoch:0007, train_loss=1.89976, train_acc=0.54669, val_loss=2.06335, val_acc=0.48723, time=1.22201
Epoch:0008, train_loss=1.88790, train_acc=0.55236, val_loss=2.06348, val_acc=0.47263, time=1.19802
Epoch:0009, train_loss=1.87920, train_acc=0.54709, val_loss=2.06328, val_acc=0.46168, time=1.22201
Epoch:0010, train_loss=1.86483, train_acc=0.55094, val_loss=2.06263, val_acc=0.46898, time=1.18799
Epoch:0011, train_loss=1.84436, train_acc=0.57727, val_loss=2.06216, val_acc=0.46898, time=1.25802
Epoch:0012, train_loss=1.82506, train_acc=0.59915, val_loss=2.06214, val_acc=0.48175, time=1.28001
Epoch:0013, train_loss=1.81022, train_acc=0.60421, val_loss=2.06227, val_acc=0.47810, time=1.26500
Epoch:0014, train_loss=1.79788, train_acc=0.60340, val_loss=2.06223, val_acc=0.47628, time=1.13899
Epoch:0015, train_loss=1.78556, train_acc=0.60887, val_loss=2.06200, val_acc=0.47263, time=1.24901
Epoch:0016, train_loss=1.77296, train_acc=0.61799, val_loss=2.06172, val_acc=0.47080, time=1.17801
Epoch:0017, train_loss=1.76138, train_acc=0.62994, val_loss=2.06158, val_acc=0.47080, time=1.18203
Epoch:0018, train_loss=1.75212, train_acc=0.64027, val_loss=2.06166, val_acc=0.46168, time=1.14999
Epoch:0019, train_loss=1.74545, train_acc=0.65222, val_loss=2.06189, val_acc=0.45803, time=1.26701
Epoch:0020, train_loss=1.74047, train_acc=0.65748, val_loss=2.06216, val_acc=0.45620, time=1.16701
Early stopping...

Optimization Finished!

Test set results: loss= 2.01113, accuracy= 0.43810, time= 0.35600

Test Precision, Recall and F1-Score...
              precision    recall  f1-score   support

           0     0.1333    0.0460    0.0684        87
           1     0.5019    0.7230    0.5925      1083
           2     0.3263    0.2457    0.2803       696
           3     0.0000    0.0000    0.0000        10
           4     0.0000    0.0000    0.0000        75
           5     0.0000    0.0000    0.0000       121
           6     0.0000    0.0000    0.0000        36
           7     0.0909    0.0123    0.0217        81

    accuracy                         0.4381      2189
   macro avg     0.1316    0.1284    0.1204      2189
weighted avg     0.3607    0.4381    0.3858      2189


Macro average Test Precision, Recall and F1-Score...
(0.13156267237851207, 0.1283755044287131, 0.12036894758958232, None)

Micro average Test Precision, Recall and F1-Score...
(0.4380995888533577, 0.4380995888533577, 0.4380995888533577, None)

Embeddings:
Word_embeddings:7688
Train_doc_embeddings:5485
Test_doc_embeddings:2189
