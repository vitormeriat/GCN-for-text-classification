
==========: 280954788308500
Epoch:0001, train_loss=2.21228, train_acc=0.22706, val_loss=2.08947, val_acc=0.27007, time=1.40100
Epoch:0002, train_loss=2.19845, train_acc=0.22888, val_loss=2.08813, val_acc=0.26825, time=1.25401
Epoch:0003, train_loss=2.18478, train_acc=0.23273, val_loss=2.08680, val_acc=0.26825, time=1.27701
Epoch:0004, train_loss=2.17128, train_acc=0.23456, val_loss=2.08550, val_acc=0.27372, time=1.28205
Epoch:0005, train_loss=2.15795, train_acc=0.23901, val_loss=2.08421, val_acc=0.27007, time=1.21197
Epoch:0006, train_loss=2.14480, train_acc=0.24448, val_loss=2.08295, val_acc=0.27372, time=1.30203
Epoch:0007, train_loss=2.13184, train_acc=0.25015, val_loss=2.08171, val_acc=0.28102, time=1.20100
Epoch:0008, train_loss=2.11908, train_acc=0.25501, val_loss=2.08049, val_acc=0.28467, time=1.26801
Epoch:0009, train_loss=2.10656, train_acc=0.26231, val_loss=2.07930, val_acc=0.29745, time=1.23401
Epoch:0010, train_loss=2.09427, train_acc=0.27223, val_loss=2.07814, val_acc=0.30109, time=1.28902
Epoch:0011, train_loss=2.08223, train_acc=0.28357, val_loss=2.07700, val_acc=0.31752, time=1.16600
Epoch:0012, train_loss=2.07047, train_acc=0.29593, val_loss=2.07590, val_acc=0.32117, time=1.23500
Epoch:0013, train_loss=2.05902, train_acc=0.30768, val_loss=2.07484, val_acc=0.32847, time=1.19001
Epoch:0014, train_loss=2.04787, train_acc=0.31963, val_loss=2.07381, val_acc=0.32299, time=1.28401
Epoch:0015, train_loss=2.03706, train_acc=0.32975, val_loss=2.07282, val_acc=0.33212, time=1.25799
Epoch:0016, train_loss=2.02660, train_acc=0.33846, val_loss=2.07187, val_acc=0.34307, time=1.19101
Epoch:0017, train_loss=2.01651, train_acc=0.35123, val_loss=2.07096, val_acc=0.35766, time=1.17901
Epoch:0018, train_loss=2.00682, train_acc=0.36196, val_loss=2.07009, val_acc=0.37774, time=1.19298
Epoch:0019, train_loss=1.99753, train_acc=0.37330, val_loss=2.06927, val_acc=0.39051, time=1.19001
Epoch:0020, train_loss=1.98865, train_acc=0.38728, val_loss=2.06849, val_acc=0.41058, time=1.26902
Epoch:0021, train_loss=1.98021, train_acc=0.39781, val_loss=2.06776, val_acc=0.42336, time=1.21300
Epoch:0022, train_loss=1.97219, train_acc=0.40895, val_loss=2.06707, val_acc=0.43066, time=1.22801
Epoch:0023, train_loss=1.96462, train_acc=0.41705, val_loss=2.06643, val_acc=0.43431, time=1.16302
Epoch:0024, train_loss=1.95750, train_acc=0.42779, val_loss=2.06583, val_acc=0.44343, time=1.29002
Epoch:0025, train_loss=1.95081, train_acc=0.43812, val_loss=2.06528, val_acc=0.45438, time=1.24400
Epoch:0026, train_loss=1.94455, train_acc=0.44379, val_loss=2.06477, val_acc=0.45438, time=1.23701
Epoch:0027, train_loss=1.93871, train_acc=0.45068, val_loss=2.06431, val_acc=0.45803, time=1.10800
Epoch:0028, train_loss=1.93327, train_acc=0.45513, val_loss=2.06388, val_acc=0.45985, time=1.21801
Epoch:0029, train_loss=1.92820, train_acc=0.45757, val_loss=2.06348, val_acc=0.45255, time=1.27500
Epoch:0030, train_loss=1.92350, train_acc=0.46506, val_loss=2.06312, val_acc=0.46168, time=1.12302
Epoch:0031, train_loss=1.91913, train_acc=0.46911, val_loss=2.06279, val_acc=0.45803, time=1.27600
Epoch:0032, train_loss=1.91506, train_acc=0.47478, val_loss=2.06248, val_acc=0.46533, time=1.13501
Epoch:0033, train_loss=1.91129, train_acc=0.47762, val_loss=2.06220, val_acc=0.47080, time=1.18501
Epoch:0034, train_loss=1.90777, train_acc=0.48147, val_loss=2.06195, val_acc=0.47080, time=1.21500
Epoch:0035, train_loss=1.90449, train_acc=0.48288, val_loss=2.06171, val_acc=0.47628, time=1.16802
Epoch:0036, train_loss=1.90142, train_acc=0.48572, val_loss=2.06149, val_acc=0.47445, time=1.21501
Epoch:0037, train_loss=1.89855, train_acc=0.48714, val_loss=2.06129, val_acc=0.46898, time=1.20902
Epoch:0038, train_loss=1.89585, train_acc=0.48916, val_loss=2.06110, val_acc=0.47263, time=1.17201
Epoch:0039, train_loss=1.89331, train_acc=0.49018, val_loss=2.06092, val_acc=0.46898, time=1.20799
Epoch:0040, train_loss=1.89090, train_acc=0.49139, val_loss=2.06076, val_acc=0.47080, time=1.13001
Epoch:0041, train_loss=1.88861, train_acc=0.49301, val_loss=2.06061, val_acc=0.47263, time=1.12401
Epoch:0042, train_loss=1.88643, train_acc=0.49483, val_loss=2.06046, val_acc=0.47263, time=1.27500
Epoch:0043, train_loss=1.88435, train_acc=0.49666, val_loss=2.06032, val_acc=0.47445, time=1.28501
Epoch:0044, train_loss=1.88237, train_acc=0.49767, val_loss=2.06020, val_acc=0.47628, time=1.13901
Epoch:0045, train_loss=1.88046, train_acc=0.49848, val_loss=2.06007, val_acc=0.47628, time=1.22103
Epoch:0046, train_loss=1.87862, train_acc=0.50091, val_loss=2.05996, val_acc=0.47445, time=1.24600
Epoch:0047, train_loss=1.87684, train_acc=0.50192, val_loss=2.05985, val_acc=0.47445, time=1.16501
Epoch:0048, train_loss=1.87513, train_acc=0.50375, val_loss=2.05975, val_acc=0.47445, time=1.24701
Epoch:0049, train_loss=1.87346, train_acc=0.50456, val_loss=2.05966, val_acc=0.47628, time=1.15900
Epoch:0050, train_loss=1.87185, train_acc=0.50618, val_loss=2.05957, val_acc=0.47445, time=1.19601
Epoch:0051, train_loss=1.87028, train_acc=0.50719, val_loss=2.05949, val_acc=0.47628, time=1.18701
Epoch:0052, train_loss=1.86875, train_acc=0.50901, val_loss=2.05941, val_acc=0.47445, time=1.22901
Epoch:0053, train_loss=1.86725, train_acc=0.51023, val_loss=2.05934, val_acc=0.47445, time=1.19600
Epoch:0054, train_loss=1.86579, train_acc=0.51165, val_loss=2.05928, val_acc=0.47263, time=1.27801
Epoch:0055, train_loss=1.86436, train_acc=0.51408, val_loss=2.05921, val_acc=0.47445, time=1.20102
Epoch:0056, train_loss=1.86295, train_acc=0.51509, val_loss=2.05916, val_acc=0.47263, time=1.33300
Epoch:0057, train_loss=1.86156, train_acc=0.51610, val_loss=2.05911, val_acc=0.47263, time=1.25702
Epoch:0058, train_loss=1.86020, train_acc=0.51772, val_loss=2.05906, val_acc=0.47263, time=1.28601
Epoch:0059, train_loss=1.85885, train_acc=0.51914, val_loss=2.05902, val_acc=0.47263, time=1.18902
Epoch:0060, train_loss=1.85752, train_acc=0.52036, val_loss=2.05898, val_acc=0.47263, time=1.27899
Epoch:0061, train_loss=1.85621, train_acc=0.52218, val_loss=2.05894, val_acc=0.47445, time=1.14301
Epoch:0062, train_loss=1.85491, train_acc=0.52400, val_loss=2.05891, val_acc=0.47445, time=1.21001
Epoch:0063, train_loss=1.85363, train_acc=0.52502, val_loss=2.05888, val_acc=0.47445, time=1.24700
Epoch:0064, train_loss=1.85236, train_acc=0.52623, val_loss=2.05886, val_acc=0.47445, time=1.17199
Epoch:0065, train_loss=1.85111, train_acc=0.52724, val_loss=2.05884, val_acc=0.47080, time=1.19500
Epoch:0066, train_loss=1.84986, train_acc=0.52866, val_loss=2.05882, val_acc=0.47080, time=1.15001
Epoch:0067, train_loss=1.84863, train_acc=0.52927, val_loss=2.05880, val_acc=0.46715, time=1.20100
Epoch:0068, train_loss=1.84742, train_acc=0.52967, val_loss=2.05878, val_acc=0.46715, time=1.22401
Epoch:0069, train_loss=1.84622, train_acc=0.53089, val_loss=2.05877, val_acc=0.46715, time=1.30600
Epoch:0070, train_loss=1.84503, train_acc=0.53352, val_loss=2.05876, val_acc=0.46715, time=1.21101
Epoch:0071, train_loss=1.84385, train_acc=0.53494, val_loss=2.05875, val_acc=0.46715, time=1.22101
Epoch:0072, train_loss=1.84269, train_acc=0.53575, val_loss=2.05874, val_acc=0.46715, time=1.14102
Epoch:0073, train_loss=1.84154, train_acc=0.53737, val_loss=2.05873, val_acc=0.46533, time=1.23600
Epoch:0074, train_loss=1.84041, train_acc=0.53838, val_loss=2.05873, val_acc=0.46533, time=1.11001
Epoch:0075, train_loss=1.83929, train_acc=0.53980, val_loss=2.05872, val_acc=0.46168, time=1.24601
Epoch:0076, train_loss=1.83818, train_acc=0.54000, val_loss=2.05871, val_acc=0.46168, time=1.16201
Epoch:0077, train_loss=1.83708, train_acc=0.54122, val_loss=2.05871, val_acc=0.45985, time=1.20501
Epoch:0078, train_loss=1.83599, train_acc=0.54223, val_loss=2.05870, val_acc=0.45803, time=1.11200
Epoch:0079, train_loss=1.83492, train_acc=0.54304, val_loss=2.05870, val_acc=0.45803, time=1.20201
Epoch:0080, train_loss=1.83386, train_acc=0.54365, val_loss=2.05869, val_acc=0.45803, time=1.23201
Epoch:0081, train_loss=1.83280, train_acc=0.54466, val_loss=2.05869, val_acc=0.45985, time=1.27901
Epoch:0082, train_loss=1.83176, train_acc=0.54568, val_loss=2.05869, val_acc=0.46168, time=1.27501
Epoch:0083, train_loss=1.83073, train_acc=0.54669, val_loss=2.05868, val_acc=0.46168, time=1.22400
Epoch:0084, train_loss=1.82970, train_acc=0.54811, val_loss=2.05868, val_acc=0.46168, time=1.15300
Epoch:0085, train_loss=1.82869, train_acc=0.54851, val_loss=2.05867, val_acc=0.45985, time=1.27901
Epoch:0086, train_loss=1.82768, train_acc=0.54892, val_loss=2.05867, val_acc=0.45985, time=1.25700
Epoch:0087, train_loss=1.82668, train_acc=0.54952, val_loss=2.05867, val_acc=0.45985, time=1.19901
Epoch:0088, train_loss=1.82569, train_acc=0.55074, val_loss=2.05866, val_acc=0.45985, time=1.19500
Epoch:0089, train_loss=1.82471, train_acc=0.55276, val_loss=2.05866, val_acc=0.45985, time=1.27800
Epoch:0090, train_loss=1.82374, train_acc=0.55358, val_loss=2.05866, val_acc=0.45985, time=1.22101
Epoch:0091, train_loss=1.82278, train_acc=0.55479, val_loss=2.05866, val_acc=0.45985, time=1.26001
Epoch:0092, train_loss=1.82182, train_acc=0.55682, val_loss=2.05866, val_acc=0.45620, time=1.07701
Epoch:0093, train_loss=1.82087, train_acc=0.55783, val_loss=2.05866, val_acc=0.45620, time=1.21800
Epoch:0094, train_loss=1.81993, train_acc=0.55803, val_loss=2.05866, val_acc=0.45620, time=1.22401
Epoch:0095, train_loss=1.81899, train_acc=0.55945, val_loss=2.05866, val_acc=0.45620, time=1.16099
Epoch:0096, train_loss=1.81806, train_acc=0.56026, val_loss=2.05866, val_acc=0.45620, time=1.25404
Epoch:0097, train_loss=1.81714, train_acc=0.56026, val_loss=2.05866, val_acc=0.45620, time=1.24899
Early stopping...

Optimization Finished!

Test set results: loss= 2.00206, accuracy= 0.45683, time= 0.34900

Test Precision, Recall and F1-Score...
              precision    recall  f1-score   support

           0     0.0000    0.0000    0.0000        87
           1     0.4944    0.8199    0.6169      1083
           2     0.3137    0.1609    0.2127       696
           3     0.0000    0.0000    0.0000        10
           4     0.0000    0.0000    0.0000        75
           5     0.0000    0.0000    0.0000       121
           6     0.0000    0.0000    0.0000        36
           7     0.0000    0.0000    0.0000        81

    accuracy                         0.4568      2189
   macro avg     0.1010    0.1226    0.1037      2189
weighted avg     0.3444    0.4568    0.3728      2189


Macro average Test Precision, Recall and F1-Score...
(0.10101969518319577, 0.12260801732097941, 0.10370080093363641, None)

Micro average Test Precision, Recall and F1-Score...
(0.4568296025582458, 0.4568296025582458, 0.4568296025582458, None)

Embeddings:
Word_embeddings:7688
Train_doc_embeddings:5485
Test_doc_embeddings:2189
