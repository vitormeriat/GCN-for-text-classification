
==========: 279505527748700
Epoch:0001, train_loss=2.43487, train_acc=0.04456, val_loss=2.08371, val_acc=0.25730, time=1.22301
Epoch:0002, train_loss=2.11410, train_acc=0.24286, val_loss=2.06430, val_acc=0.41971, time=1.14901
Epoch:0003, train_loss=1.92877, train_acc=0.43549, val_loss=2.06042, val_acc=0.46898, time=1.23902
Epoch:0004, train_loss=1.88643, train_acc=0.51610, val_loss=2.06266, val_acc=0.48905, time=1.15701
Epoch:0005, train_loss=1.90042, train_acc=0.53150, val_loss=2.06357, val_acc=0.49635, time=1.17199
Epoch:0006, train_loss=1.90467, train_acc=0.54304, val_loss=2.06378, val_acc=0.47993, time=1.30002
Epoch:0007, train_loss=1.90217, train_acc=0.55276, val_loss=2.06433, val_acc=0.45803, time=1.21101
Epoch:0008, train_loss=1.89871, train_acc=0.54770, val_loss=2.06408, val_acc=0.45620, time=1.30101
Epoch:0009, train_loss=1.88333, train_acc=0.56573, val_loss=2.06313, val_acc=0.47263, time=1.16101
Epoch:0010, train_loss=1.85910, train_acc=0.58679, val_loss=2.06227, val_acc=0.47993, time=1.16000
Epoch:0011, train_loss=1.83472, train_acc=0.59469, val_loss=2.06163, val_acc=0.47993, time=1.18200
Epoch:0012, train_loss=1.81295, train_acc=0.59874, val_loss=2.06107, val_acc=0.47445, time=1.17201
Epoch:0013, train_loss=1.79312, train_acc=0.60300, val_loss=2.06065, val_acc=0.47628, time=1.18401
Epoch:0014, train_loss=1.77617, train_acc=0.61576, val_loss=2.06064, val_acc=0.47628, time=1.22501
Epoch:0015, train_loss=1.76420, train_acc=0.62771, val_loss=2.06112, val_acc=0.46533, time=1.23001
Epoch:0016, train_loss=1.75761, train_acc=0.63784, val_loss=2.06190, val_acc=0.46715, time=1.19001
Epoch:0017, train_loss=1.75415, train_acc=0.64310, val_loss=2.06267, val_acc=0.45985, time=1.33700
Epoch:0018, train_loss=1.75086, train_acc=0.64817, val_loss=2.06328, val_acc=0.44708, time=1.20101
Epoch:0019, train_loss=1.74609, train_acc=0.65607, val_loss=2.06370, val_acc=0.44526, time=1.28701
Epoch:0020, train_loss=1.73978, train_acc=0.66235, val_loss=2.06400, val_acc=0.44161, time=1.21401
Epoch:0021, train_loss=1.73266, train_acc=0.67065, val_loss=2.06425, val_acc=0.42883, time=1.20900
Epoch:0022, train_loss=1.72544, train_acc=0.67207, val_loss=2.06447, val_acc=0.43796, time=1.13100
Epoch:0023, train_loss=1.71848, train_acc=0.67004, val_loss=2.06466, val_acc=0.43796, time=1.33501
Epoch:0024, train_loss=1.71188, train_acc=0.67470, val_loss=2.06483, val_acc=0.44343, time=1.31201
Epoch:0025, train_loss=1.70568, train_acc=0.68017, val_loss=2.06501, val_acc=0.44526, time=1.35200
Epoch:0026, train_loss=1.70004, train_acc=0.68280, val_loss=2.06523, val_acc=0.44708, time=1.12700
Epoch:0027, train_loss=1.69512, train_acc=0.68908, val_loss=2.06551, val_acc=0.45985, time=1.25400
Epoch:0028, train_loss=1.69094, train_acc=0.69718, val_loss=2.06584, val_acc=0.45620, time=1.16901
Epoch:0029, train_loss=1.68728, train_acc=0.70124, val_loss=2.06620, val_acc=0.45803, time=1.18401
Epoch:0030, train_loss=1.68379, train_acc=0.70610, val_loss=2.06656, val_acc=0.45620, time=1.24402
Epoch:0031, train_loss=1.68019, train_acc=0.70853, val_loss=2.06693, val_acc=0.46168, time=1.17100
Epoch:0032, train_loss=1.67643, train_acc=0.71136, val_loss=2.06729, val_acc=0.46168, time=1.20002
Epoch:0033, train_loss=1.67261, train_acc=0.71278, val_loss=2.06765, val_acc=0.45985, time=1.22501
Epoch:0034, train_loss=1.66884, train_acc=0.71724, val_loss=2.06800, val_acc=0.45438, time=1.28201
Epoch:0035, train_loss=1.66518, train_acc=0.72109, val_loss=2.06835, val_acc=0.45255, time=1.17101
Epoch:0036, train_loss=1.66165, train_acc=0.72878, val_loss=2.06870, val_acc=0.44343, time=1.20400
Epoch:0037, train_loss=1.65828, train_acc=0.73668, val_loss=2.06907, val_acc=0.44708, time=1.16902
Epoch:0038, train_loss=1.65512, train_acc=0.74276, val_loss=2.06947, val_acc=0.44891, time=1.15800
Epoch:0039, train_loss=1.65212, train_acc=0.74701, val_loss=2.06989, val_acc=0.44343, time=1.25501
Epoch:0040, train_loss=1.64918, train_acc=0.75430, val_loss=2.07033, val_acc=0.44161, time=1.21501
Epoch:0041, train_loss=1.64620, train_acc=0.75957, val_loss=2.07077, val_acc=0.43796, time=1.20002
Epoch:0042, train_loss=1.64315, train_acc=0.76241, val_loss=2.07123, val_acc=0.43613, time=1.16700
Epoch:0043, train_loss=1.64010, train_acc=0.76403, val_loss=2.07170, val_acc=0.43796, time=1.15502
Epoch:0044, train_loss=1.63714, train_acc=0.76585, val_loss=2.07217, val_acc=0.43613, time=1.26999
Epoch:0045, train_loss=1.63434, train_acc=0.76828, val_loss=2.07263, val_acc=0.43613, time=1.31401
Epoch:0046, train_loss=1.63168, train_acc=0.77112, val_loss=2.07308, val_acc=0.43796, time=1.18200
Epoch:0047, train_loss=1.62914, train_acc=0.77294, val_loss=2.07351, val_acc=0.42883, time=1.14301
Epoch:0048, train_loss=1.62669, train_acc=0.77699, val_loss=2.07392, val_acc=0.43248, time=1.16701
Epoch:0049, train_loss=1.62430, train_acc=0.78003, val_loss=2.07433, val_acc=0.42883, time=1.10101
Epoch:0050, train_loss=1.62192, train_acc=0.78347, val_loss=2.07474, val_acc=0.42883, time=1.25802
Epoch:0051, train_loss=1.61953, train_acc=0.78671, val_loss=2.07515, val_acc=0.42883, time=1.04800
Epoch:0052, train_loss=1.61711, train_acc=0.78833, val_loss=2.07557, val_acc=0.42701, time=1.28002
Epoch:0053, train_loss=1.61469, train_acc=0.79036, val_loss=2.07600, val_acc=0.42701, time=1.26500
Epoch:0054, train_loss=1.61232, train_acc=0.79238, val_loss=2.07643, val_acc=0.42701, time=1.17300
Epoch:0055, train_loss=1.61002, train_acc=0.79380, val_loss=2.07686, val_acc=0.42883, time=1.33600
Epoch:0056, train_loss=1.60780, train_acc=0.79785, val_loss=2.07730, val_acc=0.43066, time=1.19302
Epoch:0057, train_loss=1.60566, train_acc=0.80170, val_loss=2.07775, val_acc=0.43066, time=1.20701
Epoch:0058, train_loss=1.60359, train_acc=0.80474, val_loss=2.07821, val_acc=0.42883, time=1.37201
Epoch:0059, train_loss=1.60157, train_acc=0.80717, val_loss=2.07869, val_acc=0.42518, time=1.10702
Epoch:0060, train_loss=1.59956, train_acc=0.81041, val_loss=2.07917, val_acc=0.42518, time=1.30501
Epoch:0061, train_loss=1.59754, train_acc=0.81365, val_loss=2.07966, val_acc=0.42518, time=1.15500
Epoch:0062, train_loss=1.59552, train_acc=0.81710, val_loss=2.08016, val_acc=0.42518, time=1.17101
Epoch:0063, train_loss=1.59351, train_acc=0.81993, val_loss=2.08064, val_acc=0.42336, time=1.26500
Epoch:0064, train_loss=1.59153, train_acc=0.82297, val_loss=2.08112, val_acc=0.42518, time=1.25201
Epoch:0065, train_loss=1.58961, train_acc=0.82418, val_loss=2.08160, val_acc=0.42518, time=1.36001
Epoch:0066, train_loss=1.58776, train_acc=0.82439, val_loss=2.08208, val_acc=0.42518, time=1.25103
Epoch:0067, train_loss=1.58596, train_acc=0.82560, val_loss=2.08256, val_acc=0.42153, time=1.22199
Epoch:0068, train_loss=1.58418, train_acc=0.82743, val_loss=2.08305, val_acc=0.42518, time=1.23601
Epoch:0069, train_loss=1.58241, train_acc=0.82986, val_loss=2.08353, val_acc=0.42336, time=1.26902
Epoch:0070, train_loss=1.58064, train_acc=0.83148, val_loss=2.08401, val_acc=0.42153, time=1.18001
Epoch:0071, train_loss=1.57889, train_acc=0.83391, val_loss=2.08448, val_acc=0.42518, time=1.21901
Epoch:0072, train_loss=1.57715, train_acc=0.83634, val_loss=2.08493, val_acc=0.42701, time=1.17502
Epoch:0073, train_loss=1.57544, train_acc=0.83816, val_loss=2.08538, val_acc=0.42336, time=1.19600
Epoch:0074, train_loss=1.57378, train_acc=0.84079, val_loss=2.08584, val_acc=0.41788, time=1.16402
Epoch:0075, train_loss=1.57215, train_acc=0.84221, val_loss=2.08630, val_acc=0.41423, time=1.26400
Epoch:0076, train_loss=1.57055, train_acc=0.84505, val_loss=2.08677, val_acc=0.41423, time=1.30402
Epoch:0077, train_loss=1.56897, train_acc=0.84626, val_loss=2.08725, val_acc=0.41423, time=1.17001
Epoch:0078, train_loss=1.56740, train_acc=0.84788, val_loss=2.08773, val_acc=0.41241, time=1.19200
Epoch:0079, train_loss=1.56585, train_acc=0.85052, val_loss=2.08821, val_acc=0.41058, time=1.18001
Epoch:0080, train_loss=1.56431, train_acc=0.85173, val_loss=2.08869, val_acc=0.41058, time=1.17801
Epoch:0081, train_loss=1.56279, train_acc=0.85396, val_loss=2.08918, val_acc=0.40876, time=1.27501
Epoch:0082, train_loss=1.56131, train_acc=0.85659, val_loss=2.08968, val_acc=0.40693, time=1.24601
Epoch:0083, train_loss=1.55985, train_acc=0.85821, val_loss=2.09018, val_acc=0.40693, time=1.13202
Epoch:0084, train_loss=1.55842, train_acc=0.86145, val_loss=2.09067, val_acc=0.40511, time=1.24200
Epoch:0085, train_loss=1.55700, train_acc=0.86267, val_loss=2.09115, val_acc=0.40511, time=1.25201
Epoch:0086, train_loss=1.55560, train_acc=0.86388, val_loss=2.09163, val_acc=0.40511, time=1.12501
Epoch:0087, train_loss=1.55422, train_acc=0.86591, val_loss=2.09211, val_acc=0.40328, time=1.27200
Epoch:0088, train_loss=1.55286, train_acc=0.86814, val_loss=2.09259, val_acc=0.39781, time=1.12500
Epoch:0089, train_loss=1.55151, train_acc=0.86976, val_loss=2.09309, val_acc=0.39599, time=1.17201
Epoch:0090, train_loss=1.55019, train_acc=0.87077, val_loss=2.09358, val_acc=0.39416, time=1.17002
Epoch:0091, train_loss=1.54889, train_acc=0.87199, val_loss=2.09407, val_acc=0.39234, time=1.19601
Epoch:0092, train_loss=1.54761, train_acc=0.87401, val_loss=2.09456, val_acc=0.38869, time=1.26801
Epoch:0093, train_loss=1.54634, train_acc=0.87563, val_loss=2.09506, val_acc=0.39051, time=1.30301
Epoch:0094, train_loss=1.54509, train_acc=0.87644, val_loss=2.09556, val_acc=0.38869, time=1.16202
Epoch:0095, train_loss=1.54386, train_acc=0.87746, val_loss=2.09606, val_acc=0.38869, time=1.15501
Epoch:0096, train_loss=1.54265, train_acc=0.87908, val_loss=2.09656, val_acc=0.38869, time=1.24499
Epoch:0097, train_loss=1.54145, train_acc=0.88130, val_loss=2.09706, val_acc=0.38869, time=1.16101
Epoch:0098, train_loss=1.54027, train_acc=0.88232, val_loss=2.09756, val_acc=0.38869, time=1.09400
Epoch:0099, train_loss=1.53911, train_acc=0.88292, val_loss=2.09806, val_acc=0.38869, time=1.15301
Epoch:0100, train_loss=1.53797, train_acc=0.88455, val_loss=2.09855, val_acc=0.38869, time=1.20400
Epoch:0101, train_loss=1.53684, train_acc=0.88576, val_loss=2.09905, val_acc=0.39051, time=1.20701
Epoch:0102, train_loss=1.53572, train_acc=0.88637, val_loss=2.09954, val_acc=0.39051, time=1.30302
Early stopping...

Optimization Finished!

Test set results: loss= 2.17123, accuracy= 0.40064, time= 0.33301

Test Precision, Recall and F1-Score...
              precision    recall  f1-score   support

           0     0.0612    0.0345    0.0441        87
           1     0.5011    0.6085    0.5496      1083
           2     0.3318    0.3060    0.3184       696
           3     0.0000    0.0000    0.0000        10
           4     0.0179    0.0133    0.0153        75
           5     0.0000    0.0000    0.0000       121
           6     0.0000    0.0000    0.0000        36
           7     0.0222    0.0123    0.0159        81

    accuracy                         0.4006      2189
   macro avg     0.1168    0.1218    0.1179      2189
weighted avg     0.3573    0.4006    0.3760      2189


Macro average Test Precision, Recall and F1-Score...
(0.11677753002756365, 0.12183639690491267, 0.11790852199599247, None)

Micro average Test Precision, Recall and F1-Score...
(0.40063956144358154, 0.40063956144358154, 0.40063956144358154, None)

Embeddings:
Word_embeddings:7688
Train_doc_embeddings:5485
Test_doc_embeddings:2189
