
==========: 127693498964500
Epoch:0001, train_loss=2.20408, train_acc=0.41624, val_loss=2.06036, val_acc=0.58759, time=1.46110
Epoch:0002, train_loss=1.93838, train_acc=0.55398, val_loss=2.04473, val_acc=0.65876, time=1.39808
Epoch:0003, train_loss=1.78264, train_acc=0.66883, val_loss=2.03720, val_acc=0.71715, time=1.32207
Epoch:0004, train_loss=1.70727, train_acc=0.71541, val_loss=2.03124, val_acc=0.77555, time=1.32913
Epoch:0005, train_loss=1.64822, train_acc=0.76099, val_loss=2.02601, val_acc=0.80474, time=1.36006
Epoch:0006, train_loss=1.59566, train_acc=0.81689, val_loss=2.02216, val_acc=0.85219, time=1.37708
Epoch:0007, train_loss=1.55626, train_acc=0.86733, val_loss=2.01983, val_acc=0.88139, time=1.40906
Epoch:0008, train_loss=1.53152, train_acc=0.90500, val_loss=2.01843, val_acc=0.88869, time=1.38324
Epoch:0009, train_loss=1.51599, train_acc=0.92181, val_loss=2.01736, val_acc=0.89964, time=1.30604
Epoch:0010, train_loss=1.50407, train_acc=0.93235, val_loss=2.01635, val_acc=0.90511, time=1.36008
Epoch:0011, train_loss=1.49320, train_acc=0.94085, val_loss=2.01534, val_acc=0.92153, time=1.31806
Epoch:0012, train_loss=1.48288, train_acc=0.94977, val_loss=2.01438, val_acc=0.92336, time=1.43308
Epoch:0013, train_loss=1.47331, train_acc=0.95564, val_loss=2.01354, val_acc=0.92883, time=1.39609
Epoch:0014, train_loss=1.46487, train_acc=0.96111, val_loss=2.01286, val_acc=0.93066, time=1.32708
Epoch:0015, train_loss=1.45784, train_acc=0.96516, val_loss=2.01234, val_acc=0.93613, time=1.39206
Epoch:0016, train_loss=1.45237, train_acc=0.96901, val_loss=2.01198, val_acc=0.94526, time=1.24106
Epoch:0017, train_loss=1.44839, train_acc=0.97326, val_loss=2.01174, val_acc=0.94891, time=1.29307
Epoch:0018, train_loss=1.44563, train_acc=0.97407, val_loss=2.01159, val_acc=0.95255, time=1.38904
Epoch:0019, train_loss=1.44364, train_acc=0.97407, val_loss=2.01147, val_acc=0.95438, time=1.30405
Epoch:0020, train_loss=1.44194, train_acc=0.97630, val_loss=2.01136, val_acc=0.95620, time=1.28508
Epoch:0021, train_loss=1.44014, train_acc=0.97691, val_loss=2.01123, val_acc=0.95620, time=1.30706
Epoch:0022, train_loss=1.43814, train_acc=0.97914, val_loss=2.01110, val_acc=0.95255, time=1.30803
Epoch:0023, train_loss=1.43598, train_acc=0.98177, val_loss=2.01097, val_acc=0.94708, time=1.31507
Epoch:0024, train_loss=1.43386, train_acc=0.98380, val_loss=2.01085, val_acc=0.95073, time=1.36321
Epoch:0025, train_loss=1.43198, train_acc=0.98582, val_loss=2.01077, val_acc=0.95073, time=1.31506
Epoch:0026, train_loss=1.43044, train_acc=0.98825, val_loss=2.01071, val_acc=0.95073, time=1.34906
Epoch:0027, train_loss=1.42922, train_acc=0.98866, val_loss=2.01066, val_acc=0.95073, time=1.30609
Epoch:0028, train_loss=1.42826, train_acc=0.98886, val_loss=2.01062, val_acc=0.95073, time=1.34905
Epoch:0029, train_loss=1.42744, train_acc=0.98926, val_loss=2.01057, val_acc=0.95073, time=1.30006
Epoch:0030, train_loss=1.42668, train_acc=0.98926, val_loss=2.01051, val_acc=0.95073, time=1.27208
Epoch:0031, train_loss=1.42594, train_acc=0.98947, val_loss=2.01044, val_acc=0.95255, time=1.34907
Epoch:0032, train_loss=1.42520, train_acc=0.99048, val_loss=2.01036, val_acc=0.95438, time=1.28208
Epoch:0033, train_loss=1.42448, train_acc=0.99129, val_loss=2.01028, val_acc=0.95438, time=1.27306
Epoch:0034, train_loss=1.42378, train_acc=0.99230, val_loss=2.01020, val_acc=0.95620, time=1.22635
Epoch:0035, train_loss=1.42312, train_acc=0.99271, val_loss=2.01013, val_acc=0.95620, time=1.37303
Epoch:0036, train_loss=1.42250, train_acc=0.99271, val_loss=2.01007, val_acc=0.95803, time=1.31107
Epoch:0037, train_loss=1.42194, train_acc=0.99372, val_loss=2.01003, val_acc=0.95803, time=1.32907
Epoch:0038, train_loss=1.42143, train_acc=0.99392, val_loss=2.01000, val_acc=0.95985, time=1.28307
Epoch:0039, train_loss=1.42097, train_acc=0.99433, val_loss=2.00999, val_acc=0.95985, time=1.37307
Epoch:0040, train_loss=1.42054, train_acc=0.99554, val_loss=2.00998, val_acc=0.95985, time=1.35406
Epoch:0041, train_loss=1.42015, train_acc=0.99595, val_loss=2.00999, val_acc=0.95985, time=1.33107
Epoch:0042, train_loss=1.41978, train_acc=0.99595, val_loss=2.01001, val_acc=0.95985, time=1.27706
Epoch:0043, train_loss=1.41944, train_acc=0.99595, val_loss=2.01003, val_acc=0.95985, time=1.38506
Epoch:0044, train_loss=1.41913, train_acc=0.99595, val_loss=2.01005, val_acc=0.95985, time=1.29805
Early stopping...

Optimization Finished!

Test set results: loss= 1.80374, accuracy= 0.95706, time= 0.39801

Test Precision, Recall and F1-Score...
              precision    recall  f1-score   support

           0     0.9674    0.9871    0.9771      1083
           1     0.9792    0.9454    0.9620       696
           2     0.8696    0.9195    0.8939        87
           3     0.8939    0.9752    0.9328       121
           4     0.9615    0.6944    0.8065        36
           5     0.8861    0.9333    0.9091        75
           6     0.9286    0.8025    0.8609        81
           7     0.7692    1.0000    0.8696        10

    accuracy                         0.9571      2189
   macro avg     0.9069    0.9072    0.9015      2189
weighted avg     0.9580    0.9571    0.9566      2189


Macro average Test Precision, Recall and F1-Score...
(0.9069385876480939, 0.9071836249259783, 0.9014790436187085, None)

Micro average Test Precision, Recall and F1-Score...
(0.9570580173595249, 0.9570580173595249, 0.9570580173595249, None)

Embeddings:
Word_embeddings:7688
Train_doc_embeddings:5485
Test_doc_embeddings:2189
