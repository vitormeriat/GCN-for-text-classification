
==========: 299735253632500
Epoch:0001, train_loss=2.01279, train_acc=0.16989, val_loss=1.93792, val_acc=0.32804, time=0.12501
Epoch:0002, train_loss=1.87610, train_acc=0.32045, val_loss=1.93297, val_acc=0.35979, time=0.11700
Epoch:0003, train_loss=1.80169, train_acc=0.40246, val_loss=1.92626, val_acc=0.43386, time=0.11101
Epoch:0004, train_loss=1.72254, train_acc=0.49092, val_loss=1.91926, val_acc=0.56085, time=0.13000
Epoch:0005, train_loss=1.64921, train_acc=0.61394, val_loss=1.91275, val_acc=0.61376, time=0.12100
Epoch:0006, train_loss=1.58541, train_acc=0.71412, val_loss=1.90739, val_acc=0.70370, time=0.11202
Epoch:0007, train_loss=1.53355, train_acc=0.77563, val_loss=1.90334, val_acc=0.73545, time=0.11900
Epoch:0008, train_loss=1.49297, train_acc=0.79613, val_loss=1.90014, val_acc=0.75661, time=0.14601
Epoch:0009, train_loss=1.45900, train_acc=0.80258, val_loss=1.89747, val_acc=0.76720, time=0.13499
Epoch:0010, train_loss=1.42850, train_acc=0.81371, val_loss=1.89526, val_acc=0.75661, time=0.11800
Epoch:0011, train_loss=1.40099, train_acc=0.82542, val_loss=1.89356, val_acc=0.74074, time=0.12501
Epoch:0012, train_loss=1.37669, train_acc=0.84886, val_loss=1.89228, val_acc=0.75132, time=0.13001
Epoch:0013, train_loss=1.35532, train_acc=0.86467, val_loss=1.89134, val_acc=0.75661, time=0.12402
Epoch:0014, train_loss=1.33641, train_acc=0.87463, val_loss=1.89064, val_acc=0.75132, time=0.12801
Epoch:0015, train_loss=1.31973, train_acc=0.88869, val_loss=1.89014, val_acc=0.74603, time=0.12600
Epoch:0016, train_loss=1.30518, train_acc=0.89572, val_loss=1.88975, val_acc=0.75132, time=0.11302
Epoch:0017, train_loss=1.29242, train_acc=0.90627, val_loss=1.88939, val_acc=0.75661, time=0.12998
Epoch:0018, train_loss=1.28086, train_acc=0.91388, val_loss=1.88900, val_acc=0.75132, time=0.10699
Epoch:0019, train_loss=1.26991, train_acc=0.92267, val_loss=1.88858, val_acc=0.75132, time=0.13302
Epoch:0020, train_loss=1.25938, train_acc=0.92853, val_loss=1.88819, val_acc=0.76190, time=0.11801
Epoch:0021, train_loss=1.24945, train_acc=0.93556, val_loss=1.88790, val_acc=0.76190, time=0.13002
Epoch:0022, train_loss=1.24039, train_acc=0.93849, val_loss=1.88776, val_acc=0.76190, time=0.12102
Epoch:0023, train_loss=1.23230, train_acc=0.94376, val_loss=1.88776, val_acc=0.76190, time=0.11201
Epoch:0024, train_loss=1.22506, train_acc=0.95079, val_loss=1.88786, val_acc=0.75661, time=0.12600
Epoch:0025, train_loss=1.21844, train_acc=0.95723, val_loss=1.88802, val_acc=0.75132, time=0.12800
Epoch:0026, train_loss=1.21227, train_acc=0.96134, val_loss=1.88821, val_acc=0.75132, time=0.12800
Epoch:0027, train_loss=1.20641, train_acc=0.96544, val_loss=1.88839, val_acc=0.75132, time=0.12201
Early stopping...

Optimization Finished!

Test set results: loss= 1.73115, accuracy= 0.72291, time= 0.03800

Test Precision, Recall and F1-Score...
              precision    recall  f1-score   support

           0     0.8168    0.7643    0.7897       140
           1     0.6098    0.5556    0.5814        45
           2     0.7045    0.7686    0.7352       121
           3     0.6915    0.7065    0.6989        92
           4     0.6636    0.6293    0.6460       116
           5     0.8667    0.6000    0.7091        65
           6     0.7143    0.7940    0.7520       233

    accuracy                         0.7229       812
   macro avg     0.7239    0.6883    0.7018       812
weighted avg     0.7271    0.7229    0.7220       812


Macro average Test Precision, Recall and F1-Score...
(0.723881935932439, 0.688322830204374, 0.7017581386918252, None)

Micro average Test Precision, Recall and F1-Score...
(0.7229064039408867, 0.7229064039408867, 0.7229064039408866, None)

Embeddings:
Word_embeddings:1343
Train_doc_embeddings:1896
Test_doc_embeddings:812
