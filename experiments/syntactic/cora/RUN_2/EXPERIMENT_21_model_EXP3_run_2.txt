
==========: 299872625307100
Epoch:0001, train_loss=2.03365, train_acc=0.12478, val_loss=1.93712, val_acc=0.36508, time=0.12400
Epoch:0002, train_loss=1.86375, train_acc=0.33861, val_loss=1.92872, val_acc=0.42328, time=0.13200
Epoch:0003, train_loss=1.79198, train_acc=0.42531, val_loss=1.92260, val_acc=0.48148, time=0.12799
Epoch:0004, train_loss=1.73038, train_acc=0.53251, val_loss=1.91714, val_acc=0.55026, time=0.13301
Epoch:0005, train_loss=1.66950, train_acc=0.60516, val_loss=1.91197, val_acc=0.62963, time=0.12001
Epoch:0006, train_loss=1.60813, train_acc=0.68834, val_loss=1.90768, val_acc=0.70370, time=0.12100
Epoch:0007, train_loss=1.55344, train_acc=0.73052, val_loss=1.90445, val_acc=0.71958, time=0.10600
Epoch:0008, train_loss=1.50840, train_acc=0.76216, val_loss=1.90183, val_acc=0.73545, time=0.12999
Epoch:0009, train_loss=1.46989, train_acc=0.78735, val_loss=1.89941, val_acc=0.75132, time=0.12801
Epoch:0010, train_loss=1.43497, train_acc=0.80609, val_loss=1.89722, val_acc=0.75661, time=0.11400
Epoch:0011, train_loss=1.40387, train_acc=0.83890, val_loss=1.89549, val_acc=0.75132, time=0.12201
Epoch:0012, train_loss=1.37820, train_acc=0.85940, val_loss=1.89432, val_acc=0.76190, time=0.13502
Epoch:0013, train_loss=1.35823, train_acc=0.87346, val_loss=1.89356, val_acc=0.76190, time=0.12000
Epoch:0014, train_loss=1.34242, train_acc=0.88694, val_loss=1.89295, val_acc=0.75661, time=0.10402
Epoch:0015, train_loss=1.32854, train_acc=0.89104, val_loss=1.89228, val_acc=0.74603, time=0.10799
Epoch:0016, train_loss=1.31490, train_acc=0.90041, val_loss=1.89150, val_acc=0.75661, time=0.10900
Epoch:0017, train_loss=1.30104, train_acc=0.90685, val_loss=1.89072, val_acc=0.77249, time=0.12801
Epoch:0018, train_loss=1.28739, train_acc=0.91681, val_loss=1.89007, val_acc=0.77778, time=0.12801
Epoch:0019, train_loss=1.27458, train_acc=0.92384, val_loss=1.88963, val_acc=0.77249, time=0.12100
Epoch:0020, train_loss=1.26308, train_acc=0.92970, val_loss=1.88944, val_acc=0.77778, time=0.10702
Epoch:0021, train_loss=1.25303, train_acc=0.92970, val_loss=1.88948, val_acc=0.77778, time=0.12199
Epoch:0022, train_loss=1.24435, train_acc=0.93673, val_loss=1.88969, val_acc=0.76720, time=0.12401
Epoch:0023, train_loss=1.23680, train_acc=0.94025, val_loss=1.89000, val_acc=0.76190, time=0.11602
Epoch:0024, train_loss=1.23001, train_acc=0.94259, val_loss=1.89031, val_acc=0.76190, time=0.10100
Epoch:0025, train_loss=1.22360, train_acc=0.95079, val_loss=1.89057, val_acc=0.76720, time=0.12799
Early stopping...

Optimization Finished!

Test set results: loss= 1.72919, accuracy= 0.73030, time= 0.03803

Test Precision, Recall and F1-Score...
              precision    recall  f1-score   support

           0     0.8284    0.7929    0.8102       140
           1     0.7059    0.5333    0.6076        45
           2     0.6838    0.7686    0.7237       121
           3     0.7753    0.7500    0.7624        92
           4     0.6121    0.6121    0.6121       116
           5     0.8113    0.6615    0.7288        65
           6     0.7280    0.7811    0.7536       233

    accuracy                         0.7303       812
   macro avg     0.7350    0.6999    0.7141       812
weighted avg     0.7330    0.7303    0.7296       812


Macro average Test Precision, Recall and F1-Score...
(0.7349621014883988, 0.6999298320566886, 0.7140694251204218, None)

Micro average Test Precision, Recall and F1-Score...
(0.7302955665024631, 0.7302955665024631, 0.7302955665024631, None)

Embeddings:
Word_embeddings:1343
Train_doc_embeddings:1896
Test_doc_embeddings:812
