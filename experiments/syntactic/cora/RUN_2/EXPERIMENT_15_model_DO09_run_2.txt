
==========: 299814420613900
Epoch:0001, train_loss=2.11353, train_acc=0.15114, val_loss=1.94124, val_acc=0.25926, time=0.10800
Epoch:0002, train_loss=1.90447, train_acc=0.27006, val_loss=1.93388, val_acc=0.37037, time=0.12600
Epoch:0003, train_loss=1.86258, train_acc=0.32982, val_loss=1.92882, val_acc=0.39683, time=0.13201
Epoch:0004, train_loss=1.80858, train_acc=0.37551, val_loss=1.92194, val_acc=0.47090, time=0.11199
Epoch:0005, train_loss=1.72382, train_acc=0.45343, val_loss=1.91575, val_acc=0.55026, time=0.12503
Epoch:0006, train_loss=1.64186, train_acc=0.60340, val_loss=1.91152, val_acc=0.62963, time=0.12499
Epoch:0007, train_loss=1.58066, train_acc=0.72466, val_loss=1.90893, val_acc=0.68783, time=0.13101
Epoch:0008, train_loss=1.53927, train_acc=0.78032, val_loss=1.90697, val_acc=0.68783, time=0.12900
Epoch:0009, train_loss=1.50818, train_acc=0.79086, val_loss=1.90470, val_acc=0.69312, time=0.12701
Epoch:0010, train_loss=1.47856, train_acc=0.80844, val_loss=1.90182, val_acc=0.69841, time=0.12701
Epoch:0011, train_loss=1.44701, train_acc=0.82250, val_loss=1.89872, val_acc=0.71958, time=0.11699
Epoch:0012, train_loss=1.41573, train_acc=0.84769, val_loss=1.89595, val_acc=0.73545, time=0.13202
Epoch:0013, train_loss=1.38858, train_acc=0.85882, val_loss=1.89386, val_acc=0.74603, time=0.13100
Epoch:0014, train_loss=1.36736, train_acc=0.86467, val_loss=1.89240, val_acc=0.73545, time=0.13600
Epoch:0015, train_loss=1.35102, train_acc=0.86467, val_loss=1.89138, val_acc=0.74603, time=0.12200
Epoch:0016, train_loss=1.33741, train_acc=0.87405, val_loss=1.89059, val_acc=0.73545, time=0.11000
Epoch:0017, train_loss=1.32476, train_acc=0.87932, val_loss=1.88993, val_acc=0.73545, time=0.13000
Epoch:0018, train_loss=1.31223, train_acc=0.88518, val_loss=1.88939, val_acc=0.77249, time=0.10000
Epoch:0019, train_loss=1.29975, train_acc=0.89631, val_loss=1.88899, val_acc=0.76720, time=0.12300
Epoch:0020, train_loss=1.28765, train_acc=0.90334, val_loss=1.88877, val_acc=0.77249, time=0.12201
Epoch:0021, train_loss=1.27637, train_acc=0.91798, val_loss=1.88872, val_acc=0.76190, time=0.12200
Epoch:0022, train_loss=1.26629, train_acc=0.92794, val_loss=1.88884, val_acc=0.77249, time=0.12600
Epoch:0023, train_loss=1.25754, train_acc=0.93263, val_loss=1.88905, val_acc=0.77249, time=0.14101
Epoch:0024, train_loss=1.24993, train_acc=0.93497, val_loss=1.88928, val_acc=0.76190, time=0.12001
Epoch:0025, train_loss=1.24307, train_acc=0.93966, val_loss=1.88943, val_acc=0.76190, time=0.10801
Epoch:0026, train_loss=1.23652, train_acc=0.94259, val_loss=1.88945, val_acc=0.75661, time=0.13101
Early stopping...

Optimization Finished!

Test set results: loss= 1.73045, accuracy= 0.71921, time= 0.03700

Test Precision, Recall and F1-Score...
              precision    recall  f1-score   support

           0     0.8374    0.7357    0.7833       140
           1     0.5510    0.6000    0.5745        45
           2     0.6783    0.8017    0.7348       121
           3     0.6947    0.7174    0.7059        92
           4     0.6538    0.5862    0.6182       116
           5     0.8864    0.6000    0.7156        65
           6     0.7244    0.7897    0.7556       233

    accuracy                         0.7192       812
   macro avg     0.7180    0.6901    0.6983       812
weighted avg     0.7269    0.7192    0.7188       812


Macro average Test Precision, Recall and F1-Score...
(0.7180137916575191, 0.6900949928558957, 0.6982705500826769, None)

Micro average Test Precision, Recall and F1-Score...
(0.7192118226600985, 0.7192118226600985, 0.7192118226600985, None)

Embeddings:
Word_embeddings:1343
Train_doc_embeddings:1896
Test_doc_embeddings:812
