
==========: 299202216737000
Epoch:0001, train_loss=2.21076, train_acc=0.08729, val_loss=1.96222, val_acc=0.11111, time=0.12499
Epoch:0002, train_loss=2.06020, train_acc=0.12830, val_loss=1.94855, val_acc=0.16931, time=0.13001
Epoch:0003, train_loss=1.94888, train_acc=0.20152, val_loss=1.93937, val_acc=0.23280, time=0.12900
Epoch:0004, train_loss=1.87444, train_acc=0.30287, val_loss=1.93440, val_acc=0.29630, time=0.09900
Epoch:0005, train_loss=1.83365, train_acc=0.35501, val_loss=1.93192, val_acc=0.32275, time=0.12701
Epoch:0006, train_loss=1.81130, train_acc=0.35677, val_loss=1.92959, val_acc=0.35979, time=0.13500
Epoch:0007, train_loss=1.78738, train_acc=0.37141, val_loss=1.92641, val_acc=0.37037, time=0.12099
Epoch:0008, train_loss=1.75381, train_acc=0.39543, val_loss=1.92260, val_acc=0.39683, time=0.10301
Epoch:0009, train_loss=1.71380, train_acc=0.44640, val_loss=1.91884, val_acc=0.47090, time=0.10301
Epoch:0010, train_loss=1.67386, train_acc=0.52314, val_loss=1.91562, val_acc=0.53968, time=0.13000
Epoch:0011, train_loss=1.63851, train_acc=0.58875, val_loss=1.91304, val_acc=0.59788, time=0.11999
Epoch:0012, train_loss=1.60872, train_acc=0.65202, val_loss=1.91096, val_acc=0.64550, time=0.11300
Epoch:0013, train_loss=1.58311, train_acc=0.71353, val_loss=1.90917, val_acc=0.66667, time=0.12801
Epoch:0014, train_loss=1.55985, train_acc=0.74341, val_loss=1.90753, val_acc=0.70370, time=0.12700
Epoch:0015, train_loss=1.53779, train_acc=0.76801, val_loss=1.90598, val_acc=0.71958, time=0.11302
Epoch:0016, train_loss=1.51664, train_acc=0.78969, val_loss=1.90457, val_acc=0.74603, time=0.11301
Epoch:0017, train_loss=1.49673, train_acc=0.81195, val_loss=1.90333, val_acc=0.74074, time=0.13201
Epoch:0018, train_loss=1.47850, train_acc=0.82191, val_loss=1.90227, val_acc=0.74074, time=0.12900
Epoch:0019, train_loss=1.46215, train_acc=0.83480, val_loss=1.90134, val_acc=0.73016, time=0.13001
Epoch:0020, train_loss=1.44753, train_acc=0.83831, val_loss=1.90050, val_acc=0.74074, time=0.13001
Epoch:0021, train_loss=1.43418, train_acc=0.84300, val_loss=1.89966, val_acc=0.74074, time=0.12900
Epoch:0022, train_loss=1.42156, train_acc=0.84241, val_loss=1.89879, val_acc=0.75132, time=0.13501
Epoch:0023, train_loss=1.40931, train_acc=0.84710, val_loss=1.89787, val_acc=0.75661, time=0.13100
Epoch:0024, train_loss=1.39730, train_acc=0.85003, val_loss=1.89692, val_acc=0.76720, time=0.11800
Epoch:0025, train_loss=1.38562, train_acc=0.85530, val_loss=1.89597, val_acc=0.76720, time=0.11701
Epoch:0026, train_loss=1.37446, train_acc=0.86526, val_loss=1.89507, val_acc=0.76190, time=0.11699
Epoch:0027, train_loss=1.36401, train_acc=0.86936, val_loss=1.89425, val_acc=0.76720, time=0.12701
Epoch:0028, train_loss=1.35438, train_acc=0.87053, val_loss=1.89351, val_acc=0.77249, time=0.12498
Epoch:0029, train_loss=1.34559, train_acc=0.87932, val_loss=1.89287, val_acc=0.76720, time=0.13101
Epoch:0030, train_loss=1.33753, train_acc=0.88811, val_loss=1.89232, val_acc=0.77778, time=0.12801
Epoch:0031, train_loss=1.33005, train_acc=0.89104, val_loss=1.89184, val_acc=0.77249, time=0.13200
Epoch:0032, train_loss=1.32296, train_acc=0.89572, val_loss=1.89142, val_acc=0.77249, time=0.10300
Epoch:0033, train_loss=1.31613, train_acc=0.89865, val_loss=1.89105, val_acc=0.77778, time=0.10300
Epoch:0034, train_loss=1.30944, train_acc=0.90451, val_loss=1.89073, val_acc=0.78307, time=0.12801
Epoch:0035, train_loss=1.30288, train_acc=0.91095, val_loss=1.89046, val_acc=0.78836, time=0.11800
Epoch:0036, train_loss=1.29646, train_acc=0.91271, val_loss=1.89024, val_acc=0.78836, time=0.12300
Epoch:0037, train_loss=1.29027, train_acc=0.91916, val_loss=1.89008, val_acc=0.78307, time=0.12500
Epoch:0038, train_loss=1.28437, train_acc=0.92150, val_loss=1.88996, val_acc=0.76720, time=0.12902
Epoch:0039, train_loss=1.27881, train_acc=0.92619, val_loss=1.88988, val_acc=0.76720, time=0.10901
Epoch:0040, train_loss=1.27360, train_acc=0.92794, val_loss=1.88982, val_acc=0.76720, time=0.13101
Epoch:0041, train_loss=1.26871, train_acc=0.93029, val_loss=1.88978, val_acc=0.76190, time=0.12499
Epoch:0042, train_loss=1.26408, train_acc=0.93380, val_loss=1.88972, val_acc=0.76720, time=0.11400
Epoch:0043, train_loss=1.25964, train_acc=0.94025, val_loss=1.88964, val_acc=0.76720, time=0.11600
Epoch:0044, train_loss=1.25535, train_acc=0.94259, val_loss=1.88954, val_acc=0.76190, time=0.13101
Epoch:0045, train_loss=1.25118, train_acc=0.94669, val_loss=1.88942, val_acc=0.76190, time=0.13099
Epoch:0046, train_loss=1.24712, train_acc=0.95138, val_loss=1.88929, val_acc=0.76190, time=0.12800
Epoch:0047, train_loss=1.24319, train_acc=0.95372, val_loss=1.88915, val_acc=0.76190, time=0.12500
Epoch:0048, train_loss=1.23941, train_acc=0.95665, val_loss=1.88902, val_acc=0.76190, time=0.12201
Epoch:0049, train_loss=1.23579, train_acc=0.96016, val_loss=1.88891, val_acc=0.77249, time=0.12800
Epoch:0050, train_loss=1.23235, train_acc=0.96134, val_loss=1.88881, val_acc=0.76720, time=0.12601
Epoch:0051, train_loss=1.22906, train_acc=0.96426, val_loss=1.88875, val_acc=0.76720, time=0.11401
Epoch:0052, train_loss=1.22592, train_acc=0.96719, val_loss=1.88871, val_acc=0.76720, time=0.13301
Epoch:0053, train_loss=1.22289, train_acc=0.96719, val_loss=1.88869, val_acc=0.77249, time=0.12800
Epoch:0054, train_loss=1.21997, train_acc=0.97247, val_loss=1.88870, val_acc=0.77249, time=0.10599
Epoch:0055, train_loss=1.21715, train_acc=0.97305, val_loss=1.88874, val_acc=0.77249, time=0.11801
Epoch:0056, train_loss=1.21442, train_acc=0.97364, val_loss=1.88879, val_acc=0.77249, time=0.11701
Epoch:0057, train_loss=1.21180, train_acc=0.97481, val_loss=1.88885, val_acc=0.77249, time=0.14000
Early stopping...

Optimization Finished!

Test set results: loss= 1.72781, accuracy= 0.71182, time= 0.03100

Test Precision, Recall and F1-Score...
              precision    recall  f1-score   support

           0     0.8254    0.7429    0.7820       140
           1     0.5676    0.4667    0.5122        45
           2     0.7165    0.7521    0.7339       121
           3     0.7000    0.6848    0.6923        92
           4     0.6396    0.6121    0.6256       116
           5     0.8163    0.6154    0.7018        65
           6     0.6912    0.8069    0.7446       233

    accuracy                         0.7118       812
   macro avg     0.7081    0.6687    0.6846       812
weighted avg     0.7149    0.7118    0.7102       812


Macro average Test Precision, Recall and F1-Score...
(0.7080917809821969, 0.6686704382304997, 0.6845983102031858, None)

Micro average Test Precision, Recall and F1-Score...
(0.7118226600985221, 0.7118226600985221, 0.711822660098522, None)

Embeddings:
Word_embeddings:1343
Train_doc_embeddings:1896
Test_doc_embeddings:812
