
==========: 299744857963900
Epoch:0001, train_loss=2.00223, train_acc=0.15173, val_loss=1.93450, val_acc=0.33862, time=0.12102
Epoch:0002, train_loss=1.87895, train_acc=0.30756, val_loss=1.92803, val_acc=0.41270, time=0.11100
Epoch:0003, train_loss=1.80148, train_acc=0.37493, val_loss=1.92068, val_acc=0.49206, time=0.13001
Epoch:0004, train_loss=1.71151, train_acc=0.51552, val_loss=1.91428, val_acc=0.57143, time=0.11200
Epoch:0005, train_loss=1.63226, train_acc=0.64148, val_loss=1.90940, val_acc=0.64021, time=0.09901
Epoch:0006, train_loss=1.57081, train_acc=0.73989, val_loss=1.90565, val_acc=0.65608, time=0.11100
Epoch:0007, train_loss=1.52303, train_acc=0.78207, val_loss=1.90257, val_acc=0.67196, time=0.12502
Epoch:0008, train_loss=1.48341, train_acc=0.79906, val_loss=1.89971, val_acc=0.69312, time=0.12499
Epoch:0009, train_loss=1.44736, train_acc=0.82015, val_loss=1.89711, val_acc=0.71429, time=0.12900
Epoch:0010, train_loss=1.41444, train_acc=0.83597, val_loss=1.89505, val_acc=0.75132, time=0.12501
Epoch:0011, train_loss=1.38627, train_acc=0.85647, val_loss=1.89362, val_acc=0.77249, time=0.13200
Epoch:0012, train_loss=1.36305, train_acc=0.86643, val_loss=1.89270, val_acc=0.77778, time=0.12001
Epoch:0013, train_loss=1.34358, train_acc=0.87229, val_loss=1.89206, val_acc=0.77249, time=0.12701
Epoch:0014, train_loss=1.32656, train_acc=0.87873, val_loss=1.89155, val_acc=0.76190, time=0.12799
Epoch:0015, train_loss=1.31117, train_acc=0.89514, val_loss=1.89106, val_acc=0.76190, time=0.13201
Epoch:0016, train_loss=1.29691, train_acc=0.90334, val_loss=1.89055, val_acc=0.76720, time=0.10700
Epoch:0017, train_loss=1.28356, train_acc=0.91271, val_loss=1.89002, val_acc=0.77778, time=0.13001
Epoch:0018, train_loss=1.27115, train_acc=0.92209, val_loss=1.88954, val_acc=0.77778, time=0.12902
Epoch:0019, train_loss=1.25995, train_acc=0.93087, val_loss=1.88919, val_acc=0.77778, time=0.12901
Epoch:0020, train_loss=1.25011, train_acc=0.93556, val_loss=1.88898, val_acc=0.76720, time=0.11401
Epoch:0021, train_loss=1.24144, train_acc=0.93849, val_loss=1.88888, val_acc=0.76190, time=0.13100
Epoch:0022, train_loss=1.23352, train_acc=0.94318, val_loss=1.88886, val_acc=0.76190, time=0.11301
Epoch:0023, train_loss=1.22596, train_acc=0.94786, val_loss=1.88890, val_acc=0.76720, time=0.11001
Epoch:0024, train_loss=1.21866, train_acc=0.95548, val_loss=1.88898, val_acc=0.76720, time=0.13200
Epoch:0025, train_loss=1.21174, train_acc=0.96134, val_loss=1.88913, val_acc=0.76190, time=0.13300
Epoch:0026, train_loss=1.20539, train_acc=0.96719, val_loss=1.88933, val_acc=0.76720, time=0.12899
Early stopping...

Optimization Finished!

Test set results: loss= 1.72725, accuracy= 0.72906, time= 0.02900

Test Precision, Recall and F1-Score...
              precision    recall  f1-score   support

           0     0.8321    0.7786    0.8044       140
           1     0.6667    0.5778    0.6190        45
           2     0.7016    0.7190    0.7102       121
           3     0.7128    0.7283    0.7204        92
           4     0.6638    0.6638    0.6638       116
           5     0.8077    0.6462    0.7179        65
           6     0.7188    0.7897    0.7526       233

    accuracy                         0.7291       812
   macro avg     0.7290    0.7005    0.7126       812
weighted avg     0.7314    0.7291    0.7287       812


Macro average Test Precision, Recall and F1-Score...
(0.7290488581688794, 0.7004664086849723, 0.7126297015862005, None)

Micro average Test Precision, Recall and F1-Score...
(0.729064039408867, 0.729064039408867, 0.729064039408867, None)

Embeddings:
Word_embeddings:1343
Train_doc_embeddings:1896
Test_doc_embeddings:812
