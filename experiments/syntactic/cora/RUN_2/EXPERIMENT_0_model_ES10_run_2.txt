
==========: 299050215669100
Epoch:0001, train_loss=2.05323, train_acc=0.20855, val_loss=1.93652, val_acc=0.35450, time=0.12200
Epoch:0002, train_loss=1.89116, train_acc=0.30639, val_loss=1.93138, val_acc=0.37566, time=0.12001
Epoch:0003, train_loss=1.81250, train_acc=0.35384, val_loss=1.92741, val_acc=0.39683, time=0.11500
Epoch:0004, train_loss=1.75791, train_acc=0.42824, val_loss=1.92002, val_acc=0.50794, time=0.11500
Epoch:0005, train_loss=1.68221, train_acc=0.55595, val_loss=1.91246, val_acc=0.58201, time=0.12400
Epoch:0006, train_loss=1.60881, train_acc=0.67252, val_loss=1.90692, val_acc=0.62963, time=0.12600
Epoch:0007, train_loss=1.55425, train_acc=0.73521, val_loss=1.90323, val_acc=0.66667, time=0.11601
Epoch:0008, train_loss=1.51514, train_acc=0.76450, val_loss=1.90051, val_acc=0.68783, time=0.12901
Epoch:0009, train_loss=1.48303, train_acc=0.78559, val_loss=1.89824, val_acc=0.70370, time=0.10501
Epoch:0010, train_loss=1.45325, train_acc=0.81019, val_loss=1.89617, val_acc=0.70899, time=0.12700
Epoch:0011, train_loss=1.42448, train_acc=0.83714, val_loss=1.89429, val_acc=0.73545, time=0.11702
Epoch:0012, train_loss=1.39742, train_acc=0.85413, val_loss=1.89268, val_acc=0.74603, time=0.11501
Epoch:0013, train_loss=1.37352, train_acc=0.86760, val_loss=1.89140, val_acc=0.76720, time=0.13000
Epoch:0014, train_loss=1.35359, train_acc=0.87698, val_loss=1.89043, val_acc=0.76720, time=0.12399
Epoch:0015, train_loss=1.33720, train_acc=0.88342, val_loss=1.88968, val_acc=0.76720, time=0.10798
Epoch:0016, train_loss=1.32324, train_acc=0.88459, val_loss=1.88908, val_acc=0.77249, time=0.14799
Epoch:0017, train_loss=1.31053, train_acc=0.89221, val_loss=1.88855, val_acc=0.76720, time=0.12399
Epoch:0018, train_loss=1.29831, train_acc=0.89924, val_loss=1.88806, val_acc=0.76190, time=0.10600
Epoch:0019, train_loss=1.28630, train_acc=0.90627, val_loss=1.88761, val_acc=0.76720, time=0.12200
Epoch:0020, train_loss=1.27462, train_acc=0.91681, val_loss=1.88722, val_acc=0.76720, time=0.12200
Epoch:0021, train_loss=1.26369, train_acc=0.92560, val_loss=1.88695, val_acc=0.77249, time=0.12700
Epoch:0022, train_loss=1.25398, train_acc=0.93322, val_loss=1.88682, val_acc=0.76720, time=0.12999
Epoch:0023, train_loss=1.24563, train_acc=0.93790, val_loss=1.88681, val_acc=0.76720, time=0.13099
Epoch:0024, train_loss=1.23837, train_acc=0.94376, val_loss=1.88684, val_acc=0.77778, time=0.12801
Epoch:0025, train_loss=1.23167, train_acc=0.94786, val_loss=1.88684, val_acc=0.77778, time=0.13101
Epoch:0026, train_loss=1.22507, train_acc=0.95313, val_loss=1.88680, val_acc=0.77778, time=0.11401
Epoch:0027, train_loss=1.21848, train_acc=0.95431, val_loss=1.88677, val_acc=0.76720, time=0.12300
Epoch:0028, train_loss=1.21212, train_acc=0.96075, val_loss=1.88680, val_acc=0.76190, time=0.12600
Epoch:0029, train_loss=1.20626, train_acc=0.96426, val_loss=1.88692, val_acc=0.76190, time=0.10401
Epoch:0030, train_loss=1.20102, train_acc=0.96895, val_loss=1.88713, val_acc=0.76190, time=0.13201
Early stopping...

Optimization Finished!

Test set results: loss= 1.73045, accuracy= 0.73030, time= 0.03801

Test Precision, Recall and F1-Score...
              precision    recall  f1-score   support

           0     0.8425    0.7643    0.8015       140
           1     0.6757    0.5556    0.6098        45
           2     0.7317    0.7438    0.7377       121
           3     0.7010    0.7391    0.7196        92
           4     0.6261    0.6207    0.6234       116
           5     0.8077    0.6462    0.7179        65
           6     0.7241    0.8112    0.7652       233

    accuracy                         0.7303       812
   macro avg     0.7298    0.6973    0.7107       812
weighted avg     0.7331    0.7303    0.7295       812


Macro average Test Precision, Recall and F1-Score...
(0.7298358286959711, 0.6972536653037089, 0.7107204842959236, None)

Micro average Test Precision, Recall and F1-Score...
(0.7302955665024631, 0.7302955665024631, 0.7302955665024631, None)

Embeddings:
Word_embeddings:1343
Train_doc_embeddings:1896
Test_doc_embeddings:812
