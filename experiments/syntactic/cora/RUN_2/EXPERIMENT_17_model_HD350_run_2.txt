
==========: 299834492968600
Epoch:0001, train_loss=2.07325, train_acc=0.19508, val_loss=1.93985, val_acc=0.31746, time=0.12300
Epoch:0002, train_loss=1.90230, train_acc=0.29291, val_loss=1.93149, val_acc=0.39153, time=0.12701
Epoch:0003, train_loss=1.82803, train_acc=0.40129, val_loss=1.92480, val_acc=0.48677, time=0.12602
Epoch:0004, train_loss=1.76362, train_acc=0.49619, val_loss=1.91906, val_acc=0.57672, time=0.10601
Epoch:0005, train_loss=1.69990, train_acc=0.57528, val_loss=1.91374, val_acc=0.62434, time=0.13001
Epoch:0006, train_loss=1.63556, train_acc=0.65847, val_loss=1.90911, val_acc=0.65608, time=0.12800
Epoch:0007, train_loss=1.57631, train_acc=0.71763, val_loss=1.90528, val_acc=0.68254, time=0.11100
Epoch:0008, train_loss=1.52464, train_acc=0.75161, val_loss=1.90211, val_acc=0.70370, time=0.10900
Epoch:0009, train_loss=1.47989, train_acc=0.79438, val_loss=1.89960, val_acc=0.70370, time=0.12901
Epoch:0010, train_loss=1.44232, train_acc=0.82074, val_loss=1.89777, val_acc=0.70370, time=0.13101
Epoch:0011, train_loss=1.41272, train_acc=0.84534, val_loss=1.89654, val_acc=0.71958, time=0.12100
Epoch:0012, train_loss=1.39029, train_acc=0.85179, val_loss=1.89565, val_acc=0.70899, time=0.12500
Epoch:0013, train_loss=1.37223, train_acc=0.85647, val_loss=1.89475, val_acc=0.73016, time=0.11600
Epoch:0014, train_loss=1.35526, train_acc=0.86760, val_loss=1.89367, val_acc=0.75132, time=0.12101
Epoch:0015, train_loss=1.33764, train_acc=0.87639, val_loss=1.89249, val_acc=0.76720, time=0.11899
Epoch:0016, train_loss=1.31980, train_acc=0.88928, val_loss=1.89143, val_acc=0.77249, time=0.13701
Epoch:0017, train_loss=1.30314, train_acc=0.89865, val_loss=1.89064, val_acc=0.76720, time=0.13300
Epoch:0018, train_loss=1.28862, train_acc=0.90685, val_loss=1.89017, val_acc=0.76190, time=0.12400
Epoch:0019, train_loss=1.27638, train_acc=0.91447, val_loss=1.88998, val_acc=0.76190, time=0.11400
Epoch:0020, train_loss=1.26603, train_acc=0.92150, val_loss=1.88997, val_acc=0.74603, time=0.13100
Epoch:0021, train_loss=1.25694, train_acc=0.92794, val_loss=1.89006, val_acc=0.73545, time=0.11801
Epoch:0022, train_loss=1.24853, train_acc=0.93204, val_loss=1.89017, val_acc=0.74074, time=0.13201
Epoch:0023, train_loss=1.24037, train_acc=0.93907, val_loss=1.89024, val_acc=0.74074, time=0.13202
Epoch:0024, train_loss=1.23230, train_acc=0.94493, val_loss=1.89029, val_acc=0.74603, time=0.12201
Epoch:0025, train_loss=1.22436, train_acc=0.94903, val_loss=1.89033, val_acc=0.75132, time=0.12701
Epoch:0026, train_loss=1.21677, train_acc=0.95606, val_loss=1.89043, val_acc=0.75661, time=0.13201
Early stopping...

Optimization Finished!

Test set results: loss= 1.72924, accuracy= 0.70567, time= 0.03802

Test Precision, Recall and F1-Score...
              precision    recall  f1-score   support

           0     0.7895    0.7500    0.7692       140
           1     0.5227    0.5111    0.5169        45
           2     0.7176    0.7769    0.7460       121
           3     0.7303    0.7065    0.7182        92
           4     0.5897    0.5948    0.5923       116
           5     0.7500    0.5538    0.6372        65
           6     0.7240    0.7768    0.7495       233

    accuracy                         0.7057       812
   macro avg     0.6891    0.6671    0.6756       812
weighted avg     0.7068    0.7057    0.7045       812


Macro average Test Precision, Recall and F1-Score...
(0.6891198396059244, 0.6671414469659417, 0.6756105304866432, None)

Micro average Test Precision, Recall and F1-Score...
(0.7056650246305419, 0.7056650246305419, 0.7056650246305419, None)

Embeddings:
Word_embeddings:1343
Train_doc_embeddings:1896
Test_doc_embeddings:812
