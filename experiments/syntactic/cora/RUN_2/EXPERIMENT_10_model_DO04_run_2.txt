
==========: 299764429438400
Epoch:0001, train_loss=2.07354, train_acc=0.12009, val_loss=1.94131, val_acc=0.32804, time=0.12600
Epoch:0002, train_loss=1.90521, train_acc=0.31576, val_loss=1.93444, val_acc=0.35979, time=0.11501
Epoch:0003, train_loss=1.82424, train_acc=0.37610, val_loss=1.92796, val_acc=0.44444, time=0.13199
Epoch:0004, train_loss=1.74782, train_acc=0.48213, val_loss=1.92225, val_acc=0.51852, time=0.13000
Epoch:0005, train_loss=1.68450, train_acc=0.59754, val_loss=1.91691, val_acc=0.58730, time=0.10399
Epoch:0006, train_loss=1.62754, train_acc=0.66257, val_loss=1.91150, val_acc=0.61376, time=0.12901
Epoch:0007, train_loss=1.57064, train_acc=0.71353, val_loss=1.90666, val_acc=0.66138, time=0.12699
Epoch:0008, train_loss=1.51861, train_acc=0.76684, val_loss=1.90302, val_acc=0.67725, time=0.11003
Epoch:0009, train_loss=1.47689, train_acc=0.79731, val_loss=1.90051, val_acc=0.68254, time=0.12399
Epoch:0010, train_loss=1.44490, train_acc=0.81839, val_loss=1.89866, val_acc=0.68783, time=0.13000
Epoch:0011, train_loss=1.41899, train_acc=0.83480, val_loss=1.89709, val_acc=0.70899, time=0.12903
Epoch:0012, train_loss=1.39646, train_acc=0.84651, val_loss=1.89567, val_acc=0.72487, time=0.13001
Epoch:0013, train_loss=1.37612, train_acc=0.85940, val_loss=1.89432, val_acc=0.73545, time=0.12100
Epoch:0014, train_loss=1.35728, train_acc=0.86878, val_loss=1.89304, val_acc=0.75132, time=0.12199
Epoch:0015, train_loss=1.33956, train_acc=0.88225, val_loss=1.89188, val_acc=0.74603, time=0.11801
Epoch:0016, train_loss=1.32299, train_acc=0.88576, val_loss=1.89091, val_acc=0.75661, time=0.12300
Epoch:0017, train_loss=1.30783, train_acc=0.89748, val_loss=1.89018, val_acc=0.77778, time=0.12203
Epoch:0018, train_loss=1.29437, train_acc=0.90158, val_loss=1.88969, val_acc=0.76720, time=0.11599
Epoch:0019, train_loss=1.28262, train_acc=0.91037, val_loss=1.88940, val_acc=0.75132, time=0.12499
Epoch:0020, train_loss=1.27235, train_acc=0.91623, val_loss=1.88923, val_acc=0.74603, time=0.13401
Epoch:0021, train_loss=1.26318, train_acc=0.92736, val_loss=1.88914, val_acc=0.75132, time=0.12900
Epoch:0022, train_loss=1.25475, train_acc=0.93204, val_loss=1.88908, val_acc=0.74603, time=0.12700
Epoch:0023, train_loss=1.24677, train_acc=0.93673, val_loss=1.88903, val_acc=0.74603, time=0.11301
Epoch:0024, train_loss=1.23907, train_acc=0.94552, val_loss=1.88899, val_acc=0.75132, time=0.16000
Epoch:0025, train_loss=1.23157, train_acc=0.94903, val_loss=1.88898, val_acc=0.74603, time=0.19000
Epoch:0026, train_loss=1.22433, train_acc=0.95431, val_loss=1.88903, val_acc=0.74603, time=0.11701
Epoch:0027, train_loss=1.21755, train_acc=0.95958, val_loss=1.88917, val_acc=0.74074, time=0.13200
Epoch:0028, train_loss=1.21143, train_acc=0.96251, val_loss=1.88940, val_acc=0.74074, time=0.10999
Early stopping...

Optimization Finished!

Test set results: loss= 1.72701, accuracy= 0.73768, time= 0.03701

Test Precision, Recall and F1-Score...
              precision    recall  f1-score   support

           0     0.7986    0.7929    0.7957       140
           1     0.7179    0.6222    0.6667        45
           2     0.7521    0.7521    0.7521       121
           3     0.7128    0.7283    0.7204        92
           4     0.6466    0.6466    0.6466       116
           5     0.8200    0.6308    0.7130        65
           6     0.7352    0.7983    0.7654       233

    accuracy                         0.7377       812
   macro avg     0.7404    0.7101    0.7228       812
weighted avg     0.7393    0.7377    0.7369       812


Macro average Test Precision, Recall and F1-Score...
(0.740438790275389, 0.7101443667223998, 0.7228413022559205, None)

Micro average Test Precision, Recall and F1-Score...
(0.7376847290640394, 0.7376847290640394, 0.7376847290640394, None)

Embeddings:
Word_embeddings:1343
Train_doc_embeddings:1896
Test_doc_embeddings:812
