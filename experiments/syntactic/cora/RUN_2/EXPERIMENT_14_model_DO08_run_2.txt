
==========: 299804814008600
Epoch:0001, train_loss=2.02209, train_acc=0.13357, val_loss=1.93705, val_acc=0.33862, time=0.11400
Epoch:0002, train_loss=1.87800, train_acc=0.32220, val_loss=1.93120, val_acc=0.40212, time=0.12301
Epoch:0003, train_loss=1.80523, train_acc=0.38957, val_loss=1.92571, val_acc=0.43915, time=0.12000
Epoch:0004, train_loss=1.73303, train_acc=0.47217, val_loss=1.91892, val_acc=0.48677, time=0.11601
Epoch:0005, train_loss=1.65547, train_acc=0.58407, val_loss=1.91255, val_acc=0.60317, time=0.12399
Epoch:0006, train_loss=1.58572, train_acc=0.69127, val_loss=1.90777, val_acc=0.62963, time=0.11700
Epoch:0007, train_loss=1.53205, train_acc=0.77153, val_loss=1.90452, val_acc=0.67725, time=0.12801
Epoch:0008, train_loss=1.49255, train_acc=0.80141, val_loss=1.90214, val_acc=0.70370, time=0.12801
Epoch:0009, train_loss=1.46088, train_acc=0.81839, val_loss=1.90002, val_acc=0.72487, time=0.13001
Epoch:0010, train_loss=1.43128, train_acc=0.82601, val_loss=1.89795, val_acc=0.73545, time=0.11701
Epoch:0011, train_loss=1.40206, train_acc=0.84710, val_loss=1.89611, val_acc=0.72487, time=0.11201
Epoch:0012, train_loss=1.37499, train_acc=0.86292, val_loss=1.89472, val_acc=0.73545, time=0.11800
Epoch:0013, train_loss=1.35229, train_acc=0.87815, val_loss=1.89381, val_acc=0.71958, time=0.12301
Epoch:0014, train_loss=1.33433, train_acc=0.88635, val_loss=1.89323, val_acc=0.71429, time=0.12500
Epoch:0015, train_loss=1.31974, train_acc=0.88694, val_loss=1.89276, val_acc=0.71958, time=0.12501
Epoch:0016, train_loss=1.30666, train_acc=0.89162, val_loss=1.89227, val_acc=0.71958, time=0.12900
Epoch:0017, train_loss=1.29384, train_acc=0.89865, val_loss=1.89174, val_acc=0.73016, time=0.12600
Epoch:0018, train_loss=1.28107, train_acc=0.90803, val_loss=1.89123, val_acc=0.74074, time=0.12800
Epoch:0019, train_loss=1.26879, train_acc=0.91857, val_loss=1.89081, val_acc=0.75132, time=0.12401
Epoch:0020, train_loss=1.25762, train_acc=0.93146, val_loss=1.89054, val_acc=0.75132, time=0.12001
Epoch:0021, train_loss=1.24793, train_acc=0.93790, val_loss=1.89042, val_acc=0.75661, time=0.12100
Epoch:0022, train_loss=1.23964, train_acc=0.94376, val_loss=1.89043, val_acc=0.75661, time=0.12901
Epoch:0023, train_loss=1.23228, train_acc=0.94786, val_loss=1.89049, val_acc=0.75661, time=0.12601
Epoch:0024, train_loss=1.22530, train_acc=0.95196, val_loss=1.89058, val_acc=0.75661, time=0.13001
Epoch:0025, train_loss=1.21841, train_acc=0.95665, val_loss=1.89069, val_acc=0.75132, time=0.13100
Epoch:0026, train_loss=1.21164, train_acc=0.96134, val_loss=1.89084, val_acc=0.74603, time=0.13000
Epoch:0027, train_loss=1.20528, train_acc=0.96485, val_loss=1.89106, val_acc=0.74603, time=0.12502
Early stopping...

Optimization Finished!

Test set results: loss= 1.72812, accuracy= 0.72414, time= 0.02800

Test Precision, Recall and F1-Score...
              precision    recall  f1-score   support

           0     0.8140    0.7500    0.7807       140
           1     0.5909    0.5778    0.5843        45
           2     0.6929    0.7273    0.7097       121
           3     0.7263    0.7500    0.7380        92
           4     0.6491    0.6379    0.6435       116
           5     0.8431    0.6615    0.7414        65
           6     0.7262    0.7854    0.7546       233

    accuracy                         0.7241       812
   macro avg     0.7204    0.6986    0.7074       812
weighted avg     0.7272    0.7241    0.7242       812


Macro average Test Precision, Recall and F1-Score...
(0.7203631846702315, 0.6985611037705163, 0.7074401268811752, None)

Micro average Test Precision, Recall and F1-Score...
(0.7241379310344828, 0.7241379310344828, 0.7241379310344829, None)

Embeddings:
Word_embeddings:1343
Train_doc_embeddings:1896
Test_doc_embeddings:812
