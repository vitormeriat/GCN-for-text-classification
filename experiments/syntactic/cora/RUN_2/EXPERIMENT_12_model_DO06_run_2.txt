
==========: 299784679227400
Epoch:0001, train_loss=2.04357, train_acc=0.12244, val_loss=1.93716, val_acc=0.32275, time=0.12598
Epoch:0002, train_loss=1.87668, train_acc=0.31693, val_loss=1.93106, val_acc=0.36508, time=0.10401
Epoch:0003, train_loss=1.82913, train_acc=0.35149, val_loss=1.92435, val_acc=0.42328, time=0.12200
Epoch:0004, train_loss=1.75445, train_acc=0.42121, val_loss=1.91839, val_acc=0.53439, time=0.11200
Epoch:0005, train_loss=1.68169, train_acc=0.58407, val_loss=1.91402, val_acc=0.62963, time=0.12301
Epoch:0006, train_loss=1.62499, train_acc=0.69127, val_loss=1.91002, val_acc=0.68254, time=0.11299
Epoch:0007, train_loss=1.57495, train_acc=0.74165, val_loss=1.90601, val_acc=0.69312, time=0.13200
Epoch:0008, train_loss=1.52742, train_acc=0.76919, val_loss=1.90241, val_acc=0.70370, time=0.12401
Epoch:0009, train_loss=1.48409, train_acc=0.80141, val_loss=1.89972, val_acc=0.73545, time=0.12001
Epoch:0010, train_loss=1.44868, train_acc=0.82660, val_loss=1.89806, val_acc=0.73545, time=0.12799
Epoch:0011, train_loss=1.42215, train_acc=0.83538, val_loss=1.89701, val_acc=0.72487, time=0.13502
Epoch:0012, train_loss=1.40151, train_acc=0.84827, val_loss=1.89605, val_acc=0.72487, time=0.13097
Epoch:0013, train_loss=1.38271, train_acc=0.85706, val_loss=1.89493, val_acc=0.72487, time=0.11700
Epoch:0014, train_loss=1.36379, train_acc=0.86526, val_loss=1.89370, val_acc=0.73016, time=0.12100
Epoch:0015, train_loss=1.34508, train_acc=0.87698, val_loss=1.89249, val_acc=0.73016, time=0.11101
Epoch:0016, train_loss=1.32768, train_acc=0.88752, val_loss=1.89141, val_acc=0.73016, time=0.12700
Epoch:0017, train_loss=1.31234, train_acc=0.89748, val_loss=1.89052, val_acc=0.73016, time=0.11600
Epoch:0018, train_loss=1.29926, train_acc=0.90334, val_loss=1.88983, val_acc=0.73016, time=0.12400
Epoch:0019, train_loss=1.28822, train_acc=0.91037, val_loss=1.88934, val_acc=0.73545, time=0.12499
Epoch:0020, train_loss=1.27861, train_acc=0.91681, val_loss=1.88902, val_acc=0.74603, time=0.10400
Epoch:0021, train_loss=1.26962, train_acc=0.92443, val_loss=1.88882, val_acc=0.74603, time=0.11402
Epoch:0022, train_loss=1.26069, train_acc=0.92912, val_loss=1.88875, val_acc=0.75132, time=0.13003
Epoch:0023, train_loss=1.25179, train_acc=0.93146, val_loss=1.88881, val_acc=0.75661, time=0.17098
Epoch:0024, train_loss=1.24326, train_acc=0.93615, val_loss=1.88899, val_acc=0.75661, time=0.12500
Epoch:0025, train_loss=1.23542, train_acc=0.94259, val_loss=1.88926, val_acc=0.75132, time=0.10998
Epoch:0026, train_loss=1.22836, train_acc=0.94962, val_loss=1.88957, val_acc=0.74603, time=0.11800
Early stopping...

Optimization Finished!

Test set results: loss= 1.72577, accuracy= 0.72783, time= 0.03101

Test Precision, Recall and F1-Score...
              precision    recall  f1-score   support

           0     0.8209    0.7857    0.8029       140
           1     0.6944    0.5556    0.6173        45
           2     0.7438    0.7438    0.7438       121
           3     0.6771    0.7065    0.6915        92
           4     0.6765    0.5948    0.6330       116
           5     0.8750    0.6462    0.7434        65
           6     0.6909    0.8155    0.7480       233

    accuracy                         0.7278       812
   macro avg     0.7398    0.6926    0.7114       812
weighted avg     0.7325    0.7278    0.7264       812


Macro average Test Precision, Recall and F1-Score...
(0.7398006617432549, 0.6925750442043437, 0.7114166462997643, None)

Micro average Test Precision, Recall and F1-Score...
(0.7278325123152709, 0.7278325123152709, 0.7278325123152709, None)

Embeddings:
Word_embeddings:1343
Train_doc_embeddings:1896
Test_doc_embeddings:812
