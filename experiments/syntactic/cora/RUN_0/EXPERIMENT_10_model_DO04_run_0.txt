
==========: 298044106637200
Epoch:0001, train_loss=2.25998, train_acc=0.11131, val_loss=1.95620, val_acc=0.15873, time=0.13201
Epoch:0002, train_loss=1.99120, train_acc=0.18102, val_loss=1.94054, val_acc=0.25926, time=0.13099
Epoch:0003, train_loss=1.85876, train_acc=0.31693, val_loss=1.93554, val_acc=0.36508, time=0.13100
Epoch:0004, train_loss=1.82340, train_acc=0.34564, val_loss=1.93180, val_acc=0.39683, time=0.12500
Epoch:0005, train_loss=1.78468, train_acc=0.36087, val_loss=1.92534, val_acc=0.42328, time=0.11801
Epoch:0006, train_loss=1.71413, train_acc=0.43468, val_loss=1.91908, val_acc=0.48677, time=0.13000
Epoch:0007, train_loss=1.64473, train_acc=0.56591, val_loss=1.91462, val_acc=0.55026, time=0.12600
Epoch:0008, train_loss=1.59244, train_acc=0.67838, val_loss=1.91123, val_acc=0.60847, time=0.12201
Epoch:0009, train_loss=1.55147, train_acc=0.73286, val_loss=1.90803, val_acc=0.65079, time=0.12200
Epoch:0010, train_loss=1.51451, train_acc=0.76977, val_loss=1.90481, val_acc=0.67725, time=0.12800
Epoch:0011, train_loss=1.47979, train_acc=0.81136, val_loss=1.90190, val_acc=0.69841, time=0.12999
Epoch:0012, train_loss=1.44935, train_acc=0.83714, val_loss=1.89969, val_acc=0.68254, time=0.10600
Epoch:0013, train_loss=1.42536, train_acc=0.84944, val_loss=1.89817, val_acc=0.67725, time=0.12200
Epoch:0014, train_loss=1.40728, train_acc=0.84593, val_loss=1.89703, val_acc=0.69841, time=0.13100
Epoch:0015, train_loss=1.39186, train_acc=0.84417, val_loss=1.89586, val_acc=0.69312, time=0.11401
Epoch:0016, train_loss=1.37585, train_acc=0.84886, val_loss=1.89457, val_acc=0.70899, time=0.13399
Epoch:0017, train_loss=1.35835, train_acc=0.86175, val_loss=1.89330, val_acc=0.69841, time=0.13000
Epoch:0018, train_loss=1.34070, train_acc=0.87170, val_loss=1.89227, val_acc=0.72487, time=0.12900
Epoch:0019, train_loss=1.32478, train_acc=0.87991, val_loss=1.89162, val_acc=0.74074, time=0.11500
Epoch:0020, train_loss=1.31165, train_acc=0.88987, val_loss=1.89134, val_acc=0.75132, time=0.12002
Epoch:0021, train_loss=1.30121, train_acc=0.89924, val_loss=1.89131, val_acc=0.75132, time=0.10201
Epoch:0022, train_loss=1.29254, train_acc=0.90627, val_loss=1.89138, val_acc=0.75132, time=0.11498
Epoch:0023, train_loss=1.28461, train_acc=0.91095, val_loss=1.89143, val_acc=0.74603, time=0.13101
Epoch:0024, train_loss=1.27670, train_acc=0.91623, val_loss=1.89139, val_acc=0.74603, time=0.11600
Epoch:0025, train_loss=1.26861, train_acc=0.92267, val_loss=1.89128, val_acc=0.74074, time=0.13011
Epoch:0026, train_loss=1.26048, train_acc=0.92736, val_loss=1.89112, val_acc=0.73016, time=0.12100
Epoch:0027, train_loss=1.25259, train_acc=0.93497, val_loss=1.89093, val_acc=0.73016, time=0.13100
Epoch:0028, train_loss=1.24518, train_acc=0.94025, val_loss=1.89073, val_acc=0.73545, time=0.13101
Epoch:0029, train_loss=1.23836, train_acc=0.94318, val_loss=1.89056, val_acc=0.73545, time=0.12599
Epoch:0030, train_loss=1.23216, train_acc=0.94786, val_loss=1.89043, val_acc=0.73545, time=0.13200
Epoch:0031, train_loss=1.22652, train_acc=0.95431, val_loss=1.89035, val_acc=0.73016, time=0.11602
Epoch:0032, train_loss=1.22136, train_acc=0.95665, val_loss=1.89034, val_acc=0.74074, time=0.10000
Epoch:0033, train_loss=1.21661, train_acc=0.95782, val_loss=1.89040, val_acc=0.75132, time=0.12501
Epoch:0034, train_loss=1.21217, train_acc=0.96192, val_loss=1.89052, val_acc=0.76190, time=0.11599
Epoch:0035, train_loss=1.20795, train_acc=0.96426, val_loss=1.89069, val_acc=0.77249, time=0.12200
Early stopping...

Optimization Finished!

Test set results: loss= 1.72942, accuracy= 0.72906, time= 0.03401

Test Precision, Recall and F1-Score...
              precision    recall  f1-score   support

           0     0.8154    0.7571    0.7852       140
           1     0.6471    0.4889    0.5570        45
           2     0.7222    0.7521    0.7368       121
           3     0.6857    0.7826    0.7310        92
           4     0.6207    0.6207    0.6207       116
           5     0.8776    0.6615    0.7544        65
           6     0.7381    0.7983    0.7670       233

    accuracy                         0.7291       812
   macro avg     0.7295    0.6945    0.7074       812
weighted avg     0.7325    0.7291    0.7280       812


Macro average Test Precision, Recall and F1-Score...
(0.7295308372180501, 0.6944597051285498, 0.7074342445904171, None)

Micro average Test Precision, Recall and F1-Score...
(0.729064039408867, 0.729064039408867, 0.729064039408867, None)

Embeddings:
Word_embeddings:1343
Train_doc_embeddings:1896
Test_doc_embeddings:812
