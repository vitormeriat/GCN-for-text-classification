
==========: 298153296142900
Epoch:0001, train_loss=2.09773, train_acc=0.10486, val_loss=1.93885, val_acc=0.27513, time=0.13199
Epoch:0002, train_loss=1.88984, train_acc=0.27944, val_loss=1.93090, val_acc=0.37037, time=0.13100
Epoch:0003, train_loss=1.83268, train_acc=0.34153, val_loss=1.92530, val_acc=0.39683, time=0.11800
Epoch:0004, train_loss=1.77363, train_acc=0.39016, val_loss=1.91859, val_acc=0.47619, time=0.11999
Epoch:0005, train_loss=1.69568, train_acc=0.51494, val_loss=1.91303, val_acc=0.58730, time=0.12299
Epoch:0006, train_loss=1.62805, train_acc=0.61921, val_loss=1.90882, val_acc=0.64021, time=0.13200
Epoch:0007, train_loss=1.57457, train_acc=0.70885, val_loss=1.90563, val_acc=0.69841, time=0.12500
Epoch:0008, train_loss=1.53147, train_acc=0.75161, val_loss=1.90301, val_acc=0.71429, time=0.12400
Epoch:0009, train_loss=1.49406, train_acc=0.78559, val_loss=1.90068, val_acc=0.73016, time=0.12200
Epoch:0010, train_loss=1.46004, train_acc=0.81722, val_loss=1.89869, val_acc=0.71958, time=0.12201
Epoch:0011, train_loss=1.42985, train_acc=0.83948, val_loss=1.89715, val_acc=0.73545, time=0.11900
Epoch:0012, train_loss=1.40429, train_acc=0.84886, val_loss=1.89599, val_acc=0.73545, time=0.12998
Epoch:0013, train_loss=1.38291, train_acc=0.85589, val_loss=1.89504, val_acc=0.71429, time=0.12500
Epoch:0014, train_loss=1.36427, train_acc=0.86292, val_loss=1.89415, val_acc=0.70370, time=0.13000
Epoch:0015, train_loss=1.34727, train_acc=0.87170, val_loss=1.89327, val_acc=0.70370, time=0.11500
Epoch:0016, train_loss=1.33154, train_acc=0.87991, val_loss=1.89244, val_acc=0.73545, time=0.12901
Epoch:0017, train_loss=1.31699, train_acc=0.88811, val_loss=1.89168, val_acc=0.74603, time=0.13101
Epoch:0018, train_loss=1.30349, train_acc=0.89338, val_loss=1.89102, val_acc=0.76190, time=0.12301
Epoch:0019, train_loss=1.29099, train_acc=0.89982, val_loss=1.89052, val_acc=0.76720, time=0.11800
Epoch:0020, train_loss=1.27960, train_acc=0.90978, val_loss=1.89019, val_acc=0.76720, time=0.11801
Epoch:0021, train_loss=1.26947, train_acc=0.91798, val_loss=1.89005, val_acc=0.75661, time=0.13401
Epoch:0022, train_loss=1.26058, train_acc=0.92501, val_loss=1.89004, val_acc=0.75661, time=0.11101
Epoch:0023, train_loss=1.25265, train_acc=0.92912, val_loss=1.89009, val_acc=0.75661, time=0.13301
Epoch:0024, train_loss=1.24522, train_acc=0.93322, val_loss=1.89015, val_acc=0.75661, time=0.13102
Epoch:0025, train_loss=1.23796, train_acc=0.94025, val_loss=1.89019, val_acc=0.75132, time=0.13200
Epoch:0026, train_loss=1.23078, train_acc=0.94318, val_loss=1.89023, val_acc=0.75661, time=0.12301
Epoch:0027, train_loss=1.22384, train_acc=0.95021, val_loss=1.89029, val_acc=0.75661, time=0.11701
Epoch:0028, train_loss=1.21737, train_acc=0.95723, val_loss=1.89041, val_acc=0.75661, time=0.10101
Early stopping...

Optimization Finished!

Test set results: loss= 1.72648, accuracy= 0.71798, time= 0.03700

Test Precision, Recall and F1-Score...
              precision    recall  f1-score   support

           0     0.8106    0.7643    0.7868       140
           1     0.6000    0.5333    0.5647        45
           2     0.7049    0.7107    0.7078       121
           3     0.7283    0.7283    0.7283        92
           4     0.6000    0.6207    0.6102       116
           5     0.8571    0.6462    0.7368        65
           6     0.7198    0.7940    0.7551       233

    accuracy                         0.7180       812
   macro avg     0.7173    0.6854    0.6985       812
weighted avg     0.7215    0.7180    0.7178       812


Macro average Test Precision, Recall and F1-Score...
(0.7172531682968106, 0.6853512337817758, 0.6985234322066531, None)

Micro average Test Precision, Recall and F1-Score...
(0.7179802955665024, 0.7179802955665024, 0.7179802955665024, None)

Embeddings:
Word_embeddings:1343
Train_doc_embeddings:1896
Test_doc_embeddings:812
