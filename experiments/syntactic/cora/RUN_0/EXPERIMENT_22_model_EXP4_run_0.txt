
==========: 298163019432900
Epoch:0001, train_loss=1.98254, train_acc=0.27827, val_loss=1.93421, val_acc=0.34921, time=0.13200
Epoch:0002, train_loss=1.85268, train_acc=0.33802, val_loss=1.92695, val_acc=0.49735, time=0.11699
Epoch:0003, train_loss=1.77527, train_acc=0.46514, val_loss=1.91955, val_acc=0.57143, time=0.13202
Epoch:0004, train_loss=1.69987, train_acc=0.54247, val_loss=1.91298, val_acc=0.59788, time=0.13100
Epoch:0005, train_loss=1.62761, train_acc=0.63035, val_loss=1.90825, val_acc=0.61905, time=0.12800
Epoch:0006, train_loss=1.56851, train_acc=0.67604, val_loss=1.90482, val_acc=0.65079, time=0.12501
Epoch:0007, train_loss=1.52073, train_acc=0.74927, val_loss=1.90221, val_acc=0.68254, time=0.12400
Epoch:0008, train_loss=1.48196, train_acc=0.79262, val_loss=1.89994, val_acc=0.70899, time=0.12000
Epoch:0009, train_loss=1.44852, train_acc=0.82250, val_loss=1.89768, val_acc=0.72487, time=0.12300
Epoch:0010, train_loss=1.41760, train_acc=0.84710, val_loss=1.89554, val_acc=0.72487, time=0.11401
Epoch:0011, train_loss=1.38975, train_acc=0.86292, val_loss=1.89375, val_acc=0.74074, time=0.13001
Epoch:0012, train_loss=1.36628, train_acc=0.87815, val_loss=1.89240, val_acc=0.74603, time=0.12001
Epoch:0013, train_loss=1.34702, train_acc=0.88049, val_loss=1.89145, val_acc=0.75132, time=0.13000
Epoch:0014, train_loss=1.33054, train_acc=0.88459, val_loss=1.89078, val_acc=0.76720, time=0.13201
Epoch:0015, train_loss=1.31542, train_acc=0.89104, val_loss=1.89033, val_acc=0.76190, time=0.13500
Epoch:0016, train_loss=1.30103, train_acc=0.89924, val_loss=1.89007, val_acc=0.76190, time=0.12400
Epoch:0017, train_loss=1.28758, train_acc=0.91154, val_loss=1.89000, val_acc=0.75661, time=0.12500
Epoch:0018, train_loss=1.27550, train_acc=0.91916, val_loss=1.89005, val_acc=0.74603, time=0.12400
Epoch:0019, train_loss=1.26489, train_acc=0.92794, val_loss=1.89015, val_acc=0.75132, time=0.12201
Epoch:0020, train_loss=1.25539, train_acc=0.93497, val_loss=1.89021, val_acc=0.74603, time=0.13000
Epoch:0021, train_loss=1.24654, train_acc=0.93966, val_loss=1.89021, val_acc=0.74074, time=0.12001
Epoch:0022, train_loss=1.23815, train_acc=0.94552, val_loss=1.89019, val_acc=0.75132, time=0.11899
Epoch:0023, train_loss=1.23023, train_acc=0.94786, val_loss=1.89019, val_acc=0.74603, time=0.13000
Epoch:0024, train_loss=1.22290, train_acc=0.95079, val_loss=1.89023, val_acc=0.75132, time=0.13399
Early stopping...

Optimization Finished!

Test set results: loss= 1.72529, accuracy= 0.72167, time= 0.03500

Test Precision, Recall and F1-Score...
              precision    recall  f1-score   support

           0     0.7899    0.7786    0.7842       140
           1     0.6579    0.5556    0.6024        45
           2     0.7154    0.7273    0.7213       121
           3     0.7216    0.7609    0.7407        92
           4     0.6364    0.6034    0.6195       116
           5     0.7593    0.6308    0.6891        65
           6     0.7262    0.7854    0.7546       233

    accuracy                         0.7217       812
   macro avg     0.7152    0.6917    0.7017       812
weighted avg     0.7211    0.7217    0.7202       812


Macro average Test Precision, Recall and F1-Score...
(0.7152371171609817, 0.6916992155100415, 0.7016883355191147, None)

Micro average Test Precision, Recall and F1-Score...
(0.7216748768472906, 0.7216748768472906, 0.7216748768472906, None)

Embeddings:
Word_embeddings:1343
Train_doc_embeddings:1896
Test_doc_embeddings:812
