
==========: 298064983286500
Epoch:0001, train_loss=2.07809, train_acc=0.12888, val_loss=1.93598, val_acc=0.36508, time=0.13101
Epoch:0002, train_loss=1.89597, train_acc=0.30580, val_loss=1.92903, val_acc=0.39683, time=0.10803
Epoch:0003, train_loss=1.83283, train_acc=0.35677, val_loss=1.92141, val_acc=0.47619, time=0.10800
Epoch:0004, train_loss=1.75649, train_acc=0.44991, val_loss=1.91471, val_acc=0.56085, time=0.11903
Epoch:0005, train_loss=1.68317, train_acc=0.55243, val_loss=1.90922, val_acc=0.66138, time=0.12402
Epoch:0006, train_loss=1.61686, train_acc=0.66022, val_loss=1.90522, val_acc=0.70370, time=0.12400
Epoch:0007, train_loss=1.56279, train_acc=0.73404, val_loss=1.90247, val_acc=0.73016, time=0.12801
Epoch:0008, train_loss=1.51987, train_acc=0.77270, val_loss=1.90045, val_acc=0.75661, time=0.14900
Epoch:0009, train_loss=1.48413, train_acc=0.79906, val_loss=1.89887, val_acc=0.75132, time=0.13400
Epoch:0010, train_loss=1.45360, train_acc=0.81957, val_loss=1.89753, val_acc=0.72487, time=0.14600
Epoch:0011, train_loss=1.42715, train_acc=0.82660, val_loss=1.89623, val_acc=0.71958, time=0.10899
Epoch:0012, train_loss=1.40341, train_acc=0.83421, val_loss=1.89486, val_acc=0.72487, time=0.13101
Epoch:0013, train_loss=1.38141, train_acc=0.84359, val_loss=1.89345, val_acc=0.74603, time=0.13601
Epoch:0014, train_loss=1.36110, train_acc=0.85706, val_loss=1.89210, val_acc=0.74603, time=0.13100
Epoch:0015, train_loss=1.34295, train_acc=0.87522, val_loss=1.89095, val_acc=0.74603, time=0.11301
Epoch:0016, train_loss=1.32729, train_acc=0.88459, val_loss=1.89004, val_acc=0.77249, time=0.13200
Epoch:0017, train_loss=1.31395, train_acc=0.89514, val_loss=1.88935, val_acc=0.77778, time=0.12500
Epoch:0018, train_loss=1.30235, train_acc=0.89982, val_loss=1.88884, val_acc=0.77778, time=0.11899
Epoch:0019, train_loss=1.29174, train_acc=0.90334, val_loss=1.88845, val_acc=0.77249, time=0.12902
Epoch:0020, train_loss=1.28147, train_acc=0.90920, val_loss=1.88819, val_acc=0.77778, time=0.12900
Epoch:0021, train_loss=1.27137, train_acc=0.91974, val_loss=1.88807, val_acc=0.77778, time=0.13501
Epoch:0022, train_loss=1.26164, train_acc=0.92619, val_loss=1.88811, val_acc=0.76720, time=0.11600
Epoch:0023, train_loss=1.25260, train_acc=0.93029, val_loss=1.88829, val_acc=0.77249, time=0.10300
Epoch:0024, train_loss=1.24446, train_acc=0.93732, val_loss=1.88858, val_acc=0.77778, time=0.11601
Epoch:0025, train_loss=1.23720, train_acc=0.94376, val_loss=1.88890, val_acc=0.77778, time=0.11501
Early stopping...

Optimization Finished!

Test set results: loss= 1.72323, accuracy= 0.73276, time= 0.02901

Test Precision, Recall and F1-Score...
              precision    recall  f1-score   support

           0     0.8043    0.7929    0.7986       140
           1     0.6486    0.5333    0.5854        45
           2     0.7000    0.7521    0.7251       121
           3     0.7805    0.6957    0.7356        92
           4     0.6792    0.6207    0.6486       116
           5     0.7636    0.6462    0.7000        65
           6     0.7235    0.8197    0.7686       233

    accuracy                         0.7328       812
   macro avg     0.7286    0.6944    0.7088       812
weighted avg     0.7331    0.7328    0.7308       812


Macro average Test Precision, Recall and F1-Score...
(0.7285501106791049, 0.6943563937718065, 0.7088455869868735, None)

Micro average Test Precision, Recall and F1-Score...
(0.7327586206896551, 0.7327586206896551, 0.732758620689655, None)

Embeddings:
Word_embeddings:1343
Train_doc_embeddings:1896
Test_doc_embeddings:812
