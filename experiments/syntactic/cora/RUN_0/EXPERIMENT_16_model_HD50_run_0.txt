
==========: 298104900741100
Epoch:0001, train_loss=2.00780, train_acc=0.17047, val_loss=1.93734, val_acc=0.33862, time=0.12000
Epoch:0002, train_loss=1.87916, train_acc=0.31810, val_loss=1.93063, val_acc=0.39153, time=0.10200
Epoch:0003, train_loss=1.80629, train_acc=0.41476, val_loss=1.92343, val_acc=0.43915, time=0.11400
Epoch:0004, train_loss=1.72787, train_acc=0.52783, val_loss=1.91616, val_acc=0.52910, time=0.12801
Epoch:0005, train_loss=1.64718, train_acc=0.61219, val_loss=1.91032, val_acc=0.62434, time=0.12601
Epoch:0006, train_loss=1.57912, train_acc=0.70592, val_loss=1.90640, val_acc=0.65079, time=0.13000
Epoch:0007, train_loss=1.52948, train_acc=0.76216, val_loss=1.90373, val_acc=0.68254, time=0.13001
Epoch:0008, train_loss=1.49232, train_acc=0.79496, val_loss=1.90133, val_acc=0.70370, time=0.13699
Epoch:0009, train_loss=1.45876, train_acc=0.81722, val_loss=1.89880, val_acc=0.70899, time=0.13098
Epoch:0010, train_loss=1.42561, train_acc=0.83890, val_loss=1.89638, val_acc=0.74074, time=0.13099
Epoch:0011, train_loss=1.39488, train_acc=0.85706, val_loss=1.89442, val_acc=0.74603, time=0.12700
Epoch:0012, train_loss=1.36909, train_acc=0.86936, val_loss=1.89302, val_acc=0.76190, time=0.12000
Epoch:0013, train_loss=1.34852, train_acc=0.87405, val_loss=1.89209, val_acc=0.76720, time=0.12301
Epoch:0014, train_loss=1.33164, train_acc=0.88108, val_loss=1.89144, val_acc=0.76720, time=0.11501
Epoch:0015, train_loss=1.31663, train_acc=0.88869, val_loss=1.89094, val_acc=0.76720, time=0.12800
Epoch:0016, train_loss=1.30241, train_acc=0.89690, val_loss=1.89050, val_acc=0.76720, time=0.12700
Epoch:0017, train_loss=1.28870, train_acc=0.90568, val_loss=1.89014, val_acc=0.78307, time=0.12800
Epoch:0018, train_loss=1.27573, train_acc=0.91564, val_loss=1.88987, val_acc=0.77249, time=0.10201
Epoch:0019, train_loss=1.26388, train_acc=0.92326, val_loss=1.88972, val_acc=0.76190, time=0.11799
Epoch:0020, train_loss=1.25336, train_acc=0.93615, val_loss=1.88967, val_acc=0.76720, time=0.13001
Epoch:0021, train_loss=1.24415, train_acc=0.93966, val_loss=1.88970, val_acc=0.76720, time=0.13299
Epoch:0022, train_loss=1.23597, train_acc=0.94318, val_loss=1.88977, val_acc=0.75661, time=0.12302
Epoch:0023, train_loss=1.22847, train_acc=0.95196, val_loss=1.88984, val_acc=0.77249, time=0.13302
Epoch:0024, train_loss=1.22134, train_acc=0.95782, val_loss=1.88991, val_acc=0.76720, time=0.13399
Epoch:0025, train_loss=1.21444, train_acc=0.96544, val_loss=1.89000, val_acc=0.76720, time=0.12602
Epoch:0026, train_loss=1.20784, train_acc=0.96719, val_loss=1.89013, val_acc=0.76190, time=0.11901
Early stopping...

Optimization Finished!

Test set results: loss= 1.72548, accuracy= 0.74261, time= 0.03099

Test Precision, Recall and F1-Score...
              precision    recall  f1-score   support

           0     0.7931    0.8214    0.8070       140
           1     0.6136    0.6000    0.6067        45
           2     0.7244    0.7603    0.7419       121
           3     0.7831    0.7065    0.7429        92
           4     0.6783    0.6724    0.6753       116
           5     0.8462    0.6769    0.7521        65
           6     0.7398    0.7811    0.7599       233

    accuracy                         0.7426       812
   macro avg     0.7398    0.7170    0.7266       812
weighted avg     0.7443    0.7426    0.7422       812


Macro average Test Precision, Recall and F1-Score...
(0.7397905578492362, 0.7169619484180364, 0.7265613805394294, None)

Micro average Test Precision, Recall and F1-Score...
(0.7426108374384236, 0.7426108374384236, 0.7426108374384235, None)

Embeddings:
Word_embeddings:1343
Train_doc_embeddings:1896
Test_doc_embeddings:812
