
==========: 298014090030300
Epoch:0001, train_loss=1.99770, train_acc=0.16872, val_loss=1.93500, val_acc=0.33862, time=0.11900
Epoch:0002, train_loss=1.87180, train_acc=0.32162, val_loss=1.92796, val_acc=0.38624, time=0.11902
Epoch:0003, train_loss=1.79174, train_acc=0.41945, val_loss=1.92087, val_acc=0.47619, time=0.13201
Epoch:0004, train_loss=1.70857, train_acc=0.54540, val_loss=1.91425, val_acc=0.60847, time=0.11201
Epoch:0005, train_loss=1.63044, train_acc=0.65026, val_loss=1.90918, val_acc=0.67196, time=0.12301
Epoch:0006, train_loss=1.56717, train_acc=0.72408, val_loss=1.90565, val_acc=0.69312, time=0.13501
Epoch:0007, train_loss=1.51874, train_acc=0.76743, val_loss=1.90283, val_acc=0.71958, time=0.12700
Epoch:0008, train_loss=1.47855, train_acc=0.79731, val_loss=1.90023, val_acc=0.74074, time=0.12299
Epoch:0009, train_loss=1.44240, train_acc=0.81664, val_loss=1.89781, val_acc=0.74603, time=0.12598
Epoch:0010, train_loss=1.41021, train_acc=0.84183, val_loss=1.89572, val_acc=0.74603, time=0.13100
Epoch:0011, train_loss=1.38284, train_acc=0.86116, val_loss=1.89403, val_acc=0.76190, time=0.11699
Epoch:0012, train_loss=1.36003, train_acc=0.87053, val_loss=1.89270, val_acc=0.76190, time=0.11900
Epoch:0013, train_loss=1.34052, train_acc=0.87756, val_loss=1.89165, val_acc=0.76720, time=0.11999
Epoch:0014, train_loss=1.32293, train_acc=0.88459, val_loss=1.89086, val_acc=0.77778, time=0.11901
Epoch:0015, train_loss=1.30674, train_acc=0.89748, val_loss=1.89033, val_acc=0.78307, time=0.13401
Epoch:0016, train_loss=1.29205, train_acc=0.90568, val_loss=1.89004, val_acc=0.78836, time=0.12100
Epoch:0017, train_loss=1.27897, train_acc=0.91740, val_loss=1.88992, val_acc=0.78836, time=0.12401
Epoch:0018, train_loss=1.26735, train_acc=0.92326, val_loss=1.88987, val_acc=0.77778, time=0.12000
Epoch:0019, train_loss=1.25682, train_acc=0.92912, val_loss=1.88983, val_acc=0.76720, time=0.12800
Epoch:0020, train_loss=1.24705, train_acc=0.93673, val_loss=1.88976, val_acc=0.76720, time=0.12801
Epoch:0021, train_loss=1.23787, train_acc=0.94142, val_loss=1.88968, val_acc=0.76720, time=0.13400
Epoch:0022, train_loss=1.22929, train_acc=0.94845, val_loss=1.88964, val_acc=0.76190, time=0.11302
Epoch:0023, train_loss=1.22143, train_acc=0.95431, val_loss=1.88966, val_acc=0.76720, time=0.12200
Epoch:0024, train_loss=1.21433, train_acc=0.96075, val_loss=1.88977, val_acc=0.75661, time=0.12801
Epoch:0025, train_loss=1.20795, train_acc=0.96485, val_loss=1.88995, val_acc=0.75661, time=0.12400
Early stopping...

Optimization Finished!

Test set results: loss= 1.72728, accuracy= 0.72044, time= 0.03801

Test Precision, Recall and F1-Score...
              precision    recall  f1-score   support

           0     0.8231    0.7643    0.7926       140
           1     0.5957    0.6222    0.6087        45
           2     0.6977    0.7438    0.7200       121
           3     0.7079    0.6848    0.6961        92
           4     0.6186    0.6293    0.6239       116
           5     0.8269    0.6615    0.7350        65
           6     0.7328    0.7768    0.7542       233

    accuracy                         0.7204       812
   macro avg     0.7147    0.6975    0.7044       812
weighted avg     0.7239    0.7204    0.7209       812


Macro average Test Precision, Recall and F1-Score...
(0.7146745511512654, 0.6975378626852804, 0.7043659810132306, None)

Micro average Test Precision, Recall and F1-Score...
(0.7204433497536946, 0.7204433497536946, 0.7204433497536946, None)

Embeddings:
Word_embeddings:1343
Train_doc_embeddings:1896
Test_doc_embeddings:812
