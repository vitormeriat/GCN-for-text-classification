
==========: 298182771555400
Epoch:0001, train_loss=1.99198, train_acc=0.15290, val_loss=1.93572, val_acc=0.38624, time=0.12500
Epoch:0002, train_loss=1.87294, train_acc=0.31869, val_loss=1.92969, val_acc=0.41270, time=0.12101
Epoch:0003, train_loss=1.78810, train_acc=0.40598, val_loss=1.92303, val_acc=0.46561, time=0.11001
Epoch:0004, train_loss=1.70262, train_acc=0.54364, val_loss=1.91622, val_acc=0.55026, time=0.11799
Epoch:0005, train_loss=1.62617, train_acc=0.68248, val_loss=1.91009, val_acc=0.61376, time=0.12601
Epoch:0006, train_loss=1.56328, train_acc=0.75220, val_loss=1.90511, val_acc=0.67196, time=0.12301
Epoch:0007, train_loss=1.51340, train_acc=0.78910, val_loss=1.90119, val_acc=0.71429, time=0.13199
Epoch:0008, train_loss=1.47194, train_acc=0.80258, val_loss=1.89825, val_acc=0.71429, time=0.12300
Epoch:0009, train_loss=1.43652, train_acc=0.82074, val_loss=1.89610, val_acc=0.70899, time=0.12301
Epoch:0010, train_loss=1.40595, train_acc=0.83597, val_loss=1.89451, val_acc=0.70899, time=0.12000
Epoch:0011, train_loss=1.37922, train_acc=0.85354, val_loss=1.89330, val_acc=0.71958, time=0.12800
Epoch:0012, train_loss=1.35564, train_acc=0.86585, val_loss=1.89242, val_acc=0.73016, time=0.11801
Epoch:0013, train_loss=1.33512, train_acc=0.88049, val_loss=1.89180, val_acc=0.74074, time=0.12801
Epoch:0014, train_loss=1.31766, train_acc=0.89104, val_loss=1.89137, val_acc=0.74603, time=0.12599
Epoch:0015, train_loss=1.30278, train_acc=0.90803, val_loss=1.89100, val_acc=0.74074, time=0.11502
Epoch:0016, train_loss=1.28953, train_acc=0.91154, val_loss=1.89058, val_acc=0.74603, time=0.12901
Epoch:0017, train_loss=1.27705, train_acc=0.91740, val_loss=1.89012, val_acc=0.75661, time=0.12701
Epoch:0018, train_loss=1.26508, train_acc=0.92501, val_loss=1.88971, val_acc=0.75661, time=0.12602
Epoch:0019, train_loss=1.25386, train_acc=0.93087, val_loss=1.88942, val_acc=0.75132, time=0.12299
Epoch:0020, train_loss=1.24375, train_acc=0.93439, val_loss=1.88930, val_acc=0.75661, time=0.12001
Epoch:0021, train_loss=1.23491, train_acc=0.94376, val_loss=1.88932, val_acc=0.76190, time=0.12300
Epoch:0022, train_loss=1.22709, train_acc=0.94962, val_loss=1.88943, val_acc=0.75132, time=0.11900
Epoch:0023, train_loss=1.21987, train_acc=0.95548, val_loss=1.88960, val_acc=0.76190, time=0.12302
Epoch:0024, train_loss=1.21293, train_acc=0.95841, val_loss=1.88980, val_acc=0.76190, time=0.12699
Epoch:0025, train_loss=1.20625, train_acc=0.96602, val_loss=1.89005, val_acc=0.75661, time=0.11800
Early stopping...

Optimization Finished!

Test set results: loss= 1.72776, accuracy= 0.72537, time= 0.03899

Test Precision, Recall and F1-Score...
              precision    recall  f1-score   support

           0     0.8110    0.7357    0.7715       140
           1     0.6154    0.5333    0.5714        45
           2     0.7165    0.7521    0.7339       121
           3     0.7087    0.7935    0.7487        92
           4     0.6731    0.6034    0.6364       116
           5     0.8302    0.6769    0.7458        65
           6     0.7104    0.7897    0.7480       233

    accuracy                         0.7254       812
   macro avg     0.7236    0.6978    0.7079       812
weighted avg     0.7275    0.7254    0.7241       812


Macro average Test Precision, Recall and F1-Score...
(0.7236245496181875, 0.697808988460037, 0.70794955661652, None)

Micro average Test Precision, Recall and F1-Score...
(0.7253694581280788, 0.7253694581280788, 0.7253694581280788, None)

Embeddings:
Word_embeddings:1343
Train_doc_embeddings:1896
Test_doc_embeddings:812
