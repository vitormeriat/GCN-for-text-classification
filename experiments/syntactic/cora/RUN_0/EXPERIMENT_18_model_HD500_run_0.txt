
==========: 298123686186200
Epoch:0001, train_loss=2.15598, train_acc=0.10193, val_loss=1.94191, val_acc=0.31746, time=0.13600
Epoch:0002, train_loss=1.92700, train_acc=0.28764, val_loss=1.93417, val_acc=0.39153, time=0.13201
Epoch:0003, train_loss=1.85266, train_acc=0.35911, val_loss=1.92927, val_acc=0.44444, time=0.12099
Epoch:0004, train_loss=1.79724, train_acc=0.44288, val_loss=1.92320, val_acc=0.50265, time=0.10701
Epoch:0005, train_loss=1.73340, train_acc=0.54247, val_loss=1.91682, val_acc=0.61376, time=0.11500
Epoch:0006, train_loss=1.66802, train_acc=0.64733, val_loss=1.91167, val_acc=0.65608, time=0.13001
Epoch:0007, train_loss=1.61291, train_acc=0.70299, val_loss=1.90767, val_acc=0.67725, time=0.13699
Epoch:0008, train_loss=1.56574, train_acc=0.73169, val_loss=1.90424, val_acc=0.67196, time=0.12301
Epoch:0009, train_loss=1.52144, train_acc=0.75220, val_loss=1.90114, val_acc=0.68254, time=0.12601
Epoch:0010, train_loss=1.47912, train_acc=0.78032, val_loss=1.89852, val_acc=0.71958, time=0.12799
Epoch:0011, train_loss=1.44148, train_acc=0.81312, val_loss=1.89668, val_acc=0.73545, time=0.13101
Epoch:0012, train_loss=1.41142, train_acc=0.83187, val_loss=1.89567, val_acc=0.74074, time=0.13299
Epoch:0013, train_loss=1.38955, train_acc=0.84944, val_loss=1.89522, val_acc=0.75132, time=0.13001
Epoch:0014, train_loss=1.37362, train_acc=0.86292, val_loss=1.89490, val_acc=0.74074, time=0.12200
Epoch:0015, train_loss=1.36022, train_acc=0.86350, val_loss=1.89435, val_acc=0.74603, time=0.12199
Epoch:0016, train_loss=1.34669, train_acc=0.86819, val_loss=1.89346, val_acc=0.75661, time=0.12900
Epoch:0017, train_loss=1.33215, train_acc=0.87405, val_loss=1.89237, val_acc=0.76720, time=0.13102
Epoch:0018, train_loss=1.31718, train_acc=0.88284, val_loss=1.89131, val_acc=0.77249, time=0.13100
Epoch:0019, train_loss=1.30291, train_acc=0.89455, val_loss=1.89044, val_acc=0.77249, time=0.14800
Epoch:0020, train_loss=1.29009, train_acc=0.89807, val_loss=1.88985, val_acc=0.76190, time=0.12500
Epoch:0021, train_loss=1.27893, train_acc=0.90861, val_loss=1.88950, val_acc=0.75132, time=0.10601
Epoch:0022, train_loss=1.26924, train_acc=0.91740, val_loss=1.88934, val_acc=0.74603, time=0.13199
Epoch:0023, train_loss=1.26070, train_acc=0.92501, val_loss=1.88931, val_acc=0.75132, time=0.12100
Epoch:0024, train_loss=1.25303, train_acc=0.93146, val_loss=1.88935, val_acc=0.75132, time=0.11999
Epoch:0025, train_loss=1.24599, train_acc=0.93556, val_loss=1.88943, val_acc=0.75661, time=0.12900
Epoch:0026, train_loss=1.23934, train_acc=0.93966, val_loss=1.88949, val_acc=0.75661, time=0.12401
Epoch:0027, train_loss=1.23291, train_acc=0.94669, val_loss=1.88954, val_acc=0.76720, time=0.12900
Epoch:0028, train_loss=1.22660, train_acc=0.95138, val_loss=1.88958, val_acc=0.76720, time=0.13100
Epoch:0029, train_loss=1.22046, train_acc=0.95372, val_loss=1.88964, val_acc=0.76720, time=0.12700
Early stopping...

Optimization Finished!

Test set results: loss= 1.72828, accuracy= 0.71059, time= 0.03802

Test Precision, Recall and F1-Score...
              precision    recall  f1-score   support

           0     0.7692    0.7857    0.7774       140
           1     0.6190    0.5778    0.5977        45
           2     0.6815    0.7603    0.7188       121
           3     0.7412    0.6848    0.7119        92
           4     0.6429    0.6207    0.6316       116
           5     0.8372    0.5538    0.6667        65
           6     0.7063    0.7639    0.7340       233

    accuracy                         0.7106       812
   macro avg     0.7139    0.6782    0.6911       812
weighted avg     0.7140    0.7106    0.7091       812


Macro average Test Precision, Recall and F1-Score...
(0.7139074274114338, 0.6781556510818225, 0.6911381354010483, None)

Micro average Test Precision, Recall and F1-Score...
(0.7105911330049262, 0.7105911330049262, 0.7105911330049262, None)

Embeddings:
Word_embeddings:1343
Train_doc_embeddings:1896
Test_doc_embeddings:812
