
==========: 298074425235000
Epoch:0001, train_loss=2.17372, train_acc=0.16520, val_loss=1.93820, val_acc=0.28042, time=0.12799
Epoch:0002, train_loss=1.91776, train_acc=0.26655, val_loss=1.93128, val_acc=0.35450, time=0.14101
Epoch:0003, train_loss=1.86914, train_acc=0.32806, val_loss=1.92776, val_acc=0.40212, time=0.13303
Epoch:0004, train_loss=1.83096, train_acc=0.39660, val_loss=1.92381, val_acc=0.46032, time=0.12600
Epoch:0005, train_loss=1.78094, train_acc=0.47452, val_loss=1.91900, val_acc=0.53439, time=0.12300
Epoch:0006, train_loss=1.72029, train_acc=0.54716, val_loss=1.91358, val_acc=0.58201, time=0.12601
Epoch:0007, train_loss=1.65300, train_acc=0.62683, val_loss=1.90845, val_acc=0.66667, time=0.13700
Epoch:0008, train_loss=1.58841, train_acc=0.69596, val_loss=1.90433, val_acc=0.71958, time=0.12800
Epoch:0009, train_loss=1.53370, train_acc=0.76567, val_loss=1.90141, val_acc=0.75132, time=0.12800
Epoch:0010, train_loss=1.49147, train_acc=0.80375, val_loss=1.89962, val_acc=0.75132, time=0.12301
Epoch:0011, train_loss=1.46112, train_acc=0.82074, val_loss=1.89866, val_acc=0.72487, time=0.12399
Epoch:0012, train_loss=1.43976, train_acc=0.82953, val_loss=1.89802, val_acc=0.70899, time=0.12001
Epoch:0013, train_loss=1.42285, train_acc=0.82660, val_loss=1.89720, val_acc=0.69312, time=0.13200
Epoch:0014, train_loss=1.40584, train_acc=0.82953, val_loss=1.89594, val_acc=0.69841, time=0.12699
Epoch:0015, train_loss=1.38659, train_acc=0.84534, val_loss=1.89436, val_acc=0.70899, time=0.12999
Epoch:0016, train_loss=1.36593, train_acc=0.86233, val_loss=1.89274, val_acc=0.75132, time=0.11800
Epoch:0017, train_loss=1.34611, train_acc=0.87346, val_loss=1.89135, val_acc=0.75661, time=0.12902
Epoch:0018, train_loss=1.32896, train_acc=0.88576, val_loss=1.89032, val_acc=0.76190, time=0.11601
Epoch:0019, train_loss=1.31512, train_acc=0.89338, val_loss=1.88964, val_acc=0.77778, time=0.12599
Epoch:0020, train_loss=1.30414, train_acc=0.89748, val_loss=1.88924, val_acc=0.77778, time=0.11802
Epoch:0021, train_loss=1.29512, train_acc=0.89924, val_loss=1.88900, val_acc=0.76720, time=0.12801
Epoch:0022, train_loss=1.28709, train_acc=0.90744, val_loss=1.88885, val_acc=0.77249, time=0.13199
Epoch:0023, train_loss=1.27933, train_acc=0.90920, val_loss=1.88873, val_acc=0.77249, time=0.13001
Epoch:0024, train_loss=1.27139, train_acc=0.91447, val_loss=1.88862, val_acc=0.76720, time=0.11700
Epoch:0025, train_loss=1.26312, train_acc=0.91798, val_loss=1.88851, val_acc=0.77249, time=0.11100
Epoch:0026, train_loss=1.25466, train_acc=0.92501, val_loss=1.88843, val_acc=0.77249, time=0.11001
Epoch:0027, train_loss=1.24630, train_acc=0.93263, val_loss=1.88842, val_acc=0.77778, time=0.11600
Epoch:0028, train_loss=1.23844, train_acc=0.94025, val_loss=1.88851, val_acc=0.76190, time=0.13200
Epoch:0029, train_loss=1.23138, train_acc=0.94786, val_loss=1.88869, val_acc=0.76190, time=0.11801
Epoch:0030, train_loss=1.22525, train_acc=0.95255, val_loss=1.88897, val_acc=0.76190, time=0.12100
Early stopping...

Optimization Finished!

Test set results: loss= 1.72819, accuracy= 0.71059, time= 0.04201

Test Precision, Recall and F1-Score...
              precision    recall  f1-score   support

           0     0.7926    0.7643    0.7782       140
           1     0.5946    0.4889    0.5366        45
           2     0.6842    0.7521    0.7165       121
           3     0.7065    0.7065    0.7065        92
           4     0.6228    0.6121    0.6174       116
           5     0.8511    0.6154    0.7143        65
           6     0.7126    0.7768    0.7433       233

    accuracy                         0.7106       812
   macro avg     0.7092    0.6737    0.6875       812
weighted avg     0.7132    0.7106    0.7094       812


Macro average Test Precision, Recall and F1-Score...
(0.7091983893087651, 0.6737200104634483, 0.6875468376538121, None)

Micro average Test Precision, Recall and F1-Score...
(0.7105911330049262, 0.7105911330049262, 0.7105911330049262, None)

Embeddings:
Word_embeddings:1343
Train_doc_embeddings:1896
Test_doc_embeddings:812
