
==========: 298972155305800
Epoch:0001, train_loss=2.05710, train_acc=0.11716, val_loss=1.93894, val_acc=0.26984, time=0.11301
Epoch:0002, train_loss=1.88098, train_acc=0.31166, val_loss=1.93481, val_acc=0.32275, time=0.12098
Epoch:0003, train_loss=1.83383, train_acc=0.36965, val_loss=1.92788, val_acc=0.37037, time=0.12500
Epoch:0004, train_loss=1.75356, train_acc=0.43292, val_loss=1.91965, val_acc=0.43386, time=0.13001
Epoch:0005, train_loss=1.66405, train_acc=0.55653, val_loss=1.91340, val_acc=0.60317, time=0.11801
Epoch:0006, train_loss=1.59715, train_acc=0.69772, val_loss=1.90931, val_acc=0.70370, time=0.13800
Epoch:0007, train_loss=1.55306, train_acc=0.76508, val_loss=1.90597, val_acc=0.71958, time=0.11700
Epoch:0008, train_loss=1.51692, train_acc=0.80316, val_loss=1.90262, val_acc=0.72487, time=0.11801
Epoch:0009, train_loss=1.48067, train_acc=0.81839, val_loss=1.89942, val_acc=0.73016, time=0.13000
Epoch:0010, train_loss=1.44498, train_acc=0.82894, val_loss=1.89678, val_acc=0.74074, time=0.11901
Epoch:0011, train_loss=1.41325, train_acc=0.84241, val_loss=1.89492, val_acc=0.74074, time=0.10800
Epoch:0012, train_loss=1.38731, train_acc=0.85120, val_loss=1.89373, val_acc=0.73545, time=0.12901
Epoch:0013, train_loss=1.36658, train_acc=0.85589, val_loss=1.89294, val_acc=0.72487, time=0.13201
Epoch:0014, train_loss=1.34935, train_acc=0.86233, val_loss=1.89230, val_acc=0.74074, time=0.10700
Epoch:0015, train_loss=1.33395, train_acc=0.87053, val_loss=1.89164, val_acc=0.75132, time=0.11901
Epoch:0016, train_loss=1.31930, train_acc=0.87815, val_loss=1.89090, val_acc=0.76190, time=0.13500
Epoch:0017, train_loss=1.30506, train_acc=0.89104, val_loss=1.89014, val_acc=0.76190, time=0.13001
Epoch:0018, train_loss=1.29155, train_acc=0.90803, val_loss=1.88948, val_acc=0.76190, time=0.13202
Epoch:0019, train_loss=1.27929, train_acc=0.91388, val_loss=1.88899, val_acc=0.77778, time=0.11799
Epoch:0020, train_loss=1.26858, train_acc=0.92033, val_loss=1.88871, val_acc=0.76720, time=0.13200
Epoch:0021, train_loss=1.25932, train_acc=0.92560, val_loss=1.88861, val_acc=0.77249, time=0.11099
Epoch:0022, train_loss=1.25110, train_acc=0.93497, val_loss=1.88863, val_acc=0.76720, time=0.13300
Epoch:0023, train_loss=1.24341, train_acc=0.93966, val_loss=1.88872, val_acc=0.76190, time=0.11299
Epoch:0024, train_loss=1.23589, train_acc=0.94200, val_loss=1.88883, val_acc=0.76190, time=0.12501
Epoch:0025, train_loss=1.22845, train_acc=0.95079, val_loss=1.88897, val_acc=0.75661, time=0.12700
Epoch:0026, train_loss=1.22133, train_acc=0.95489, val_loss=1.88916, val_acc=0.76190, time=0.12800
Epoch:0027, train_loss=1.21479, train_acc=0.95958, val_loss=1.88940, val_acc=0.76190, time=0.12301
Early stopping...

Optimization Finished!

Test set results: loss= 1.72579, accuracy= 0.72660, time= 0.03600

Test Precision, Recall and F1-Score...
              precision    recall  f1-score   support

           0     0.8492    0.7643    0.8045       140
           1     0.6154    0.5333    0.5714        45
           2     0.7099    0.7686    0.7381       121
           3     0.7609    0.7609    0.7609        92
           4     0.6161    0.5948    0.6053       116
           5     0.8113    0.6615    0.7288        65
           6     0.7104    0.7897    0.7480       233

    accuracy                         0.7266       812
   macro avg     0.7247    0.6962    0.7081       812
weighted avg     0.7293    0.7266    0.7260       812


Macro average Test Precision, Recall and F1-Score...
(0.7247430125205163, 0.6961641818170803, 0.7081355499754654, None)

Micro average Test Precision, Recall and F1-Score...
(0.7266009852216748, 0.7266009852216748, 0.7266009852216749, None)

Embeddings:
Word_embeddings:1343
Train_doc_embeddings:1896
Test_doc_embeddings:812
