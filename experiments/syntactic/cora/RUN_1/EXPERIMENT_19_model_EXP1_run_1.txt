
==========: 298991518610800
Epoch:0001, train_loss=2.01449, train_acc=0.19098, val_loss=1.93960, val_acc=0.28571, time=0.10300
Epoch:0002, train_loss=1.92282, train_acc=0.28530, val_loss=1.93469, val_acc=0.33333, time=0.12701
Epoch:0003, train_loss=1.85779, train_acc=0.33392, val_loss=1.93153, val_acc=0.38095, time=0.13300
Epoch:0004, train_loss=1.81133, train_acc=0.38781, val_loss=1.92892, val_acc=0.39153, time=0.12600
Epoch:0005, train_loss=1.77464, train_acc=0.43761, val_loss=1.92584, val_acc=0.39153, time=0.12900
Epoch:0006, train_loss=1.73814, train_acc=0.48330, val_loss=1.92221, val_acc=0.47090, time=0.13200
Epoch:0007, train_loss=1.69929, train_acc=0.54130, val_loss=1.91843, val_acc=0.51323, time=0.11700
Epoch:0008, train_loss=1.66050, train_acc=0.59754, val_loss=1.91489, val_acc=0.54497, time=0.11302
Epoch:0009, train_loss=1.62428, train_acc=0.64323, val_loss=1.91178, val_acc=0.61376, time=0.12301
Epoch:0010, train_loss=1.59169, train_acc=0.68366, val_loss=1.90913, val_acc=0.65079, time=0.11901
Epoch:0011, train_loss=1.56260, train_acc=0.71529, val_loss=1.90689, val_acc=0.66138, time=0.11600
Epoch:0012, train_loss=1.53663, train_acc=0.74692, val_loss=1.90502, val_acc=0.67725, time=0.10601
Epoch:0013, train_loss=1.51343, train_acc=0.77973, val_loss=1.90345, val_acc=0.70899, time=0.10300
Epoch:0014, train_loss=1.49248, train_acc=0.79438, val_loss=1.90206, val_acc=0.71429, time=0.13401
Epoch:0015, train_loss=1.47314, train_acc=0.81371, val_loss=1.90078, val_acc=0.71429, time=0.12401
Epoch:0016, train_loss=1.45482, train_acc=0.82835, val_loss=1.89955, val_acc=0.70899, time=0.12201
Epoch:0017, train_loss=1.43728, train_acc=0.84359, val_loss=1.89837, val_acc=0.71429, time=0.12601
Epoch:0018, train_loss=1.42060, train_acc=0.85003, val_loss=1.89728, val_acc=0.71958, time=0.13200
Epoch:0019, train_loss=1.40502, train_acc=0.85706, val_loss=1.89629, val_acc=0.71958, time=0.12901
Epoch:0020, train_loss=1.39074, train_acc=0.86409, val_loss=1.89543, val_acc=0.71958, time=0.12501
Epoch:0021, train_loss=1.37776, train_acc=0.87346, val_loss=1.89468, val_acc=0.72487, time=0.13301
Epoch:0022, train_loss=1.36592, train_acc=0.87698, val_loss=1.89402, val_acc=0.74074, time=0.12299
Epoch:0023, train_loss=1.35498, train_acc=0.88284, val_loss=1.89343, val_acc=0.74074, time=0.12600
Epoch:0024, train_loss=1.34472, train_acc=0.88928, val_loss=1.89290, val_acc=0.75132, time=0.11101
Epoch:0025, train_loss=1.33499, train_acc=0.89514, val_loss=1.89242, val_acc=0.75132, time=0.12700
Epoch:0026, train_loss=1.32569, train_acc=0.90217, val_loss=1.89197, val_acc=0.75132, time=0.12801
Epoch:0027, train_loss=1.31678, train_acc=0.90744, val_loss=1.89157, val_acc=0.74603, time=0.11400
Epoch:0028, train_loss=1.30824, train_acc=0.91447, val_loss=1.89122, val_acc=0.74603, time=0.11900
Epoch:0029, train_loss=1.30012, train_acc=0.91857, val_loss=1.89092, val_acc=0.75132, time=0.12100
Epoch:0030, train_loss=1.29245, train_acc=0.92326, val_loss=1.89067, val_acc=0.75132, time=0.11602
Epoch:0031, train_loss=1.28527, train_acc=0.92677, val_loss=1.89048, val_acc=0.75132, time=0.12203
Epoch:0032, train_loss=1.27856, train_acc=0.93146, val_loss=1.89032, val_acc=0.75661, time=0.10800
Epoch:0033, train_loss=1.27224, train_acc=0.93146, val_loss=1.89020, val_acc=0.75661, time=0.12401
Epoch:0034, train_loss=1.26623, train_acc=0.93615, val_loss=1.89009, val_acc=0.75132, time=0.11200
Epoch:0035, train_loss=1.26046, train_acc=0.94025, val_loss=1.89000, val_acc=0.75132, time=0.12500
Epoch:0036, train_loss=1.25490, train_acc=0.94318, val_loss=1.88993, val_acc=0.75132, time=0.11601
Epoch:0037, train_loss=1.24955, train_acc=0.95021, val_loss=1.88987, val_acc=0.75661, time=0.12302
Epoch:0038, train_loss=1.24442, train_acc=0.95372, val_loss=1.88984, val_acc=0.76190, time=0.13100
Epoch:0039, train_loss=1.23953, train_acc=0.95723, val_loss=1.88982, val_acc=0.76190, time=0.12300
Epoch:0040, train_loss=1.23490, train_acc=0.96134, val_loss=1.88983, val_acc=0.76720, time=0.12200
Epoch:0041, train_loss=1.23050, train_acc=0.96309, val_loss=1.88984, val_acc=0.76720, time=0.11102
Epoch:0042, train_loss=1.22634, train_acc=0.96485, val_loss=1.88987, val_acc=0.76720, time=0.12999
Early stopping...

Optimization Finished!

Test set results: loss= 1.72335, accuracy= 0.72783, time= 0.02601

Test Precision, Recall and F1-Score...
              precision    recall  f1-score   support

           0     0.8074    0.7786    0.7927       140
           1     0.6053    0.5111    0.5542        45
           2     0.7031    0.7438    0.7229       121
           3     0.7416    0.7174    0.7293        92
           4     0.6579    0.6466    0.6522       116
           5     0.7963    0.6615    0.7227        65
           6     0.7283    0.7940    0.7598       233

    accuracy                         0.7278       812
   macro avg     0.7200    0.6933    0.7048       812
weighted avg     0.7283    0.7278    0.7268       812


Macro average Test Precision, Recall and F1-Score...
(0.7199865841201891, 0.6932795855583332, 0.7048191509315574, None)

Micro average Test Precision, Recall and F1-Score...
(0.7278325123152709, 0.7278325123152709, 0.7278325123152709, None)

Embeddings:
Word_embeddings:1343
Train_doc_embeddings:1896
Test_doc_embeddings:812
