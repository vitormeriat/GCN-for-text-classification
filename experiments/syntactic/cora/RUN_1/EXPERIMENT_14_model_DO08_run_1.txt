
==========: 298943361568200
Epoch:0001, train_loss=2.15744, train_acc=0.08963, val_loss=1.94495, val_acc=0.20635, time=0.13000
Epoch:0002, train_loss=1.92549, train_acc=0.24780, val_loss=1.93360, val_acc=0.35450, time=0.13101
Epoch:0003, train_loss=1.85101, train_acc=0.34271, val_loss=1.92713, val_acc=0.40741, time=0.11099
Epoch:0004, train_loss=1.79793, train_acc=0.37844, val_loss=1.92065, val_acc=0.46561, time=0.10601
Epoch:0005, train_loss=1.73111, train_acc=0.48037, val_loss=1.91573, val_acc=0.58730, time=0.11000
Epoch:0006, train_loss=1.67211, train_acc=0.59168, val_loss=1.91200, val_acc=0.64550, time=0.13102
Epoch:0007, train_loss=1.62167, train_acc=0.66022, val_loss=1.90878, val_acc=0.68783, time=0.11101
Epoch:0008, train_loss=1.57609, train_acc=0.73462, val_loss=1.90584, val_acc=0.70899, time=0.12501
Epoch:0009, train_loss=1.53383, train_acc=0.76333, val_loss=1.90319, val_acc=0.69312, time=0.13001
Epoch:0010, train_loss=1.49508, train_acc=0.78969, val_loss=1.90097, val_acc=0.73016, time=0.12701
Epoch:0011, train_loss=1.46138, train_acc=0.80492, val_loss=1.89927, val_acc=0.72487, time=0.12899
Epoch:0012, train_loss=1.43363, train_acc=0.82660, val_loss=1.89797, val_acc=0.71958, time=0.11901
Epoch:0013, train_loss=1.41104, train_acc=0.83421, val_loss=1.89691, val_acc=0.71958, time=0.13101
Epoch:0014, train_loss=1.39198, train_acc=0.84183, val_loss=1.89593, val_acc=0.73016, time=0.12400
Epoch:0015, train_loss=1.37490, train_acc=0.85764, val_loss=1.89497, val_acc=0.73545, time=0.12801
Epoch:0016, train_loss=1.35895, train_acc=0.86819, val_loss=1.89402, val_acc=0.74074, time=0.12400
Epoch:0017, train_loss=1.34385, train_acc=0.88284, val_loss=1.89311, val_acc=0.73545, time=0.12101
Epoch:0018, train_loss=1.32970, train_acc=0.89690, val_loss=1.89226, val_acc=0.73545, time=0.12301
Epoch:0019, train_loss=1.31666, train_acc=0.90217, val_loss=1.89152, val_acc=0.75132, time=0.12999
Epoch:0020, train_loss=1.30480, train_acc=0.90803, val_loss=1.89087, val_acc=0.77249, time=0.12400
Epoch:0021, train_loss=1.29394, train_acc=0.91213, val_loss=1.89033, val_acc=0.77778, time=0.11903
Epoch:0022, train_loss=1.28380, train_acc=0.91330, val_loss=1.88990, val_acc=0.78307, time=0.13400
Epoch:0023, train_loss=1.27422, train_acc=0.91623, val_loss=1.88960, val_acc=0.79365, time=0.13200
Epoch:0024, train_loss=1.26524, train_acc=0.92091, val_loss=1.88948, val_acc=0.79365, time=0.13000
Epoch:0025, train_loss=1.25707, train_acc=0.92209, val_loss=1.88952, val_acc=0.78307, time=0.12401
Epoch:0026, train_loss=1.24982, train_acc=0.92970, val_loss=1.88969, val_acc=0.78307, time=0.13000
Epoch:0027, train_loss=1.24336, train_acc=0.93497, val_loss=1.88992, val_acc=0.77778, time=0.11101
Epoch:0028, train_loss=1.23736, train_acc=0.93907, val_loss=1.89013, val_acc=0.77249, time=0.10102
Epoch:0029, train_loss=1.23149, train_acc=0.94200, val_loss=1.89029, val_acc=0.77249, time=0.12299
Early stopping...

Optimization Finished!

Test set results: loss= 1.72597, accuracy= 0.72660, time= 0.04800

Test Precision, Recall and F1-Score...
              precision    recall  f1-score   support

           0     0.7914    0.7857    0.7885       140
           1     0.6765    0.5111    0.5823        45
           2     0.7090    0.7851    0.7451       121
           3     0.7356    0.6957    0.7151        92
           4     0.6633    0.5603    0.6075       116
           5     0.7414    0.6615    0.6992        65
           6     0.7252    0.8155    0.7677       233

    accuracy                         0.7266       812
   macro avg     0.7203    0.6878    0.7008       812
weighted avg     0.7251    0.7266    0.7233       812


Macro average Test Precision, Recall and F1-Score...
(0.7203229083800986, 0.6878479243688689, 0.7007615971602217, None)

Micro average Test Precision, Recall and F1-Score...
(0.7266009852216748, 0.7266009852216748, 0.7266009852216749, None)

Embeddings:
Word_embeddings:1343
Train_doc_embeddings:1896
Test_doc_embeddings:812
