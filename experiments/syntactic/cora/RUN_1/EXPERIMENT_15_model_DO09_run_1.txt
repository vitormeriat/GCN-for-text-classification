
==========: 298953095668600
Epoch:0001, train_loss=2.04672, train_acc=0.13005, val_loss=1.93574, val_acc=0.30159, time=0.12999
Epoch:0002, train_loss=1.87826, train_acc=0.30053, val_loss=1.92912, val_acc=0.39683, time=0.12600
Epoch:0003, train_loss=1.81066, train_acc=0.37493, val_loss=1.92296, val_acc=0.48148, time=0.12300
Epoch:0004, train_loss=1.73326, train_acc=0.48858, val_loss=1.91687, val_acc=0.52910, time=0.12500
Epoch:0005, train_loss=1.65707, train_acc=0.58465, val_loss=1.91103, val_acc=0.60317, time=0.11502
Epoch:0006, train_loss=1.58796, train_acc=0.67311, val_loss=1.90613, val_acc=0.66667, time=0.11901
Epoch:0007, train_loss=1.53157, train_acc=0.74165, val_loss=1.90243, val_acc=0.73545, time=0.12600
Epoch:0008, train_loss=1.48795, train_acc=0.79028, val_loss=1.89980, val_acc=0.74074, time=0.12199
Epoch:0009, train_loss=1.45376, train_acc=0.82191, val_loss=1.89789, val_acc=0.73016, time=0.12302
Epoch:0010, train_loss=1.42548, train_acc=0.83890, val_loss=1.89630, val_acc=0.74603, time=0.12100
Epoch:0011, train_loss=1.39994, train_acc=0.85296, val_loss=1.89482, val_acc=0.75132, time=0.12900
Epoch:0012, train_loss=1.37580, train_acc=0.86292, val_loss=1.89346, val_acc=0.76190, time=0.11899
Epoch:0013, train_loss=1.35359, train_acc=0.87405, val_loss=1.89232, val_acc=0.75661, time=0.12001
Epoch:0014, train_loss=1.33424, train_acc=0.88635, val_loss=1.89142, val_acc=0.76720, time=0.11101
Epoch:0015, train_loss=1.31797, train_acc=0.89514, val_loss=1.89074, val_acc=0.76720, time=0.12901
Epoch:0016, train_loss=1.30431, train_acc=0.89924, val_loss=1.89020, val_acc=0.77778, time=0.12001
Epoch:0017, train_loss=1.29252, train_acc=0.90275, val_loss=1.88979, val_acc=0.77778, time=0.12199
Epoch:0018, train_loss=1.28182, train_acc=0.90393, val_loss=1.88945, val_acc=0.77778, time=0.12301
Epoch:0019, train_loss=1.27154, train_acc=0.90861, val_loss=1.88918, val_acc=0.77778, time=0.09900
Epoch:0020, train_loss=1.26137, train_acc=0.91564, val_loss=1.88900, val_acc=0.77778, time=0.12200
Epoch:0021, train_loss=1.25145, train_acc=0.92501, val_loss=1.88893, val_acc=0.77778, time=0.12502
Epoch:0022, train_loss=1.24224, train_acc=0.93263, val_loss=1.88900, val_acc=0.77778, time=0.12900
Epoch:0023, train_loss=1.23407, train_acc=0.94493, val_loss=1.88916, val_acc=0.77778, time=0.13100
Epoch:0024, train_loss=1.22697, train_acc=0.95255, val_loss=1.88937, val_acc=0.77778, time=0.12602
Epoch:0025, train_loss=1.22069, train_acc=0.95723, val_loss=1.88958, val_acc=0.77249, time=0.11400
Early stopping...

Optimization Finished!

Test set results: loss= 1.72695, accuracy= 0.72660, time= 0.03800

Test Precision, Recall and F1-Score...
              precision    recall  f1-score   support

           0     0.8372    0.7714    0.8030       140
           1     0.5814    0.5556    0.5682        45
           2     0.7132    0.7603    0.7360       121
           3     0.6832    0.7500    0.7150        92
           4     0.6907    0.5776    0.6291       116
           5     0.7679    0.6615    0.7107        65
           6     0.7237    0.7983    0.7592       233

    accuracy                         0.7266       812
   macro avg     0.7139    0.6964    0.7030       812
weighted avg     0.7281    0.7266    0.7252       812


Macro average Test Precision, Recall and F1-Score...
(0.7138950662100154, 0.696388947962016, 0.7030310227079534, None)

Micro average Test Precision, Recall and F1-Score...
(0.7266009852216748, 0.7266009852216748, 0.7266009852216749, None)

Embeddings:
Word_embeddings:1343
Train_doc_embeddings:1896
Test_doc_embeddings:812
