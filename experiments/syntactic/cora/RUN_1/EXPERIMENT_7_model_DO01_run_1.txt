
==========: 298872736602600
Epoch:0001, train_loss=2.15322, train_acc=0.12419, val_loss=1.94216, val_acc=0.27513, time=0.12599
Epoch:0002, train_loss=1.90749, train_acc=0.26889, val_loss=1.93564, val_acc=0.38095, time=0.13200
Epoch:0003, train_loss=1.84400, train_acc=0.34095, val_loss=1.93221, val_acc=0.40212, time=0.13101
Epoch:0004, train_loss=1.79786, train_acc=0.40422, val_loss=1.92611, val_acc=0.43915, time=0.13100
Epoch:0005, train_loss=1.72561, train_acc=0.48565, val_loss=1.91987, val_acc=0.52381, time=0.13001
Epoch:0006, train_loss=1.65480, train_acc=0.57411, val_loss=1.91457, val_acc=0.56085, time=0.13299
Epoch:0007, train_loss=1.59626, train_acc=0.66198, val_loss=1.91000, val_acc=0.61905, time=0.13100
Epoch:0008, train_loss=1.54758, train_acc=0.73462, val_loss=1.90618, val_acc=0.67196, time=0.12700
Epoch:0009, train_loss=1.50732, train_acc=0.78207, val_loss=1.90325, val_acc=0.71429, time=0.12901
Epoch:0010, train_loss=1.47474, train_acc=0.81195, val_loss=1.90117, val_acc=0.72487, time=0.13200
Epoch:0011, train_loss=1.44816, train_acc=0.81547, val_loss=1.89974, val_acc=0.71958, time=0.14201
Epoch:0012, train_loss=1.42565, train_acc=0.82425, val_loss=1.89864, val_acc=0.70899, time=0.12299
Epoch:0013, train_loss=1.40520, train_acc=0.83597, val_loss=1.89757, val_acc=0.69841, time=0.13201
Epoch:0014, train_loss=1.38526, train_acc=0.85413, val_loss=1.89644, val_acc=0.69841, time=0.11099
Epoch:0015, train_loss=1.36569, train_acc=0.86585, val_loss=1.89532, val_acc=0.70370, time=0.12101
Epoch:0016, train_loss=1.34732, train_acc=0.87463, val_loss=1.89434, val_acc=0.73016, time=0.10200
Epoch:0017, train_loss=1.33113, train_acc=0.88576, val_loss=1.89355, val_acc=0.73016, time=0.13301
Epoch:0018, train_loss=1.31753, train_acc=0.89162, val_loss=1.89294, val_acc=0.71958, time=0.13400
Epoch:0019, train_loss=1.30616, train_acc=0.89279, val_loss=1.89244, val_acc=0.71958, time=0.13101
Epoch:0020, train_loss=1.29623, train_acc=0.89748, val_loss=1.89197, val_acc=0.73545, time=0.11200
Epoch:0021, train_loss=1.28702, train_acc=0.90100, val_loss=1.89150, val_acc=0.73016, time=0.12802
Epoch:0022, train_loss=1.27804, train_acc=0.90393, val_loss=1.89104, val_acc=0.73545, time=0.12801
Epoch:0023, train_loss=1.26909, train_acc=0.91037, val_loss=1.89063, val_acc=0.75132, time=0.12099
Epoch:0024, train_loss=1.26021, train_acc=0.91974, val_loss=1.89032, val_acc=0.75132, time=0.12600
Epoch:0025, train_loss=1.25159, train_acc=0.93146, val_loss=1.89012, val_acc=0.76720, time=0.12599
Epoch:0026, train_loss=1.24349, train_acc=0.93790, val_loss=1.89006, val_acc=0.75661, time=0.13001
Epoch:0027, train_loss=1.23617, train_acc=0.94669, val_loss=1.89013, val_acc=0.75661, time=0.12801
Epoch:0028, train_loss=1.22973, train_acc=0.95021, val_loss=1.89030, val_acc=0.76190, time=0.13101
Epoch:0029, train_loss=1.22410, train_acc=0.95255, val_loss=1.89052, val_acc=0.76720, time=0.12400
Epoch:0030, train_loss=1.21903, train_acc=0.95431, val_loss=1.89071, val_acc=0.76190, time=0.11301
Early stopping...

Optimization Finished!

Test set results: loss= 1.72493, accuracy= 0.73030, time= 0.03000

Test Precision, Recall and F1-Score...
              precision    recall  f1-score   support

           0     0.8226    0.7286    0.7727       140
           1     0.5294    0.6000    0.5625        45
           2     0.7385    0.7934    0.7649       121
           3     0.7071    0.7609    0.7330        92
           4     0.6762    0.6121    0.6425       116
           5     0.8333    0.6923    0.7563        65
           6     0.7309    0.7811    0.7552       233

    accuracy                         0.7303       812
   macro avg     0.7197    0.7098    0.7125       812
weighted avg     0.7344    0.7303    0.7305       812


Macro average Test Precision, Recall and F1-Score...
(0.7197103085289064, 0.7097602801705923, 0.7124535692309291, None)

Micro average Test Precision, Recall and F1-Score...
(0.7302955665024631, 0.7302955665024631, 0.7302955665024631, None)

Embeddings:
Word_embeddings:1343
Train_doc_embeddings:1896
Test_doc_embeddings:812
