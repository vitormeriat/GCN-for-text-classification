
==========: 298203281637800
Epoch:0001, train_loss=2.04579, train_acc=0.15876, val_loss=1.93561, val_acc=0.32275, time=0.12100
Epoch:0002, train_loss=1.88451, train_acc=0.30697, val_loss=1.92812, val_acc=0.39153, time=0.11499
Epoch:0003, train_loss=1.79814, train_acc=0.39719, val_loss=1.92460, val_acc=0.43386, time=0.12800
Epoch:0004, train_loss=1.74499, train_acc=0.45987, val_loss=1.91904, val_acc=0.51852, time=0.12401
Epoch:0005, train_loss=1.67832, train_acc=0.54071, val_loss=1.91269, val_acc=0.57672, time=0.12900
Epoch:0006, train_loss=1.60734, train_acc=0.65085, val_loss=1.90795, val_acc=0.62434, time=0.12900
Epoch:0007, train_loss=1.55254, train_acc=0.71178, val_loss=1.90477, val_acc=0.66138, time=0.13101
Epoch:0008, train_loss=1.51250, train_acc=0.74927, val_loss=1.90224, val_acc=0.67196, time=0.13100
Epoch:0009, train_loss=1.47869, train_acc=0.77856, val_loss=1.90002, val_acc=0.68783, time=0.12199
Epoch:0010, train_loss=1.44765, train_acc=0.81488, val_loss=1.89813, val_acc=0.71958, time=0.12501
Epoch:0011, train_loss=1.41991, train_acc=0.84066, val_loss=1.89661, val_acc=0.74603, time=0.13101
Epoch:0012, train_loss=1.39588, train_acc=0.85882, val_loss=1.89534, val_acc=0.76190, time=0.12900
Epoch:0013, train_loss=1.37483, train_acc=0.86936, val_loss=1.89418, val_acc=0.76720, time=0.12499
Epoch:0014, train_loss=1.35587, train_acc=0.88225, val_loss=1.89311, val_acc=0.75661, time=0.12602
Epoch:0015, train_loss=1.33863, train_acc=0.88928, val_loss=1.89215, val_acc=0.75661, time=0.11400
Epoch:0016, train_loss=1.32312, train_acc=0.89279, val_loss=1.89133, val_acc=0.76190, time=0.13100
Epoch:0017, train_loss=1.30925, train_acc=0.90217, val_loss=1.89064, val_acc=0.77249, time=0.10000
Epoch:0018, train_loss=1.29675, train_acc=0.90920, val_loss=1.89008, val_acc=0.77778, time=0.12603
Epoch:0019, train_loss=1.28540, train_acc=0.91506, val_loss=1.88964, val_acc=0.77778, time=0.12900
Epoch:0020, train_loss=1.27506, train_acc=0.92091, val_loss=1.88932, val_acc=0.77778, time=0.12100
Epoch:0021, train_loss=1.26558, train_acc=0.92677, val_loss=1.88914, val_acc=0.77778, time=0.13100
Epoch:0022, train_loss=1.25678, train_acc=0.93263, val_loss=1.88908, val_acc=0.77778, time=0.12600
Epoch:0023, train_loss=1.24840, train_acc=0.93673, val_loss=1.88912, val_acc=0.77778, time=0.11901
Epoch:0024, train_loss=1.24028, train_acc=0.94376, val_loss=1.88927, val_acc=0.76190, time=0.12101
Epoch:0025, train_loss=1.23251, train_acc=0.94962, val_loss=1.88952, val_acc=0.76720, time=0.12000
Epoch:0026, train_loss=1.22532, train_acc=0.95548, val_loss=1.88986, val_acc=0.76720, time=0.12900
Epoch:0027, train_loss=1.21888, train_acc=0.95841, val_loss=1.89027, val_acc=0.77778, time=0.13001
Epoch:0028, train_loss=1.21317, train_acc=0.96134, val_loss=1.89069, val_acc=0.77778, time=0.12601
Epoch:0029, train_loss=1.20801, train_acc=0.96368, val_loss=1.89106, val_acc=0.77778, time=0.12999
Epoch:0030, train_loss=1.20314, train_acc=0.96778, val_loss=1.89135, val_acc=0.77249, time=0.13001
Epoch:0031, train_loss=1.19842, train_acc=0.97422, val_loss=1.89154, val_acc=0.77778, time=0.13102
Epoch:0032, train_loss=1.19384, train_acc=0.97774, val_loss=1.89167, val_acc=0.77778, time=0.09701
Epoch:0033, train_loss=1.18949, train_acc=0.98067, val_loss=1.89175, val_acc=0.78307, time=0.11801
Epoch:0034, train_loss=1.18548, train_acc=0.98243, val_loss=1.89183, val_acc=0.78307, time=0.10400
Epoch:0035, train_loss=1.18185, train_acc=0.98535, val_loss=1.89193, val_acc=0.77778, time=0.11300
Epoch:0036, train_loss=1.17859, train_acc=0.98711, val_loss=1.89206, val_acc=0.76720, time=0.09701
Epoch:0037, train_loss=1.17562, train_acc=0.98770, val_loss=1.89223, val_acc=0.76720, time=0.08400
Epoch:0038, train_loss=1.17289, train_acc=0.98887, val_loss=1.89244, val_acc=0.76720, time=0.09901
Epoch:0039, train_loss=1.17033, train_acc=0.99063, val_loss=1.89267, val_acc=0.77249, time=0.11802
Epoch:0040, train_loss=1.16792, train_acc=0.99356, val_loss=1.89292, val_acc=0.77249, time=0.12900
Epoch:0041, train_loss=1.16563, train_acc=0.99531, val_loss=1.89317, val_acc=0.77778, time=0.10301
Epoch:0042, train_loss=1.16347, train_acc=0.99590, val_loss=1.89344, val_acc=0.77249, time=0.08501
Epoch:0043, train_loss=1.16143, train_acc=0.99707, val_loss=1.89370, val_acc=0.77778, time=0.11099
Epoch:0044, train_loss=1.15955, train_acc=0.99883, val_loss=1.89397, val_acc=0.77249, time=0.12901
Epoch:0045, train_loss=1.15782, train_acc=0.99883, val_loss=1.89423, val_acc=0.76720, time=0.10602
Epoch:0046, train_loss=1.15624, train_acc=0.99883, val_loss=1.89448, val_acc=0.77249, time=0.12201
Epoch:0047, train_loss=1.15478, train_acc=0.99883, val_loss=1.89472, val_acc=0.77249, time=0.08501
Epoch:0048, train_loss=1.15343, train_acc=0.99883, val_loss=1.89493, val_acc=0.76720, time=0.08499
Epoch:0049, train_loss=1.15215, train_acc=0.99883, val_loss=1.89512, val_acc=0.77249, time=0.11601
Epoch:0050, train_loss=1.15094, train_acc=0.99941, val_loss=1.89529, val_acc=0.76720, time=0.11001
Epoch:0051, train_loss=1.14980, train_acc=1.00000, val_loss=1.89544, val_acc=0.76720, time=0.11501
Epoch:0052, train_loss=1.14872, train_acc=1.00000, val_loss=1.89559, val_acc=0.75661, time=0.09601
Epoch:0053, train_loss=1.14771, train_acc=1.00000, val_loss=1.89574, val_acc=0.75661, time=0.09000
Epoch:0054, train_loss=1.14677, train_acc=1.00000, val_loss=1.89588, val_acc=0.76190, time=0.11000
Epoch:0055, train_loss=1.14590, train_acc=1.00000, val_loss=1.89604, val_acc=0.76190, time=0.09700
Epoch:0056, train_loss=1.14508, train_acc=1.00000, val_loss=1.89620, val_acc=0.75661, time=0.08500
Epoch:0057, train_loss=1.14433, train_acc=1.00000, val_loss=1.89638, val_acc=0.75661, time=0.08401
Epoch:0058, train_loss=1.14362, train_acc=1.00000, val_loss=1.89656, val_acc=0.75661, time=0.08300
Epoch:0059, train_loss=1.14295, train_acc=1.00000, val_loss=1.89675, val_acc=0.75661, time=0.10901
Epoch:0060, train_loss=1.14232, train_acc=1.00000, val_loss=1.89695, val_acc=0.75661, time=0.09902
Epoch:0061, train_loss=1.14172, train_acc=1.00000, val_loss=1.89715, val_acc=0.75661, time=0.08502
Epoch:0062, train_loss=1.14115, train_acc=1.00000, val_loss=1.89734, val_acc=0.75661, time=0.08400
Epoch:0063, train_loss=1.14061, train_acc=1.00000, val_loss=1.89753, val_acc=0.75661, time=0.09499
Epoch:0064, train_loss=1.14011, train_acc=1.00000, val_loss=1.89772, val_acc=0.75661, time=0.11800
Epoch:0065, train_loss=1.13963, train_acc=1.00000, val_loss=1.89790, val_acc=0.75132, time=0.11801
Epoch:0066, train_loss=1.13918, train_acc=1.00000, val_loss=1.89807, val_acc=0.75132, time=0.08401
Epoch:0067, train_loss=1.13876, train_acc=1.00000, val_loss=1.89824, val_acc=0.75132, time=0.12601
Epoch:0068, train_loss=1.13836, train_acc=1.00000, val_loss=1.89839, val_acc=0.75661, time=0.08500
Epoch:0069, train_loss=1.13798, train_acc=1.00000, val_loss=1.89854, val_acc=0.75661, time=0.12101
Epoch:0070, train_loss=1.13761, train_acc=1.00000, val_loss=1.89868, val_acc=0.75132, time=0.08402
Epoch:0071, train_loss=1.13727, train_acc=1.00000, val_loss=1.89882, val_acc=0.75132, time=0.10600
Epoch:0072, train_loss=1.13694, train_acc=1.00000, val_loss=1.89896, val_acc=0.75132, time=0.08401
Epoch:0073, train_loss=1.13663, train_acc=1.00000, val_loss=1.89910, val_acc=0.75132, time=0.10501
Epoch:0074, train_loss=1.13633, train_acc=1.00000, val_loss=1.89923, val_acc=0.75132, time=0.13000
Epoch:0075, train_loss=1.13605, train_acc=1.00000, val_loss=1.89937, val_acc=0.75132, time=0.10399
Epoch:0076, train_loss=1.13578, train_acc=1.00000, val_loss=1.89950, val_acc=0.75132, time=0.10000
Epoch:0077, train_loss=1.13552, train_acc=1.00000, val_loss=1.89963, val_acc=0.75132, time=0.12601
Epoch:0078, train_loss=1.13527, train_acc=1.00000, val_loss=1.89975, val_acc=0.75132, time=0.11701
Epoch:0079, train_loss=1.13503, train_acc=1.00000, val_loss=1.89988, val_acc=0.75661, time=0.12802
Epoch:0080, train_loss=1.13481, train_acc=1.00000, val_loss=1.90000, val_acc=0.75661, time=0.10301
Epoch:0081, train_loss=1.13459, train_acc=1.00000, val_loss=1.90012, val_acc=0.76190, time=0.08500
Epoch:0082, train_loss=1.13439, train_acc=1.00000, val_loss=1.90024, val_acc=0.76190, time=0.08501
Epoch:0083, train_loss=1.13419, train_acc=1.00000, val_loss=1.90035, val_acc=0.76190, time=0.12000
Epoch:0084, train_loss=1.13400, train_acc=1.00000, val_loss=1.90047, val_acc=0.76190, time=0.11401
Epoch:0085, train_loss=1.13381, train_acc=1.00000, val_loss=1.90059, val_acc=0.76190, time=0.08400
Epoch:0086, train_loss=1.13363, train_acc=1.00000, val_loss=1.90070, val_acc=0.75661, time=0.08600
Epoch:0087, train_loss=1.13346, train_acc=1.00000, val_loss=1.90082, val_acc=0.75661, time=0.10600
Epoch:0088, train_loss=1.13330, train_acc=1.00000, val_loss=1.90094, val_acc=0.75661, time=0.10202
Epoch:0089, train_loss=1.13314, train_acc=1.00000, val_loss=1.90106, val_acc=0.75661, time=0.12901
Epoch:0090, train_loss=1.13299, train_acc=1.00000, val_loss=1.90118, val_acc=0.75661, time=0.12800
Epoch:0091, train_loss=1.13285, train_acc=1.00000, val_loss=1.90129, val_acc=0.75661, time=0.12800
Epoch:0092, train_loss=1.13270, train_acc=1.00000, val_loss=1.90141, val_acc=0.75661, time=0.11701
Epoch:0093, train_loss=1.13257, train_acc=1.00000, val_loss=1.90152, val_acc=0.75661, time=0.10799
Epoch:0094, train_loss=1.13244, train_acc=1.00000, val_loss=1.90164, val_acc=0.75661, time=0.08801
Epoch:0095, train_loss=1.13231, train_acc=1.00000, val_loss=1.90174, val_acc=0.75661, time=0.08401
Epoch:0096, train_loss=1.13218, train_acc=1.00000, val_loss=1.90185, val_acc=0.75661, time=0.11499
Epoch:0097, train_loss=1.13207, train_acc=1.00000, val_loss=1.90196, val_acc=0.75661, time=0.13000
Epoch:0098, train_loss=1.13195, train_acc=1.00000, val_loss=1.90206, val_acc=0.75661, time=0.10700
Epoch:0099, train_loss=1.13184, train_acc=1.00000, val_loss=1.90216, val_acc=0.75132, time=0.11500
Epoch:0100, train_loss=1.13173, train_acc=1.00000, val_loss=1.90225, val_acc=0.74603, time=0.10201
Epoch:0101, train_loss=1.13163, train_acc=1.00000, val_loss=1.90235, val_acc=0.74074, time=0.11200
Epoch:0102, train_loss=1.13152, train_acc=1.00000, val_loss=1.90245, val_acc=0.74074, time=0.08700
Early stopping...

Optimization Finished!

Test set results: loss= 1.78886, accuracy= 0.70936, time= 0.02801

Test Precision, Recall and F1-Score...
              precision    recall  f1-score   support

           0     0.8438    0.7714    0.8060       140
           1     0.5000    0.5778    0.5361        45
           2     0.6791    0.7521    0.7137       121
           3     0.7079    0.6848    0.6961        92
           4     0.6364    0.6034    0.6195       116
           5     0.7925    0.6462    0.7119        65
           6     0.7154    0.7554    0.7349       233

    accuracy                         0.7094       812
   macro avg     0.6964    0.6844    0.6883       812
weighted avg     0.7142    0.7094    0.7102       812


Macro average Test Precision, Recall and F1-Score...
(0.6964261810250181, 0.6844317146410498, 0.6883012063309052, None)

Micro average Test Precision, Recall and F1-Score...
(0.7093596059113301, 0.7093596059113301, 0.7093596059113301, None)

Embeddings:
Word_embeddings:1343
Train_doc_embeddings:1896
Test_doc_embeddings:812
