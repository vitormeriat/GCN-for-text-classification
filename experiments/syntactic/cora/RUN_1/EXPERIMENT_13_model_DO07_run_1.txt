
==========: 298933446207700
Epoch:0001, train_loss=2.04747, train_acc=0.13064, val_loss=1.93706, val_acc=0.32804, time=0.12600
Epoch:0002, train_loss=1.88369, train_acc=0.30463, val_loss=1.93185, val_acc=0.36508, time=0.13101
Epoch:0003, train_loss=1.81532, train_acc=0.36145, val_loss=1.92643, val_acc=0.42857, time=0.12701
Epoch:0004, train_loss=1.73955, train_acc=0.46749, val_loss=1.92002, val_acc=0.53439, time=0.13300
Epoch:0005, train_loss=1.66388, train_acc=0.57645, val_loss=1.91339, val_acc=0.61376, time=0.13000
Epoch:0006, train_loss=1.59450, train_acc=0.69596, val_loss=1.90792, val_acc=0.65079, time=0.11901
Epoch:0007, train_loss=1.53953, train_acc=0.77153, val_loss=1.90405, val_acc=0.70899, time=0.11900
Epoch:0008, train_loss=1.49914, train_acc=0.80023, val_loss=1.90124, val_acc=0.75132, time=0.12100
Epoch:0009, train_loss=1.46644, train_acc=0.81019, val_loss=1.89890, val_acc=0.75132, time=0.10800
Epoch:0010, train_loss=1.43592, train_acc=0.82367, val_loss=1.89685, val_acc=0.76190, time=0.13001
Epoch:0011, train_loss=1.40674, train_acc=0.84124, val_loss=1.89515, val_acc=0.76720, time=0.11701
Epoch:0012, train_loss=1.38034, train_acc=0.85706, val_loss=1.89383, val_acc=0.76720, time=0.11902
Epoch:0013, train_loss=1.35785, train_acc=0.87522, val_loss=1.89287, val_acc=0.75132, time=0.12701
Epoch:0014, train_loss=1.33910, train_acc=0.88459, val_loss=1.89214, val_acc=0.75661, time=0.11401
Epoch:0015, train_loss=1.32311, train_acc=0.88928, val_loss=1.89158, val_acc=0.74603, time=0.12801
Epoch:0016, train_loss=1.30892, train_acc=0.89807, val_loss=1.89113, val_acc=0.74074, time=0.12900
Epoch:0017, train_loss=1.29594, train_acc=0.90803, val_loss=1.89077, val_acc=0.73545, time=0.12400
Epoch:0018, train_loss=1.28392, train_acc=0.91506, val_loss=1.89048, val_acc=0.71958, time=0.13000
Epoch:0019, train_loss=1.27272, train_acc=0.92443, val_loss=1.89022, val_acc=0.73016, time=0.12200
Epoch:0020, train_loss=1.26225, train_acc=0.92970, val_loss=1.89001, val_acc=0.72487, time=0.14000
Epoch:0021, train_loss=1.25252, train_acc=0.93380, val_loss=1.88985, val_acc=0.73016, time=0.12999
Epoch:0022, train_loss=1.24363, train_acc=0.93966, val_loss=1.88977, val_acc=0.73545, time=0.13200
Epoch:0023, train_loss=1.23562, train_acc=0.94259, val_loss=1.88976, val_acc=0.75132, time=0.12801
Epoch:0024, train_loss=1.22838, train_acc=0.95021, val_loss=1.88980, val_acc=0.75132, time=0.12200
Epoch:0025, train_loss=1.22174, train_acc=0.95138, val_loss=1.88988, val_acc=0.75132, time=0.12400
Epoch:0026, train_loss=1.21555, train_acc=0.95899, val_loss=1.88999, val_acc=0.75132, time=0.12901
Epoch:0027, train_loss=1.20972, train_acc=0.96485, val_loss=1.89015, val_acc=0.74603, time=0.12100
Early stopping...

Optimization Finished!

Test set results: loss= 1.72602, accuracy= 0.73892, time= 0.02999

Test Precision, Recall and F1-Score...
              precision    recall  f1-score   support

           0     0.8042    0.8214    0.8127       140
           1     0.7037    0.4222    0.5278        45
           2     0.7460    0.7769    0.7611       121
           3     0.7158    0.7391    0.7273        92
           4     0.6325    0.6379    0.6352       116
           5     0.8235    0.6462    0.7241        65
           6     0.7431    0.8069    0.7737       233

    accuracy                         0.7389       812
   macro avg     0.7384    0.6929    0.7088       812
weighted avg     0.7394    0.7389    0.7359       812


Macro average Test Precision, Recall and F1-Score...
(0.7384016822587675, 0.6929417951417056, 0.7088426531239901, None)

Micro average Test Precision, Recall and F1-Score...
(0.7389162561576355, 0.7389162561576355, 0.7389162561576355, None)

Embeddings:
Word_embeddings:1343
Train_doc_embeddings:1896
Test_doc_embeddings:812
