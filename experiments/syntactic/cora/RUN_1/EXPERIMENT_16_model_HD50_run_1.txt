
==========: 298962379805700
Epoch:0001, train_loss=2.12337, train_acc=0.13533, val_loss=1.94562, val_acc=0.29101, time=0.12599
Epoch:0002, train_loss=1.94108, train_acc=0.29877, val_loss=1.93534, val_acc=0.33333, time=0.12601
Epoch:0003, train_loss=1.84102, train_acc=0.36438, val_loss=1.92780, val_acc=0.39683, time=0.11201
Epoch:0004, train_loss=1.76916, train_acc=0.46104, val_loss=1.92258, val_acc=0.47619, time=0.11499
Epoch:0005, train_loss=1.71928, train_acc=0.52021, val_loss=1.91723, val_acc=0.51852, time=0.12902
Epoch:0006, train_loss=1.66402, train_acc=0.55126, val_loss=1.91149, val_acc=0.56614, time=0.12402
Epoch:0007, train_loss=1.60115, train_acc=0.63562, val_loss=1.90691, val_acc=0.63492, time=0.12301
Epoch:0008, train_loss=1.54622, train_acc=0.70299, val_loss=1.90395, val_acc=0.68254, time=0.11399
Epoch:0009, train_loss=1.50460, train_acc=0.75806, val_loss=1.90219, val_acc=0.70899, time=0.12800
Epoch:0010, train_loss=1.47367, train_acc=0.78852, val_loss=1.90110, val_acc=0.73545, time=0.11701
Epoch:0011, train_loss=1.44959, train_acc=0.81781, val_loss=1.90018, val_acc=0.75132, time=0.13000
Epoch:0012, train_loss=1.42865, train_acc=0.82601, val_loss=1.89908, val_acc=0.75132, time=0.12102
Epoch:0013, train_loss=1.40803, train_acc=0.83890, val_loss=1.89765, val_acc=0.74074, time=0.12900
Epoch:0014, train_loss=1.38665, train_acc=0.85530, val_loss=1.89600, val_acc=0.74603, time=0.12001
Epoch:0015, train_loss=1.36534, train_acc=0.86936, val_loss=1.89436, val_acc=0.76190, time=0.12000
Epoch:0016, train_loss=1.34566, train_acc=0.88049, val_loss=1.89296, val_acc=0.78836, time=0.12399
Epoch:0017, train_loss=1.32888, train_acc=0.89104, val_loss=1.89189, val_acc=0.79365, time=0.12899
Epoch:0018, train_loss=1.31524, train_acc=0.89162, val_loss=1.89116, val_acc=0.78307, time=0.11800
Epoch:0019, train_loss=1.30410, train_acc=0.89748, val_loss=1.89068, val_acc=0.77249, time=0.13101
Epoch:0020, train_loss=1.29437, train_acc=0.89924, val_loss=1.89035, val_acc=0.78836, time=0.12500
Epoch:0021, train_loss=1.28512, train_acc=0.90451, val_loss=1.89011, val_acc=0.79365, time=0.13399
Epoch:0022, train_loss=1.27582, train_acc=0.91037, val_loss=1.88994, val_acc=0.78307, time=0.11202
Epoch:0023, train_loss=1.26639, train_acc=0.92267, val_loss=1.88984, val_acc=0.78307, time=0.12901
Epoch:0024, train_loss=1.25709, train_acc=0.92970, val_loss=1.88983, val_acc=0.78836, time=0.12600
Epoch:0025, train_loss=1.24827, train_acc=0.93849, val_loss=1.88994, val_acc=0.78836, time=0.10903
Epoch:0026, train_loss=1.24024, train_acc=0.94025, val_loss=1.89015, val_acc=0.78836, time=0.12400
Epoch:0027, train_loss=1.23313, train_acc=0.94728, val_loss=1.89045, val_acc=0.78307, time=0.13400
Early stopping...

Optimization Finished!

Test set results: loss= 1.72790, accuracy= 0.72537, time= 0.03001

Test Precision, Recall and F1-Score...
              precision    recall  f1-score   support

           0     0.8140    0.7500    0.7807       140
           1     0.6279    0.6000    0.6136        45
           2     0.7244    0.7603    0.7419       121
           3     0.6505    0.7283    0.6872        92
           4     0.6759    0.6293    0.6518       116
           5     0.8571    0.6462    0.7368        65
           6     0.7233    0.7854    0.7531       233

    accuracy                         0.7254       812
   macro avg     0.7247    0.6999    0.7093       812
weighted avg     0.7295    0.7254    0.7252       812


Macro average Test Precision, Recall and F1-Score...
(0.7247348988571328, 0.699923337768705, 0.7093049598528843, None)

Micro average Test Precision, Recall and F1-Score...
(0.7253694581280788, 0.7253694581280788, 0.7253694581280788, None)

Embeddings:
Word_embeddings:1343
Train_doc_embeddings:1896
Test_doc_embeddings:812
