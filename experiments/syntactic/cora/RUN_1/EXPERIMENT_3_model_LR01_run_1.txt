
==========: 298336209381300
Epoch:0001, train_loss=1.97758, train_acc=0.25308, val_loss=2.00734, val_acc=0.28042, time=0.12001
Epoch:0002, train_loss=2.40910, train_acc=0.31927, val_loss=1.92220, val_acc=0.49735, time=0.13100
Epoch:0003, train_loss=1.65157, train_acc=0.55946, val_loss=1.92026, val_acc=0.52381, time=0.13199
Epoch:0004, train_loss=1.62572, train_acc=0.59520, val_loss=1.92483, val_acc=0.53968, time=0.10001
Epoch:0005, train_loss=1.63851, train_acc=0.59461, val_loss=1.91209, val_acc=0.57143, time=0.12900
Epoch:0006, train_loss=1.49528, train_acc=0.70064, val_loss=1.90104, val_acc=0.67196, time=0.12899
Epoch:0007, train_loss=1.37630, train_acc=0.81078, val_loss=1.89793, val_acc=0.70899, time=0.12701
Epoch:0008, train_loss=1.32777, train_acc=0.85647, val_loss=1.89939, val_acc=0.70899, time=0.12800
Epoch:0009, train_loss=1.31850, train_acc=0.85764, val_loss=1.90118, val_acc=0.68254, time=0.12000
Epoch:0010, train_loss=1.31573, train_acc=0.85296, val_loss=1.90136, val_acc=0.68783, time=0.11400
Epoch:0011, train_loss=1.30378, train_acc=0.85706, val_loss=1.90019, val_acc=0.72487, time=0.12102
Epoch:0012, train_loss=1.28362, train_acc=0.88166, val_loss=1.89871, val_acc=0.73545, time=0.12100
Epoch:0013, train_loss=1.26278, train_acc=0.90568, val_loss=1.89755, val_acc=0.74074, time=0.12702
Epoch:0014, train_loss=1.24579, train_acc=0.92150, val_loss=1.89666, val_acc=0.75661, time=0.10700
Epoch:0015, train_loss=1.23196, train_acc=0.92794, val_loss=1.89589, val_acc=0.74074, time=0.11201
Epoch:0016, train_loss=1.21953, train_acc=0.94318, val_loss=1.89528, val_acc=0.73545, time=0.12501
Epoch:0017, train_loss=1.20832, train_acc=0.95489, val_loss=1.89494, val_acc=0.73016, time=0.11101
Epoch:0018, train_loss=1.19904, train_acc=0.96544, val_loss=1.89494, val_acc=0.74074, time=0.11899
Epoch:0019, train_loss=1.19178, train_acc=0.96895, val_loss=1.89526, val_acc=0.74603, time=0.12802
Epoch:0020, train_loss=1.18593, train_acc=0.97071, val_loss=1.89580, val_acc=0.72487, time=0.11300
Epoch:0021, train_loss=1.18085, train_acc=0.97715, val_loss=1.89651, val_acc=0.71958, time=0.12902
Epoch:0022, train_loss=1.17622, train_acc=0.97832, val_loss=1.89734, val_acc=0.71429, time=0.11099
Early stopping...

Optimization Finished!

Test set results: loss= 1.77125, accuracy= 0.68842, time= 0.03801

Test Precision, Recall and F1-Score...
              precision    recall  f1-score   support

           0     0.8387    0.7429    0.7879       140
           1     0.4118    0.6222    0.4956        45
           2     0.7524    0.6529    0.6991       121
           3     0.8333    0.5978    0.6962        92
           4     0.5528    0.5862    0.5690       116
           5     0.7736    0.6308    0.6949        65
           6     0.6740    0.7897    0.7273       233

    accuracy                         0.6884       812
   macro avg     0.6909    0.6604    0.6671       812
weighted avg     0.7083    0.6884    0.6920       812


Macro average Test Precision, Recall and F1-Score...
(0.69094453958919, 0.6603533874508233, 0.6671424604892664, None)

Micro average Test Precision, Recall and F1-Score...
(0.6884236453201971, 0.6884236453201971, 0.6884236453201971, None)

Embeddings:
Word_embeddings:1343
Train_doc_embeddings:1896
Test_doc_embeddings:812
