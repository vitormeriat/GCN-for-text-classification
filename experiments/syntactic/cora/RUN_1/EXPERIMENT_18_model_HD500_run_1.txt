
==========: 298981752938100
Epoch:0001, train_loss=2.02383, train_acc=0.19215, val_loss=1.94061, val_acc=0.32275, time=0.13101
Epoch:0002, train_loss=1.88602, train_acc=0.30814, val_loss=1.93368, val_acc=0.37037, time=0.11901
Epoch:0003, train_loss=1.82762, train_acc=0.37258, val_loss=1.92710, val_acc=0.48677, time=0.12401
Epoch:0004, train_loss=1.75997, train_acc=0.46397, val_loss=1.92064, val_acc=0.49735, time=0.12699
Epoch:0005, train_loss=1.68642, train_acc=0.57001, val_loss=1.91477, val_acc=0.60317, time=0.12901
Epoch:0006, train_loss=1.61659, train_acc=0.67545, val_loss=1.91035, val_acc=0.66667, time=0.12701
Epoch:0007, train_loss=1.56098, train_acc=0.75044, val_loss=1.90748, val_acc=0.68783, time=0.13001
Epoch:0008, train_loss=1.52046, train_acc=0.77270, val_loss=1.90548, val_acc=0.67725, time=0.11799
Epoch:0009, train_loss=1.48857, train_acc=0.78559, val_loss=1.90359, val_acc=0.66667, time=0.14600
Epoch:0010, train_loss=1.45921, train_acc=0.79496, val_loss=1.90151, val_acc=0.68783, time=0.11501
Epoch:0011, train_loss=1.43036, train_acc=0.81429, val_loss=1.89931, val_acc=0.69312, time=0.11901
Epoch:0012, train_loss=1.40288, train_acc=0.83773, val_loss=1.89718, val_acc=0.70370, time=0.12399
Epoch:0013, train_loss=1.37816, train_acc=0.85354, val_loss=1.89530, val_acc=0.71429, time=0.12900
Epoch:0014, train_loss=1.35715, train_acc=0.87053, val_loss=1.89377, val_acc=0.74603, time=0.12201
Epoch:0015, train_loss=1.33991, train_acc=0.88225, val_loss=1.89260, val_acc=0.76720, time=0.12000
Epoch:0016, train_loss=1.32563, train_acc=0.89104, val_loss=1.89175, val_acc=0.77249, time=0.12602
Epoch:0017, train_loss=1.31311, train_acc=0.89572, val_loss=1.89111, val_acc=0.76720, time=0.12901
Epoch:0018, train_loss=1.30119, train_acc=0.90393, val_loss=1.89060, val_acc=0.77249, time=0.12999
Epoch:0019, train_loss=1.28932, train_acc=0.90920, val_loss=1.89023, val_acc=0.76720, time=0.11200
Epoch:0020, train_loss=1.27759, train_acc=0.91974, val_loss=1.89002, val_acc=0.77249, time=0.10499
Epoch:0021, train_loss=1.26649, train_acc=0.92501, val_loss=1.88997, val_acc=0.75661, time=0.11299
Epoch:0022, train_loss=1.25654, train_acc=0.93263, val_loss=1.89007, val_acc=0.76190, time=0.11901
Epoch:0023, train_loss=1.24789, train_acc=0.93732, val_loss=1.89025, val_acc=0.75132, time=0.11900
Epoch:0024, train_loss=1.24032, train_acc=0.94025, val_loss=1.89041, val_acc=0.75132, time=0.13100
Epoch:0025, train_loss=1.23345, train_acc=0.94259, val_loss=1.89050, val_acc=0.76190, time=0.12201
Epoch:0026, train_loss=1.22694, train_acc=0.95021, val_loss=1.89049, val_acc=0.76190, time=0.12499
Early stopping...

Optimization Finished!

Test set results: loss= 1.72729, accuracy= 0.71798, time= 0.03901

Test Precision, Recall and F1-Score...
              precision    recall  f1-score   support

           0     0.8000    0.7429    0.7704       140
           1     0.5349    0.5111    0.5227        45
           2     0.6809    0.7934    0.7328       121
           3     0.7750    0.6739    0.7209        92
           4     0.6396    0.6121    0.6256       116
           5     0.7963    0.6615    0.7227        65
           6     0.7273    0.7897    0.7572       233

    accuracy                         0.7180       812
   macro avg     0.7077    0.6835    0.6932       812
weighted avg     0.7206    0.7180    0.7172       812


Macro average Test Precision, Recall and F1-Score...
(0.707706206852669, 0.6835109607242477, 0.6931848122357768, None)

Micro average Test Precision, Recall and F1-Score...
(0.7179802955665024, 0.7179802955665024, 0.7179802955665024, None)

Embeddings:
Word_embeddings:1343
Train_doc_embeddings:1896
Test_doc_embeddings:812
