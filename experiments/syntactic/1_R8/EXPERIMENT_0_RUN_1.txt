
==========: 106329424732100
Epoch:0001, train_loss=2.22890, train_acc=0.03869, val_loss=2.06405, val_acc=0.54927, time=1.43908
Epoch:0002, train_loss=1.94051, train_acc=0.52420, val_loss=2.04721, val_acc=0.67336, time=1.34505
Epoch:0003, train_loss=1.79271, train_acc=0.67025, val_loss=2.03802, val_acc=0.73358, time=1.62509
Epoch:0004, train_loss=1.71039, train_acc=0.73891, val_loss=2.03203, val_acc=0.77190, time=1.47708
Epoch:0005, train_loss=1.65452, train_acc=0.77760, val_loss=2.02746, val_acc=0.80474, time=1.40508
Epoch:0006, train_loss=1.60944, train_acc=0.81710, val_loss=2.02389, val_acc=0.83759, time=1.36708
Epoch:0007, train_loss=1.57246, train_acc=0.86024, val_loss=2.02124, val_acc=0.86314, time=1.33507
Epoch:0008, train_loss=1.54449, train_acc=0.90237, val_loss=2.01927, val_acc=0.88686, time=1.33006
Epoch:0009, train_loss=1.52425, train_acc=0.93174, val_loss=2.01772, val_acc=0.89964, time=1.32907
Epoch:0010, train_loss=1.50923, train_acc=0.94835, val_loss=2.01647, val_acc=0.91606, time=1.51908
Epoch:0011, train_loss=1.49784, train_acc=0.95665, val_loss=2.01547, val_acc=0.92336, time=1.77110
Epoch:0012, train_loss=1.48916, train_acc=0.96091, val_loss=2.01467, val_acc=0.93066, time=1.62309
Epoch:0013, train_loss=1.48227, train_acc=0.96354, val_loss=2.01401, val_acc=0.93431, time=1.40707
Epoch:0014, train_loss=1.47633, train_acc=0.96536, val_loss=2.01347, val_acc=0.93978, time=1.62610
Epoch:0015, train_loss=1.47084, train_acc=0.96739, val_loss=2.01301, val_acc=0.93796, time=1.43409
Epoch:0016, train_loss=1.46572, train_acc=0.96921, val_loss=2.01265, val_acc=0.93613, time=1.46408
Epoch:0017, train_loss=1.46105, train_acc=0.97043, val_loss=2.01236, val_acc=0.93613, time=1.52709
Epoch:0018, train_loss=1.45693, train_acc=0.97347, val_loss=2.01212, val_acc=0.93613, time=1.52907
Epoch:0019, train_loss=1.45332, train_acc=0.97407, val_loss=2.01193, val_acc=0.93978, time=1.48108
Epoch:0020, train_loss=1.45016, train_acc=0.97407, val_loss=2.01176, val_acc=0.94526, time=1.43108
Epoch:0021, train_loss=1.44734, train_acc=0.97590, val_loss=2.01161, val_acc=0.94708, time=1.37507
Epoch:0022, train_loss=1.44481, train_acc=0.97731, val_loss=2.01147, val_acc=0.94708, time=1.41108
Epoch:0023, train_loss=1.44254, train_acc=0.97954, val_loss=2.01134, val_acc=0.94708, time=1.35108
Epoch:0024, train_loss=1.44049, train_acc=0.98015, val_loss=2.01122, val_acc=0.94891, time=1.41010
Epoch:0025, train_loss=1.43861, train_acc=0.98137, val_loss=2.01111, val_acc=0.94891, time=1.43007
Epoch:0026, train_loss=1.43687, train_acc=0.98258, val_loss=2.01101, val_acc=0.95073, time=1.34408
Epoch:0027, train_loss=1.43521, train_acc=0.98339, val_loss=2.01092, val_acc=0.95255, time=1.34107
Epoch:0028, train_loss=1.43362, train_acc=0.98461, val_loss=2.01083, val_acc=0.95255, time=1.40507
Epoch:0029, train_loss=1.43210, train_acc=0.98623, val_loss=2.01075, val_acc=0.95438, time=1.40508
Epoch:0030, train_loss=1.43065, train_acc=0.98724, val_loss=2.01068, val_acc=0.95438, time=1.41309
Epoch:0031, train_loss=1.42928, train_acc=0.98724, val_loss=2.01062, val_acc=0.95620, time=1.38808
Epoch:0032, train_loss=1.42802, train_acc=0.98785, val_loss=2.01056, val_acc=0.95620, time=1.37908
Epoch:0033, train_loss=1.42687, train_acc=0.98845, val_loss=2.01051, val_acc=0.95803, time=1.40508
Epoch:0034, train_loss=1.42582, train_acc=0.98967, val_loss=2.01047, val_acc=0.95803, time=1.42509
Epoch:0035, train_loss=1.42487, train_acc=0.99068, val_loss=2.01042, val_acc=0.95803, time=1.36706
Epoch:0036, train_loss=1.42401, train_acc=0.99109, val_loss=2.01037, val_acc=0.95803, time=1.40708
Epoch:0037, train_loss=1.42323, train_acc=0.99170, val_loss=2.01032, val_acc=0.95803, time=1.30909
Epoch:0038, train_loss=1.42253, train_acc=0.99291, val_loss=2.01028, val_acc=0.95985, time=1.42708
Epoch:0039, train_loss=1.42191, train_acc=0.99332, val_loss=2.01024, val_acc=0.95985, time=1.37207
Epoch:0040, train_loss=1.42136, train_acc=0.99352, val_loss=2.01021, val_acc=0.95985, time=1.32907
Epoch:0041, train_loss=1.42087, train_acc=0.99433, val_loss=2.01018, val_acc=0.95985, time=1.51109
Epoch:0042, train_loss=1.42043, train_acc=0.99494, val_loss=2.01017, val_acc=0.95985, time=1.40608
Epoch:0043, train_loss=1.42004, train_acc=0.99494, val_loss=2.01016, val_acc=0.96168, time=1.32108
Epoch:0044, train_loss=1.41967, train_acc=0.99534, val_loss=2.01016, val_acc=0.96168, time=1.33508
Epoch:0045, train_loss=1.41934, train_acc=0.99554, val_loss=2.01017, val_acc=0.96168, time=1.34507
Epoch:0046, train_loss=1.41903, train_acc=0.99575, val_loss=2.01018, val_acc=0.96168, time=1.43509
Epoch:0047, train_loss=1.41874, train_acc=0.99635, val_loss=2.01020, val_acc=0.95985, time=1.39107
Epoch:0048, train_loss=1.41847, train_acc=0.99676, val_loss=2.01021, val_acc=0.95803, time=1.44708
Early stopping...

Optimization Finished!

Test set results: loss= 1.80343, accuracy= 0.95660, time= 0.44502

Test Precision, Recall and F1-Score...
              precision    recall  f1-score   support

           0     0.9692    0.9871    0.9780      1083
           1     0.9792    0.9483    0.9635       696
           2     0.8696    0.9195    0.8939        87
           3     0.8992    0.9587    0.9280       121
           4     0.9231    0.6667    0.7742        36
           5     0.8750    0.9333    0.9032        75
           6     0.8904    0.8025    0.8442        81
           7     0.8333    1.0000    0.9091        10

    accuracy                         0.9566      2189
   macro avg     0.9049    0.9020    0.8993      2189
weighted avg     0.9570    0.9566    0.9561      2189


Macro average Test Precision, Recall and F1-Score...
(0.904876837861081, 0.9020044824035756, 0.8992583240407102, None)

Micro average Test Precision, Recall and F1-Score...
(0.9566011877569667, 0.9566011877569667, 0.9566011877569667, None)

Embeddings:
Word_embeddings:7688
Train_doc_embeddings:5485
Test_doc_embeddings:2189
