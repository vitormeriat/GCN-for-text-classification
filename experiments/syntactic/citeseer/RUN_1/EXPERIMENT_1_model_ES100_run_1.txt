
==========: 304445184488200
Epoch:0001, train_loss=1.79711, train_acc=0.14990, val_loss=1.79259, val_acc=0.18182, time=0.29198
Epoch:0002, train_loss=1.77681, train_acc=0.22557, val_loss=1.79341, val_acc=0.21212, time=0.30200
Epoch:0003, train_loss=1.77208, train_acc=0.22989, val_loss=1.79407, val_acc=0.17316, time=0.33102
Epoch:0004, train_loss=1.76815, train_acc=0.27826, val_loss=1.79423, val_acc=0.17749, time=0.33401
Epoch:0005, train_loss=1.76140, train_acc=0.37548, val_loss=1.79404, val_acc=0.17316, time=0.27398
Epoch:0006, train_loss=1.75332, train_acc=0.38554, val_loss=1.79363, val_acc=0.18615, time=0.29700
Epoch:0007, train_loss=1.74504, train_acc=0.39176, val_loss=1.79312, val_acc=0.18615, time=0.27301
Epoch:0008, train_loss=1.73696, train_acc=0.40996, val_loss=1.79269, val_acc=0.19481, time=0.27499
Epoch:0009, train_loss=1.72968, train_acc=0.41858, val_loss=1.79246, val_acc=0.19481, time=0.32304
Epoch:0010, train_loss=1.72311, train_acc=0.44875, val_loss=1.79242, val_acc=0.19048, time=0.30599
Epoch:0011, train_loss=1.71662, train_acc=0.49521, val_loss=1.79255, val_acc=0.18615, time=0.30600
Epoch:0012, train_loss=1.70985, train_acc=0.52203, val_loss=1.79279, val_acc=0.16450, time=0.31401
Epoch:0013, train_loss=1.70270, train_acc=0.52443, val_loss=1.79310, val_acc=0.17316, time=0.28600
Epoch:0014, train_loss=1.69504, train_acc=0.52778, val_loss=1.79340, val_acc=0.18615, time=0.29701
Epoch:0015, train_loss=1.68682, train_acc=0.54454, val_loss=1.79375, val_acc=0.18615, time=0.21799
Epoch:0016, train_loss=1.67859, train_acc=0.55316, val_loss=1.79422, val_acc=0.18615, time=0.32503
Epoch:0017, train_loss=1.67070, train_acc=0.56034, val_loss=1.79480, val_acc=0.17749, time=0.25899
Epoch:0018, train_loss=1.66282, train_acc=0.57423, val_loss=1.79547, val_acc=0.19048, time=0.25401
Epoch:0019, train_loss=1.65501, train_acc=0.58573, val_loss=1.79609, val_acc=0.18615, time=0.21000
Epoch:0020, train_loss=1.64725, train_acc=0.59339, val_loss=1.79651, val_acc=0.18182, time=0.27801
Epoch:0021, train_loss=1.63896, train_acc=0.60057, val_loss=1.79681, val_acc=0.19048, time=0.25599
Epoch:0022, train_loss=1.63058, train_acc=0.60105, val_loss=1.79714, val_acc=0.19048, time=0.29801
Epoch:0023, train_loss=1.62211, train_acc=0.61207, val_loss=1.79759, val_acc=0.19048, time=0.27001
Epoch:0024, train_loss=1.61380, train_acc=0.62356, val_loss=1.79810, val_acc=0.18182, time=0.32801
Epoch:0025, train_loss=1.60595, train_acc=0.62979, val_loss=1.79862, val_acc=0.19048, time=0.25500
Epoch:0026, train_loss=1.59795, train_acc=0.64320, val_loss=1.79929, val_acc=0.19481, time=0.21000
Epoch:0027, train_loss=1.58977, train_acc=0.65086, val_loss=1.80017, val_acc=0.19481, time=0.26600
Epoch:0028, train_loss=1.58127, train_acc=0.66379, val_loss=1.80115, val_acc=0.18182, time=0.26501
Epoch:0029, train_loss=1.57317, train_acc=0.67577, val_loss=1.80204, val_acc=0.18615, time=0.31399
Epoch:0030, train_loss=1.56530, train_acc=0.68487, val_loss=1.80291, val_acc=0.19048, time=0.25200
Epoch:0031, train_loss=1.55750, train_acc=0.69205, val_loss=1.80373, val_acc=0.18182, time=0.26700
Epoch:0032, train_loss=1.54945, train_acc=0.70259, val_loss=1.80437, val_acc=0.18182, time=0.26202
Epoch:0033, train_loss=1.54143, train_acc=0.71456, val_loss=1.80502, val_acc=0.17749, time=0.28299
Epoch:0034, train_loss=1.53379, train_acc=0.72222, val_loss=1.80593, val_acc=0.17316, time=0.28701
Epoch:0035, train_loss=1.52626, train_acc=0.73324, val_loss=1.80691, val_acc=0.17316, time=0.23599
Epoch:0036, train_loss=1.51856, train_acc=0.73994, val_loss=1.80793, val_acc=0.17749, time=0.20602
Epoch:0037, train_loss=1.51093, train_acc=0.75096, val_loss=1.80912, val_acc=0.17749, time=0.27400
Epoch:0038, train_loss=1.50356, train_acc=0.76102, val_loss=1.81020, val_acc=0.17316, time=0.24200
Epoch:0039, train_loss=1.49631, train_acc=0.76964, val_loss=1.81106, val_acc=0.17749, time=0.27200
Epoch:0040, train_loss=1.48896, train_acc=0.78017, val_loss=1.81201, val_acc=0.18182, time=0.23101
Epoch:0041, train_loss=1.48175, train_acc=0.79310, val_loss=1.81296, val_acc=0.17749, time=0.30801
Epoch:0042, train_loss=1.47481, train_acc=0.80651, val_loss=1.81403, val_acc=0.17316, time=0.24800
Epoch:0043, train_loss=1.46784, train_acc=0.81082, val_loss=1.81533, val_acc=0.17316, time=0.29001
Epoch:0044, train_loss=1.46094, train_acc=0.82088, val_loss=1.81653, val_acc=0.17316, time=0.28000
Epoch:0045, train_loss=1.45430, train_acc=0.82567, val_loss=1.81769, val_acc=0.17316, time=0.26300
Epoch:0046, train_loss=1.44771, train_acc=0.83477, val_loss=1.81872, val_acc=0.16883, time=0.27500
Epoch:0047, train_loss=1.44116, train_acc=0.84818, val_loss=1.81972, val_acc=0.16883, time=0.24002
Epoch:0048, train_loss=1.43488, train_acc=0.86015, val_loss=1.82099, val_acc=0.16883, time=0.26800
Epoch:0049, train_loss=1.42870, train_acc=0.86734, val_loss=1.82212, val_acc=0.16883, time=0.25802
Epoch:0050, train_loss=1.42261, train_acc=0.87787, val_loss=1.82378, val_acc=0.16450, time=0.26400
Epoch:0051, train_loss=1.41693, train_acc=0.88266, val_loss=1.82450, val_acc=0.16450, time=0.23700
Epoch:0052, train_loss=1.41205, train_acc=0.88649, val_loss=1.82654, val_acc=0.16883, time=0.21201
Epoch:0053, train_loss=1.40786, train_acc=0.88889, val_loss=1.82659, val_acc=0.16450, time=0.29100
Epoch:0054, train_loss=1.40283, train_acc=0.90086, val_loss=1.82778, val_acc=0.16450, time=0.20900
Epoch:0055, train_loss=1.39632, train_acc=0.91619, val_loss=1.82970, val_acc=0.17316, time=0.26199
Epoch:0056, train_loss=1.39338, train_acc=0.90661, val_loss=1.82985, val_acc=0.16883, time=0.25500
Epoch:0057, train_loss=1.38878, train_acc=0.91858, val_loss=1.83092, val_acc=0.16450, time=0.25402
Epoch:0058, train_loss=1.38346, train_acc=0.92481, val_loss=1.83277, val_acc=0.17316, time=0.27201
Epoch:0059, train_loss=1.38125, train_acc=0.92481, val_loss=1.83274, val_acc=0.16450, time=0.25101
Epoch:0060, train_loss=1.37577, train_acc=0.93678, val_loss=1.83369, val_acc=0.17316, time=0.20901
Epoch:0061, train_loss=1.37276, train_acc=0.93918, val_loss=1.83543, val_acc=0.17316, time=0.26198
Epoch:0062, train_loss=1.36899, train_acc=0.94684, val_loss=1.83624, val_acc=0.17749, time=0.30901
Epoch:0063, train_loss=1.36502, train_acc=0.94875, val_loss=1.83680, val_acc=0.17749, time=0.28800
Epoch:0064, train_loss=1.36233, train_acc=0.95019, val_loss=1.83790, val_acc=0.17749, time=0.32801
Epoch:0065, train_loss=1.35814, train_acc=0.95690, val_loss=1.83930, val_acc=0.17749, time=0.30100
Epoch:0066, train_loss=1.35587, train_acc=0.95929, val_loss=1.83981, val_acc=0.17749, time=0.25002
Epoch:0067, train_loss=1.35198, train_acc=0.96408, val_loss=1.84077, val_acc=0.18182, time=0.22199
Epoch:0068, train_loss=1.34942, train_acc=0.96552, val_loss=1.84218, val_acc=0.18182, time=0.30699
Epoch:0069, train_loss=1.34623, train_acc=0.96935, val_loss=1.84316, val_acc=0.18615, time=0.26900
Epoch:0070, train_loss=1.34341, train_acc=0.97126, val_loss=1.84367, val_acc=0.19048, time=0.31700
Epoch:0071, train_loss=1.34079, train_acc=0.97270, val_loss=1.84473, val_acc=0.18182, time=0.26302
Epoch:0072, train_loss=1.33783, train_acc=0.97414, val_loss=1.84614, val_acc=0.18182, time=0.31800
Epoch:0073, train_loss=1.33556, train_acc=0.97701, val_loss=1.84688, val_acc=0.18182, time=0.32601
Epoch:0074, train_loss=1.33267, train_acc=0.97893, val_loss=1.84772, val_acc=0.18182, time=0.30601
Epoch:0075, train_loss=1.33054, train_acc=0.97845, val_loss=1.84886, val_acc=0.18182, time=0.23400
Epoch:0076, train_loss=1.32786, train_acc=0.98515, val_loss=1.84992, val_acc=0.18615, time=0.23401
Epoch:0077, train_loss=1.32577, train_acc=0.98467, val_loss=1.85063, val_acc=0.18182, time=0.24301
Epoch:0078, train_loss=1.32340, train_acc=0.98851, val_loss=1.85163, val_acc=0.18182, time=0.27401
Epoch:0079, train_loss=1.32127, train_acc=0.98898, val_loss=1.85289, val_acc=0.18182, time=0.28200
Epoch:0080, train_loss=1.31918, train_acc=0.98994, val_loss=1.85378, val_acc=0.18182, time=0.28500
Epoch:0081, train_loss=1.31705, train_acc=0.99042, val_loss=1.85449, val_acc=0.17749, time=0.24701
Epoch:0082, train_loss=1.31518, train_acc=0.98946, val_loss=1.85554, val_acc=0.17749, time=0.24801
Epoch:0083, train_loss=1.31314, train_acc=0.99138, val_loss=1.85670, val_acc=0.17749, time=0.21199
Epoch:0084, train_loss=1.31140, train_acc=0.99186, val_loss=1.85753, val_acc=0.17749, time=0.31801
Epoch:0085, train_loss=1.30948, train_acc=0.99138, val_loss=1.85842, val_acc=0.17749, time=0.24601
Epoch:0086, train_loss=1.30783, train_acc=0.99186, val_loss=1.85947, val_acc=0.18182, time=0.28700
Epoch:0087, train_loss=1.30606, train_acc=0.99234, val_loss=1.86044, val_acc=0.18182, time=0.22002
Epoch:0088, train_loss=1.30447, train_acc=0.99282, val_loss=1.86123, val_acc=0.18182, time=0.23899
Epoch:0089, train_loss=1.30286, train_acc=0.99282, val_loss=1.86219, val_acc=0.17749, time=0.24099
Epoch:0090, train_loss=1.30133, train_acc=0.99377, val_loss=1.86330, val_acc=0.18182, time=0.30300
Epoch:0091, train_loss=1.29985, train_acc=0.99569, val_loss=1.86418, val_acc=0.18182, time=0.28201
Epoch:0092, train_loss=1.29839, train_acc=0.99521, val_loss=1.86497, val_acc=0.17749, time=0.26699
Epoch:0093, train_loss=1.29702, train_acc=0.99425, val_loss=1.86593, val_acc=0.17749, time=0.22703
Epoch:0094, train_loss=1.29564, train_acc=0.99521, val_loss=1.86696, val_acc=0.17749, time=0.24799
Epoch:0095, train_loss=1.29437, train_acc=0.99569, val_loss=1.86783, val_acc=0.17316, time=0.28101
Epoch:0096, train_loss=1.29307, train_acc=0.99569, val_loss=1.86869, val_acc=0.17316, time=0.23401
Epoch:0097, train_loss=1.29188, train_acc=0.99473, val_loss=1.86965, val_acc=0.17316, time=0.20901
Epoch:0098, train_loss=1.29067, train_acc=0.99617, val_loss=1.87059, val_acc=0.17316, time=0.23700
Epoch:0099, train_loss=1.28955, train_acc=0.99617, val_loss=1.87139, val_acc=0.17316, time=0.26799
Epoch:0100, train_loss=1.28842, train_acc=0.99569, val_loss=1.87228, val_acc=0.17316, time=0.29800
Epoch:0101, train_loss=1.28736, train_acc=0.99617, val_loss=1.87327, val_acc=0.17316, time=0.20700
Epoch:0102, train_loss=1.28632, train_acc=0.99665, val_loss=1.87415, val_acc=0.17316, time=0.30101
Early stopping...

Optimization Finished!

Test set results: loss= 2.12470, accuracy= 0.17321, time= 0.06102

Test Precision, Recall and F1-Score...
              precision    recall  f1-score   support

           0     0.1371    0.1270    0.1319       189
           1     0.1500    0.1400    0.1448       150
           2     0.1990    0.1961    0.1975       204
           3     0.2026    0.2260    0.2136       208
           4     0.1771    0.1965    0.1863       173
           5     0.1132    0.0870    0.0984        69

    accuracy                         0.1732       993
   macro avg     0.1632    0.1621    0.1621       993
weighted avg     0.1708    0.1732    0.1716       993


Macro average Test Precision, Recall and F1-Score...
(0.1631708199444886, 0.1620854017441432, 0.16208749525160693, None)

Micro average Test Precision, Recall and F1-Score...
(0.1732124874118832, 0.1732124874118832, 0.1732124874118832, None)

Embeddings:
Word_embeddings:3515
Train_doc_embeddings:2319
Test_doc_embeddings:993
