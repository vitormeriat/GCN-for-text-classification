
==========: 304780820504900
Epoch:0001, train_loss=1.81541, train_acc=0.17337, val_loss=1.79325, val_acc=0.16883, time=0.30802
Epoch:0002, train_loss=1.81213, train_acc=0.17337, val_loss=1.79298, val_acc=0.16883, time=0.28799
Epoch:0003, train_loss=1.80903, train_acc=0.17337, val_loss=1.79273, val_acc=0.16883, time=0.32900
Epoch:0004, train_loss=1.80611, train_acc=0.17337, val_loss=1.79250, val_acc=0.16883, time=0.28200
Epoch:0005, train_loss=1.80337, train_acc=0.17433, val_loss=1.79229, val_acc=0.16883, time=0.33000
Epoch:0006, train_loss=1.80080, train_acc=0.17481, val_loss=1.79211, val_acc=0.16883, time=0.27802
Epoch:0007, train_loss=1.79841, train_acc=0.17481, val_loss=1.79194, val_acc=0.16883, time=0.32899
Epoch:0008, train_loss=1.79619, train_acc=0.17385, val_loss=1.79179, val_acc=0.15584, time=0.32501
Epoch:0009, train_loss=1.79412, train_acc=0.17337, val_loss=1.79166, val_acc=0.16017, time=0.30502
Epoch:0010, train_loss=1.79221, train_acc=0.17577, val_loss=1.79155, val_acc=0.15152, time=0.29802
Epoch:0011, train_loss=1.79045, train_acc=0.17193, val_loss=1.79145, val_acc=0.16883, time=0.25600
Epoch:0012, train_loss=1.78883, train_acc=0.17864, val_loss=1.79137, val_acc=0.18182, time=0.30801
Epoch:0013, train_loss=1.78733, train_acc=0.18870, val_loss=1.79130, val_acc=0.19481, time=0.30200
Epoch:0014, train_loss=1.78596, train_acc=0.19397, val_loss=1.79125, val_acc=0.19048, time=0.31900
Epoch:0015, train_loss=1.78470, train_acc=0.20546, val_loss=1.79121, val_acc=0.21212, time=0.31903
Epoch:0016, train_loss=1.78354, train_acc=0.21121, val_loss=1.79118, val_acc=0.19048, time=0.33100
Epoch:0017, train_loss=1.78247, train_acc=0.21887, val_loss=1.79116, val_acc=0.22511, time=0.33601
Epoch:0018, train_loss=1.78148, train_acc=0.22126, val_loss=1.79115, val_acc=0.23377, time=0.28501
Epoch:0019, train_loss=1.78057, train_acc=0.22079, val_loss=1.79115, val_acc=0.23810, time=0.28702
Epoch:0020, train_loss=1.77972, train_acc=0.22366, val_loss=1.79115, val_acc=0.24675, time=0.30700
Epoch:0021, train_loss=1.77893, train_acc=0.22701, val_loss=1.79116, val_acc=0.25108, time=0.32601
Epoch:0022, train_loss=1.77819, train_acc=0.22653, val_loss=1.79118, val_acc=0.24242, time=0.30399
Epoch:0023, train_loss=1.77750, train_acc=0.23036, val_loss=1.79120, val_acc=0.24675, time=0.26601
Early stopping...

Optimization Finished!

Test set results: loss= 1.78758, accuracy= 0.19940, time= 0.08500

Test Precision, Recall and F1-Score...
              precision    recall  f1-score   support

           0     0.1538    0.0635    0.0899       189
           1     0.0000    0.0000    0.0000       150
           2     0.1811    0.2157    0.1969       204
           3     0.2127    0.6442    0.3198       208
           4     0.2222    0.0462    0.0766       173
           5     0.0000    0.0000    0.0000        69

    accuracy                         0.1994       993
   macro avg     0.1283    0.1616    0.1139       993
weighted avg     0.1497    0.1994    0.1379       993


Macro average Test Precision, Recall and F1-Score...
(0.12830612460242088, 0.16160864696651844, 0.11385329042230659, None)

Micro average Test Precision, Recall and F1-Score...
(0.19939577039274925, 0.19939577039274925, 0.19939577039274925, None)

Embeddings:
Word_embeddings:3515
Train_doc_embeddings:2319
Test_doc_embeddings:993
