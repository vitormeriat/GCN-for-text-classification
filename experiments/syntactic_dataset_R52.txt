
==================== Torch Seed: 13196137664100
Epoch:0001, train_loss=4.05827, train_acc=0.01480, val_loss=3.92709, val_acc=0.37825, time=0.90601
Epoch:0002, train_loss=3.73059, train_acc=0.38306, val_loss=3.90000, val_acc=0.56508, time=0.81100
Epoch:0003, train_loss=3.47714, train_acc=0.56455, val_loss=3.88198, val_acc=0.60949, time=0.88200
Epoch:0004, train_loss=3.31021, train_acc=0.61813, val_loss=3.87108, val_acc=0.63093, time=1.02601
Epoch:0005, train_loss=3.20823, train_acc=0.64246, val_loss=3.86416, val_acc=0.65237, time=0.89102
Epoch:0006, train_loss=3.14084, train_acc=0.66899, val_loss=3.85872, val_acc=0.67994, time=0.84602
Epoch:0007, train_loss=3.08643, train_acc=0.69757, val_loss=3.85381, val_acc=0.70597, time=0.95600
Epoch:0008, train_loss=3.03778, train_acc=0.73312, val_loss=3.84942, val_acc=0.74732, time=1.02499
Epoch:0009, train_loss=2.99498, train_acc=0.76884, val_loss=3.84577, val_acc=0.77335, time=0.83001
Epoch:0010, train_loss=2.95967, train_acc=0.80150, val_loss=3.84300, val_acc=0.81776, time=0.92202
Epoch:0011, train_loss=2.93249, train_acc=0.83143, val_loss=3.84100, val_acc=0.83920, time=1.01501
Epoch:0012, train_loss=2.91174, train_acc=0.84946, val_loss=3.83946, val_acc=0.84380, time=1.04701
Epoch:0013, train_loss=2.89462, train_acc=0.86358, val_loss=3.83812, val_acc=0.84992, time=0.90401
Epoch:0014, train_loss=2.87925, train_acc=0.87515, val_loss=3.83688, val_acc=0.84839, time=1.05002
Epoch:0015, train_loss=2.86482, train_acc=0.88757, val_loss=3.83569, val_acc=0.85452, time=0.85801
Epoch:0016, train_loss=2.85118, train_acc=0.89692, val_loss=3.83453, val_acc=0.87289, time=0.84102
Epoch:0017, train_loss=2.83837, train_acc=0.90628, val_loss=3.83341, val_acc=0.87749, time=1.02301
Epoch:0018, train_loss=2.82646, train_acc=0.91495, val_loss=3.83234, val_acc=0.87902, time=1.00901
Epoch:0019, train_loss=2.81549, train_acc=0.92210, val_loss=3.83133, val_acc=0.88208, time=0.84902
Epoch:0020, train_loss=2.80550, train_acc=0.92839, val_loss=3.83040, val_acc=0.88668, time=0.87199
Epoch:0021, train_loss=2.79642, train_acc=0.93247, val_loss=3.82954, val_acc=0.88974, time=0.90602
Epoch:0022, train_loss=2.78816, train_acc=0.93655, val_loss=3.82875, val_acc=0.89587, time=1.00701
Epoch:0023, train_loss=2.78056, train_acc=0.93945, val_loss=3.82799, val_acc=0.90046, time=0.92302
Epoch:0024, train_loss=2.77341, train_acc=0.94336, val_loss=3.82726, val_acc=0.90505, time=1.11901
Epoch:0025, train_loss=2.76659, train_acc=0.94608, val_loss=3.82655, val_acc=0.90505, time=0.89001
Epoch:0026, train_loss=2.76003, train_acc=0.94897, val_loss=3.82587, val_acc=0.90505, time=0.85902
Epoch:0027, train_loss=2.75378, train_acc=0.95152, val_loss=3.82522, val_acc=0.90505, time=0.96801
Epoch:0028, train_loss=2.74791, train_acc=0.95441, val_loss=3.82462, val_acc=0.90199, time=0.93900
Epoch:0029, train_loss=2.74246, train_acc=0.95782, val_loss=3.82406, val_acc=0.90352, time=0.84802
Epoch:0030, train_loss=2.73746, train_acc=0.95901, val_loss=3.82356, val_acc=0.90812, time=0.95002
Epoch:0031, train_loss=2.73289, train_acc=0.96020, val_loss=3.82311, val_acc=0.90505, time=0.89200
Epoch:0032, train_loss=2.72874, train_acc=0.96190, val_loss=3.82270, val_acc=0.90658, time=0.82200
Epoch:0033, train_loss=2.72497, train_acc=0.96462, val_loss=3.82233, val_acc=0.90812, time=1.03702
Epoch:0034, train_loss=2.72154, train_acc=0.96666, val_loss=3.82199, val_acc=0.91271, time=1.05302
Epoch:0035, train_loss=2.71841, train_acc=0.96870, val_loss=3.82168, val_acc=0.91271, time=1.01701
Epoch:0036, train_loss=2.71554, train_acc=0.97125, val_loss=3.82140, val_acc=0.91271, time=1.06801
Epoch:0037, train_loss=2.71289, train_acc=0.97244, val_loss=3.82114, val_acc=0.91271, time=0.95903
Epoch:0038, train_loss=2.71041, train_acc=0.97500, val_loss=3.82090, val_acc=0.91577, time=0.96802
Epoch:0039, train_loss=2.70805, train_acc=0.97636, val_loss=3.82067, val_acc=0.91424, time=0.90500
Epoch:0040, train_loss=2.70579, train_acc=0.97772, val_loss=3.82045, val_acc=0.91577, time=0.86101
Epoch:0041, train_loss=2.70361, train_acc=0.97874, val_loss=3.82025, val_acc=0.92037, time=0.97702
Epoch:0042, train_loss=2.70151, train_acc=0.97976, val_loss=3.82005, val_acc=0.92343, time=0.90501
Epoch:0043, train_loss=2.69950, train_acc=0.98146, val_loss=3.81987, val_acc=0.92343, time=0.85801
Epoch:0044, train_loss=2.69760, train_acc=0.98333, val_loss=3.81970, val_acc=0.92343, time=1.04302
Epoch:0045, train_loss=2.69580, train_acc=0.98401, val_loss=3.81954, val_acc=0.92343, time=0.88001
Epoch:0046, train_loss=2.69413, train_acc=0.98452, val_loss=3.81940, val_acc=0.92496, time=0.91001
Epoch:0047, train_loss=2.69257, train_acc=0.98571, val_loss=3.81928, val_acc=0.92496, time=0.91501
Epoch:0048, train_loss=2.69113, train_acc=0.98690, val_loss=3.81917, val_acc=0.92496, time=0.84401
Epoch:0049, train_loss=2.68981, train_acc=0.98826, val_loss=3.81907, val_acc=0.92190, time=1.09701
Epoch:0050, train_loss=2.68858, train_acc=0.98877, val_loss=3.81898, val_acc=0.92343, time=0.90401
Epoch:0051, train_loss=2.68744, train_acc=0.98928, val_loss=3.81890, val_acc=0.92496, time=0.85102
Epoch:0052, train_loss=2.68637, train_acc=0.98962, val_loss=3.81882, val_acc=0.92649, time=0.84302
Epoch:0053, train_loss=2.68536, train_acc=0.99047, val_loss=3.81875, val_acc=0.92649, time=0.99101
Epoch:0054, train_loss=2.68441, train_acc=0.99064, val_loss=3.81868, val_acc=0.92649, time=0.97102
Epoch:0055, train_loss=2.68350, train_acc=0.99081, val_loss=3.81861, val_acc=0.92649, time=0.86901
Epoch:0056, train_loss=2.68263, train_acc=0.99098, val_loss=3.81854, val_acc=0.92496, time=1.01402
Epoch:0057, train_loss=2.68180, train_acc=0.99133, val_loss=3.81848, val_acc=0.92649, time=1.19997
Epoch:0058, train_loss=2.68102, train_acc=0.99218, val_loss=3.81842, val_acc=0.92649, time=0.90201
Epoch:0059, train_loss=2.68028, train_acc=0.99286, val_loss=3.81836, val_acc=0.92649, time=0.93601
Epoch:0060, train_loss=2.67957, train_acc=0.99320, val_loss=3.81831, val_acc=0.92649, time=0.91101
Epoch:0061, train_loss=2.67891, train_acc=0.99337, val_loss=3.81825, val_acc=0.92649, time=0.96201
Epoch:0062, train_loss=2.67828, train_acc=0.99388, val_loss=3.81821, val_acc=0.92649, time=0.81001
Epoch:0063, train_loss=2.67768, train_acc=0.99405, val_loss=3.81816, val_acc=0.92649, time=1.10301
Epoch:0064, train_loss=2.67712, train_acc=0.99388, val_loss=3.81812, val_acc=0.92649, time=0.87301
Epoch:0065, train_loss=2.67658, train_acc=0.99354, val_loss=3.81808, val_acc=0.92649, time=0.90101
Epoch:0066, train_loss=2.67607, train_acc=0.99371, val_loss=3.81804, val_acc=0.92649, time=1.05402
Epoch:0067, train_loss=2.67558, train_acc=0.99371, val_loss=3.81801, val_acc=0.92649, time=0.91802
Epoch:0068, train_loss=2.67511, train_acc=0.99388, val_loss=3.81797, val_acc=0.92649, time=0.86100
Epoch:0069, train_loss=2.67466, train_acc=0.99388, val_loss=3.81794, val_acc=0.92802, time=0.82203
Epoch:0070, train_loss=2.67423, train_acc=0.99405, val_loss=3.81791, val_acc=0.92956, time=0.90702
Epoch:0071, train_loss=2.67381, train_acc=0.99422, val_loss=3.81788, val_acc=0.93109, time=0.98802
Epoch:0072, train_loss=2.67342, train_acc=0.99439, val_loss=3.81785, val_acc=0.93109, time=0.99700
Epoch:0073, train_loss=2.67304, train_acc=0.99473, val_loss=3.81782, val_acc=0.93109, time=0.84502
Epoch:0074, train_loss=2.67267, train_acc=0.99473, val_loss=3.81779, val_acc=0.93109, time=1.02302
Epoch:0075, train_loss=2.67233, train_acc=0.99490, val_loss=3.81777, val_acc=0.93109, time=0.96902
Epoch:0076, train_loss=2.67199, train_acc=0.99490, val_loss=3.81775, val_acc=0.93262, time=0.81102
Epoch:0077, train_loss=2.67168, train_acc=0.99490, val_loss=3.81772, val_acc=0.93262, time=0.85901
Epoch:0078, train_loss=2.67137, train_acc=0.99490, val_loss=3.81770, val_acc=0.93262, time=1.08002
Epoch:0079, train_loss=2.67109, train_acc=0.99592, val_loss=3.81768, val_acc=0.93262, time=0.87601
Epoch:0080, train_loss=2.67081, train_acc=0.99609, val_loss=3.81767, val_acc=0.93262, time=0.82601
Epoch:0081, train_loss=2.67055, train_acc=0.99609, val_loss=3.81765, val_acc=0.93262, time=0.83001
Epoch:0082, train_loss=2.67030, train_acc=0.99609, val_loss=3.81763, val_acc=0.93262, time=0.90301
Epoch:0083, train_loss=2.67006, train_acc=0.99609, val_loss=3.81762, val_acc=0.93415, time=0.93301
Epoch:0084, train_loss=2.66983, train_acc=0.99609, val_loss=3.81760, val_acc=0.93415, time=0.87101
Epoch:0085, train_loss=2.66962, train_acc=0.99626, val_loss=3.81759, val_acc=0.93415, time=0.90501
Epoch:0086, train_loss=2.66941, train_acc=0.99643, val_loss=3.81758, val_acc=0.93415, time=0.98001
Epoch:0087, train_loss=2.66921, train_acc=0.99643, val_loss=3.81757, val_acc=0.93415, time=0.88901
Epoch:0088, train_loss=2.66903, train_acc=0.99643, val_loss=3.81756, val_acc=0.93415, time=0.98402
Epoch:0089, train_loss=2.66885, train_acc=0.99643, val_loss=3.81756, val_acc=0.93415, time=0.81502
Epoch:0090, train_loss=2.66868, train_acc=0.99643, val_loss=3.81755, val_acc=0.93262, time=0.90301
Epoch:0091, train_loss=2.66851, train_acc=0.99643, val_loss=3.81754, val_acc=0.93262, time=0.81602
Epoch:0092, train_loss=2.66835, train_acc=0.99643, val_loss=3.81754, val_acc=0.93262, time=0.94701
Epoch:0093, train_loss=2.66820, train_acc=0.99643, val_loss=3.81753, val_acc=0.93262, time=0.92302
Epoch:0094, train_loss=2.66806, train_acc=0.99660, val_loss=3.81753, val_acc=0.93262, time=0.86502
Epoch:0095, train_loss=2.66792, train_acc=0.99660, val_loss=3.81753, val_acc=0.93262, time=0.96101
Epoch:0096, train_loss=2.66778, train_acc=0.99677, val_loss=3.81752, val_acc=0.93262, time=0.91202
Epoch:0097, train_loss=2.66765, train_acc=0.99677, val_loss=3.81752, val_acc=0.93262, time=1.01601
Epoch:0098, train_loss=2.66752, train_acc=0.99694, val_loss=3.81752, val_acc=0.93262, time=0.77601
Epoch:0099, train_loss=2.66740, train_acc=0.99694, val_loss=3.81751, val_acc=0.93262, time=0.93102
Epoch:0100, train_loss=2.66728, train_acc=0.99694, val_loss=3.81751, val_acc=0.93262, time=0.93301
Epoch:0101, train_loss=2.66717, train_acc=0.99694, val_loss=3.81751, val_acc=0.93415, time=0.96401
Epoch:0102, train_loss=2.66706, train_acc=0.99711, val_loss=3.81750, val_acc=0.93415, time=0.97201
Epoch:0103, train_loss=2.66695, train_acc=0.99711, val_loss=3.81750, val_acc=0.93262, time=1.05701
Epoch:0104, train_loss=2.66685, train_acc=0.99711, val_loss=3.81750, val_acc=0.93262, time=0.93101
Epoch:0105, train_loss=2.66675, train_acc=0.99711, val_loss=3.81749, val_acc=0.93262, time=0.97500
Epoch:0106, train_loss=2.66665, train_acc=0.99711, val_loss=3.81749, val_acc=0.93262, time=1.00602
Epoch:0107, train_loss=2.66655, train_acc=0.99711, val_loss=3.81749, val_acc=0.93262, time=0.87302
Epoch:0108, train_loss=2.66646, train_acc=0.99711, val_loss=3.81748, val_acc=0.93262, time=0.89302
Epoch:0109, train_loss=2.66637, train_acc=0.99711, val_loss=3.81748, val_acc=0.93262, time=0.81000
Epoch:0110, train_loss=2.66628, train_acc=0.99728, val_loss=3.81747, val_acc=0.93415, time=0.89402
Epoch:0111, train_loss=2.66620, train_acc=0.99728, val_loss=3.81747, val_acc=0.93415, time=0.97300
Epoch:0112, train_loss=2.66611, train_acc=0.99728, val_loss=3.81747, val_acc=0.93415, time=0.95202
Epoch:0113, train_loss=2.66603, train_acc=0.99728, val_loss=3.81746, val_acc=0.93415, time=0.95300
Epoch:0114, train_loss=2.66595, train_acc=0.99728, val_loss=3.81746, val_acc=0.93415, time=0.93201
Epoch:0115, train_loss=2.66588, train_acc=0.99728, val_loss=3.81746, val_acc=0.93415, time=0.83201
Epoch:0116, train_loss=2.66580, train_acc=0.99728, val_loss=3.81745, val_acc=0.93415, time=0.85601
Epoch:0117, train_loss=2.66573, train_acc=0.99728, val_loss=3.81745, val_acc=0.93415, time=0.95002
Epoch:0118, train_loss=2.66566, train_acc=0.99728, val_loss=3.81744, val_acc=0.93415, time=0.88002
Epoch:0119, train_loss=2.66559, train_acc=0.99728, val_loss=3.81744, val_acc=0.93415, time=0.90801
Epoch:0120, train_loss=2.66552, train_acc=0.99728, val_loss=3.81744, val_acc=0.93415, time=1.04401
Epoch:0121, train_loss=2.66546, train_acc=0.99728, val_loss=3.81744, val_acc=0.93415, time=1.02801
Epoch:0122, train_loss=2.66539, train_acc=0.99728, val_loss=3.81743, val_acc=0.93415, time=0.85602
Epoch:0123, train_loss=2.66533, train_acc=0.99728, val_loss=3.81743, val_acc=0.93415, time=0.89701
Epoch:0124, train_loss=2.66527, train_acc=0.99728, val_loss=3.81743, val_acc=0.93415, time=0.88702
Epoch:0125, train_loss=2.66521, train_acc=0.99745, val_loss=3.81742, val_acc=0.93415, time=1.02001
Epoch:0126, train_loss=2.66515, train_acc=0.99745, val_loss=3.81742, val_acc=0.93415, time=1.11401
Epoch:0127, train_loss=2.66509, train_acc=0.99745, val_loss=3.81742, val_acc=0.93415, time=0.86502
Epoch:0128, train_loss=2.66504, train_acc=0.99745, val_loss=3.81742, val_acc=0.93415, time=0.91101
Epoch:0129, train_loss=2.66498, train_acc=0.99762, val_loss=3.81742, val_acc=0.93415, time=0.88300
Epoch:0130, train_loss=2.66493, train_acc=0.99762, val_loss=3.81741, val_acc=0.93415, time=0.99201
Epoch:0131, train_loss=2.66488, train_acc=0.99762, val_loss=3.81741, val_acc=0.93415, time=1.06701
Epoch:0132, train_loss=2.66482, train_acc=0.99762, val_loss=3.81741, val_acc=0.93415, time=0.89700
Epoch:0133, train_loss=2.66477, train_acc=0.99779, val_loss=3.81741, val_acc=0.93415, time=1.04301
Epoch:0134, train_loss=2.66472, train_acc=0.99779, val_loss=3.81741, val_acc=0.93415, time=0.96502
Epoch:0135, train_loss=2.66468, train_acc=0.99779, val_loss=3.81741, val_acc=0.93568, time=0.89702
Epoch:0136, train_loss=2.66463, train_acc=0.99779, val_loss=3.81741, val_acc=0.93568, time=0.88201
Epoch:0137, train_loss=2.66458, train_acc=0.99779, val_loss=3.81741, val_acc=0.93568, time=0.83801
Epoch:0138, train_loss=2.66454, train_acc=0.99779, val_loss=3.81741, val_acc=0.93568, time=0.87402
Epoch:0139, train_loss=2.66449, train_acc=0.99779, val_loss=3.81741, val_acc=0.93568, time=0.88200
Epoch:0140, train_loss=2.66445, train_acc=0.99779, val_loss=3.81740, val_acc=0.93568, time=0.91402
Epoch:0141, train_loss=2.66441, train_acc=0.99779, val_loss=3.81740, val_acc=0.93568, time=0.90102
Epoch:0142, train_loss=2.66437, train_acc=0.99796, val_loss=3.81740, val_acc=0.93568, time=0.94001
Epoch:0143, train_loss=2.66432, train_acc=0.99813, val_loss=3.81740, val_acc=0.93568, time=0.90601
Epoch:0144, train_loss=2.66428, train_acc=0.99813, val_loss=3.81740, val_acc=0.93568, time=0.86302
Epoch:0145, train_loss=2.66424, train_acc=0.99813, val_loss=3.81740, val_acc=0.93568, time=1.07501
Epoch:0146, train_loss=2.66421, train_acc=0.99813, val_loss=3.81740, val_acc=0.93568, time=0.80502
Epoch:0147, train_loss=2.66417, train_acc=0.99813, val_loss=3.81740, val_acc=0.93568, time=0.93101
Epoch:0148, train_loss=2.66413, train_acc=0.99813, val_loss=3.81740, val_acc=0.93568, time=0.95101
Epoch:0149, train_loss=2.66409, train_acc=0.99813, val_loss=3.81740, val_acc=0.93568, time=0.86501
Epoch:0150, train_loss=2.66406, train_acc=0.99830, val_loss=3.81740, val_acc=0.93568, time=1.02100
Epoch:0151, train_loss=2.66402, train_acc=0.99830, val_loss=3.81740, val_acc=0.93568, time=0.94101
Epoch:0152, train_loss=2.66399, train_acc=0.99830, val_loss=3.81740, val_acc=0.93568, time=1.01301
Epoch:0153, train_loss=2.66395, train_acc=0.99830, val_loss=3.81740, val_acc=0.93568, time=1.04502
Epoch:0154, train_loss=2.66392, train_acc=0.99830, val_loss=3.81740, val_acc=0.93568, time=0.87901
Epoch:0155, train_loss=2.66389, train_acc=0.99864, val_loss=3.81740, val_acc=0.93568, time=1.05300
Epoch:0156, train_loss=2.66385, train_acc=0.99864, val_loss=3.81740, val_acc=0.93568, time=0.96602
Epoch:0157, train_loss=2.66382, train_acc=0.99864, val_loss=3.81740, val_acc=0.93568, time=0.92601
Epoch:0158, train_loss=2.66379, train_acc=0.99864, val_loss=3.81740, val_acc=0.93568, time=0.87801
Epoch:0159, train_loss=2.66376, train_acc=0.99864, val_loss=3.81740, val_acc=0.93568, time=0.95503
Epoch:0160, train_loss=2.66373, train_acc=0.99864, val_loss=3.81740, val_acc=0.93568, time=0.89501
Epoch:0161, train_loss=2.66370, train_acc=0.99864, val_loss=3.81740, val_acc=0.93568, time=0.88301
Early stopping...

Optimization Finished!

Test set results: loss= 3.44022, accuracy= 0.92445, time= 0.34601

Test Precision, Recall and F1-Score...
              precision    recall  f1-score   support

           0     0.9597    0.9898    0.9745      1083
           1     1.0000    0.9167    0.9565        12
           2     0.9602    0.9368    0.9484       696
           3     0.7172    0.9467    0.8161        75
           4     0.8061    0.9080    0.8541        87
           5     0.9565    1.0000    0.9778        22
           6     1.0000    1.0000    1.0000         2
           7     0.9333    0.9333    0.9333        15
           8     0.8571    0.7500    0.8000         8
           9     0.8406    0.9587    0.8958       121
          10     0.8966    0.7222    0.8000        36
          11     0.8438    0.9643    0.9000        28
          12     0.8000    0.3333    0.4706        12
          13     1.0000    1.0000    1.0000         1
          14     0.8519    0.9200    0.8846        25
          15     0.8378    0.7654    0.8000        81
          16     1.0000    0.8235    0.9032        17
          17     1.0000    0.8889    0.9412         9
          18     1.0000    1.0000    1.0000         9
          19     0.8333    0.7692    0.8000        13
          20     1.0000    0.9000    0.9474        10
          21     0.8889    0.8000    0.8421        20
          22     0.6667    0.8000    0.7273        10
          23     0.8000    0.7273    0.7619        11
          24     1.0000    0.9167    0.9565        12
          25     0.9231    0.6316    0.7500        19
          26     1.0000    0.7778    0.8750         9
          27     1.0000    1.0000    1.0000         9
          28     0.7143    0.5556    0.6250         9
          29     0.5000    1.0000    0.6667         1
          30     0.3333    0.1667    0.2222         6
          31     0.8182    0.7500    0.7826        12
          32     1.0000    0.9333    0.9655        15
          33     1.0000    0.6000    0.7500         5
          34     0.0000    0.0000    0.0000         1
          35     0.0000    0.0000    0.0000         6
          36     1.0000    0.2000    0.3333         5
          37     0.9167    1.0000    0.9565        11
          38     0.0000    0.0000    0.0000         1
          39     0.0000    0.0000    0.0000         3
          40     1.0000    0.3333    0.5000         3
          41     1.0000    0.5000    0.6667         4
          42     1.0000    1.0000    1.0000         4
          43     1.0000    1.0000    1.0000         3
          44     0.0000    0.0000    0.0000         1
          45     0.0000    0.0000    0.0000         1
          46     1.0000    0.1429    0.2500         7
          47     0.0000    0.0000    0.0000         4
          48     0.6000    0.7500    0.6667         4
          49     0.0000    0.0000    0.0000         2
          50     0.8333    1.0000    0.9091         5
          51     1.0000    0.3333    0.5000         3

    accuracy                         0.9245      2568
   macro avg     0.7517    0.6605    0.6790      2568
weighted avg     0.9213    0.9245    0.9187      2568


Macro average Test Precision, Recall and F1-Score...
(0.7517034433941905, 0.6604867863481576, 0.679048342216476, None)

Micro average Test Precision, Recall and F1-Score...
(0.9244548286604362, 0.9244548286604362, 0.9244548286604362, None)

Embeddings:
Word_embeddings:8892
Train_doc_embeddings:6532
Test_doc_embeddings:2568
