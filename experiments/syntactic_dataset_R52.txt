
==================== Torch Seed: 48697069028200
Epoch:0001, train_loss=3.88508, train_acc=0.08709, val_loss=3.91429, val_acc=0.49158, time=0.94901
Epoch:0002, train_loss=3.60098, train_acc=0.49294, val_loss=3.89235, val_acc=0.57580, time=0.95101
Epoch:0003, train_loss=3.39429, train_acc=0.59755, val_loss=3.87855, val_acc=0.59265, time=1.01403
Epoch:0004, train_loss=3.25844, train_acc=0.63514, val_loss=3.86982, val_acc=0.61103, time=1.12299
Epoch:0005, train_loss=3.16975, train_acc=0.66117, val_loss=3.86347, val_acc=0.65391, time=0.91701
Epoch:0006, train_loss=3.10471, train_acc=0.69280, val_loss=3.85847, val_acc=0.67688, time=0.95602
Epoch:0007, train_loss=3.05341, train_acc=0.72801, val_loss=3.85456, val_acc=0.71822, time=0.88002
Epoch:0008, train_loss=3.01288, train_acc=0.75982, val_loss=3.85144, val_acc=0.75038, time=0.88401
Epoch:0009, train_loss=2.98010, train_acc=0.78874, val_loss=3.84877, val_acc=0.76570, time=0.86201
Epoch:0010, train_loss=2.95222, train_acc=0.81459, val_loss=3.84640, val_acc=0.77795, time=0.89600
Epoch:0011, train_loss=2.92798, train_acc=0.83620, val_loss=3.84427, val_acc=0.80398, time=0.87704
Epoch:0012, train_loss=2.90679, train_acc=0.85865, val_loss=3.84234, val_acc=0.81776, time=0.86199
Epoch:0013, train_loss=2.88801, train_acc=0.87107, val_loss=3.84053, val_acc=0.82542, time=0.82301
Epoch:0014, train_loss=2.87092, train_acc=0.88076, val_loss=3.83882, val_acc=0.83308, time=0.92901
Epoch:0015, train_loss=2.85502, train_acc=0.88961, val_loss=3.83719, val_acc=0.84380, time=0.89800
Epoch:0016, train_loss=2.84012, train_acc=0.89811, val_loss=3.83567, val_acc=0.84992, time=0.84102
Epoch:0017, train_loss=2.82621, train_acc=0.90645, val_loss=3.83426, val_acc=0.85758, time=0.91901
Epoch:0018, train_loss=2.81340, train_acc=0.91580, val_loss=3.83300, val_acc=0.86830, time=0.96800
Epoch:0019, train_loss=2.80175, train_acc=0.92312, val_loss=3.83187, val_acc=0.87902, time=0.90501
Epoch:0020, train_loss=2.79127, train_acc=0.93230, val_loss=3.83088, val_acc=0.88208, time=0.87402
Epoch:0021, train_loss=2.78197, train_acc=0.93706, val_loss=3.83002, val_acc=0.88668, time=0.90300
Epoch:0022, train_loss=2.77377, train_acc=0.94200, val_loss=3.82926, val_acc=0.88668, time=0.87300
Epoch:0023, train_loss=2.76651, train_acc=0.94812, val_loss=3.82860, val_acc=0.88821, time=1.09601
Epoch:0024, train_loss=2.75997, train_acc=0.95152, val_loss=3.82800, val_acc=0.88821, time=0.99302
Epoch:0025, train_loss=2.75395, train_acc=0.95475, val_loss=3.82744, val_acc=0.88974, time=0.96302
Epoch:0026, train_loss=2.74828, train_acc=0.95680, val_loss=3.82691, val_acc=0.89433, time=0.99200
Epoch:0027, train_loss=2.74288, train_acc=0.96037, val_loss=3.82642, val_acc=0.89433, time=1.03601
Epoch:0028, train_loss=2.73774, train_acc=0.96343, val_loss=3.82595, val_acc=0.89280, time=0.91102
Epoch:0029, train_loss=2.73290, train_acc=0.96632, val_loss=3.82552, val_acc=0.89740, time=0.91300
Epoch:0030, train_loss=2.72839, train_acc=0.96887, val_loss=3.82513, val_acc=0.90046, time=1.06701
Epoch:0031, train_loss=2.72420, train_acc=0.97074, val_loss=3.82478, val_acc=0.90046, time=0.84402
Epoch:0032, train_loss=2.72033, train_acc=0.97261, val_loss=3.82446, val_acc=0.90352, time=1.02902
Epoch:0033, train_loss=2.71675, train_acc=0.97432, val_loss=3.82417, val_acc=0.90046, time=0.92401
Epoch:0034, train_loss=2.71346, train_acc=0.97687, val_loss=3.82390, val_acc=0.90505, time=0.88801
Epoch:0035, train_loss=2.71043, train_acc=0.97823, val_loss=3.82366, val_acc=0.90658, time=0.97400
Epoch:0036, train_loss=2.70763, train_acc=0.97925, val_loss=3.82343, val_acc=0.90658, time=0.90201
Epoch:0037, train_loss=2.70503, train_acc=0.98061, val_loss=3.82321, val_acc=0.90505, time=1.01703
Epoch:0038, train_loss=2.70261, train_acc=0.98163, val_loss=3.82300, val_acc=0.90505, time=0.86100
Epoch:0039, train_loss=2.70036, train_acc=0.98316, val_loss=3.82280, val_acc=0.90658, time=0.84301
Epoch:0040, train_loss=2.69826, train_acc=0.98435, val_loss=3.82261, val_acc=0.91118, time=1.17101
Epoch:0041, train_loss=2.69630, train_acc=0.98486, val_loss=3.82242, val_acc=0.91424, time=0.90102
Epoch:0042, train_loss=2.69448, train_acc=0.98520, val_loss=3.82226, val_acc=0.91271, time=0.86001
Epoch:0043, train_loss=2.69277, train_acc=0.98486, val_loss=3.82210, val_acc=0.91118, time=1.00503
Epoch:0044, train_loss=2.69119, train_acc=0.98588, val_loss=3.82196, val_acc=0.91118, time=0.83902
Epoch:0045, train_loss=2.68971, train_acc=0.98690, val_loss=3.82183, val_acc=0.91118, time=0.96099
Epoch:0046, train_loss=2.68833, train_acc=0.98826, val_loss=3.82172, val_acc=0.91118, time=0.89203
Epoch:0047, train_loss=2.68705, train_acc=0.98894, val_loss=3.82161, val_acc=0.91118, time=0.81200
Epoch:0048, train_loss=2.68584, train_acc=0.98928, val_loss=3.82152, val_acc=0.90965, time=0.89403
Epoch:0049, train_loss=2.68471, train_acc=0.98945, val_loss=3.82143, val_acc=0.91118, time=0.92602
Epoch:0050, train_loss=2.68365, train_acc=0.99030, val_loss=3.82134, val_acc=0.91118, time=0.91701
Epoch:0051, train_loss=2.68264, train_acc=0.99064, val_loss=3.82126, val_acc=0.91118, time=0.86101
Epoch:0052, train_loss=2.68169, train_acc=0.99133, val_loss=3.82119, val_acc=0.91118, time=1.02302
Epoch:0053, train_loss=2.68078, train_acc=0.99184, val_loss=3.82112, val_acc=0.90965, time=1.03300
Epoch:0054, train_loss=2.67992, train_acc=0.99252, val_loss=3.82105, val_acc=0.91118, time=0.92803
Epoch:0055, train_loss=2.67912, train_acc=0.99286, val_loss=3.82099, val_acc=0.91271, time=0.94099
Epoch:0056, train_loss=2.67836, train_acc=0.99320, val_loss=3.82093, val_acc=0.91118, time=0.88203
Epoch:0057, train_loss=2.67765, train_acc=0.99337, val_loss=3.82088, val_acc=0.90965, time=1.04801
Epoch:0058, train_loss=2.67698, train_acc=0.99337, val_loss=3.82084, val_acc=0.91118, time=0.94301
Epoch:0059, train_loss=2.67636, train_acc=0.99371, val_loss=3.82079, val_acc=0.91271, time=0.93801
Epoch:0060, train_loss=2.67579, train_acc=0.99405, val_loss=3.82076, val_acc=0.91271, time=0.89701
Epoch:0061, train_loss=2.67525, train_acc=0.99422, val_loss=3.82072, val_acc=0.91118, time=0.84301
Epoch:0062, train_loss=2.67474, train_acc=0.99422, val_loss=3.82069, val_acc=0.91118, time=1.14602
Epoch:0063, train_loss=2.67426, train_acc=0.99490, val_loss=3.82066, val_acc=0.91118, time=1.06701
Epoch:0064, train_loss=2.67381, train_acc=0.99507, val_loss=3.82062, val_acc=0.91118, time=0.87102
Epoch:0065, train_loss=2.67339, train_acc=0.99524, val_loss=3.82059, val_acc=0.91118, time=1.04000
Epoch:0066, train_loss=2.67298, train_acc=0.99524, val_loss=3.82056, val_acc=0.91271, time=0.91002
Epoch:0067, train_loss=2.67259, train_acc=0.99541, val_loss=3.82052, val_acc=0.91271, time=1.27100
Epoch:0068, train_loss=2.67222, train_acc=0.99541, val_loss=3.82049, val_acc=0.91424, time=0.89103
Epoch:0069, train_loss=2.67186, train_acc=0.99541, val_loss=3.82045, val_acc=0.91424, time=0.95301
Epoch:0070, train_loss=2.67152, train_acc=0.99558, val_loss=3.82041, val_acc=0.91424, time=1.00402
Epoch:0071, train_loss=2.67119, train_acc=0.99575, val_loss=3.82038, val_acc=0.91424, time=1.02902
Epoch:0072, train_loss=2.67089, train_acc=0.99592, val_loss=3.82035, val_acc=0.91424, time=0.90400
Epoch:0073, train_loss=2.67059, train_acc=0.99626, val_loss=3.82031, val_acc=0.91577, time=1.04900
Epoch:0074, train_loss=2.67031, train_acc=0.99643, val_loss=3.82028, val_acc=0.91577, time=0.92301
Epoch:0075, train_loss=2.67005, train_acc=0.99660, val_loss=3.82025, val_acc=0.91577, time=0.87601
Epoch:0076, train_loss=2.66980, train_acc=0.99677, val_loss=3.82023, val_acc=0.91577, time=0.92002
Epoch:0077, train_loss=2.66956, train_acc=0.99677, val_loss=3.82020, val_acc=0.91577, time=0.91201
Epoch:0078, train_loss=2.66933, train_acc=0.99677, val_loss=3.82017, val_acc=0.91577, time=1.03600
Epoch:0079, train_loss=2.66911, train_acc=0.99677, val_loss=3.82015, val_acc=0.91577, time=0.99002
Epoch:0080, train_loss=2.66891, train_acc=0.99677, val_loss=3.82012, val_acc=0.91577, time=0.99500
Epoch:0081, train_loss=2.66871, train_acc=0.99694, val_loss=3.82010, val_acc=0.91424, time=1.23802
Epoch:0082, train_loss=2.66852, train_acc=0.99694, val_loss=3.82008, val_acc=0.91424, time=0.99902
Epoch:0083, train_loss=2.66834, train_acc=0.99694, val_loss=3.82006, val_acc=0.91424, time=0.92899
Epoch:0084, train_loss=2.66816, train_acc=0.99694, val_loss=3.82003, val_acc=0.91424, time=0.93600
Epoch:0085, train_loss=2.66799, train_acc=0.99694, val_loss=3.82001, val_acc=0.91424, time=0.86002
Epoch:0086, train_loss=2.66783, train_acc=0.99711, val_loss=3.81999, val_acc=0.91424, time=0.93102
Epoch:0087, train_loss=2.66768, train_acc=0.99728, val_loss=3.81997, val_acc=0.91424, time=0.99500
Epoch:0088, train_loss=2.66753, train_acc=0.99745, val_loss=3.81995, val_acc=0.91424, time=0.95102
Epoch:0089, train_loss=2.66738, train_acc=0.99762, val_loss=3.81993, val_acc=0.91424, time=0.89001
Epoch:0090, train_loss=2.66725, train_acc=0.99762, val_loss=3.81991, val_acc=0.91424, time=1.10201
Epoch:0091, train_loss=2.66711, train_acc=0.99762, val_loss=3.81989, val_acc=0.91424, time=1.13300
Epoch:0092, train_loss=2.66699, train_acc=0.99762, val_loss=3.81988, val_acc=0.91424, time=0.95902
Epoch:0093, train_loss=2.66686, train_acc=0.99762, val_loss=3.81986, val_acc=0.91424, time=1.07802
Epoch:0094, train_loss=2.66674, train_acc=0.99762, val_loss=3.81984, val_acc=0.91424, time=0.97800
Epoch:0095, train_loss=2.66663, train_acc=0.99762, val_loss=3.81982, val_acc=0.91424, time=0.92300
Epoch:0096, train_loss=2.66652, train_acc=0.99762, val_loss=3.81981, val_acc=0.91577, time=0.92001
Epoch:0097, train_loss=2.66641, train_acc=0.99762, val_loss=3.81979, val_acc=0.91577, time=0.90802
Epoch:0098, train_loss=2.66631, train_acc=0.99762, val_loss=3.81978, val_acc=0.91730, time=1.06201
Epoch:0099, train_loss=2.66621, train_acc=0.99779, val_loss=3.81976, val_acc=0.91730, time=0.96501
Epoch:0100, train_loss=2.66612, train_acc=0.99779, val_loss=3.81975, val_acc=0.91730, time=0.88301
Epoch:0101, train_loss=2.66602, train_acc=0.99796, val_loss=3.81974, val_acc=0.91730, time=0.91601
Epoch:0102, train_loss=2.66593, train_acc=0.99796, val_loss=3.81972, val_acc=0.91730, time=0.96101
Epoch:0103, train_loss=2.66585, train_acc=0.99813, val_loss=3.81971, val_acc=0.91730, time=0.95501
Epoch:0104, train_loss=2.66576, train_acc=0.99830, val_loss=3.81970, val_acc=0.91730, time=0.86502
Epoch:0105, train_loss=2.66568, train_acc=0.99830, val_loss=3.81969, val_acc=0.91577, time=0.88401
Epoch:0106, train_loss=2.66560, train_acc=0.99830, val_loss=3.81968, val_acc=0.91730, time=0.94901
Epoch:0107, train_loss=2.66552, train_acc=0.99830, val_loss=3.81967, val_acc=0.91730, time=0.92700
Epoch:0108, train_loss=2.66544, train_acc=0.99830, val_loss=3.81966, val_acc=0.91730, time=1.07001
Epoch:0109, train_loss=2.66537, train_acc=0.99830, val_loss=3.81966, val_acc=0.91730, time=0.87201
Epoch:0110, train_loss=2.66530, train_acc=0.99830, val_loss=3.81965, val_acc=0.91730, time=0.89000
Epoch:0111, train_loss=2.66523, train_acc=0.99830, val_loss=3.81964, val_acc=0.91730, time=0.86201
Epoch:0112, train_loss=2.66516, train_acc=0.99830, val_loss=3.81964, val_acc=0.91730, time=0.93102
Epoch:0113, train_loss=2.66509, train_acc=0.99830, val_loss=3.81963, val_acc=0.91730, time=0.93201
Epoch:0114, train_loss=2.66503, train_acc=0.99830, val_loss=3.81963, val_acc=0.91730, time=0.82901
Epoch:0115, train_loss=2.66496, train_acc=0.99830, val_loss=3.81962, val_acc=0.91730, time=0.86902
Epoch:0116, train_loss=2.66490, train_acc=0.99830, val_loss=3.81962, val_acc=0.91730, time=0.85000
Epoch:0117, train_loss=2.66484, train_acc=0.99830, val_loss=3.81962, val_acc=0.91730, time=0.89600
Epoch:0118, train_loss=2.66478, train_acc=0.99830, val_loss=3.81961, val_acc=0.91730, time=0.92102
Epoch:0119, train_loss=2.66473, train_acc=0.99830, val_loss=3.81961, val_acc=0.91730, time=0.90701
Epoch:0120, train_loss=2.66467, train_acc=0.99847, val_loss=3.81961, val_acc=0.91730, time=1.11200
Epoch:0121, train_loss=2.66462, train_acc=0.99847, val_loss=3.81961, val_acc=0.91730, time=1.03901
Epoch:0122, train_loss=2.66456, train_acc=0.99847, val_loss=3.81960, val_acc=0.91730, time=0.90802
Epoch:0123, train_loss=2.66451, train_acc=0.99847, val_loss=3.81960, val_acc=0.91730, time=0.95201
Epoch:0124, train_loss=2.66446, train_acc=0.99847, val_loss=3.81960, val_acc=0.91730, time=0.98601
Epoch:0125, train_loss=2.66441, train_acc=0.99864, val_loss=3.81960, val_acc=0.91730, time=1.11202
Epoch:0126, train_loss=2.66436, train_acc=0.99864, val_loss=3.81960, val_acc=0.91730, time=1.04800
Epoch:0127, train_loss=2.66431, train_acc=0.99864, val_loss=3.81960, val_acc=0.91730, time=1.01602
Epoch:0128, train_loss=2.66426, train_acc=0.99864, val_loss=3.81960, val_acc=0.91730, time=0.93802
Epoch:0129, train_loss=2.66422, train_acc=0.99864, val_loss=3.81959, val_acc=0.91730, time=0.91100
Epoch:0130, train_loss=2.66417, train_acc=0.99881, val_loss=3.81959, val_acc=0.91730, time=0.97099
Epoch:0131, train_loss=2.66413, train_acc=0.99881, val_loss=3.81959, val_acc=0.91730, time=0.94101
Epoch:0132, train_loss=2.66409, train_acc=0.99881, val_loss=3.81959, val_acc=0.91730, time=0.92402
Epoch:0133, train_loss=2.66404, train_acc=0.99881, val_loss=3.81959, val_acc=0.91730, time=0.94001
Epoch:0134, train_loss=2.66400, train_acc=0.99881, val_loss=3.81959, val_acc=0.91730, time=1.05401
Epoch:0135, train_loss=2.66396, train_acc=0.99881, val_loss=3.81959, val_acc=0.91884, time=1.00801
Epoch:0136, train_loss=2.66392, train_acc=0.99881, val_loss=3.81959, val_acc=0.91884, time=0.87802
Epoch:0137, train_loss=2.66388, train_acc=0.99881, val_loss=3.81959, val_acc=0.91884, time=1.04800
Epoch:0138, train_loss=2.66385, train_acc=0.99881, val_loss=3.81959, val_acc=0.91884, time=0.94001
Epoch:0139, train_loss=2.66381, train_acc=0.99881, val_loss=3.81959, val_acc=0.91884, time=0.99902
Epoch:0140, train_loss=2.66377, train_acc=0.99881, val_loss=3.81958, val_acc=0.91884, time=1.13200
Epoch:0141, train_loss=2.66374, train_acc=0.99881, val_loss=3.81958, val_acc=0.91884, time=1.01801
Epoch:0142, train_loss=2.66370, train_acc=0.99881, val_loss=3.81958, val_acc=0.91884, time=1.05301
Epoch:0143, train_loss=2.66367, train_acc=0.99881, val_loss=3.81958, val_acc=0.91884, time=1.01601
Epoch:0144, train_loss=2.66363, train_acc=0.99881, val_loss=3.81958, val_acc=0.91884, time=1.11602
Epoch:0145, train_loss=2.66360, train_acc=0.99881, val_loss=3.81958, val_acc=0.91884, time=1.03600
Epoch:0146, train_loss=2.66357, train_acc=0.99881, val_loss=3.81958, val_acc=0.91884, time=0.95500
Epoch:0147, train_loss=2.66354, train_acc=0.99881, val_loss=3.81958, val_acc=0.91884, time=1.02301
Epoch:0148, train_loss=2.66350, train_acc=0.99898, val_loss=3.81958, val_acc=0.91884, time=1.05601
Epoch:0149, train_loss=2.66347, train_acc=0.99898, val_loss=3.81958, val_acc=0.91884, time=1.05201
Epoch:0150, train_loss=2.66344, train_acc=0.99898, val_loss=3.81958, val_acc=0.91884, time=0.86901
Epoch:0151, train_loss=2.66341, train_acc=0.99898, val_loss=3.81958, val_acc=0.91884, time=0.96602
Epoch:0152, train_loss=2.66338, train_acc=0.99898, val_loss=3.81958, val_acc=0.92037, time=1.01901
Epoch:0153, train_loss=2.66336, train_acc=0.99898, val_loss=3.81958, val_acc=0.92037, time=0.93100
Epoch:0154, train_loss=2.66333, train_acc=0.99898, val_loss=3.81958, val_acc=0.92037, time=0.90202
Epoch:0155, train_loss=2.66330, train_acc=0.99915, val_loss=3.81958, val_acc=0.92037, time=0.89201
Epoch:0156, train_loss=2.66327, train_acc=0.99915, val_loss=3.81958, val_acc=0.92037, time=0.98801
Epoch:0157, train_loss=2.66325, train_acc=0.99915, val_loss=3.81958, val_acc=0.92037, time=0.91301
Early stopping...

Optimization Finished!

Test set results: loss= 3.43911, accuracy= 0.92251, time= 0.34200

Test Precision, Recall and F1-Score...
              precision    recall  f1-score   support

           0     0.7614    0.8933    0.8221        75
           1     0.9623    0.9898    0.9759      1083
           2     0.9545    0.9339    0.9441       696
           3     0.7857    1.0000    0.8800        11
           4     0.6000    0.9000    0.7200        10
           5     1.0000    0.8889    0.9412         9
           6     1.0000    0.7778    0.8750         9
           7     0.7722    0.7531    0.7625        81
           8     0.9565    1.0000    0.9778        22
           9     0.8310    0.9752    0.8973       121
          10     0.8000    0.3333    0.4706        12
          11     1.0000    0.7778    0.8750         9
          12     0.0000    0.0000    0.0000         1
          13     1.0000    0.8667    0.9286        15
          14     0.8021    0.8851    0.8415        87
          15     1.0000    0.9167    0.9565        12
          16     1.0000    0.8235    0.9032        17
          17     0.8438    0.9643    0.9000        28
          18     0.8750    0.9333    0.9032        15
          19     0.8889    0.8000    0.8421        20
          20     1.0000    0.6000    0.7500         5
          21     0.6000    0.7500    0.6667         4
          22     1.0000    0.2000    0.3333         5
          23     0.9583    0.9200    0.9388        25
          24     1.0000    0.3333    0.5000         3
          25     0.5556    0.5556    0.5556         9
          26     1.0000    1.0000    1.0000        12
          27     0.8621    0.6944    0.7692        36
          28     1.0000    1.0000    1.0000         3
          29     0.9000    0.9000    0.9000        10
          30     0.0000    0.0000    0.0000         1
          31     1.0000    0.7692    0.8696        13
          32     1.0000    0.6842    0.8125        19
          33     0.5000    0.3333    0.4000         3
          34     0.6667    1.0000    0.8000         2
          35     0.8889    0.7273    0.8000        11
          36     0.0000    0.0000    0.0000         1
          37     0.7778    0.8750    0.8235         8
          38     0.3333    0.1667    0.2222         6
          39     1.0000    1.0000    1.0000         1
          40     1.0000    1.0000    1.0000         1
          41     1.0000    1.0000    1.0000         4
          42     0.0000    0.0000    0.0000         6
          43     0.9091    0.8333    0.8696        12
          44     0.0000    0.0000    0.0000         2
          45     1.0000    1.0000    1.0000         9
          46     0.0000    0.0000    0.0000         4
          47     1.0000    1.0000    1.0000         5
          48     0.0000    0.0000    0.0000         1
          49     1.0000    0.3333    0.5000         3
          50     1.0000    0.5000    0.6667         4
          51     1.0000    0.1429    0.2500         7

    accuracy                         0.9225      2568
   macro avg     0.7651    0.6679    0.6893      2568
weighted avg     0.9207    0.9225    0.9175      2568


Macro average Test Precision, Recall and F1-Score...
(0.7650952280911297, 0.6679089547441065, 0.6893118264645426, None)

Micro average Test Precision, Recall and F1-Score...
(0.9225077881619937, 0.9225077881619937, 0.9225077881619937, None)

Embeddings:
Word_embeddings:8892
Train_doc_embeddings:6532
Test_doc_embeddings:2568
