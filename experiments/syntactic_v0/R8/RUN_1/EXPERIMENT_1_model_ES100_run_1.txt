
==========: 151105150630700
Epoch:0001, train_loss=2.28393, train_acc=0.03747, val_loss=2.07228, val_acc=0.41788, time=1.53500
Epoch:0002, train_loss=2.00328, train_acc=0.42394, val_loss=2.06341, val_acc=0.48905, time=1.36301
Epoch:0003, train_loss=1.91226, train_acc=0.50476, val_loss=2.06164, val_acc=0.49270, time=1.10700
Epoch:0004, train_loss=1.88985, train_acc=0.51691, val_loss=2.06162, val_acc=0.48540, time=1.12001
Epoch:0005, train_loss=1.88607, train_acc=0.51469, val_loss=2.06330, val_acc=0.44891, time=1.24600
Epoch:0006, train_loss=1.89372, train_acc=0.50820, val_loss=2.06373, val_acc=0.45073, time=1.32801
Epoch:0007, train_loss=1.88332, train_acc=0.53838, val_loss=2.06351, val_acc=0.49088, time=1.15200
Epoch:0008, train_loss=1.86411, train_acc=0.56593, val_loss=2.06358, val_acc=0.50000, time=1.06300
Epoch:0009, train_loss=1.84748, train_acc=0.57788, val_loss=2.06336, val_acc=0.49088, time=1.16301
Epoch:0010, train_loss=1.82999, train_acc=0.57970, val_loss=2.06268, val_acc=0.48540, time=1.08899
Epoch:0011, train_loss=1.81027, train_acc=0.59145, val_loss=2.06215, val_acc=0.47445, time=1.12901
Epoch:0012, train_loss=1.79342, train_acc=0.60887, val_loss=2.06229, val_acc=0.45620, time=1.12000
Epoch:0013, train_loss=1.78333, train_acc=0.62082, val_loss=2.06296, val_acc=0.43613, time=1.13100
Epoch:0014, train_loss=1.77797, train_acc=0.63257, val_loss=2.06366, val_acc=0.43066, time=1.22801
Epoch:0015, train_loss=1.77263, train_acc=0.64088, val_loss=2.06414, val_acc=0.43796, time=1.18901
Epoch:0016, train_loss=1.76504, train_acc=0.64776, val_loss=2.06443, val_acc=0.44891, time=1.18001
Epoch:0017, train_loss=1.75581, train_acc=0.65364, val_loss=2.06465, val_acc=0.45255, time=1.08301
Epoch:0018, train_loss=1.74638, train_acc=0.65343, val_loss=2.06483, val_acc=0.46533, time=1.00900
Epoch:0019, train_loss=1.73750, train_acc=0.65262, val_loss=2.06497, val_acc=0.46533, time=1.09600
Epoch:0020, train_loss=1.72921, train_acc=0.65647, val_loss=2.06507, val_acc=0.46533, time=1.12201
Epoch:0021, train_loss=1.72143, train_acc=0.66255, val_loss=2.06517, val_acc=0.45620, time=1.07000
Epoch:0022, train_loss=1.71444, train_acc=0.67207, val_loss=2.06537, val_acc=0.45073, time=1.00100
Epoch:0023, train_loss=1.70874, train_acc=0.68037, val_loss=2.06571, val_acc=0.43431, time=1.12201
Epoch:0024, train_loss=1.70435, train_acc=0.68787, val_loss=2.06616, val_acc=0.42336, time=1.17801
Epoch:0025, train_loss=1.70063, train_acc=0.69354, val_loss=2.06663, val_acc=0.42336, time=1.37000
Epoch:0026, train_loss=1.69676, train_acc=0.69901, val_loss=2.06709, val_acc=0.42883, time=1.17599
Epoch:0027, train_loss=1.69241, train_acc=0.70326, val_loss=2.06755, val_acc=0.43066, time=1.12001
Epoch:0028, train_loss=1.68787, train_acc=0.70630, val_loss=2.06803, val_acc=0.43796, time=1.11602
Epoch:0029, train_loss=1.68349, train_acc=0.70488, val_loss=2.06850, val_acc=0.44343, time=1.05899
Epoch:0030, train_loss=1.67933, train_acc=0.70873, val_loss=2.06894, val_acc=0.44526, time=1.17399
Epoch:0031, train_loss=1.67520, train_acc=0.71258, val_loss=2.06933, val_acc=0.43796, time=1.12102
Epoch:0032, train_loss=1.67100, train_acc=0.71947, val_loss=2.06969, val_acc=0.42701, time=1.05299
Epoch:0033, train_loss=1.66691, train_acc=0.72655, val_loss=2.07006, val_acc=0.42336, time=1.14402
Epoch:0034, train_loss=1.66317, train_acc=0.73567, val_loss=2.07047, val_acc=0.42336, time=1.14201
Epoch:0035, train_loss=1.65983, train_acc=0.74276, val_loss=2.07090, val_acc=0.42701, time=1.16601
Epoch:0036, train_loss=1.65671, train_acc=0.74782, val_loss=2.07135, val_acc=0.42701, time=1.11599
Epoch:0037, train_loss=1.65356, train_acc=0.75289, val_loss=2.07180, val_acc=0.42883, time=1.24001
Epoch:0038, train_loss=1.65032, train_acc=0.75572, val_loss=2.07226, val_acc=0.42518, time=1.29400
Epoch:0039, train_loss=1.64708, train_acc=0.76058, val_loss=2.07272, val_acc=0.42883, time=1.25002
Epoch:0040, train_loss=1.64395, train_acc=0.76200, val_loss=2.07318, val_acc=0.42518, time=1.24602
Epoch:0041, train_loss=1.64096, train_acc=0.76180, val_loss=2.07362, val_acc=0.42701, time=1.16499
Epoch:0042, train_loss=1.63802, train_acc=0.76423, val_loss=2.07404, val_acc=0.42518, time=1.26800
Epoch:0043, train_loss=1.63509, train_acc=0.76848, val_loss=2.07444, val_acc=0.42701, time=1.14500
Epoch:0044, train_loss=1.63224, train_acc=0.77233, val_loss=2.07484, val_acc=0.42518, time=1.23002
Epoch:0045, train_loss=1.62953, train_acc=0.77679, val_loss=2.07526, val_acc=0.42701, time=1.29302
Epoch:0046, train_loss=1.62695, train_acc=0.78023, val_loss=2.07569, val_acc=0.42701, time=1.13302
Epoch:0047, train_loss=1.62438, train_acc=0.78226, val_loss=2.07613, val_acc=0.42701, time=1.11300
Epoch:0048, train_loss=1.62178, train_acc=0.78428, val_loss=2.07657, val_acc=0.43066, time=1.11999
Epoch:0049, train_loss=1.61917, train_acc=0.78671, val_loss=2.07703, val_acc=0.42883, time=1.34300
Epoch:0050, train_loss=1.61664, train_acc=0.78894, val_loss=2.07749, val_acc=0.42701, time=0.99001
Epoch:0051, train_loss=1.61421, train_acc=0.78995, val_loss=2.07794, val_acc=0.43066, time=1.15001
Epoch:0052, train_loss=1.61182, train_acc=0.79441, val_loss=2.07839, val_acc=0.43066, time=1.03100
Epoch:0053, train_loss=1.60946, train_acc=0.79846, val_loss=2.07883, val_acc=0.42701, time=1.09299
Epoch:0054, train_loss=1.60716, train_acc=0.80190, val_loss=2.07929, val_acc=0.43066, time=1.14999
Epoch:0055, train_loss=1.60495, train_acc=0.80717, val_loss=2.07977, val_acc=0.42701, time=1.07400
Epoch:0056, train_loss=1.60277, train_acc=0.81183, val_loss=2.08026, val_acc=0.42701, time=1.07603
Epoch:0057, train_loss=1.60059, train_acc=0.81507, val_loss=2.08077, val_acc=0.42701, time=1.28301
Epoch:0058, train_loss=1.59840, train_acc=0.81669, val_loss=2.08128, val_acc=0.42883, time=1.27900
Epoch:0059, train_loss=1.59625, train_acc=0.81831, val_loss=2.08180, val_acc=0.43066, time=1.18300
Epoch:0060, train_loss=1.59415, train_acc=0.81932, val_loss=2.08231, val_acc=0.43066, time=1.22202
Epoch:0061, train_loss=1.59209, train_acc=0.82094, val_loss=2.08281, val_acc=0.42518, time=1.13901
Epoch:0062, train_loss=1.59005, train_acc=0.82358, val_loss=2.08330, val_acc=0.42883, time=1.25501
Epoch:0063, train_loss=1.58806, train_acc=0.82682, val_loss=2.08380, val_acc=0.42883, time=1.13902
Epoch:0064, train_loss=1.58613, train_acc=0.82925, val_loss=2.08429, val_acc=0.42701, time=1.17501
Epoch:0065, train_loss=1.58421, train_acc=0.83188, val_loss=2.08479, val_acc=0.42701, time=1.13200
Epoch:0066, train_loss=1.58230, train_acc=0.83289, val_loss=2.08530, val_acc=0.42701, time=1.03201
Epoch:0067, train_loss=1.58042, train_acc=0.83512, val_loss=2.08580, val_acc=0.42518, time=1.20500
Epoch:0068, train_loss=1.57858, train_acc=0.83674, val_loss=2.08628, val_acc=0.42336, time=1.19201
Epoch:0069, train_loss=1.57677, train_acc=0.83816, val_loss=2.08676, val_acc=0.42153, time=1.09900
Epoch:0070, train_loss=1.57498, train_acc=0.84059, val_loss=2.08724, val_acc=0.41423, time=1.07500
Epoch:0071, train_loss=1.57323, train_acc=0.84201, val_loss=2.08773, val_acc=0.41241, time=1.25800
Epoch:0072, train_loss=1.57150, train_acc=0.84525, val_loss=2.08823, val_acc=0.41058, time=1.12400
Epoch:0073, train_loss=1.56979, train_acc=0.84849, val_loss=2.08875, val_acc=0.41241, time=1.17101
Epoch:0074, train_loss=1.56810, train_acc=0.85011, val_loss=2.08926, val_acc=0.41241, time=1.07802
Epoch:0075, train_loss=1.56645, train_acc=0.85254, val_loss=2.08977, val_acc=0.41058, time=0.99702
Epoch:0076, train_loss=1.56482, train_acc=0.85436, val_loss=2.09027, val_acc=0.41058, time=1.03100
Epoch:0077, train_loss=1.56322, train_acc=0.85518, val_loss=2.09077, val_acc=0.40876, time=1.18202
Epoch:0078, train_loss=1.56165, train_acc=0.85700, val_loss=2.09128, val_acc=0.40876, time=1.09001
Epoch:0079, train_loss=1.56010, train_acc=0.85882, val_loss=2.09179, val_acc=0.40693, time=1.00300
Epoch:0080, train_loss=1.55856, train_acc=0.86145, val_loss=2.09231, val_acc=0.40511, time=1.23302
Epoch:0081, train_loss=1.55704, train_acc=0.86206, val_loss=2.09283, val_acc=0.40328, time=1.15200
Epoch:0082, train_loss=1.55556, train_acc=0.86287, val_loss=2.09335, val_acc=0.40511, time=1.19700
Epoch:0083, train_loss=1.55409, train_acc=0.86571, val_loss=2.09386, val_acc=0.39599, time=1.10701
Epoch:0084, train_loss=1.55265, train_acc=0.86814, val_loss=2.09437, val_acc=0.39599, time=1.05601
Epoch:0085, train_loss=1.55123, train_acc=0.86996, val_loss=2.09489, val_acc=0.39781, time=1.04202
Epoch:0086, train_loss=1.54983, train_acc=0.87138, val_loss=2.09542, val_acc=0.39599, time=1.10000
Epoch:0087, train_loss=1.54844, train_acc=0.87199, val_loss=2.09595, val_acc=0.39599, time=1.20301
Epoch:0088, train_loss=1.54709, train_acc=0.87300, val_loss=2.09648, val_acc=0.39781, time=1.13302
Epoch:0089, train_loss=1.54575, train_acc=0.87401, val_loss=2.09700, val_acc=0.39781, time=1.23500
Epoch:0090, train_loss=1.54443, train_acc=0.87705, val_loss=2.09751, val_acc=0.39781, time=1.11900
Epoch:0091, train_loss=1.54313, train_acc=0.87928, val_loss=2.09804, val_acc=0.39599, time=1.08299
Epoch:0092, train_loss=1.54185, train_acc=0.88029, val_loss=2.09856, val_acc=0.39599, time=1.14701
Epoch:0093, train_loss=1.54059, train_acc=0.88151, val_loss=2.09909, val_acc=0.39599, time=1.03901
Epoch:0094, train_loss=1.53935, train_acc=0.88292, val_loss=2.09961, val_acc=0.39416, time=1.12201
Epoch:0095, train_loss=1.53813, train_acc=0.88414, val_loss=2.10012, val_acc=0.39234, time=1.21700
Epoch:0096, train_loss=1.53693, train_acc=0.88515, val_loss=2.10064, val_acc=0.38869, time=1.14201
Epoch:0097, train_loss=1.53575, train_acc=0.88617, val_loss=2.10117, val_acc=0.39051, time=1.15602
Epoch:0098, train_loss=1.53458, train_acc=0.88758, val_loss=2.10170, val_acc=0.39051, time=1.20900
Epoch:0099, train_loss=1.53343, train_acc=0.89001, val_loss=2.10222, val_acc=0.39051, time=1.07002
Epoch:0100, train_loss=1.53229, train_acc=0.89123, val_loss=2.10274, val_acc=0.39234, time=1.01500
Epoch:0101, train_loss=1.53117, train_acc=0.89285, val_loss=2.10326, val_acc=0.39051, time=1.05501
Epoch:0102, train_loss=1.53007, train_acc=0.89427, val_loss=2.10379, val_acc=0.38869, time=1.12300
Early stopping...

Optimization Finished!

Test set results: loss= 2.19010, accuracy= 0.40429, time= 0.31801

Test Precision, Recall and F1-Score...
              precision    recall  f1-score   support

           0     0.0698    0.0345    0.0462        87
           1     0.5045    0.6177    0.5554      1083
           2     0.3215    0.3003    0.3105       696
           3     0.0000    0.0000    0.0000        10
           4     0.0392    0.0267    0.0317        75
           5     0.0161    0.0083    0.0109       121
           6     0.0000    0.0000    0.0000        36
           7     0.0256    0.0123    0.0167        81

    accuracy                         0.4043      2189
   macro avg     0.1221    0.1250    0.1214      2189
weighted avg     0.3578    0.4043    0.3777      2189


Macro average Test Precision, Recall and F1-Score...
(0.12210206680629433, 0.12497193191092676, 0.12143280862334187, None)

Micro average Test Precision, Recall and F1-Score...
(0.4042941982640475, 0.4042941982640475, 0.4042941982640475, None)

Embeddings:
Word_embeddings:7688
Train_doc_embeddings:5485
Test_doc_embeddings:2189
