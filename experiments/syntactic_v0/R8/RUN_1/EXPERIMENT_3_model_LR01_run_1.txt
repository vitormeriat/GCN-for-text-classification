
==========: 152346469636200
Epoch:0001, train_loss=2.07122, train_acc=0.38505, val_loss=2.09446, val_acc=0.49270, time=1.24600
Epoch:0002, train_loss=2.18773, train_acc=0.52745, val_loss=2.16519, val_acc=0.32847, time=1.33899
Epoch:0003, train_loss=2.88374, train_acc=0.29066, val_loss=2.09716, val_acc=0.33394, time=1.18401
Epoch:0004, train_loss=2.16838, train_acc=0.38384, val_loss=2.09502, val_acc=0.50182, time=1.05301
Epoch:0005, train_loss=2.05325, train_acc=0.53514, val_loss=2.10029, val_acc=0.50365, time=1.08900
Epoch:0006, train_loss=2.05210, train_acc=0.53190, val_loss=2.08839, val_acc=0.49088, time=1.29301
Epoch:0007, train_loss=1.92122, train_acc=0.54568, val_loss=2.08061, val_acc=0.45438, time=0.98302
Epoch:0008, train_loss=1.84425, train_acc=0.56350, val_loss=2.08008, val_acc=0.37956, time=0.97201
Epoch:0009, train_loss=1.83343, train_acc=0.57707, val_loss=2.08078, val_acc=0.33759, time=1.19701
Epoch:0010, train_loss=1.83146, train_acc=0.59206, val_loss=2.08094, val_acc=0.31569, time=1.16400
Epoch:0011, train_loss=1.82356, train_acc=0.61373, val_loss=2.08057, val_acc=0.33212, time=1.01500
Epoch:0012, train_loss=1.80998, train_acc=0.63460, val_loss=2.07972, val_acc=0.34307, time=1.26599
Epoch:0013, train_loss=1.79199, train_acc=0.65303, val_loss=2.07855, val_acc=0.38869, time=1.35501
Epoch:0014, train_loss=1.77123, train_acc=0.68604, val_loss=2.07729, val_acc=0.39599, time=1.13000
Epoch:0015, train_loss=1.74976, train_acc=0.69820, val_loss=2.07618, val_acc=0.41058, time=1.05601
Epoch:0016, train_loss=1.72941, train_acc=0.70427, val_loss=2.07550, val_acc=0.43431, time=1.10999
Epoch:0017, train_loss=1.71213, train_acc=0.70711, val_loss=2.07544, val_acc=0.44891, time=1.06202
Epoch:0018, train_loss=1.69945, train_acc=0.70448, val_loss=2.07601, val_acc=0.45620, time=1.05799
Epoch:0019, train_loss=1.69178, train_acc=0.69840, val_loss=2.07709, val_acc=0.45985, time=0.95500
Epoch:0020, train_loss=1.68820, train_acc=0.69334, val_loss=2.07840, val_acc=0.46168, time=0.99303
Early stopping...

Optimization Finished!

Test set results: loss= 2.07137, accuracy= 0.46231, time= 0.33500

Test Precision, Recall and F1-Score...
              precision    recall  f1-score   support

           0     0.1429    0.0115    0.0213        87
           1     0.5003    0.8292    0.6240      1083
           2     0.3333    0.1595    0.2157       696
           3     0.0000    0.0000    0.0000        10
           4     0.0769    0.0133    0.0227        75
           5     0.0000    0.0000    0.0000       121
           6     0.0000    0.0000    0.0000        36
           7     0.0476    0.0123    0.0196        81

    accuracy                         0.4623      2189
   macro avg     0.1376    0.1282    0.1129      2189
weighted avg     0.3636    0.4623    0.3797      2189


Macro average Test Precision, Recall and F1-Score...
(0.13762639403307925, 0.1282292790649407, 0.11292495339656687, None)

Micro average Test Precision, Recall and F1-Score...
(0.4623115577889447, 0.4623115577889447, 0.4623115577889447, None)

Embeddings:
Word_embeddings:7688
Train_doc_embeddings:5485
Test_doc_embeddings:2189
