
==========: 157205640855900
Epoch:0001, train_loss=2.33462, train_acc=0.17663, val_loss=2.07814, val_acc=0.35584, time=1.50400
Epoch:0002, train_loss=2.05681, train_acc=0.32084, val_loss=2.06534, val_acc=0.45620, time=1.41301
Epoch:0003, train_loss=1.91994, train_acc=0.49038, val_loss=2.06591, val_acc=0.48905, time=1.32301
Epoch:0004, train_loss=1.91545, train_acc=0.52157, val_loss=2.06629, val_acc=0.48905, time=1.05400
Epoch:0005, train_loss=1.91340, train_acc=0.52907, val_loss=2.06442, val_acc=0.48540, time=0.98401
Epoch:0006, train_loss=1.89231, train_acc=0.53676, val_loss=2.06349, val_acc=0.46168, time=0.97501
Epoch:0007, train_loss=1.87939, train_acc=0.53717, val_loss=2.06399, val_acc=0.42883, time=1.07600
Epoch:0008, train_loss=1.87554, train_acc=0.51205, val_loss=2.06344, val_acc=0.42336, time=1.08101
Epoch:0009, train_loss=1.85773, train_acc=0.52481, val_loss=2.06217, val_acc=0.44343, time=1.19902
Epoch:0010, train_loss=1.83097, train_acc=0.56877, val_loss=2.06154, val_acc=0.45803, time=1.17101
Epoch:0011, train_loss=1.80948, train_acc=0.58781, val_loss=2.06190, val_acc=0.46715, time=0.96501
Epoch:0012, train_loss=1.79736, train_acc=0.59591, val_loss=2.06256, val_acc=0.47628, time=1.09999
Epoch:0013, train_loss=1.78936, train_acc=0.59692, val_loss=2.06292, val_acc=0.47445, time=1.06601
Epoch:0014, train_loss=1.78065, train_acc=0.60361, val_loss=2.06293, val_acc=0.46715, time=1.16100
Epoch:0015, train_loss=1.77075, train_acc=0.61475, val_loss=2.06283, val_acc=0.46715, time=1.04399
Epoch:0016, train_loss=1.76138, train_acc=0.63034, val_loss=2.06281, val_acc=0.44891, time=1.12901
Epoch:0017, train_loss=1.75377, train_acc=0.64472, val_loss=2.06291, val_acc=0.43431, time=1.09604
Epoch:0018, train_loss=1.74765, train_acc=0.64493, val_loss=2.06307, val_acc=0.43978, time=1.16400
Epoch:0019, train_loss=1.74183, train_acc=0.65100, val_loss=2.06320, val_acc=0.43796, time=1.04300
Epoch:0020, train_loss=1.73534, train_acc=0.65809, val_loss=2.06330, val_acc=0.44526, time=1.22500
Epoch:0021, train_loss=1.72817, train_acc=0.66397, val_loss=2.06346, val_acc=0.44891, time=1.08601
Epoch:0022, train_loss=1.72116, train_acc=0.66781, val_loss=2.06376, val_acc=0.45803, time=1.00500
Epoch:0023, train_loss=1.71524, train_acc=0.67025, val_loss=2.06420, val_acc=0.45985, time=1.27400
Epoch:0024, train_loss=1.71063, train_acc=0.66883, val_loss=2.06469, val_acc=0.45620, time=1.42299
Epoch:0025, train_loss=1.70673, train_acc=0.66802, val_loss=2.06515, val_acc=0.45803, time=1.00700
Epoch:0026, train_loss=1.70276, train_acc=0.66761, val_loss=2.06551, val_acc=0.45803, time=1.10500
Epoch:0027, train_loss=1.69829, train_acc=0.67409, val_loss=2.06582, val_acc=0.45620, time=1.12801
Epoch:0028, train_loss=1.69351, train_acc=0.68382, val_loss=2.06613, val_acc=0.44891, time=1.02700
Epoch:0029, train_loss=1.68883, train_acc=0.69374, val_loss=2.06650, val_acc=0.44708, time=1.10899
Epoch:0030, train_loss=1.68453, train_acc=0.70407, val_loss=2.06692, val_acc=0.43978, time=1.31302
Epoch:0031, train_loss=1.68052, train_acc=0.71116, val_loss=2.06736, val_acc=0.43978, time=1.30800
Epoch:0032, train_loss=1.67659, train_acc=0.71987, val_loss=2.06782, val_acc=0.43613, time=1.08801
Epoch:0033, train_loss=1.67269, train_acc=0.72392, val_loss=2.06831, val_acc=0.43978, time=1.17200
Epoch:0034, train_loss=1.66895, train_acc=0.72797, val_loss=2.06881, val_acc=0.43796, time=1.20001
Epoch:0035, train_loss=1.66554, train_acc=0.73081, val_loss=2.06933, val_acc=0.43613, time=1.01101
Epoch:0036, train_loss=1.66246, train_acc=0.73223, val_loss=2.06983, val_acc=0.44161, time=1.08600
Epoch:0037, train_loss=1.65951, train_acc=0.73526, val_loss=2.07028, val_acc=0.43978, time=1.01602
Epoch:0038, train_loss=1.65646, train_acc=0.73911, val_loss=2.07066, val_acc=0.43796, time=1.01800
Epoch:0039, train_loss=1.65320, train_acc=0.74580, val_loss=2.07098, val_acc=0.43796, time=0.94600
Epoch:0040, train_loss=1.64984, train_acc=0.75086, val_loss=2.07129, val_acc=0.43431, time=1.08101
Epoch:0041, train_loss=1.64657, train_acc=0.75329, val_loss=2.07160, val_acc=0.43248, time=1.20201
Epoch:0042, train_loss=1.64353, train_acc=0.75998, val_loss=2.07194, val_acc=0.43248, time=1.05000
Epoch:0043, train_loss=1.64073, train_acc=0.76484, val_loss=2.07231, val_acc=0.43248, time=1.11100
Epoch:0044, train_loss=1.63807, train_acc=0.76848, val_loss=2.07270, val_acc=0.43248, time=1.07901
Epoch:0045, train_loss=1.63545, train_acc=0.77031, val_loss=2.07311, val_acc=0.43066, time=1.07301
Epoch:0046, train_loss=1.63285, train_acc=0.77193, val_loss=2.07355, val_acc=0.43431, time=1.08801
Epoch:0047, train_loss=1.63029, train_acc=0.77395, val_loss=2.07400, val_acc=0.43431, time=1.11900
Epoch:0048, train_loss=1.62780, train_acc=0.77496, val_loss=2.07445, val_acc=0.43248, time=1.21400
Epoch:0049, train_loss=1.62531, train_acc=0.77618, val_loss=2.07489, val_acc=0.43248, time=1.01201
Epoch:0050, train_loss=1.62278, train_acc=0.77962, val_loss=2.07531, val_acc=0.43248, time=1.13701
Epoch:0051, train_loss=1.62021, train_acc=0.78489, val_loss=2.07574, val_acc=0.43066, time=1.05200
Epoch:0052, train_loss=1.61770, train_acc=0.79036, val_loss=2.07619, val_acc=0.43066, time=0.99600
Epoch:0053, train_loss=1.61530, train_acc=0.79340, val_loss=2.07665, val_acc=0.43066, time=1.14901
Epoch:0054, train_loss=1.61303, train_acc=0.79684, val_loss=2.07713, val_acc=0.43066, time=0.95901
Epoch:0055, train_loss=1.61083, train_acc=0.79988, val_loss=2.07762, val_acc=0.43066, time=1.05401
Epoch:0056, train_loss=1.60864, train_acc=0.80231, val_loss=2.07811, val_acc=0.42701, time=1.07101
Epoch:0057, train_loss=1.60646, train_acc=0.80474, val_loss=2.07860, val_acc=0.42518, time=1.10999
Epoch:0058, train_loss=1.60431, train_acc=0.80596, val_loss=2.07908, val_acc=0.42336, time=1.10602
Epoch:0059, train_loss=1.60220, train_acc=0.80778, val_loss=2.07955, val_acc=0.42518, time=1.12200
Epoch:0060, train_loss=1.60010, train_acc=0.81001, val_loss=2.08000, val_acc=0.42701, time=1.01400
Epoch:0061, train_loss=1.59801, train_acc=0.81345, val_loss=2.08043, val_acc=0.43248, time=1.12300
Epoch:0062, train_loss=1.59596, train_acc=0.81588, val_loss=2.08087, val_acc=0.42883, time=1.12300
Epoch:0063, train_loss=1.59397, train_acc=0.81811, val_loss=2.08131, val_acc=0.42883, time=1.01901
Epoch:0064, train_loss=1.59203, train_acc=0.82034, val_loss=2.08177, val_acc=0.43248, time=1.11502
Epoch:0065, train_loss=1.59010, train_acc=0.82398, val_loss=2.08224, val_acc=0.42883, time=1.07199
Epoch:0066, train_loss=1.58817, train_acc=0.82540, val_loss=2.08272, val_acc=0.42883, time=1.16101
Epoch:0067, train_loss=1.58627, train_acc=0.82743, val_loss=2.08321, val_acc=0.42701, time=1.00601
Epoch:0068, train_loss=1.58441, train_acc=0.83067, val_loss=2.08368, val_acc=0.42518, time=1.17601
Epoch:0069, train_loss=1.58257, train_acc=0.83249, val_loss=2.08415, val_acc=0.42883, time=1.12600
Epoch:0070, train_loss=1.58076, train_acc=0.83391, val_loss=2.08461, val_acc=0.42883, time=1.09501
Epoch:0071, train_loss=1.57898, train_acc=0.83755, val_loss=2.08508, val_acc=0.43066, time=1.06500
Epoch:0072, train_loss=1.57724, train_acc=0.83958, val_loss=2.08555, val_acc=0.43066, time=1.15001
Epoch:0073, train_loss=1.57553, train_acc=0.84079, val_loss=2.08603, val_acc=0.43248, time=1.02900
Epoch:0074, train_loss=1.57382, train_acc=0.84221, val_loss=2.08652, val_acc=0.43066, time=1.01700
Epoch:0075, train_loss=1.57213, train_acc=0.84545, val_loss=2.08702, val_acc=0.43066, time=1.27899
Epoch:0076, train_loss=1.57047, train_acc=0.84647, val_loss=2.08751, val_acc=0.43066, time=1.17302
Epoch:0077, train_loss=1.56883, train_acc=0.84869, val_loss=2.08799, val_acc=0.43066, time=1.14202
Epoch:0078, train_loss=1.56722, train_acc=0.84950, val_loss=2.08847, val_acc=0.42883, time=1.35800
Epoch:0079, train_loss=1.56563, train_acc=0.85133, val_loss=2.08895, val_acc=0.42518, time=1.24000
Epoch:0080, train_loss=1.56406, train_acc=0.85396, val_loss=2.08943, val_acc=0.42518, time=1.12202
Epoch:0081, train_loss=1.56252, train_acc=0.85416, val_loss=2.08991, val_acc=0.42336, time=1.11901
Epoch:0082, train_loss=1.56100, train_acc=0.85538, val_loss=2.09041, val_acc=0.42336, time=1.01700
Epoch:0083, train_loss=1.55949, train_acc=0.85680, val_loss=2.09090, val_acc=0.42336, time=1.10699
Epoch:0084, train_loss=1.55800, train_acc=0.85862, val_loss=2.09139, val_acc=0.42153, time=1.03701
Epoch:0085, train_loss=1.55654, train_acc=0.86024, val_loss=2.09186, val_acc=0.41971, time=1.21999
Epoch:0086, train_loss=1.55510, train_acc=0.86186, val_loss=2.09233, val_acc=0.41971, time=1.03501
Epoch:0087, train_loss=1.55368, train_acc=0.86409, val_loss=2.09281, val_acc=0.41606, time=1.18501
Epoch:0088, train_loss=1.55228, train_acc=0.86652, val_loss=2.09329, val_acc=0.41606, time=1.11901
Epoch:0089, train_loss=1.55090, train_acc=0.86854, val_loss=2.09379, val_acc=0.41606, time=1.03500
Epoch:0090, train_loss=1.54953, train_acc=0.86976, val_loss=2.09428, val_acc=0.41606, time=1.18400
Epoch:0091, train_loss=1.54818, train_acc=0.87178, val_loss=2.09477, val_acc=0.41423, time=1.07399
Epoch:0092, train_loss=1.54686, train_acc=0.87259, val_loss=2.09525, val_acc=0.41606, time=1.04000
Epoch:0093, train_loss=1.54555, train_acc=0.87422, val_loss=2.09573, val_acc=0.41606, time=1.06001
Epoch:0094, train_loss=1.54427, train_acc=0.87543, val_loss=2.09622, val_acc=0.41788, time=1.11601
Epoch:0095, train_loss=1.54300, train_acc=0.87563, val_loss=2.09672, val_acc=0.41971, time=0.98600
Epoch:0096, train_loss=1.54174, train_acc=0.87827, val_loss=2.09723, val_acc=0.41606, time=0.99601
Epoch:0097, train_loss=1.54051, train_acc=0.88110, val_loss=2.09772, val_acc=0.41606, time=1.15201
Epoch:0098, train_loss=1.53929, train_acc=0.88272, val_loss=2.09821, val_acc=0.41606, time=1.07799
Epoch:0099, train_loss=1.53809, train_acc=0.88475, val_loss=2.09870, val_acc=0.41606, time=0.97702
Epoch:0100, train_loss=1.53691, train_acc=0.88576, val_loss=2.09919, val_acc=0.41788, time=1.11001
Epoch:0101, train_loss=1.53575, train_acc=0.88718, val_loss=2.09969, val_acc=0.41788, time=1.10602
Epoch:0102, train_loss=1.53460, train_acc=0.88799, val_loss=2.10019, val_acc=0.41606, time=1.04701
Early stopping...

Optimization Finished!

Test set results: loss= 2.17529, accuracy= 0.40566, time= 0.33101

Test Precision, Recall and F1-Score...
              precision    recall  f1-score   support

           0     0.0444    0.0230    0.0303        87
           1     0.5056    0.6242    0.5587      1083
           2     0.3270    0.2960    0.3107       696
           3     0.0000    0.0000    0.0000        10
           4     0.0364    0.0267    0.0308        75
           5     0.0159    0.0083    0.0109       121
           6     0.0000    0.0000    0.0000        36
           7     0.0263    0.0123    0.0168        81

    accuracy                         0.4057      2189
   macro avg     0.1194    0.1238    0.1198      2189
weighted avg     0.3590    0.4057    0.3787      2189


Macro average Test Precision, Recall and F1-Score...
(0.1194488233514136, 0.12380429810317689, 0.11976689173416678, None)

Micro average Test Precision, Recall and F1-Score...
(0.40566468707172226, 0.40566468707172226, 0.4056646870717222, None)

Embeddings:
Word_embeddings:7688
Train_doc_embeddings:5485
Test_doc_embeddings:2189
