
==========: 158454874082200
Epoch:0001, train_loss=2.24266, train_acc=0.04173, val_loss=2.09467, val_acc=0.04015, time=1.31102
Epoch:0002, train_loss=2.22620, train_acc=0.04780, val_loss=2.09298, val_acc=0.05839, time=1.39900
Epoch:0003, train_loss=2.21002, train_acc=0.05915, val_loss=2.09132, val_acc=0.07482, time=1.06400
Epoch:0004, train_loss=2.19416, train_acc=0.07515, val_loss=2.08969, val_acc=0.10949, time=1.08801
Epoch:0005, train_loss=2.17864, train_acc=0.09581, val_loss=2.08811, val_acc=0.14416, time=1.07200
Epoch:0006, train_loss=2.16346, train_acc=0.12072, val_loss=2.08656, val_acc=0.16788, time=1.19799
Epoch:0007, train_loss=2.14863, train_acc=0.14989, val_loss=2.08506, val_acc=0.18248, time=1.05200
Epoch:0008, train_loss=2.13417, train_acc=0.17318, val_loss=2.08360, val_acc=0.20803, time=0.98301
Epoch:0009, train_loss=2.12012, train_acc=0.19587, val_loss=2.08218, val_acc=0.22628, time=1.08302
Epoch:0010, train_loss=2.10646, train_acc=0.21997, val_loss=2.08080, val_acc=0.25730, time=1.01800
Epoch:0011, train_loss=2.09322, train_acc=0.24408, val_loss=2.07948, val_acc=0.28467, time=0.97200
Epoch:0012, train_loss=2.08042, train_acc=0.26757, val_loss=2.07820, val_acc=0.29380, time=1.35401
Epoch:0013, train_loss=2.06808, train_acc=0.28843, val_loss=2.07697, val_acc=0.30474, time=1.16201
Epoch:0014, train_loss=2.05621, train_acc=0.30747, val_loss=2.07580, val_acc=0.32847, time=1.07901
Epoch:0015, train_loss=2.04481, train_acc=0.32449, val_loss=2.07467, val_acc=0.34124, time=1.00800
Epoch:0016, train_loss=2.03388, train_acc=0.34191, val_loss=2.07360, val_acc=0.36131, time=1.06502
Epoch:0017, train_loss=2.02343, train_acc=0.35953, val_loss=2.07257, val_acc=0.36496, time=0.97301
Epoch:0018, train_loss=2.01346, train_acc=0.37108, val_loss=2.07160, val_acc=0.37956, time=1.03301
Epoch:0019, train_loss=2.00398, train_acc=0.38789, val_loss=2.07068, val_acc=0.38139, time=1.03003
Epoch:0020, train_loss=1.99497, train_acc=0.40207, val_loss=2.06982, val_acc=0.38504, time=1.02899
Epoch:0021, train_loss=1.98645, train_acc=0.41260, val_loss=2.06900, val_acc=0.38686, time=1.05501
Epoch:0022, train_loss=1.97839, train_acc=0.42657, val_loss=2.06823, val_acc=0.39781, time=1.20101
Epoch:0023, train_loss=1.97079, train_acc=0.43569, val_loss=2.06750, val_acc=0.40693, time=1.11901
Epoch:0024, train_loss=1.96364, train_acc=0.44399, val_loss=2.06683, val_acc=0.41423, time=1.01199
Epoch:0025, train_loss=1.95692, train_acc=0.44805, val_loss=2.06620, val_acc=0.42701, time=1.06601
Epoch:0026, train_loss=1.95060, train_acc=0.45453, val_loss=2.06560, val_acc=0.43796, time=1.32602
Epoch:0027, train_loss=1.94468, train_acc=0.46121, val_loss=2.06505, val_acc=0.43796, time=0.99301
Epoch:0028, train_loss=1.93912, train_acc=0.46709, val_loss=2.06454, val_acc=0.44526, time=0.97900
Epoch:0029, train_loss=1.93391, train_acc=0.47255, val_loss=2.06406, val_acc=0.44343, time=1.05899
Epoch:0030, train_loss=1.92903, train_acc=0.47802, val_loss=2.06362, val_acc=0.45073, time=1.04001
Epoch:0031, train_loss=1.92446, train_acc=0.48187, val_loss=2.06320, val_acc=0.45073, time=1.02001
Epoch:0032, train_loss=1.92016, train_acc=0.48613, val_loss=2.06282, val_acc=0.45073, time=1.09401
Epoch:0033, train_loss=1.91612, train_acc=0.48977, val_loss=2.06246, val_acc=0.45255, time=1.09900
Epoch:0034, train_loss=1.91233, train_acc=0.49321, val_loss=2.06213, val_acc=0.45255, time=1.33601
Epoch:0035, train_loss=1.90876, train_acc=0.49646, val_loss=2.06181, val_acc=0.45438, time=1.30301
Epoch:0036, train_loss=1.90540, train_acc=0.49848, val_loss=2.06152, val_acc=0.45803, time=1.05702
Epoch:0037, train_loss=1.90221, train_acc=0.50091, val_loss=2.06125, val_acc=0.45985, time=1.04099
Epoch:0038, train_loss=1.89920, train_acc=0.50435, val_loss=2.06100, val_acc=0.45985, time=1.15401
Epoch:0039, train_loss=1.89635, train_acc=0.50557, val_loss=2.06077, val_acc=0.46350, time=1.07303
Epoch:0040, train_loss=1.89365, train_acc=0.50780, val_loss=2.06055, val_acc=0.46168, time=0.95299
Epoch:0041, train_loss=1.89107, train_acc=0.51104, val_loss=2.06034, val_acc=0.46168, time=1.03301
Epoch:0042, train_loss=1.88863, train_acc=0.51144, val_loss=2.06015, val_acc=0.46533, time=1.04702
Epoch:0043, train_loss=1.88629, train_acc=0.51286, val_loss=2.05997, val_acc=0.46533, time=1.14101
Epoch:0044, train_loss=1.88406, train_acc=0.51428, val_loss=2.05980, val_acc=0.46533, time=1.09902
Epoch:0045, train_loss=1.88192, train_acc=0.51529, val_loss=2.05965, val_acc=0.46715, time=1.17401
Epoch:0046, train_loss=1.87988, train_acc=0.51610, val_loss=2.05950, val_acc=0.46715, time=1.05802
Epoch:0047, train_loss=1.87791, train_acc=0.51853, val_loss=2.05937, val_acc=0.47080, time=1.05100
Epoch:0048, train_loss=1.87602, train_acc=0.51975, val_loss=2.05924, val_acc=0.47445, time=1.04200
Epoch:0049, train_loss=1.87420, train_acc=0.52036, val_loss=2.05912, val_acc=0.47263, time=1.04501
Epoch:0050, train_loss=1.87244, train_acc=0.52218, val_loss=2.05901, val_acc=0.47263, time=0.96201
Epoch:0051, train_loss=1.87074, train_acc=0.52279, val_loss=2.05891, val_acc=0.47263, time=1.08300
Epoch:0052, train_loss=1.86909, train_acc=0.52339, val_loss=2.05882, val_acc=0.46898, time=1.09801
Epoch:0053, train_loss=1.86749, train_acc=0.52441, val_loss=2.05873, val_acc=0.46715, time=0.96201
Epoch:0054, train_loss=1.86594, train_acc=0.52664, val_loss=2.05865, val_acc=0.46715, time=0.98501
Epoch:0055, train_loss=1.86443, train_acc=0.52765, val_loss=2.05857, val_acc=0.46715, time=0.96801
Epoch:0056, train_loss=1.86295, train_acc=0.52765, val_loss=2.05850, val_acc=0.46715, time=1.06001
Epoch:0057, train_loss=1.86151, train_acc=0.52846, val_loss=2.05843, val_acc=0.46898, time=1.07801
Epoch:0058, train_loss=1.86010, train_acc=0.52947, val_loss=2.05837, val_acc=0.47080, time=0.94801
Epoch:0059, train_loss=1.85872, train_acc=0.53028, val_loss=2.05832, val_acc=0.46898, time=1.00900
Epoch:0060, train_loss=1.85736, train_acc=0.53231, val_loss=2.05826, val_acc=0.46898, time=1.06401
Epoch:0061, train_loss=1.85603, train_acc=0.53352, val_loss=2.05821, val_acc=0.46898, time=1.05900
Epoch:0062, train_loss=1.85473, train_acc=0.53595, val_loss=2.05817, val_acc=0.46715, time=1.11000
Epoch:0063, train_loss=1.85344, train_acc=0.53717, val_loss=2.05813, val_acc=0.46533, time=1.04301
Epoch:0064, train_loss=1.85218, train_acc=0.53859, val_loss=2.05809, val_acc=0.46533, time=1.07801
Epoch:0065, train_loss=1.85094, train_acc=0.53980, val_loss=2.05805, val_acc=0.46350, time=1.09001
Epoch:0066, train_loss=1.84971, train_acc=0.54102, val_loss=2.05802, val_acc=0.46533, time=1.12902
Epoch:0067, train_loss=1.84851, train_acc=0.54223, val_loss=2.05799, val_acc=0.46350, time=1.14102
Epoch:0068, train_loss=1.84732, train_acc=0.54345, val_loss=2.05796, val_acc=0.46350, time=1.03301
Epoch:0069, train_loss=1.84615, train_acc=0.54365, val_loss=2.05794, val_acc=0.46715, time=0.97299
Epoch:0070, train_loss=1.84500, train_acc=0.54385, val_loss=2.05791, val_acc=0.46898, time=1.00402
Epoch:0071, train_loss=1.84386, train_acc=0.54608, val_loss=2.05789, val_acc=0.46898, time=1.02402
Epoch:0072, train_loss=1.84274, train_acc=0.54851, val_loss=2.05787, val_acc=0.46898, time=1.04200
Epoch:0073, train_loss=1.84163, train_acc=0.54993, val_loss=2.05785, val_acc=0.46898, time=1.01100
Epoch:0074, train_loss=1.84053, train_acc=0.55155, val_loss=2.05783, val_acc=0.46533, time=1.06002
Epoch:0075, train_loss=1.83945, train_acc=0.55236, val_loss=2.05781, val_acc=0.46168, time=0.94199
Epoch:0076, train_loss=1.83838, train_acc=0.55317, val_loss=2.05780, val_acc=0.46168, time=1.10301
Epoch:0077, train_loss=1.83733, train_acc=0.55398, val_loss=2.05778, val_acc=0.46350, time=1.09202
Epoch:0078, train_loss=1.83628, train_acc=0.55479, val_loss=2.05777, val_acc=0.46350, time=1.06000
Epoch:0079, train_loss=1.83525, train_acc=0.55580, val_loss=2.05776, val_acc=0.46350, time=0.97800
Epoch:0080, train_loss=1.83423, train_acc=0.55641, val_loss=2.05775, val_acc=0.46533, time=1.03602
Epoch:0081, train_loss=1.83322, train_acc=0.55783, val_loss=2.05773, val_acc=0.46533, time=1.05500
Epoch:0082, train_loss=1.83221, train_acc=0.55844, val_loss=2.05773, val_acc=0.46350, time=0.94501
Epoch:0083, train_loss=1.83122, train_acc=0.56026, val_loss=2.05772, val_acc=0.46533, time=0.96300
Epoch:0084, train_loss=1.83024, train_acc=0.56127, val_loss=2.05771, val_acc=0.46533, time=1.04802
Epoch:0085, train_loss=1.82927, train_acc=0.56127, val_loss=2.05770, val_acc=0.46533, time=1.03500
Epoch:0086, train_loss=1.82831, train_acc=0.56188, val_loss=2.05769, val_acc=0.46533, time=1.13702
Epoch:0087, train_loss=1.82736, train_acc=0.56289, val_loss=2.05769, val_acc=0.46533, time=1.04600
Epoch:0088, train_loss=1.82641, train_acc=0.56391, val_loss=2.05768, val_acc=0.46168, time=1.16401
Epoch:0089, train_loss=1.82548, train_acc=0.56472, val_loss=2.05768, val_acc=0.46350, time=1.03301
Epoch:0090, train_loss=1.82455, train_acc=0.56492, val_loss=2.05767, val_acc=0.46350, time=1.16601
Epoch:0091, train_loss=1.82363, train_acc=0.56654, val_loss=2.05767, val_acc=0.46715, time=1.24999
Epoch:0092, train_loss=1.82272, train_acc=0.56735, val_loss=2.05767, val_acc=0.46715, time=1.26000
Epoch:0093, train_loss=1.82182, train_acc=0.56816, val_loss=2.05767, val_acc=0.46715, time=1.00601
Epoch:0094, train_loss=1.82092, train_acc=0.56937, val_loss=2.05767, val_acc=0.46898, time=0.96501
Epoch:0095, train_loss=1.82003, train_acc=0.57018, val_loss=2.05767, val_acc=0.46898, time=1.10001
Epoch:0096, train_loss=1.81915, train_acc=0.57099, val_loss=2.05767, val_acc=0.46715, time=1.00799
Epoch:0097, train_loss=1.81828, train_acc=0.57160, val_loss=2.05767, val_acc=0.46715, time=1.02600
Epoch:0098, train_loss=1.81741, train_acc=0.57241, val_loss=2.05767, val_acc=0.46533, time=1.11301
Epoch:0099, train_loss=1.81655, train_acc=0.57403, val_loss=2.05767, val_acc=0.46533, time=1.00801
Early stopping...

Optimization Finished!

Test set results: loss= 2.00086, accuracy= 0.46323, time= 0.34498

Test Precision, Recall and F1-Score...
              precision    recall  f1-score   support

           0     0.0000    0.0000    0.0000        87
           1     0.4945    0.8264    0.6187      1083
           2     0.3343    0.1710    0.2262       696
           3     0.0000    0.0000    0.0000        10
           4     0.0000    0.0000    0.0000        75
           5     0.0000    0.0000    0.0000       121
           6     0.0000    0.0000    0.0000        36
           7     0.0000    0.0000    0.0000        81

    accuracy                         0.4632      2189
   macro avg     0.1036    0.1247    0.1056      2189
weighted avg     0.3509    0.4632    0.3780      2189


Macro average Test Precision, Recall and F1-Score...
(0.10359310013036191, 0.1246731421339192, 0.10562132734185967, None)

Micro average Test Precision, Recall and F1-Score...
(0.4632252169940612, 0.4632252169940612, 0.4632252169940612, None)

Embeddings:
Word_embeddings:7688
Train_doc_embeddings:5485
Test_doc_embeddings:2189
