
==========: 161371276215800
Epoch:0001, train_loss=2.34796, train_acc=0.04476, val_loss=2.09034, val_acc=0.16423, time=1.30400
Epoch:0002, train_loss=2.17156, train_acc=0.16305, val_loss=2.07625, val_acc=0.33577, time=1.40501
Epoch:0003, train_loss=2.03492, train_acc=0.35831, val_loss=2.06753, val_acc=0.42153, time=1.32602
Epoch:0004, train_loss=1.94850, train_acc=0.45939, val_loss=2.06344, val_acc=0.43796, time=1.25200
Epoch:0005, train_loss=1.90627, train_acc=0.49139, val_loss=2.06209, val_acc=0.45073, time=0.97800
Epoch:0006, train_loss=1.88987, train_acc=0.51023, val_loss=2.06183, val_acc=0.45255, time=1.03501
Epoch:0007, train_loss=1.88366, train_acc=0.51793, val_loss=2.06187, val_acc=0.44161, time=1.08099
Epoch:0008, train_loss=1.88009, train_acc=0.52704, val_loss=2.06205, val_acc=0.44708, time=0.94701
Epoch:0009, train_loss=1.87748, train_acc=0.53393, val_loss=2.06238, val_acc=0.42883, time=0.98500
Epoch:0010, train_loss=1.87555, train_acc=0.53960, val_loss=2.06272, val_acc=0.43066, time=1.10501
Epoch:0011, train_loss=1.87266, train_acc=0.54223, val_loss=2.06287, val_acc=0.42883, time=1.06300
Epoch:0012, train_loss=1.86689, train_acc=0.55054, val_loss=2.06278, val_acc=0.43248, time=1.15901
Epoch:0013, train_loss=1.85811, train_acc=0.55945, val_loss=2.06258, val_acc=0.43796, time=1.04402
Epoch:0014, train_loss=1.84775, train_acc=0.57059, val_loss=2.06241, val_acc=0.44708, time=1.10100
Epoch:0015, train_loss=1.83731, train_acc=0.57444, val_loss=2.06230, val_acc=0.46168, time=1.03699
Epoch:0016, train_loss=1.82755, train_acc=0.57970, val_loss=2.06225, val_acc=0.47263, time=1.09600
Epoch:0017, train_loss=1.81843, train_acc=0.58416, val_loss=2.06218, val_acc=0.47445, time=1.06101
Epoch:0018, train_loss=1.80970, train_acc=0.58801, val_loss=2.06209, val_acc=0.46715, time=1.21102
Epoch:0019, train_loss=1.80121, train_acc=0.59307, val_loss=2.06199, val_acc=0.47080, time=1.09000
Epoch:0020, train_loss=1.79305, train_acc=0.59996, val_loss=2.06189, val_acc=0.45803, time=1.21403
Epoch:0021, train_loss=1.78537, train_acc=0.61069, val_loss=2.06182, val_acc=0.45620, time=0.95000
Epoch:0022, train_loss=1.77830, train_acc=0.62123, val_loss=2.06179, val_acc=0.44708, time=0.97401
Epoch:0023, train_loss=1.77182, train_acc=0.63034, val_loss=2.06180, val_acc=0.44161, time=1.10701
Epoch:0024, train_loss=1.76580, train_acc=0.63844, val_loss=2.06182, val_acc=0.44161, time=1.10500
Epoch:0025, train_loss=1.76012, train_acc=0.64331, val_loss=2.06187, val_acc=0.44708, time=1.02201
Epoch:0026, train_loss=1.75474, train_acc=0.64614, val_loss=2.06194, val_acc=0.44891, time=1.01199
Epoch:0027, train_loss=1.74970, train_acc=0.64877, val_loss=2.06204, val_acc=0.44526, time=0.94401
Early stopping...

Optimization Finished!

Test set results: loss= 2.00855, accuracy= 0.44861, time= 0.29400

Test Precision, Recall and F1-Score...
              precision    recall  f1-score   support

           0     0.0741    0.0230    0.0351        87
           1     0.4959    0.7876    0.6086      1083
           2     0.3173    0.1796    0.2294       696
           3     0.0000    0.0000    0.0000        10
           4     0.0588    0.0133    0.0217        75
           5     0.0435    0.0083    0.0139       121
           6     0.0000    0.0000    0.0000        36
           7     0.0000    0.0000    0.0000        81

    accuracy                         0.4486      2189
   macro avg     0.1237    0.1265    0.1136      2189
weighted avg     0.3536    0.4486    0.3770      2189


Macro average Test Precision, Recall and F1-Score...
(0.1236956225202843, 0.12647637064775, 0.1135883929546073, None)

Micro average Test Precision, Recall and F1-Score...
(0.44860666971219737, 0.44860666971219737, 0.4486066697121973, None)

Embeddings:
Word_embeddings:7688
Train_doc_embeddings:5485
Test_doc_embeddings:2189
