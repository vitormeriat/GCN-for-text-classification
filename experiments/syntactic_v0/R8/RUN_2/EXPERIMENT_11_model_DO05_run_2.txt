
==========: 156903166969400
Epoch:0001, train_loss=2.36755, train_acc=0.06522, val_loss=2.08337, val_acc=0.43796, time=1.24600
Epoch:0002, train_loss=2.10285, train_acc=0.45149, val_loss=2.07142, val_acc=0.49270, time=1.26000
Epoch:0003, train_loss=1.98998, train_acc=0.51063, val_loss=2.06402, val_acc=0.50000, time=1.30200
Epoch:0004, train_loss=1.92100, train_acc=0.51934, val_loss=2.05907, val_acc=0.47993, time=1.06502
Epoch:0005, train_loss=1.87566, train_acc=0.50841, val_loss=2.06007, val_acc=0.39416, time=1.10300
Epoch:0006, train_loss=1.88625, train_acc=0.43549, val_loss=2.06272, val_acc=0.37044, time=0.97101
Epoch:0007, train_loss=1.90487, train_acc=0.41260, val_loss=2.06277, val_acc=0.39051, time=0.95599
Epoch:0008, train_loss=1.89106, train_acc=0.45534, val_loss=2.06192, val_acc=0.43978, time=1.07402
Epoch:0009, train_loss=1.86389, train_acc=0.50456, val_loss=2.06201, val_acc=0.45985, time=0.97500
Epoch:0010, train_loss=1.84471, train_acc=0.54689, val_loss=2.06288, val_acc=0.49818, time=1.08002
Epoch:0011, train_loss=1.83521, train_acc=0.55702, val_loss=2.06330, val_acc=0.49453, time=0.99099
Epoch:0012, train_loss=1.82486, train_acc=0.55783, val_loss=2.06273, val_acc=0.49635, time=1.07901
Epoch:0013, train_loss=1.80852, train_acc=0.56208, val_loss=2.06155, val_acc=0.49635, time=0.97102
Epoch:0014, train_loss=1.78891, train_acc=0.57667, val_loss=2.06045, val_acc=0.50182, time=1.12700
Epoch:0015, train_loss=1.77155, train_acc=0.59611, val_loss=2.05989, val_acc=0.48175, time=1.18301
Epoch:0016, train_loss=1.75995, train_acc=0.62366, val_loss=2.05992, val_acc=0.44891, time=1.02801
Epoch:0017, train_loss=1.75368, train_acc=0.62710, val_loss=2.06029, val_acc=0.41788, time=1.04601
Epoch:0018, train_loss=1.74984, train_acc=0.61859, val_loss=2.06074, val_acc=0.41606, time=1.17301
Epoch:0019, train_loss=1.74563, train_acc=0.62123, val_loss=2.06115, val_acc=0.42518, time=0.97401
Epoch:0020, train_loss=1.74000, train_acc=0.63743, val_loss=2.06154, val_acc=0.45438, time=1.01900
Early stopping...

Optimization Finished!

Test set results: loss= 2.01170, accuracy= 0.43810, time= 0.29100

Test Precision, Recall and F1-Score...
              precision    recall  f1-score   support

           0     0.1905    0.0460    0.0741        87
           1     0.5023    0.6999    0.5849      1083
           2     0.3271    0.2773    0.3002       696
           3     0.0000    0.0000    0.0000        10
           4     0.0833    0.0400    0.0541        75
           5     0.0000    0.0000    0.0000       121
           6     0.0000    0.0000    0.0000        36
           7     0.0909    0.0123    0.0217        81

    accuracy                         0.4381      2189
   macro avg     0.1493    0.1344    0.1294      2189
weighted avg     0.3663    0.4381    0.3904      2189


Macro average Test Precision, Recall and F1-Score...
(0.14926958445234384, 0.13444115062223685, 0.1293624153460152, None)

Micro average Test Precision, Recall and F1-Score...
(0.4380995888533577, 0.4380995888533577, 0.4380995888533577, None)

Embeddings:
Word_embeddings:7688
Train_doc_embeddings:5485
Test_doc_embeddings:2189
