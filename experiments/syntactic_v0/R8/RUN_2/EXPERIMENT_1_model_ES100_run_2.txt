
==========: 154448597092200
Epoch:0001, train_loss=2.24446, train_acc=0.03524, val_loss=2.06914, val_acc=0.43978, time=1.51700
Epoch:0002, train_loss=1.98240, train_acc=0.45554, val_loss=2.05976, val_acc=0.49088, time=1.36601
Epoch:0003, train_loss=1.89864, train_acc=0.50435, val_loss=2.05799, val_acc=0.49270, time=1.33900
Epoch:0004, train_loss=1.88044, train_acc=0.50658, val_loss=2.05895, val_acc=0.45803, time=1.16100
Epoch:0005, train_loss=1.88157, train_acc=0.50111, val_loss=2.05981, val_acc=0.45985, time=1.14699
Epoch:0006, train_loss=1.87461, train_acc=0.52684, val_loss=2.06023, val_acc=0.47080, time=1.03101
Epoch:0007, train_loss=1.86009, train_acc=0.55135, val_loss=2.06039, val_acc=0.48175, time=1.01100
Epoch:0008, train_loss=1.84379, train_acc=0.56107, val_loss=2.06022, val_acc=0.48175, time=1.05202
Epoch:0009, train_loss=1.82672, train_acc=0.57282, val_loss=2.06014, val_acc=0.47993, time=1.06702
Epoch:0010, train_loss=1.81212, train_acc=0.58801, val_loss=2.06032, val_acc=0.45620, time=1.07300
Epoch:0011, train_loss=1.80070, train_acc=0.60057, val_loss=2.06045, val_acc=0.43248, time=1.03402
Epoch:0012, train_loss=1.78868, train_acc=0.61333, val_loss=2.06039, val_acc=0.44526, time=1.26100
Epoch:0013, train_loss=1.77460, train_acc=0.62386, val_loss=2.06036, val_acc=0.45073, time=1.15801
Epoch:0014, train_loss=1.76080, train_acc=0.63034, val_loss=2.06060, val_acc=0.46533, time=1.04402
Epoch:0015, train_loss=1.74979, train_acc=0.63460, val_loss=2.06107, val_acc=0.47263, time=1.08401
Epoch:0016, train_loss=1.74193, train_acc=0.63986, val_loss=2.06166, val_acc=0.47263, time=0.99499
Epoch:0017, train_loss=1.73622, train_acc=0.64391, val_loss=2.06228, val_acc=0.46533, time=1.08501
Epoch:0018, train_loss=1.73170, train_acc=0.65627, val_loss=2.06289, val_acc=0.45620, time=1.13100
Epoch:0019, train_loss=1.72760, train_acc=0.66498, val_loss=2.06346, val_acc=0.44708, time=1.08402
Epoch:0020, train_loss=1.72312, train_acc=0.66984, val_loss=2.06395, val_acc=0.43796, time=1.25801
Epoch:0021, train_loss=1.71772, train_acc=0.67632, val_loss=2.06437, val_acc=0.43613, time=1.34401
Epoch:0022, train_loss=1.71150, train_acc=0.68159, val_loss=2.06475, val_acc=0.44161, time=1.21202
Epoch:0023, train_loss=1.70498, train_acc=0.68523, val_loss=2.06511, val_acc=0.44343, time=1.09500
Epoch:0024, train_loss=1.69861, train_acc=0.68807, val_loss=2.06545, val_acc=0.44343, time=0.98200
Epoch:0025, train_loss=1.69251, train_acc=0.69070, val_loss=2.06577, val_acc=0.44526, time=0.97103
Epoch:0026, train_loss=1.68680, train_acc=0.70002, val_loss=2.06612, val_acc=0.44526, time=1.04999
Epoch:0027, train_loss=1.68167, train_acc=0.70630, val_loss=2.06653, val_acc=0.44161, time=1.07500
Epoch:0028, train_loss=1.67726, train_acc=0.71643, val_loss=2.06702, val_acc=0.43248, time=0.95101
Epoch:0029, train_loss=1.67343, train_acc=0.72271, val_loss=2.06756, val_acc=0.42883, time=1.06201
Epoch:0030, train_loss=1.66990, train_acc=0.72797, val_loss=2.06813, val_acc=0.42883, time=1.08100
Epoch:0031, train_loss=1.66643, train_acc=0.73202, val_loss=2.06869, val_acc=0.43066, time=1.08701
Epoch:0032, train_loss=1.66288, train_acc=0.73385, val_loss=2.06921, val_acc=0.43431, time=0.99301
Epoch:0033, train_loss=1.65920, train_acc=0.73749, val_loss=2.06968, val_acc=0.43248, time=0.95299
Epoch:0034, train_loss=1.65536, train_acc=0.74600, val_loss=2.07012, val_acc=0.42518, time=0.95600
Epoch:0035, train_loss=1.65148, train_acc=0.75208, val_loss=2.07055, val_acc=0.42518, time=0.95401
Epoch:0036, train_loss=1.64771, train_acc=0.75775, val_loss=2.07101, val_acc=0.42701, time=1.12301
Epoch:0037, train_loss=1.64417, train_acc=0.76261, val_loss=2.07152, val_acc=0.42336, time=0.95399
Epoch:0038, train_loss=1.64081, train_acc=0.76646, val_loss=2.07207, val_acc=0.42883, time=1.14000
Epoch:0039, train_loss=1.63759, train_acc=0.76990, val_loss=2.07265, val_acc=0.42518, time=1.11502
Epoch:0040, train_loss=1.63452, train_acc=0.77294, val_loss=2.07324, val_acc=0.41971, time=1.01701
Epoch:0041, train_loss=1.63159, train_acc=0.77456, val_loss=2.07382, val_acc=0.42153, time=1.04201
Epoch:0042, train_loss=1.62874, train_acc=0.77800, val_loss=2.07438, val_acc=0.41971, time=1.01501
Epoch:0043, train_loss=1.62594, train_acc=0.78084, val_loss=2.07492, val_acc=0.41423, time=1.02200
Epoch:0044, train_loss=1.62315, train_acc=0.78813, val_loss=2.07544, val_acc=0.41058, time=1.15400
Epoch:0045, train_loss=1.62040, train_acc=0.79178, val_loss=2.07595, val_acc=0.40693, time=1.12202
Epoch:0046, train_loss=1.61765, train_acc=0.79319, val_loss=2.07646, val_acc=0.40693, time=1.02600
Epoch:0047, train_loss=1.61493, train_acc=0.79664, val_loss=2.07698, val_acc=0.40876, time=1.09801
Epoch:0048, train_loss=1.61226, train_acc=0.79806, val_loss=2.07749, val_acc=0.40876, time=0.94700
Epoch:0049, train_loss=1.60966, train_acc=0.79988, val_loss=2.07799, val_acc=0.41058, time=1.21801
Epoch:0050, train_loss=1.60714, train_acc=0.80251, val_loss=2.07849, val_acc=0.41058, time=1.02202
Epoch:0051, train_loss=1.60469, train_acc=0.80494, val_loss=2.07898, val_acc=0.41241, time=1.03000
Epoch:0052, train_loss=1.60232, train_acc=0.80778, val_loss=2.07949, val_acc=0.41606, time=1.02100
Epoch:0053, train_loss=1.60001, train_acc=0.80940, val_loss=2.08001, val_acc=0.41423, time=1.07002
Epoch:0054, train_loss=1.59774, train_acc=0.81142, val_loss=2.08055, val_acc=0.41423, time=0.95497
Epoch:0055, train_loss=1.59547, train_acc=0.81466, val_loss=2.08109, val_acc=0.41606, time=0.94001
Epoch:0056, train_loss=1.59322, train_acc=0.81791, val_loss=2.08162, val_acc=0.41423, time=1.12401
Epoch:0057, train_loss=1.59100, train_acc=0.82094, val_loss=2.08215, val_acc=0.41423, time=1.04801
Epoch:0058, train_loss=1.58881, train_acc=0.82236, val_loss=2.08268, val_acc=0.41606, time=1.10700
Epoch:0059, train_loss=1.58667, train_acc=0.82479, val_loss=2.08322, val_acc=0.41423, time=1.06101
Epoch:0060, train_loss=1.58458, train_acc=0.82641, val_loss=2.08378, val_acc=0.41606, time=1.03901
Epoch:0061, train_loss=1.58255, train_acc=0.82925, val_loss=2.08435, val_acc=0.41423, time=1.10903
Epoch:0062, train_loss=1.58054, train_acc=0.83107, val_loss=2.08491, val_acc=0.41606, time=1.08102
Epoch:0063, train_loss=1.57858, train_acc=0.83370, val_loss=2.08547, val_acc=0.41606, time=0.96400
Epoch:0064, train_loss=1.57664, train_acc=0.83451, val_loss=2.08601, val_acc=0.41423, time=0.95000
Epoch:0065, train_loss=1.57472, train_acc=0.83634, val_loss=2.08656, val_acc=0.41423, time=0.99499
Epoch:0066, train_loss=1.57283, train_acc=0.83978, val_loss=2.08711, val_acc=0.41241, time=0.95501
Epoch:0067, train_loss=1.57098, train_acc=0.84201, val_loss=2.08768, val_acc=0.40693, time=0.96002
Epoch:0068, train_loss=1.56916, train_acc=0.84383, val_loss=2.08826, val_acc=0.40693, time=1.01299
Epoch:0069, train_loss=1.56737, train_acc=0.84647, val_loss=2.08884, val_acc=0.40876, time=1.12900
Epoch:0070, train_loss=1.56561, train_acc=0.84829, val_loss=2.08941, val_acc=0.40876, time=0.96601
Epoch:0071, train_loss=1.56388, train_acc=0.85052, val_loss=2.08998, val_acc=0.40876, time=1.15801
Epoch:0072, train_loss=1.56218, train_acc=0.85295, val_loss=2.09055, val_acc=0.40693, time=1.00898
Epoch:0073, train_loss=1.56051, train_acc=0.85518, val_loss=2.09114, val_acc=0.40511, time=0.99702
Epoch:0074, train_loss=1.55886, train_acc=0.85740, val_loss=2.09172, val_acc=0.40146, time=0.94601
Epoch:0075, train_loss=1.55724, train_acc=0.86004, val_loss=2.09229, val_acc=0.39964, time=0.99202
Epoch:0076, train_loss=1.55564, train_acc=0.86145, val_loss=2.09286, val_acc=0.39964, time=0.96500
Epoch:0077, train_loss=1.55407, train_acc=0.86470, val_loss=2.09343, val_acc=0.39781, time=1.16402
Epoch:0078, train_loss=1.55253, train_acc=0.86814, val_loss=2.09401, val_acc=0.39781, time=1.36100
Epoch:0079, train_loss=1.55102, train_acc=0.86935, val_loss=2.09459, val_acc=0.39599, time=1.05800
Epoch:0080, train_loss=1.54953, train_acc=0.87016, val_loss=2.09516, val_acc=0.39416, time=1.01601
Epoch:0081, train_loss=1.54806, train_acc=0.87158, val_loss=2.09571, val_acc=0.39416, time=1.01401
Epoch:0082, train_loss=1.54661, train_acc=0.87381, val_loss=2.09627, val_acc=0.39051, time=1.01400
Epoch:0083, train_loss=1.54519, train_acc=0.87543, val_loss=2.09684, val_acc=0.39051, time=1.00701
Epoch:0084, train_loss=1.54379, train_acc=0.87725, val_loss=2.09741, val_acc=0.39051, time=1.07401
Epoch:0085, train_loss=1.54242, train_acc=0.87806, val_loss=2.09798, val_acc=0.39051, time=1.08400
Epoch:0086, train_loss=1.54107, train_acc=0.87887, val_loss=2.09854, val_acc=0.38869, time=0.99602
Epoch:0087, train_loss=1.53973, train_acc=0.88130, val_loss=2.09910, val_acc=0.38686, time=1.01600
Epoch:0088, train_loss=1.53842, train_acc=0.88232, val_loss=2.09968, val_acc=0.38504, time=1.08301
Epoch:0089, train_loss=1.53713, train_acc=0.88272, val_loss=2.10025, val_acc=0.38504, time=1.21901
Epoch:0090, train_loss=1.53586, train_acc=0.88455, val_loss=2.10082, val_acc=0.38686, time=1.19101
Epoch:0091, train_loss=1.53461, train_acc=0.88617, val_loss=2.10139, val_acc=0.38504, time=1.04401
Epoch:0092, train_loss=1.53339, train_acc=0.88779, val_loss=2.10197, val_acc=0.38504, time=1.03200
Epoch:0093, train_loss=1.53218, train_acc=0.89001, val_loss=2.10254, val_acc=0.38321, time=0.94601
Epoch:0094, train_loss=1.53099, train_acc=0.89163, val_loss=2.10312, val_acc=0.38504, time=1.03701
Epoch:0095, train_loss=1.52981, train_acc=0.89224, val_loss=2.10369, val_acc=0.38686, time=1.08099
Epoch:0096, train_loss=1.52866, train_acc=0.89305, val_loss=2.10426, val_acc=0.38686, time=0.98501
Epoch:0097, train_loss=1.52752, train_acc=0.89508, val_loss=2.10483, val_acc=0.38869, time=1.04000
Epoch:0098, train_loss=1.52641, train_acc=0.89710, val_loss=2.10540, val_acc=0.38686, time=0.96301
Epoch:0099, train_loss=1.52530, train_acc=0.89791, val_loss=2.10597, val_acc=0.38686, time=1.05700
Epoch:0100, train_loss=1.52422, train_acc=0.89994, val_loss=2.10654, val_acc=0.38686, time=1.00802
Epoch:0101, train_loss=1.52315, train_acc=0.90055, val_loss=2.10711, val_acc=0.38686, time=1.07000
Epoch:0102, train_loss=1.52210, train_acc=0.90095, val_loss=2.10769, val_acc=0.38686, time=1.04500
Early stopping...

Optimization Finished!

Test set results: loss= 2.21148, accuracy= 0.40201, time= 0.35499

Test Precision, Recall and F1-Score...
              precision    recall  f1-score   support

           0     0.0566    0.0345    0.0429        87
           1     0.5096    0.6122    0.5562      1083
           2     0.3271    0.3032    0.3147       696
           3     0.0000    0.0000    0.0000        10
           4     0.0308    0.0267    0.0286        75
           5     0.0000    0.0000    0.0000       121
           6     0.0000    0.0000    0.0000        36
           7     0.0263    0.0123    0.0168        81

    accuracy                         0.4020      2189
   macro avg     0.1188    0.1236    0.1199      2189
weighted avg     0.3604    0.4020    0.3785      2189


Macro average Test Precision, Recall and F1-Score...
(0.11880357132805512, 0.12360554868636267, 0.11989173465806903, None)

Micro average Test Precision, Recall and F1-Score...
(0.4020100502512563, 0.4020100502512563, 0.4020100502512563, None)

Embeddings:
Word_embeddings:7688
Train_doc_embeddings:5485
Test_doc_embeddings:2189
