
==========: 148019706741300
Epoch:0001, train_loss=2.16217, train_acc=0.12619, val_loss=2.06747, val_acc=0.47263, time=1.40302
Epoch:0002, train_loss=1.95051, train_acc=0.48268, val_loss=2.06150, val_acc=0.49270, time=1.39902
Epoch:0003, train_loss=1.89111, train_acc=0.51104, val_loss=2.06099, val_acc=0.45985, time=1.27499
Epoch:0004, train_loss=1.88038, train_acc=0.50922, val_loss=2.06234, val_acc=0.44891, time=1.03501
Epoch:0005, train_loss=1.88229, train_acc=0.51853, val_loss=2.06318, val_acc=0.45255, time=0.96900
Epoch:0006, train_loss=1.87378, train_acc=0.53980, val_loss=2.06354, val_acc=0.45803, time=1.11002
Epoch:0007, train_loss=1.85891, train_acc=0.56006, val_loss=2.06366, val_acc=0.47080, time=1.11501
Epoch:0008, train_loss=1.84305, train_acc=0.57099, val_loss=2.06339, val_acc=0.45803, time=1.00500
Epoch:0009, train_loss=1.82551, train_acc=0.58274, val_loss=2.06293, val_acc=0.43613, time=1.12299
Epoch:0010, train_loss=1.80751, train_acc=0.59672, val_loss=2.06267, val_acc=0.43796, time=1.03099
Epoch:0011, train_loss=1.79159, train_acc=0.61313, val_loss=2.06277, val_acc=0.43431, time=1.10701
Epoch:0012, train_loss=1.77880, train_acc=0.62325, val_loss=2.06318, val_acc=0.43796, time=1.18901
Epoch:0013, train_loss=1.76867, train_acc=0.63237, val_loss=2.06370, val_acc=0.43431, time=1.23200
Epoch:0014, train_loss=1.75982, train_acc=0.64108, val_loss=2.06412, val_acc=0.43978, time=1.13001
Epoch:0015, train_loss=1.75076, train_acc=0.64553, val_loss=2.06437, val_acc=0.44161, time=1.11201
Epoch:0016, train_loss=1.74099, train_acc=0.64614, val_loss=2.06450, val_acc=0.44891, time=1.18500
Epoch:0017, train_loss=1.73102, train_acc=0.65343, val_loss=2.06461, val_acc=0.45255, time=1.07901
Epoch:0018, train_loss=1.72174, train_acc=0.66255, val_loss=2.06482, val_acc=0.45255, time=1.11501
Epoch:0019, train_loss=1.71389, train_acc=0.66781, val_loss=2.06519, val_acc=0.44708, time=1.06098
Epoch:0020, train_loss=1.70775, train_acc=0.67450, val_loss=2.06571, val_acc=0.44891, time=1.15501
Epoch:0021, train_loss=1.70289, train_acc=0.68058, val_loss=2.06631, val_acc=0.44343, time=1.36200
Epoch:0022, train_loss=1.69857, train_acc=0.68604, val_loss=2.06692, val_acc=0.44161, time=1.05800
Epoch:0023, train_loss=1.69424, train_acc=0.69313, val_loss=2.06752, val_acc=0.44343, time=1.02200
Epoch:0024, train_loss=1.68977, train_acc=0.70063, val_loss=2.06809, val_acc=0.44343, time=1.18901
Epoch:0025, train_loss=1.68517, train_acc=0.70205, val_loss=2.06861, val_acc=0.44161, time=1.07701
Epoch:0026, train_loss=1.68037, train_acc=0.70407, val_loss=2.06905, val_acc=0.44343, time=1.19700
Epoch:0027, train_loss=1.67530, train_acc=0.71015, val_loss=2.06943, val_acc=0.44526, time=1.11000
Epoch:0028, train_loss=1.67008, train_acc=0.71886, val_loss=2.06977, val_acc=0.44343, time=1.28400
Epoch:0029, train_loss=1.66496, train_acc=0.72676, val_loss=2.07013, val_acc=0.43613, time=1.33300
Epoch:0030, train_loss=1.66021, train_acc=0.73547, val_loss=2.07052, val_acc=0.42883, time=1.20999
Epoch:0031, train_loss=1.65588, train_acc=0.74114, val_loss=2.07096, val_acc=0.42701, time=1.03202
Epoch:0032, train_loss=1.65193, train_acc=0.74904, val_loss=2.07143, val_acc=0.42883, time=1.08301
Epoch:0033, train_loss=1.64828, train_acc=0.75349, val_loss=2.07194, val_acc=0.42883, time=0.96402
Epoch:0034, train_loss=1.64492, train_acc=0.75592, val_loss=2.07248, val_acc=0.43431, time=1.07001
Epoch:0035, train_loss=1.64178, train_acc=0.75836, val_loss=2.07302, val_acc=0.43613, time=1.14101
Epoch:0036, train_loss=1.63871, train_acc=0.76079, val_loss=2.07354, val_acc=0.43431, time=1.06600
Epoch:0037, train_loss=1.63556, train_acc=0.76544, val_loss=2.07404, val_acc=0.43431, time=1.17001
Epoch:0038, train_loss=1.63233, train_acc=0.77051, val_loss=2.07455, val_acc=0.43248, time=1.30202
Epoch:0039, train_loss=1.62908, train_acc=0.77638, val_loss=2.07507, val_acc=0.42701, time=1.02601
Epoch:0040, train_loss=1.62588, train_acc=0.77861, val_loss=2.07561, val_acc=0.42701, time=1.19202
Epoch:0041, train_loss=1.62272, train_acc=0.78286, val_loss=2.07618, val_acc=0.42701, time=1.08300
Epoch:0042, train_loss=1.61963, train_acc=0.78692, val_loss=2.07676, val_acc=0.42701, time=1.04101
Epoch:0043, train_loss=1.61667, train_acc=0.79036, val_loss=2.07735, val_acc=0.42883, time=1.18201
Epoch:0044, train_loss=1.61384, train_acc=0.79279, val_loss=2.07793, val_acc=0.42518, time=1.18700
Epoch:0045, train_loss=1.61110, train_acc=0.79522, val_loss=2.07848, val_acc=0.42336, time=1.20299
Epoch:0046, train_loss=1.60840, train_acc=0.79826, val_loss=2.07902, val_acc=0.41606, time=0.97100
Epoch:0047, train_loss=1.60576, train_acc=0.80211, val_loss=2.07956, val_acc=0.41788, time=1.02198
Epoch:0048, train_loss=1.60319, train_acc=0.80454, val_loss=2.08010, val_acc=0.42336, time=1.01202
Epoch:0049, train_loss=1.60067, train_acc=0.80940, val_loss=2.08065, val_acc=0.42518, time=1.00901
Epoch:0050, train_loss=1.59820, train_acc=0.81082, val_loss=2.08121, val_acc=0.42701, time=1.03800
Epoch:0051, train_loss=1.59578, train_acc=0.81345, val_loss=2.08177, val_acc=0.42518, time=1.00800
Epoch:0052, train_loss=1.59342, train_acc=0.81608, val_loss=2.08231, val_acc=0.42336, time=1.04302
Epoch:0053, train_loss=1.59109, train_acc=0.81912, val_loss=2.08285, val_acc=0.42336, time=1.07199
Epoch:0054, train_loss=1.58878, train_acc=0.82297, val_loss=2.08339, val_acc=0.42518, time=1.17200
Epoch:0055, train_loss=1.58651, train_acc=0.82702, val_loss=2.08393, val_acc=0.41971, time=0.97001
Epoch:0056, train_loss=1.58429, train_acc=0.82945, val_loss=2.08448, val_acc=0.42153, time=1.05301
Epoch:0057, train_loss=1.58210, train_acc=0.83107, val_loss=2.08504, val_acc=0.42518, time=0.97801
Epoch:0058, train_loss=1.57997, train_acc=0.83350, val_loss=2.08560, val_acc=0.42518, time=1.23399
Epoch:0059, train_loss=1.57790, train_acc=0.83614, val_loss=2.08615, val_acc=0.42701, time=0.98399
Epoch:0060, train_loss=1.57588, train_acc=0.83796, val_loss=2.08670, val_acc=0.42518, time=0.95501
Epoch:0061, train_loss=1.57389, train_acc=0.84019, val_loss=2.08724, val_acc=0.42336, time=1.03200
Epoch:0062, train_loss=1.57194, train_acc=0.84322, val_loss=2.08780, val_acc=0.42153, time=0.97001
Epoch:0063, train_loss=1.57001, train_acc=0.84505, val_loss=2.08837, val_acc=0.41971, time=1.03801
Epoch:0064, train_loss=1.56809, train_acc=0.84768, val_loss=2.08895, val_acc=0.41606, time=1.01501
Epoch:0065, train_loss=1.56622, train_acc=0.85031, val_loss=2.08952, val_acc=0.41423, time=1.02702
Epoch:0066, train_loss=1.56438, train_acc=0.85315, val_loss=2.09009, val_acc=0.41423, time=1.17300
Epoch:0067, train_loss=1.56258, train_acc=0.85599, val_loss=2.09066, val_acc=0.41241, time=1.07001
Epoch:0068, train_loss=1.56082, train_acc=0.85902, val_loss=2.09123, val_acc=0.41241, time=1.03501
Epoch:0069, train_loss=1.55909, train_acc=0.86085, val_loss=2.09181, val_acc=0.41058, time=1.17702
Epoch:0070, train_loss=1.55740, train_acc=0.86348, val_loss=2.09240, val_acc=0.40876, time=1.04198
Epoch:0071, train_loss=1.55574, train_acc=0.86652, val_loss=2.09298, val_acc=0.40693, time=1.18603
Epoch:0072, train_loss=1.55410, train_acc=0.86834, val_loss=2.09355, val_acc=0.40693, time=1.03700
Epoch:0073, train_loss=1.55248, train_acc=0.87077, val_loss=2.09412, val_acc=0.40511, time=1.02003
Epoch:0074, train_loss=1.55090, train_acc=0.87158, val_loss=2.09469, val_acc=0.40693, time=1.05302
Epoch:0075, train_loss=1.54935, train_acc=0.87219, val_loss=2.09526, val_acc=0.40693, time=1.00799
Epoch:0076, train_loss=1.54782, train_acc=0.87361, val_loss=2.09584, val_acc=0.40693, time=1.02502
Epoch:0077, train_loss=1.54632, train_acc=0.87422, val_loss=2.09640, val_acc=0.40511, time=1.07402
Epoch:0078, train_loss=1.54484, train_acc=0.87624, val_loss=2.09697, val_acc=0.40693, time=0.97903
Epoch:0079, train_loss=1.54340, train_acc=0.87746, val_loss=2.09755, val_acc=0.40693, time=1.04200
Epoch:0080, train_loss=1.54197, train_acc=0.87867, val_loss=2.09814, val_acc=0.40511, time=1.09901
Epoch:0081, train_loss=1.54058, train_acc=0.88029, val_loss=2.09873, val_acc=0.40511, time=1.10102
Epoch:0082, train_loss=1.53921, train_acc=0.88252, val_loss=2.09932, val_acc=0.40511, time=1.07700
Epoch:0083, train_loss=1.53785, train_acc=0.88374, val_loss=2.09991, val_acc=0.40328, time=1.26701
Epoch:0084, train_loss=1.53652, train_acc=0.88455, val_loss=2.10049, val_acc=0.40146, time=1.28499
Epoch:0085, train_loss=1.53522, train_acc=0.88617, val_loss=2.10109, val_acc=0.40328, time=1.15603
Epoch:0086, train_loss=1.53393, train_acc=0.88698, val_loss=2.10168, val_acc=0.40146, time=1.12800
Epoch:0087, train_loss=1.53267, train_acc=0.88819, val_loss=2.10227, val_acc=0.40146, time=1.00001
Epoch:0088, train_loss=1.53143, train_acc=0.88900, val_loss=2.10286, val_acc=0.40146, time=1.13400
Epoch:0089, train_loss=1.53021, train_acc=0.89042, val_loss=2.10345, val_acc=0.39781, time=1.18800
Epoch:0090, train_loss=1.52901, train_acc=0.89163, val_loss=2.10404, val_acc=0.39416, time=1.01101
Epoch:0091, train_loss=1.52783, train_acc=0.89366, val_loss=2.10463, val_acc=0.39234, time=1.09100
Epoch:0092, train_loss=1.52667, train_acc=0.89467, val_loss=2.10521, val_acc=0.39234, time=1.26600
Epoch:0093, train_loss=1.52553, train_acc=0.89650, val_loss=2.10579, val_acc=0.39234, time=1.04201
Epoch:0094, train_loss=1.52441, train_acc=0.89832, val_loss=2.10638, val_acc=0.39234, time=1.11800
Epoch:0095, train_loss=1.52330, train_acc=0.89974, val_loss=2.10698, val_acc=0.39234, time=1.03202
Epoch:0096, train_loss=1.52221, train_acc=0.90176, val_loss=2.10757, val_acc=0.39234, time=1.24101
Epoch:0097, train_loss=1.52115, train_acc=0.90176, val_loss=2.10815, val_acc=0.39234, time=1.20401
Epoch:0098, train_loss=1.52009, train_acc=0.90237, val_loss=2.10873, val_acc=0.39234, time=1.04601
Epoch:0099, train_loss=1.51906, train_acc=0.90359, val_loss=2.10932, val_acc=0.39234, time=1.08101
Epoch:0100, train_loss=1.51804, train_acc=0.90399, val_loss=2.10991, val_acc=0.39234, time=1.00801
Epoch:0101, train_loss=1.51704, train_acc=0.90561, val_loss=2.11049, val_acc=0.38869, time=1.02500
Epoch:0102, train_loss=1.51605, train_acc=0.90662, val_loss=2.11106, val_acc=0.38869, time=0.96501
Early stopping...

Optimization Finished!

Test set results: loss= 2.23083, accuracy= 0.39653, time= 0.28801

Test Precision, Recall and F1-Score...
              precision    recall  f1-score   support

           0     0.0526    0.0230    0.0320        87
           1     0.5047    0.5928    0.5452      1083
           2     0.3289    0.3175    0.3231       696
           3     0.0000    0.0000    0.0000        10
           4     0.0145    0.0133    0.0139        75
           5     0.0152    0.0083    0.0107       121
           6     0.0000    0.0000    0.0000        36
           7     0.0192    0.0123    0.0150        81

    accuracy                         0.3965      2189
   macro avg     0.1169    0.1209    0.1175      2189
weighted avg     0.3584    0.3965    0.3754      2189


Macro average Test Precision, Recall and F1-Score...
(0.11688658071299554, 0.1209073125585531, 0.11749300189757635, None)

Micro average Test Precision, Recall and F1-Score...
(0.39652809502055736, 0.39652809502055736, 0.39652809502055736, None)

Embeddings:
Word_embeddings:7688
Train_doc_embeddings:5485
Test_doc_embeddings:2189
