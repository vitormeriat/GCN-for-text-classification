
==========: 149297727892500
Epoch:0001, train_loss=2.27411, train_acc=0.06522, val_loss=2.09847, val_acc=0.07847, time=1.46401
Epoch:0002, train_loss=2.25669, train_acc=0.07110, val_loss=2.09665, val_acc=0.08029, time=1.39002
Epoch:0003, train_loss=2.23955, train_acc=0.07697, val_loss=2.09486, val_acc=0.08212, time=1.17701
Epoch:0004, train_loss=2.22271, train_acc=0.08710, val_loss=2.09311, val_acc=0.09307, time=1.02002
Epoch:0005, train_loss=2.20618, train_acc=0.09723, val_loss=2.09139, val_acc=0.10219, time=1.06701
Epoch:0006, train_loss=2.19000, train_acc=0.10756, val_loss=2.08971, val_acc=0.11314, time=1.01800
Epoch:0007, train_loss=2.17416, train_acc=0.11870, val_loss=2.08807, val_acc=0.12774, time=1.17301
Epoch:0008, train_loss=2.15866, train_acc=0.13429, val_loss=2.08647, val_acc=0.15693, time=1.02901
Epoch:0009, train_loss=2.14355, train_acc=0.15272, val_loss=2.08491, val_acc=0.15693, time=0.98899
Epoch:0010, train_loss=2.12881, train_acc=0.17338, val_loss=2.08339, val_acc=0.17883, time=1.06000
Epoch:0011, train_loss=2.11449, train_acc=0.19506, val_loss=2.08193, val_acc=0.21168, time=1.02103
Epoch:0012, train_loss=2.10059, train_acc=0.21795, val_loss=2.08050, val_acc=0.22080, time=1.16001
Epoch:0013, train_loss=2.08714, train_acc=0.23597, val_loss=2.07913, val_acc=0.24088, time=1.21401
Epoch:0014, train_loss=2.07416, train_acc=0.25744, val_loss=2.07782, val_acc=0.26460, time=1.20499
Epoch:0015, train_loss=2.06166, train_acc=0.27689, val_loss=2.07655, val_acc=0.30109, time=1.11502
Epoch:0016, train_loss=2.04967, train_acc=0.29917, val_loss=2.07535, val_acc=0.32482, time=1.14201
Epoch:0017, train_loss=2.03818, train_acc=0.32186, val_loss=2.07419, val_acc=0.34672, time=1.15802
Epoch:0018, train_loss=2.02718, train_acc=0.33522, val_loss=2.07309, val_acc=0.36131, time=1.09501
Epoch:0019, train_loss=2.01669, train_acc=0.35426, val_loss=2.07205, val_acc=0.37044, time=1.00602
Epoch:0020, train_loss=2.00672, train_acc=0.36783, val_loss=2.07107, val_acc=0.37956, time=1.05199
Epoch:0021, train_loss=1.99726, train_acc=0.38282, val_loss=2.07014, val_acc=0.38869, time=1.02201
Epoch:0022, train_loss=1.98831, train_acc=0.39720, val_loss=2.06926, val_acc=0.40328, time=1.07800
Epoch:0023, train_loss=1.97987, train_acc=0.40875, val_loss=2.06844, val_acc=0.40876, time=1.24002
Epoch:0024, train_loss=1.97194, train_acc=0.41868, val_loss=2.06767, val_acc=0.42153, time=1.24200
Epoch:0025, train_loss=1.96450, train_acc=0.42718, val_loss=2.06696, val_acc=0.44161, time=1.15502
Epoch:0026, train_loss=1.95752, train_acc=0.43609, val_loss=2.06629, val_acc=0.44526, time=1.14699
Epoch:0027, train_loss=1.95100, train_acc=0.44764, val_loss=2.06567, val_acc=0.44891, time=1.08601
Epoch:0028, train_loss=1.94491, train_acc=0.45493, val_loss=2.06510, val_acc=0.45255, time=1.17001
Epoch:0029, train_loss=1.93924, train_acc=0.46162, val_loss=2.06456, val_acc=0.45073, time=1.03101
Epoch:0030, train_loss=1.93395, train_acc=0.46628, val_loss=2.06407, val_acc=0.45255, time=1.06202
Epoch:0031, train_loss=1.92903, train_acc=0.47114, val_loss=2.06362, val_acc=0.45438, time=1.08599
Epoch:0032, train_loss=1.92445, train_acc=0.47519, val_loss=2.06320, val_acc=0.45438, time=1.03400
Epoch:0033, train_loss=1.92018, train_acc=0.47661, val_loss=2.06281, val_acc=0.45438, time=1.13400
Epoch:0034, train_loss=1.91620, train_acc=0.48045, val_loss=2.06245, val_acc=0.45255, time=1.15801
Epoch:0035, train_loss=1.91248, train_acc=0.48390, val_loss=2.06212, val_acc=0.45985, time=1.00603
Epoch:0036, train_loss=1.90899, train_acc=0.48694, val_loss=2.06181, val_acc=0.46168, time=1.00199
Epoch:0037, train_loss=1.90572, train_acc=0.48835, val_loss=2.06152, val_acc=0.46898, time=1.12501
Epoch:0038, train_loss=1.90265, train_acc=0.49018, val_loss=2.06126, val_acc=0.47628, time=1.15301
Epoch:0039, train_loss=1.89975, train_acc=0.49119, val_loss=2.06101, val_acc=0.47628, time=1.13100
Epoch:0040, train_loss=1.89701, train_acc=0.49342, val_loss=2.06077, val_acc=0.47628, time=1.00200
Epoch:0041, train_loss=1.89442, train_acc=0.49544, val_loss=2.06056, val_acc=0.47993, time=1.00599
Epoch:0042, train_loss=1.89195, train_acc=0.49625, val_loss=2.06035, val_acc=0.47993, time=1.06600
Epoch:0043, train_loss=1.88960, train_acc=0.49889, val_loss=2.06016, val_acc=0.48540, time=1.15201
Epoch:0044, train_loss=1.88736, train_acc=0.50091, val_loss=2.05998, val_acc=0.48723, time=1.21701
Epoch:0045, train_loss=1.88521, train_acc=0.50192, val_loss=2.05982, val_acc=0.48175, time=1.03500
Epoch:0046, train_loss=1.88316, train_acc=0.50354, val_loss=2.05966, val_acc=0.48358, time=1.00800
Epoch:0047, train_loss=1.88118, train_acc=0.50557, val_loss=2.05951, val_acc=0.48175, time=1.18801
Epoch:0048, train_loss=1.87929, train_acc=0.50780, val_loss=2.05937, val_acc=0.48540, time=1.12999
Epoch:0049, train_loss=1.87746, train_acc=0.50962, val_loss=2.05925, val_acc=0.48540, time=1.12500
Epoch:0050, train_loss=1.87570, train_acc=0.51185, val_loss=2.05913, val_acc=0.48540, time=1.06602
Epoch:0051, train_loss=1.87400, train_acc=0.51286, val_loss=2.05902, val_acc=0.48905, time=1.15600
Epoch:0052, train_loss=1.87235, train_acc=0.51408, val_loss=2.05891, val_acc=0.48358, time=1.22600
Epoch:0053, train_loss=1.87076, train_acc=0.51570, val_loss=2.05882, val_acc=0.48358, time=1.06802
Epoch:0054, train_loss=1.86920, train_acc=0.51610, val_loss=2.05873, val_acc=0.48175, time=1.01100
Epoch:0055, train_loss=1.86770, train_acc=0.51813, val_loss=2.05865, val_acc=0.48175, time=0.98502
Epoch:0056, train_loss=1.86623, train_acc=0.51894, val_loss=2.05857, val_acc=0.48175, time=1.06400
Epoch:0057, train_loss=1.86479, train_acc=0.51934, val_loss=2.05850, val_acc=0.48358, time=1.14202
Epoch:0058, train_loss=1.86338, train_acc=0.52096, val_loss=2.05844, val_acc=0.48358, time=1.14201
Epoch:0059, train_loss=1.86200, train_acc=0.52238, val_loss=2.05838, val_acc=0.48358, time=1.12501
Epoch:0060, train_loss=1.86064, train_acc=0.52339, val_loss=2.05832, val_acc=0.48540, time=1.09602
Epoch:0061, train_loss=1.85931, train_acc=0.52461, val_loss=2.05827, val_acc=0.48358, time=1.04901
Epoch:0062, train_loss=1.85800, train_acc=0.52542, val_loss=2.05822, val_acc=0.48540, time=1.03498
Epoch:0063, train_loss=1.85670, train_acc=0.52623, val_loss=2.05818, val_acc=0.48540, time=1.01701
Epoch:0064, train_loss=1.85542, train_acc=0.52704, val_loss=2.05814, val_acc=0.48540, time=1.27100
Epoch:0065, train_loss=1.85416, train_acc=0.52826, val_loss=2.05810, val_acc=0.48723, time=1.05503
Epoch:0066, train_loss=1.85292, train_acc=0.52886, val_loss=2.05807, val_acc=0.48723, time=1.12499
Epoch:0067, train_loss=1.85169, train_acc=0.52927, val_loss=2.05804, val_acc=0.48540, time=1.37600
Epoch:0068, train_loss=1.85048, train_acc=0.53008, val_loss=2.05801, val_acc=0.48358, time=1.27501
Epoch:0069, train_loss=1.84928, train_acc=0.53028, val_loss=2.05799, val_acc=0.48540, time=0.95703
Epoch:0070, train_loss=1.84809, train_acc=0.53089, val_loss=2.05796, val_acc=0.48540, time=0.98501
Epoch:0071, train_loss=1.84692, train_acc=0.53129, val_loss=2.05794, val_acc=0.48723, time=1.14600
Epoch:0072, train_loss=1.84577, train_acc=0.53271, val_loss=2.05792, val_acc=0.48723, time=1.08001
Epoch:0073, train_loss=1.84463, train_acc=0.53372, val_loss=2.05790, val_acc=0.48723, time=1.17402
Epoch:0074, train_loss=1.84350, train_acc=0.53494, val_loss=2.05789, val_acc=0.48723, time=1.07300
Epoch:0075, train_loss=1.84238, train_acc=0.53595, val_loss=2.05787, val_acc=0.48540, time=1.02201
Epoch:0076, train_loss=1.84128, train_acc=0.53778, val_loss=2.05786, val_acc=0.48540, time=1.16100
Epoch:0077, train_loss=1.84019, train_acc=0.53859, val_loss=2.05784, val_acc=0.48540, time=1.17001
Epoch:0078, train_loss=1.83911, train_acc=0.53899, val_loss=2.05783, val_acc=0.48540, time=1.13701
Epoch:0079, train_loss=1.83804, train_acc=0.53960, val_loss=2.05782, val_acc=0.48540, time=1.13400
Epoch:0080, train_loss=1.83698, train_acc=0.54041, val_loss=2.05781, val_acc=0.48723, time=1.07002
Epoch:0081, train_loss=1.83594, train_acc=0.54183, val_loss=2.05780, val_acc=0.48723, time=1.19700
Epoch:0082, train_loss=1.83490, train_acc=0.54243, val_loss=2.05779, val_acc=0.48723, time=1.02999
Epoch:0083, train_loss=1.83388, train_acc=0.54365, val_loss=2.05778, val_acc=0.48723, time=1.03800
Epoch:0084, train_loss=1.83286, train_acc=0.54466, val_loss=2.05777, val_acc=0.48723, time=0.96601
Epoch:0085, train_loss=1.83185, train_acc=0.54547, val_loss=2.05776, val_acc=0.48723, time=1.03001
Epoch:0086, train_loss=1.83085, train_acc=0.54608, val_loss=2.05776, val_acc=0.48723, time=1.06301
Epoch:0087, train_loss=1.82986, train_acc=0.54770, val_loss=2.05775, val_acc=0.48540, time=1.14600
Epoch:0088, train_loss=1.82888, train_acc=0.54851, val_loss=2.05774, val_acc=0.48723, time=1.00901
Epoch:0089, train_loss=1.82791, train_acc=0.55013, val_loss=2.05774, val_acc=0.49088, time=1.16500
Epoch:0090, train_loss=1.82695, train_acc=0.55094, val_loss=2.05773, val_acc=0.49088, time=0.99502
Epoch:0091, train_loss=1.82599, train_acc=0.55155, val_loss=2.05773, val_acc=0.48905, time=1.07300
Epoch:0092, train_loss=1.82505, train_acc=0.55358, val_loss=2.05772, val_acc=0.48723, time=1.09100
Epoch:0093, train_loss=1.82411, train_acc=0.55540, val_loss=2.05772, val_acc=0.48723, time=1.09001
Epoch:0094, train_loss=1.82318, train_acc=0.55540, val_loss=2.05772, val_acc=0.48723, time=1.11601
Epoch:0095, train_loss=1.82225, train_acc=0.55580, val_loss=2.05772, val_acc=0.48540, time=1.07502
Epoch:0096, train_loss=1.82134, train_acc=0.55763, val_loss=2.05771, val_acc=0.48540, time=1.10000
Epoch:0097, train_loss=1.82043, train_acc=0.55884, val_loss=2.05771, val_acc=0.48540, time=1.14701
Epoch:0098, train_loss=1.81952, train_acc=0.56046, val_loss=2.05771, val_acc=0.48723, time=1.24701
Epoch:0099, train_loss=1.81863, train_acc=0.56066, val_loss=2.05771, val_acc=0.48723, time=1.02901
Epoch:0100, train_loss=1.81774, train_acc=0.56147, val_loss=2.05771, val_acc=0.48723, time=1.05300
Epoch:0101, train_loss=1.81685, train_acc=0.56269, val_loss=2.05771, val_acc=0.48540, time=1.14300
Epoch:0102, train_loss=1.81597, train_acc=0.56249, val_loss=2.05772, val_acc=0.48358, time=0.98301
Early stopping...

Optimization Finished!

Test set results: loss= 2.00362, accuracy= 0.45546, time= 0.30700

Test Precision, Recall and F1-Score...
              precision    recall  f1-score   support

           0     0.1667    0.0230    0.0404        87
           1     0.4935    0.8033    0.6114      1083
           2     0.3360    0.1796    0.2341       696
           3     0.0000    0.0000    0.0000        10
           4     0.0000    0.0000    0.0000        75
           5     0.0000    0.0000    0.0000       121
           6     0.0000    0.0000    0.0000        36
           7     0.0000    0.0000    0.0000        81

    accuracy                         0.4555      2189
   macro avg     0.1245    0.1257    0.1107      2189
weighted avg     0.3576    0.4555    0.3785      2189


Macro average Test Precision, Recall and F1-Score...
(0.12452064997956805, 0.12573878832744292, 0.11073385457056212, None)

Micro average Test Precision, Recall and F1-Score...
(0.455459113750571, 0.455459113750571, 0.455459113750571, None)

Embeddings:
Word_embeddings:7688
Train_doc_embeddings:5485
Test_doc_embeddings:2189
