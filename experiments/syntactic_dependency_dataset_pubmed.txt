
==================== Torch Seed: 57210745401300
Epoch:0001, train_loss=1.08964, train_acc=0.36765, val_loss=1.09181, val_acc=0.56594, time=0.93399
Epoch:0002, train_loss=1.04381, train_acc=0.56263, val_loss=1.08535, val_acc=0.57029, time=0.83100
Epoch:0003, train_loss=0.98494, train_acc=0.56794, val_loss=1.07952, val_acc=0.60870, time=0.98200
Epoch:0004, train_loss=0.93098, train_acc=0.61657, val_loss=1.07586, val_acc=0.72101, time=0.77199
Epoch:0005, train_loss=0.89706, train_acc=0.71856, val_loss=1.07305, val_acc=0.73841, time=0.75600
Epoch:0006, train_loss=0.87103, train_acc=0.72847, val_loss=1.06974, val_acc=0.74348, time=0.95299
Epoch:0007, train_loss=0.84039, train_acc=0.73692, val_loss=1.06647, val_acc=0.75725, time=0.77798
Epoch:0008, train_loss=0.80952, train_acc=0.75173, val_loss=1.06419, val_acc=0.76304, time=0.73902
Epoch:0009, train_loss=0.78718, train_acc=0.75994, val_loss=1.06255, val_acc=0.76957, time=0.74998
Epoch:0010, train_loss=0.77049, train_acc=0.77033, val_loss=1.06086, val_acc=0.78188, time=0.81700
Epoch:0011, train_loss=0.75358, train_acc=0.78369, val_loss=1.05932, val_acc=0.78986, time=0.96299
Epoch:0012, train_loss=0.73834, train_acc=0.79657, val_loss=1.05848, val_acc=0.79130, time=0.88099
Epoch:0013, train_loss=0.72952, train_acc=0.80293, val_loss=1.05800, val_acc=0.79275, time=0.83899
Epoch:0014, train_loss=0.72392, train_acc=0.80245, val_loss=1.05740, val_acc=0.78986, time=0.90100
Epoch:0015, train_loss=0.71731, train_acc=0.80639, val_loss=1.05704, val_acc=0.78986, time=0.83199
Epoch:0016, train_loss=0.71292, train_acc=0.80647, val_loss=1.05695, val_acc=0.79058, time=0.76899
Epoch:0017, train_loss=0.71115, train_acc=0.80752, val_loss=1.05670, val_acc=0.78913, time=0.89700
Epoch:0018, train_loss=0.70789, train_acc=0.81009, val_loss=1.05633, val_acc=0.79493, time=0.85400
Epoch:0019, train_loss=0.70317, train_acc=0.81356, val_loss=1.05612, val_acc=0.80217, time=0.85698
Epoch:0020, train_loss=0.69986, train_acc=0.81565, val_loss=1.05592, val_acc=0.80435, time=0.81400
Epoch:0021, train_loss=0.69709, train_acc=0.81863, val_loss=1.05556, val_acc=0.80797, time=0.93000
Epoch:0022, train_loss=0.69306, train_acc=0.82225, val_loss=1.05520, val_acc=0.81449, time=0.78699
Epoch:0023, train_loss=0.68946, train_acc=0.82628, val_loss=1.05494, val_acc=0.81304, time=0.77999
Epoch:0024, train_loss=0.68717, train_acc=0.82853, val_loss=1.05466, val_acc=0.81667, time=0.93899
Epoch:0025, train_loss=0.68451, train_acc=0.82990, val_loss=1.05433, val_acc=0.81957, time=0.87698
Epoch:0026, train_loss=0.68121, train_acc=0.83215, val_loss=1.05406, val_acc=0.81957, time=0.88799
Epoch:0027, train_loss=0.67854, train_acc=0.83336, val_loss=1.05381, val_acc=0.81667, time=0.87100
Epoch:0028, train_loss=0.67621, train_acc=0.83376, val_loss=1.05346, val_acc=0.81739, time=0.80000
Epoch:0029, train_loss=0.67324, train_acc=0.83489, val_loss=1.05307, val_acc=0.82536, time=0.75199
Epoch:0030, train_loss=0.67030, train_acc=0.83908, val_loss=1.05278, val_acc=0.82754, time=0.85099
Epoch:0031, train_loss=0.66805, train_acc=0.84085, val_loss=1.05254, val_acc=0.82971, time=0.83100
Epoch:0032, train_loss=0.66577, train_acc=0.84246, val_loss=1.05231, val_acc=0.83188, time=0.79599
Epoch:0033, train_loss=0.66330, train_acc=0.84326, val_loss=1.05218, val_acc=0.82899, time=0.74599
Epoch:0034, train_loss=0.66132, train_acc=0.84399, val_loss=1.05207, val_acc=0.83261, time=0.77899
Epoch:0035, train_loss=0.65969, train_acc=0.84495, val_loss=1.05189, val_acc=0.83261, time=0.75999
Epoch:0036, train_loss=0.65787, train_acc=0.84729, val_loss=1.05169, val_acc=0.83406, time=0.92899
Epoch:0037, train_loss=0.65618, train_acc=0.84801, val_loss=1.05153, val_acc=0.83841, time=0.77299
Epoch:0038, train_loss=0.65485, train_acc=0.84833, val_loss=1.05137, val_acc=0.84130, time=0.82700
Epoch:0039, train_loss=0.65345, train_acc=0.84954, val_loss=1.05122, val_acc=0.84275, time=0.95101
Epoch:0040, train_loss=0.65196, train_acc=0.85180, val_loss=1.05107, val_acc=0.84203, time=0.83897
Epoch:0041, train_loss=0.65073, train_acc=0.85260, val_loss=1.05091, val_acc=0.84420, time=0.81698
Epoch:0042, train_loss=0.64961, train_acc=0.85461, val_loss=1.05070, val_acc=0.84783, time=0.87300
Epoch:0043, train_loss=0.64841, train_acc=0.85502, val_loss=1.05050, val_acc=0.85145, time=0.91999
Epoch:0044, train_loss=0.64738, train_acc=0.85518, val_loss=1.05034, val_acc=0.85290, time=0.81200
Epoch:0045, train_loss=0.64650, train_acc=0.85598, val_loss=1.05020, val_acc=0.85217, time=0.81400
Epoch:0046, train_loss=0.64554, train_acc=0.85671, val_loss=1.05011, val_acc=0.85145, time=0.73399
Epoch:0047, train_loss=0.64458, train_acc=0.85719, val_loss=1.05003, val_acc=0.85072, time=0.89500
Epoch:0048, train_loss=0.64372, train_acc=0.85920, val_loss=1.04993, val_acc=0.85072, time=1.04099
Epoch:0049, train_loss=0.64280, train_acc=0.86041, val_loss=1.04984, val_acc=0.85362, time=0.82999
Epoch:0050, train_loss=0.64187, train_acc=0.86137, val_loss=1.04975, val_acc=0.85435, time=0.82400
Epoch:0051, train_loss=0.64107, train_acc=0.86162, val_loss=1.04970, val_acc=0.85435, time=0.79398
Epoch:0052, train_loss=0.64028, train_acc=0.86194, val_loss=1.04965, val_acc=0.85435, time=0.80199
Epoch:0053, train_loss=0.63948, train_acc=0.86282, val_loss=1.04963, val_acc=0.85362, time=0.77199
Epoch:0054, train_loss=0.63877, train_acc=0.86355, val_loss=1.04955, val_acc=0.85507, time=0.80301
Epoch:0055, train_loss=0.63804, train_acc=0.86387, val_loss=1.04950, val_acc=0.85507, time=0.90299
Epoch:0056, train_loss=0.63730, train_acc=0.86403, val_loss=1.04935, val_acc=0.85725, time=0.76701
Epoch:0057, train_loss=0.63666, train_acc=0.86451, val_loss=1.04941, val_acc=0.85652, time=0.82300
Epoch:0058, train_loss=0.63621, train_acc=0.86419, val_loss=1.04924, val_acc=0.85652, time=0.76299
Epoch:0059, train_loss=0.63627, train_acc=0.86395, val_loss=1.04975, val_acc=0.85435, time=0.74500
Early stopping...

Optimization Finished!

Test set results: loss= 0.89000, accuracy= 0.85123, time= 0.30699

Test Precision, Recall and F1-Score...
              precision    recall  f1-score   support

           0     0.8821    0.8031    0.8408      2357
           1     0.8283    0.9007    0.8630      2356
           2     0.8451    0.8486    0.8468      1202

    accuracy                         0.8512      5915
   macro avg     0.8518    0.8508    0.8502      5915
weighted avg     0.8531    0.8512    0.8508      5915


Macro average Test Precision, Recall and F1-Score...
(0.8518119464106295, 0.8508014639602468, 0.8501832154240981, None)

Micro average Test Precision, Recall and F1-Score...
(0.8512256973795436, 0.8512256973795436, 0.8512256973795436, None)

Embeddings:
Word_embeddings:452
Train_doc_embeddings:13802
Test_doc_embeddings:5915
