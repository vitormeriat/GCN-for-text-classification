
==================== Torch Seed: 48125556082500
Epoch:0001, train_loss=1.11281, train_acc=0.32571, val_loss=1.09494, val_acc=0.41377, time=1.03098
Epoch:0002, train_loss=1.08398, train_acc=0.40501, val_loss=1.08966, val_acc=0.55507, time=0.90398
Epoch:0003, train_loss=1.03413, train_acc=0.54500, val_loss=1.08401, val_acc=0.56812, time=0.85498
Epoch:0004, train_loss=0.97638, train_acc=0.55426, val_loss=1.07993, val_acc=0.62174, time=1.07097
Epoch:0005, train_loss=0.93246, train_acc=0.61512, val_loss=1.07806, val_acc=0.71087, time=0.91397
Epoch:0006, train_loss=0.90970, train_acc=0.72074, val_loss=1.07649, val_acc=0.71232, time=0.81598
Epoch:0007, train_loss=0.89161, train_acc=0.72863, val_loss=1.07414, val_acc=0.71449, time=0.73199
Epoch:0008, train_loss=0.86882, train_acc=0.72557, val_loss=1.07123, val_acc=0.72681, time=0.85497
Epoch:0009, train_loss=0.84307, train_acc=0.73152, val_loss=1.06812, val_acc=0.74565, time=0.75098
Epoch:0010, train_loss=0.81645, train_acc=0.74803, val_loss=1.06555, val_acc=0.75580, time=0.86898
Epoch:0011, train_loss=0.79434, train_acc=0.76195, val_loss=1.06391, val_acc=0.75000, time=0.74699
Epoch:0012, train_loss=0.78024, train_acc=0.77105, val_loss=1.06228, val_acc=0.76304, time=0.74299
Epoch:0013, train_loss=0.76609, train_acc=0.78160, val_loss=1.06058, val_acc=0.77101, time=0.92798
Epoch:0014, train_loss=0.75109, train_acc=0.79142, val_loss=1.05927, val_acc=0.77464, time=0.81502
Epoch:0015, train_loss=0.73848, train_acc=0.79601, val_loss=1.05834, val_acc=0.79565, time=0.82697
Epoch:0016, train_loss=0.72833, train_acc=0.80430, val_loss=1.05806, val_acc=0.80290, time=0.84798
Epoch:0017, train_loss=0.72412, train_acc=0.80454, val_loss=1.05759, val_acc=0.80435, time=0.91398
Epoch:0018, train_loss=0.71988, train_acc=0.80470, val_loss=1.05679, val_acc=0.80652, time=0.76199
Epoch:0019, train_loss=0.71403, train_acc=0.80551, val_loss=1.05621, val_acc=0.80507, time=0.86198
Epoch:0020, train_loss=0.71043, train_acc=0.81106, val_loss=1.05586, val_acc=0.80000, time=0.70998
Epoch:0021, train_loss=0.70810, train_acc=0.81162, val_loss=1.05575, val_acc=0.80507, time=0.79599
Epoch:0022, train_loss=0.70718, train_acc=0.80840, val_loss=1.05536, val_acc=0.80725, time=0.80598
Epoch:0023, train_loss=0.70351, train_acc=0.81299, val_loss=1.05499, val_acc=0.81594, time=0.82297
Epoch:0024, train_loss=0.69967, train_acc=0.81903, val_loss=1.05479, val_acc=0.81159, time=0.74602
Epoch:0025, train_loss=0.69668, train_acc=0.82024, val_loss=1.05469, val_acc=0.81522, time=0.77193
Epoch:0026, train_loss=0.69466, train_acc=0.81943, val_loss=1.05444, val_acc=0.81667, time=0.76499
Epoch:0027, train_loss=0.69216, train_acc=0.82201, val_loss=1.05396, val_acc=0.81667, time=0.82496
Epoch:0028, train_loss=0.68859, train_acc=0.82563, val_loss=1.05360, val_acc=0.82174, time=0.82698
Epoch:0029, train_loss=0.68621, train_acc=0.82998, val_loss=1.05336, val_acc=0.82029, time=0.85199
Epoch:0030, train_loss=0.68426, train_acc=0.83103, val_loss=1.05320, val_acc=0.82174, time=0.81098
Epoch:0031, train_loss=0.68261, train_acc=0.83207, val_loss=1.05294, val_acc=0.82319, time=0.73797
Epoch:0032, train_loss=0.67995, train_acc=0.83433, val_loss=1.05275, val_acc=0.82536, time=0.73699
Epoch:0033, train_loss=0.67763, train_acc=0.83706, val_loss=1.05263, val_acc=0.82391, time=0.89197
Epoch:0034, train_loss=0.67571, train_acc=0.83610, val_loss=1.05253, val_acc=0.82681, time=0.74298
Epoch:0035, train_loss=0.67400, train_acc=0.83755, val_loss=1.05230, val_acc=0.82536, time=0.80699
Epoch:0036, train_loss=0.67183, train_acc=0.83875, val_loss=1.05202, val_acc=0.83261, time=1.01297
Epoch:0037, train_loss=0.66955, train_acc=0.84061, val_loss=1.05181, val_acc=0.83551, time=0.78098
Epoch:0038, train_loss=0.66782, train_acc=0.84181, val_loss=1.05166, val_acc=0.83188, time=0.80899
Epoch:0039, train_loss=0.66626, train_acc=0.84149, val_loss=1.05154, val_acc=0.83261, time=0.87497
Epoch:0040, train_loss=0.66466, train_acc=0.84222, val_loss=1.05143, val_acc=0.83623, time=0.83999
Epoch:0041, train_loss=0.66285, train_acc=0.84407, val_loss=1.05137, val_acc=0.83406, time=0.78098
Epoch:0042, train_loss=0.66142, train_acc=0.84560, val_loss=1.05132, val_acc=0.83406, time=0.74798
Epoch:0043, train_loss=0.66012, train_acc=0.84713, val_loss=1.05124, val_acc=0.83261, time=0.76798
Epoch:0044, train_loss=0.65886, train_acc=0.84801, val_loss=1.05110, val_acc=0.83261, time=0.87298
Epoch:0045, train_loss=0.65739, train_acc=0.84906, val_loss=1.05097, val_acc=0.83043, time=0.76099
Epoch:0046, train_loss=0.65618, train_acc=0.84930, val_loss=1.05087, val_acc=0.83116, time=0.94197
Epoch:0047, train_loss=0.65508, train_acc=0.85075, val_loss=1.05079, val_acc=0.83116, time=0.88898
Epoch:0048, train_loss=0.65405, train_acc=0.85147, val_loss=1.05072, val_acc=0.83261, time=0.76297
Epoch:0049, train_loss=0.65290, train_acc=0.85236, val_loss=1.05067, val_acc=0.83333, time=0.75099
Epoch:0050, train_loss=0.65186, train_acc=0.85332, val_loss=1.05063, val_acc=0.83333, time=0.70499
Epoch:0051, train_loss=0.65095, train_acc=0.85437, val_loss=1.05056, val_acc=0.83696, time=0.80698
Epoch:0052, train_loss=0.65005, train_acc=0.85558, val_loss=1.05047, val_acc=0.83768, time=0.76897
Epoch:0053, train_loss=0.64909, train_acc=0.85542, val_loss=1.05037, val_acc=0.83841, time=0.75599
Epoch:0054, train_loss=0.64818, train_acc=0.85711, val_loss=1.05029, val_acc=0.83551, time=0.76798
Epoch:0055, train_loss=0.64739, train_acc=0.85856, val_loss=1.05021, val_acc=0.83551, time=0.75198
Epoch:0056, train_loss=0.64658, train_acc=0.85856, val_loss=1.05016, val_acc=0.83696, time=0.85997
Epoch:0057, train_loss=0.64576, train_acc=0.86017, val_loss=1.05012, val_acc=0.83841, time=0.93799
Epoch:0058, train_loss=0.64495, train_acc=0.86089, val_loss=1.05008, val_acc=0.83913, time=0.85397
Epoch:0059, train_loss=0.64421, train_acc=0.86170, val_loss=1.05002, val_acc=0.83623, time=0.85799
Epoch:0060, train_loss=0.64346, train_acc=0.86315, val_loss=1.04995, val_acc=0.83551, time=0.82499
Epoch:0061, train_loss=0.64267, train_acc=0.86355, val_loss=1.04988, val_acc=0.83841, time=0.86299
Epoch:0062, train_loss=0.64195, train_acc=0.86307, val_loss=1.04982, val_acc=0.83551, time=0.86596
Epoch:0063, train_loss=0.64126, train_acc=0.86355, val_loss=1.04978, val_acc=0.83768, time=0.86398
Epoch:0064, train_loss=0.64057, train_acc=0.86387, val_loss=1.04975, val_acc=0.83623, time=0.80697
Epoch:0065, train_loss=0.63990, train_acc=0.86516, val_loss=1.04971, val_acc=0.83913, time=0.78397
Epoch:0066, train_loss=0.63925, train_acc=0.86492, val_loss=1.04968, val_acc=0.84130, time=0.75599
Epoch:0067, train_loss=0.63862, train_acc=0.86476, val_loss=1.04963, val_acc=0.84058, time=0.77698
Epoch:0068, train_loss=0.63799, train_acc=0.86532, val_loss=1.04957, val_acc=0.84130, time=0.95396
Epoch:0069, train_loss=0.63736, train_acc=0.86588, val_loss=1.04952, val_acc=0.84203, time=0.73399
Epoch:0070, train_loss=0.63677, train_acc=0.86604, val_loss=1.04949, val_acc=0.84058, time=0.82297
Epoch:0071, train_loss=0.63619, train_acc=0.86637, val_loss=1.04946, val_acc=0.84058, time=0.76699
Epoch:0072, train_loss=0.63560, train_acc=0.86661, val_loss=1.04945, val_acc=0.83986, time=0.79499
Epoch:0073, train_loss=0.63504, train_acc=0.86701, val_loss=1.04943, val_acc=0.83986, time=0.78397
Epoch:0074, train_loss=0.63450, train_acc=0.86717, val_loss=1.04939, val_acc=0.84130, time=0.70998
Epoch:0075, train_loss=0.63395, train_acc=0.86814, val_loss=1.04936, val_acc=0.83986, time=0.80499
Epoch:0076, train_loss=0.63341, train_acc=0.86773, val_loss=1.04933, val_acc=0.84058, time=0.81098
Epoch:0077, train_loss=0.63289, train_acc=0.86757, val_loss=1.04931, val_acc=0.84275, time=0.87398
Epoch:0078, train_loss=0.63237, train_acc=0.86806, val_loss=1.04931, val_acc=0.84203, time=0.80398
Epoch:0079, train_loss=0.63186, train_acc=0.86838, val_loss=1.04929, val_acc=0.84348, time=0.84297
Epoch:0080, train_loss=0.63136, train_acc=0.86894, val_loss=1.04927, val_acc=0.84275, time=0.85098
Epoch:0081, train_loss=0.63087, train_acc=0.86886, val_loss=1.04924, val_acc=0.84420, time=0.76099
Epoch:0082, train_loss=0.63039, train_acc=0.86902, val_loss=1.04921, val_acc=0.84420, time=0.81497
Epoch:0083, train_loss=0.62992, train_acc=0.86926, val_loss=1.04920, val_acc=0.84348, time=0.88297
Epoch:0084, train_loss=0.62945, train_acc=0.87047, val_loss=1.04918, val_acc=0.84493, time=0.84298
Epoch:0085, train_loss=0.62899, train_acc=0.87079, val_loss=1.04917, val_acc=0.84130, time=0.78599
Epoch:0086, train_loss=0.62855, train_acc=0.87184, val_loss=1.04914, val_acc=0.84275, time=0.75498
Epoch:0087, train_loss=0.62811, train_acc=0.87232, val_loss=1.04912, val_acc=0.84130, time=0.84198
Epoch:0088, train_loss=0.62767, train_acc=0.87256, val_loss=1.04909, val_acc=0.84130, time=0.72498
Epoch:0089, train_loss=0.62725, train_acc=0.87321, val_loss=1.04908, val_acc=0.84203, time=0.86497
Epoch:0090, train_loss=0.62683, train_acc=0.87345, val_loss=1.04906, val_acc=0.84275, time=0.85000
Epoch:0091, train_loss=0.62642, train_acc=0.87393, val_loss=1.04905, val_acc=0.84203, time=0.91298
Epoch:0092, train_loss=0.62601, train_acc=0.87434, val_loss=1.04903, val_acc=0.84203, time=0.77097
Epoch:0093, train_loss=0.62563, train_acc=0.87466, val_loss=1.04903, val_acc=0.84130, time=0.84799
Epoch:0094, train_loss=0.62526, train_acc=0.87458, val_loss=1.04900, val_acc=0.84203, time=0.78998
Epoch:0095, train_loss=0.62497, train_acc=0.87458, val_loss=1.04905, val_acc=0.84275, time=0.75798
Epoch:0096, train_loss=0.62487, train_acc=0.87474, val_loss=1.04907, val_acc=0.83841, time=0.77097
Early stopping...

Optimization Finished!

Test set results: loss= 0.88622, accuracy= 0.85900, time= 0.26101

Test Precision, Recall and F1-Score...
              precision    recall  f1-score   support

           0     0.8779    0.8269    0.8516      2357
           1     0.8425    0.8947    0.8678      2356
           2     0.8583    0.8519    0.8551      1202

    accuracy                         0.8590      5915
   macro avg     0.8596    0.8578    0.8582      5915
weighted avg     0.8598    0.8590    0.8588      5915


Macro average Test Precision, Recall and F1-Score...
(0.8595980752230941, 0.8578496398526158, 0.8582037417370417, None)

Micro average Test Precision, Recall and F1-Score...
(0.8590025359256128, 0.8590025359256128, 0.8590025359256127, None)

Embeddings:
Word_embeddings:452
Train_doc_embeddings:13802
Test_doc_embeddings:5915
