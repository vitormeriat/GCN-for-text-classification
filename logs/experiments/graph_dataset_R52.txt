
==================== Torch Seed: 24392484565700
Epoch:0001, train_loss=8.41711, train_acc=0.01276, val_loss=4.35251, val_acc=0.03063, time=0.79801
Epoch:0002, train_loss=7.34494, train_acc=0.03232, val_loss=4.28605, val_acc=0.04135, time=0.76601
Epoch:0003, train_loss=6.40970, train_acc=0.06838, val_loss=4.23050, val_acc=0.05972, time=1.34604
Epoch:0004, train_loss=5.63846, train_acc=0.13251, val_loss=4.18590, val_acc=0.08576, time=1.55001
Epoch:0005, train_loss=5.02824, train_acc=0.21041, val_loss=4.15077, val_acc=0.13476, time=1.12002
Epoch:0006, train_loss=4.54455, train_acc=0.29716, val_loss=4.12327, val_acc=0.18683, time=0.79000
Epoch:0007, train_loss=4.15495, train_acc=0.39531, val_loss=4.10192, val_acc=0.23277, time=0.76701
Epoch:0008, train_loss=3.83991, train_acc=0.48954, val_loss=4.08574, val_acc=0.27565, time=0.72502
Epoch:0009, train_loss=3.59074, train_acc=0.57663, val_loss=4.07406, val_acc=0.32006, time=0.90701
Epoch:0010, train_loss=3.40105, train_acc=0.64501, val_loss=4.06646, val_acc=0.34303, time=0.80600
Epoch:0011, train_loss=3.26117, train_acc=0.70658, val_loss=4.06211, val_acc=0.35835, time=0.81600
Epoch:0012, train_loss=3.15718, train_acc=0.75421, val_loss=4.05996, val_acc=0.36753, time=0.79702
Epoch:0013, train_loss=3.07468, train_acc=0.78772, val_loss=4.05900, val_acc=0.37979, time=0.73901
Epoch:0014, train_loss=3.00293, train_acc=0.81374, val_loss=4.05853, val_acc=0.39204, time=1.30902
Epoch:0015, train_loss=2.93795, train_acc=0.84538, val_loss=4.05813, val_acc=0.39204, time=0.95702
Epoch:0016, train_loss=2.87977, train_acc=0.87498, val_loss=4.05764, val_acc=0.39357, time=0.95802
Epoch:0017, train_loss=2.82922, train_acc=0.89828, val_loss=4.05703, val_acc=0.39969, time=0.74300
Epoch:0018, train_loss=2.78729, train_acc=0.92142, val_loss=4.05636, val_acc=0.40123, time=1.28602
Epoch:0019, train_loss=2.75409, train_acc=0.93945, val_loss=4.05569, val_acc=0.40123, time=0.76302
Epoch:0020, train_loss=2.72859, train_acc=0.95611, val_loss=4.05504, val_acc=0.40276, time=0.81701
Epoch:0021, train_loss=2.70959, train_acc=0.96649, val_loss=4.05444, val_acc=0.40582, time=0.76402
Epoch:0022, train_loss=2.69579, train_acc=0.97619, val_loss=4.05388, val_acc=0.40276, time=0.74900
Epoch:0023, train_loss=2.68594, train_acc=0.98265, val_loss=4.05337, val_acc=0.40123, time=0.86800
Epoch:0024, train_loss=2.67884, train_acc=0.98792, val_loss=4.05291, val_acc=0.40429, time=0.90202
Epoch:0025, train_loss=2.67364, train_acc=0.99064, val_loss=4.05250, val_acc=0.40429, time=0.83200
Epoch:0026, train_loss=2.66986, train_acc=0.99354, val_loss=4.05213, val_acc=0.40429, time=0.82402
Epoch:0027, train_loss=2.66716, train_acc=0.99524, val_loss=4.05181, val_acc=0.40276, time=0.82400
Epoch:0028, train_loss=2.66524, train_acc=0.99745, val_loss=4.05151, val_acc=0.40276, time=0.75502
Epoch:0029, train_loss=2.66391, train_acc=0.99830, val_loss=4.05125, val_acc=0.40276, time=0.75601
Epoch:0030, train_loss=2.66298, train_acc=0.99915, val_loss=4.05102, val_acc=0.40123, time=0.76501
Epoch:0031, train_loss=2.66234, train_acc=0.99915, val_loss=4.05082, val_acc=0.39663, time=0.89502
Epoch:0032, train_loss=2.66189, train_acc=0.99932, val_loss=4.05063, val_acc=0.39663, time=0.80100
Epoch:0033, train_loss=2.66155, train_acc=0.99932, val_loss=4.05048, val_acc=0.39663, time=0.91502
Epoch:0034, train_loss=2.66130, train_acc=0.99966, val_loss=4.05033, val_acc=0.39816, time=1.16501
Epoch:0035, train_loss=2.66112, train_acc=0.99966, val_loss=4.05021, val_acc=0.40123, time=0.72702
Epoch:0036, train_loss=2.66098, train_acc=0.99966, val_loss=4.05009, val_acc=0.39969, time=0.78400
Epoch:0037, train_loss=2.66086, train_acc=0.99966, val_loss=4.04999, val_acc=0.39969, time=0.80602
Epoch:0038, train_loss=2.66075, train_acc=0.99966, val_loss=4.04989, val_acc=0.39969, time=0.84901
Epoch:0039, train_loss=2.66065, train_acc=0.99966, val_loss=4.04980, val_acc=0.39816, time=0.84502
Epoch:0040, train_loss=2.66056, train_acc=0.99966, val_loss=4.04971, val_acc=0.39663, time=0.71601
Epoch:0041, train_loss=2.66048, train_acc=0.99983, val_loss=4.04963, val_acc=0.39663, time=0.83601
Epoch:0042, train_loss=2.66042, train_acc=1.00000, val_loss=4.04956, val_acc=0.39663, time=0.88201
Epoch:0043, train_loss=2.66037, train_acc=1.00000, val_loss=4.04949, val_acc=0.39510, time=0.80299
Epoch:0044, train_loss=2.66033, train_acc=1.00000, val_loss=4.04942, val_acc=0.39510, time=0.73302
Epoch:0045, train_loss=2.66030, train_acc=1.00000, val_loss=4.04936, val_acc=0.39510, time=0.75501
Epoch:0046, train_loss=2.66028, train_acc=1.00000, val_loss=4.04930, val_acc=0.39510, time=0.92701
Epoch:0047, train_loss=2.66027, train_acc=1.00000, val_loss=4.04924, val_acc=0.39510, time=0.77702
Epoch:0048, train_loss=2.66026, train_acc=1.00000, val_loss=4.04919, val_acc=0.39663, time=0.98301
Epoch:0049, train_loss=2.66025, train_acc=1.00000, val_loss=4.04914, val_acc=0.39663, time=0.76402
Epoch:0050, train_loss=2.66025, train_acc=1.00000, val_loss=4.04909, val_acc=0.39663, time=0.82201
Epoch:0051, train_loss=2.66024, train_acc=1.00000, val_loss=4.04904, val_acc=0.39663, time=1.28302
Epoch:0052, train_loss=2.66024, train_acc=1.00000, val_loss=4.04900, val_acc=0.39663, time=0.72701
Epoch:0053, train_loss=2.66024, train_acc=1.00000, val_loss=4.04895, val_acc=0.39663, time=0.93701
Epoch:0054, train_loss=2.66023, train_acc=1.00000, val_loss=4.04891, val_acc=0.39816, time=0.77502
Epoch:0055, train_loss=2.66023, train_acc=1.00000, val_loss=4.04887, val_acc=0.39816, time=0.80900
Epoch:0056, train_loss=2.66023, train_acc=1.00000, val_loss=4.04883, val_acc=0.39816, time=0.76602
Epoch:0057, train_loss=2.66023, train_acc=1.00000, val_loss=4.04880, val_acc=0.39816, time=0.82901
Epoch:0058, train_loss=2.66022, train_acc=1.00000, val_loss=4.04876, val_acc=0.39816, time=0.75001
Epoch:0059, train_loss=2.66022, train_acc=1.00000, val_loss=4.04872, val_acc=0.39969, time=0.88300
Epoch:0060, train_loss=2.66022, train_acc=1.00000, val_loss=4.04869, val_acc=0.39969, time=0.92702
Epoch:0061, train_loss=2.66022, train_acc=1.00000, val_loss=4.04866, val_acc=0.39969, time=0.85201
Epoch:0062, train_loss=2.66022, train_acc=1.00000, val_loss=4.04862, val_acc=0.39969, time=0.87200
Epoch:0063, train_loss=2.66022, train_acc=1.00000, val_loss=4.04859, val_acc=0.39969, time=0.73203
Epoch:0064, train_loss=2.66021, train_acc=1.00000, val_loss=4.04856, val_acc=0.39969, time=0.76002
Epoch:0065, train_loss=2.66021, train_acc=1.00000, val_loss=4.04853, val_acc=0.39969, time=0.82202
Epoch:0066, train_loss=2.66021, train_acc=1.00000, val_loss=4.04850, val_acc=0.39969, time=0.82901
Epoch:0067, train_loss=2.66021, train_acc=1.00000, val_loss=4.04848, val_acc=0.39969, time=0.77701
Epoch:0068, train_loss=2.66021, train_acc=1.00000, val_loss=4.04845, val_acc=0.39969, time=0.74799
Epoch:0069, train_loss=2.66021, train_acc=1.00000, val_loss=4.04842, val_acc=0.39969, time=0.99202
Epoch:0070, train_loss=2.66021, train_acc=1.00000, val_loss=4.04840, val_acc=0.39969, time=0.77702
Epoch:0071, train_loss=2.66021, train_acc=1.00000, val_loss=4.04837, val_acc=0.39969, time=0.73602
Epoch:0072, train_loss=2.66021, train_acc=1.00000, val_loss=4.04834, val_acc=0.39969, time=0.74801
Epoch:0073, train_loss=2.66021, train_acc=1.00000, val_loss=4.04832, val_acc=0.39969, time=0.86902
Epoch:0074, train_loss=2.66020, train_acc=1.00000, val_loss=4.04830, val_acc=0.39969, time=0.80301
Epoch:0075, train_loss=2.66020, train_acc=1.00000, val_loss=4.04827, val_acc=0.39969, time=1.00702
Epoch:0076, train_loss=2.66020, train_acc=1.00000, val_loss=4.04825, val_acc=0.39969, time=0.80901
Epoch:0077, train_loss=2.66020, train_acc=1.00000, val_loss=4.04822, val_acc=0.40123, time=0.74900
Epoch:0078, train_loss=2.66020, train_acc=1.00000, val_loss=4.04820, val_acc=0.40123, time=0.92003
Epoch:0079, train_loss=2.66020, train_acc=1.00000, val_loss=4.04818, val_acc=0.40123, time=0.76300
Epoch:0080, train_loss=2.66020, train_acc=1.00000, val_loss=4.04816, val_acc=0.40123, time=0.86602
Epoch:0081, train_loss=2.66020, train_acc=1.00000, val_loss=4.04814, val_acc=0.40123, time=0.76501
Epoch:0082, train_loss=2.66020, train_acc=1.00000, val_loss=4.04812, val_acc=0.40276, time=0.85600
Epoch:0083, train_loss=2.66020, train_acc=1.00000, val_loss=4.04809, val_acc=0.40276, time=0.72200
Epoch:0084, train_loss=2.66020, train_acc=1.00000, val_loss=4.04807, val_acc=0.40276, time=0.96303
Epoch:0085, train_loss=2.66020, train_acc=1.00000, val_loss=4.04805, val_acc=0.40276, time=0.89702
Epoch:0086, train_loss=2.66020, train_acc=1.00000, val_loss=4.04803, val_acc=0.40276, time=0.81302
Epoch:0087, train_loss=2.66020, train_acc=1.00000, val_loss=4.04801, val_acc=0.40276, time=0.99300
Epoch:0088, train_loss=2.66020, train_acc=1.00000, val_loss=4.04799, val_acc=0.40276, time=0.74601
Epoch:0089, train_loss=2.66020, train_acc=1.00000, val_loss=4.04798, val_acc=0.40276, time=0.75202
Epoch:0090, train_loss=2.66020, train_acc=1.00000, val_loss=4.04796, val_acc=0.40276, time=0.83601
Epoch:0091, train_loss=2.66019, train_acc=1.00000, val_loss=4.04794, val_acc=0.40276, time=0.77500
Epoch:0092, train_loss=2.66019, train_acc=1.00000, val_loss=4.04792, val_acc=0.40276, time=0.83201
Epoch:0093, train_loss=2.66019, train_acc=1.00000, val_loss=4.04790, val_acc=0.40276, time=0.84902
Epoch:0094, train_loss=2.66019, train_acc=1.00000, val_loss=4.04788, val_acc=0.40276, time=0.77801
Epoch:0095, train_loss=2.66019, train_acc=1.00000, val_loss=4.04786, val_acc=0.40276, time=0.79601
Epoch:0096, train_loss=2.66019, train_acc=1.00000, val_loss=4.04785, val_acc=0.40429, time=0.82301
Epoch:0097, train_loss=2.66019, train_acc=1.00000, val_loss=4.04783, val_acc=0.40429, time=0.74201
Epoch:0098, train_loss=2.66019, train_acc=1.00000, val_loss=4.04781, val_acc=0.40429, time=0.72102
Epoch:0099, train_loss=2.66019, train_acc=1.00000, val_loss=4.04779, val_acc=0.40429, time=0.77801
Epoch:0100, train_loss=2.66019, train_acc=1.00000, val_loss=4.04778, val_acc=0.40582, time=0.80601
Epoch:0101, train_loss=2.66019, train_acc=1.00000, val_loss=4.04776, val_acc=0.40582, time=0.86002
Epoch:0102, train_loss=2.66019, train_acc=1.00000, val_loss=4.04774, val_acc=0.40582, time=0.75199
Epoch:0103, train_loss=2.66019, train_acc=1.00000, val_loss=4.04773, val_acc=0.40582, time=0.78602
Epoch:0104, train_loss=2.66019, train_acc=1.00000, val_loss=4.04771, val_acc=0.40582, time=0.78602
Epoch:0105, train_loss=2.66019, train_acc=1.00000, val_loss=4.04769, val_acc=0.40888, time=1.03100
Epoch:0106, train_loss=2.66019, train_acc=1.00000, val_loss=4.04768, val_acc=0.40888, time=1.50903
Epoch:0107, train_loss=2.66019, train_acc=1.00000, val_loss=4.04766, val_acc=0.40888, time=0.78002
Epoch:0108, train_loss=2.66019, train_acc=1.00000, val_loss=4.04765, val_acc=0.40888, time=0.87199
Epoch:0109, train_loss=2.66019, train_acc=1.00000, val_loss=4.04763, val_acc=0.41041, time=0.78701
Epoch:0110, train_loss=2.66019, train_acc=1.00000, val_loss=4.04761, val_acc=0.41041, time=0.82302
Epoch:0111, train_loss=2.66019, train_acc=1.00000, val_loss=4.04760, val_acc=0.41041, time=0.75602
Epoch:0112, train_loss=2.66019, train_acc=1.00000, val_loss=4.04758, val_acc=0.41041, time=0.74201
Epoch:0113, train_loss=2.66019, train_acc=1.00000, val_loss=4.04757, val_acc=0.41041, time=0.93301
Epoch:0114, train_loss=2.66019, train_acc=1.00000, val_loss=4.04755, val_acc=0.41041, time=0.78301
Epoch:0115, train_loss=2.66019, train_acc=1.00000, val_loss=4.04754, val_acc=0.41041, time=0.73003
Epoch:0116, train_loss=2.66019, train_acc=1.00000, val_loss=4.04752, val_acc=0.41041, time=0.85300
Epoch:0117, train_loss=2.66019, train_acc=1.00000, val_loss=4.04751, val_acc=0.41041, time=0.97901
Epoch:0118, train_loss=2.66019, train_acc=1.00000, val_loss=4.04749, val_acc=0.41041, time=0.87002
Epoch:0119, train_loss=2.66019, train_acc=1.00000, val_loss=4.04748, val_acc=0.41041, time=0.77301
Epoch:0120, train_loss=2.66019, train_acc=1.00000, val_loss=4.04746, val_acc=0.41041, time=0.84202
Epoch:0121, train_loss=2.66019, train_acc=1.00000, val_loss=4.04745, val_acc=0.41041, time=0.76401
Epoch:0122, train_loss=2.66019, train_acc=1.00000, val_loss=4.04743, val_acc=0.41041, time=0.85302
Epoch:0123, train_loss=2.66018, train_acc=1.00000, val_loss=4.04742, val_acc=0.41041, time=0.82601
Epoch:0124, train_loss=2.66018, train_acc=1.00000, val_loss=4.04741, val_acc=0.41041, time=0.94101
Epoch:0125, train_loss=2.66018, train_acc=1.00000, val_loss=4.04739, val_acc=0.41041, time=0.83201
Epoch:0126, train_loss=2.66018, train_acc=1.00000, val_loss=4.04738, val_acc=0.41041, time=0.77501
Epoch:0127, train_loss=2.66018, train_acc=1.00000, val_loss=4.04736, val_acc=0.41041, time=0.77702
Epoch:0128, train_loss=2.66018, train_acc=1.00000, val_loss=4.04735, val_acc=0.41041, time=0.83001
Epoch:0129, train_loss=2.66018, train_acc=1.00000, val_loss=4.04734, val_acc=0.41041, time=0.89801
Epoch:0130, train_loss=2.66018, train_acc=1.00000, val_loss=4.04732, val_acc=0.41041, time=0.97900
Epoch:0131, train_loss=2.66018, train_acc=1.00000, val_loss=4.04731, val_acc=0.41041, time=0.74301
Epoch:0132, train_loss=2.66018, train_acc=1.00000, val_loss=4.04730, val_acc=0.41041, time=0.76002
Epoch:0133, train_loss=2.66018, train_acc=1.00000, val_loss=4.04728, val_acc=0.41041, time=0.84800
Epoch:0134, train_loss=2.66018, train_acc=1.00000, val_loss=4.04727, val_acc=0.41041, time=0.78502
Epoch:0135, train_loss=2.66018, train_acc=1.00000, val_loss=4.04726, val_acc=0.41041, time=0.78801
Epoch:0136, train_loss=2.66018, train_acc=1.00000, val_loss=4.04724, val_acc=0.41041, time=0.80501
Epoch:0137, train_loss=2.66018, train_acc=1.00000, val_loss=4.04723, val_acc=0.41041, time=0.78401
Epoch:0138, train_loss=2.66018, train_acc=1.00000, val_loss=4.04722, val_acc=0.41041, time=1.32302
Epoch:0139, train_loss=2.66018, train_acc=1.00000, val_loss=4.04720, val_acc=0.41041, time=0.90701
Epoch:0140, train_loss=2.66018, train_acc=1.00000, val_loss=4.04719, val_acc=0.40888, time=0.87800
Epoch:0141, train_loss=2.66018, train_acc=1.00000, val_loss=4.04718, val_acc=0.40888, time=0.80102
Epoch:0142, train_loss=2.66018, train_acc=1.00000, val_loss=4.04717, val_acc=0.40888, time=0.81702
Epoch:0143, train_loss=2.66018, train_acc=1.00000, val_loss=4.04715, val_acc=0.40888, time=0.84801
Epoch:0144, train_loss=2.66018, train_acc=1.00000, val_loss=4.04714, val_acc=0.40888, time=0.99202
Epoch:0145, train_loss=2.66018, train_acc=1.00000, val_loss=4.04713, val_acc=0.40888, time=0.87701
Epoch:0146, train_loss=2.66018, train_acc=1.00000, val_loss=4.04712, val_acc=0.40888, time=0.80300
Epoch:0147, train_loss=2.66018, train_acc=1.00000, val_loss=4.04710, val_acc=0.40888, time=0.85400
Epoch:0148, train_loss=2.66018, train_acc=1.00000, val_loss=4.04709, val_acc=0.40888, time=0.79202
Epoch:0149, train_loss=2.66018, train_acc=1.00000, val_loss=4.04708, val_acc=0.40888, time=0.94501
Epoch:0150, train_loss=2.66018, train_acc=1.00000, val_loss=4.04707, val_acc=0.40888, time=0.79401
Epoch:0151, train_loss=2.66018, train_acc=1.00000, val_loss=4.04705, val_acc=0.40888, time=0.77101
Epoch:0152, train_loss=2.66018, train_acc=1.00000, val_loss=4.04704, val_acc=0.40888, time=0.92301
Epoch:0153, train_loss=2.66018, train_acc=1.00000, val_loss=4.04703, val_acc=0.40888, time=0.77802
Epoch:0154, train_loss=2.66018, train_acc=1.00000, val_loss=4.04702, val_acc=0.40888, time=0.73101
Epoch:0155, train_loss=2.66018, train_acc=1.00000, val_loss=4.04701, val_acc=0.40888, time=0.86402
Epoch:0156, train_loss=2.66018, train_acc=1.00000, val_loss=4.04700, val_acc=0.40888, time=0.73302
Epoch:0157, train_loss=2.66018, train_acc=1.00000, val_loss=4.04698, val_acc=0.40888, time=1.33201
Epoch:0158, train_loss=2.66018, train_acc=1.00000, val_loss=4.04697, val_acc=0.40888, time=0.72902
Epoch:0159, train_loss=2.66018, train_acc=1.00000, val_loss=4.04696, val_acc=0.40888, time=1.28000
Epoch:0160, train_loss=2.66018, train_acc=1.00000, val_loss=4.04695, val_acc=0.40888, time=0.93102
Epoch:0161, train_loss=2.66018, train_acc=1.00000, val_loss=4.04694, val_acc=0.40888, time=0.90201
Epoch:0162, train_loss=2.66018, train_acc=1.00000, val_loss=4.04693, val_acc=0.40888, time=0.96402
Epoch:0163, train_loss=2.66018, train_acc=1.00000, val_loss=4.04692, val_acc=0.40888, time=0.78801
Epoch:0164, train_loss=2.66018, train_acc=1.00000, val_loss=4.04690, val_acc=0.40888, time=0.89501
Epoch:0165, train_loss=2.66018, train_acc=1.00000, val_loss=4.04689, val_acc=0.40888, time=0.82201
Epoch:0166, train_loss=2.66018, train_acc=1.00000, val_loss=4.04688, val_acc=0.40888, time=0.77601
Epoch:0167, train_loss=2.66018, train_acc=1.00000, val_loss=4.04687, val_acc=0.40888, time=0.80502
Epoch:0168, train_loss=2.66018, train_acc=1.00000, val_loss=4.04686, val_acc=0.40888, time=0.76701
Epoch:0169, train_loss=2.66018, train_acc=1.00000, val_loss=4.04685, val_acc=0.40888, time=0.90601
Epoch:0170, train_loss=2.66018, train_acc=1.00000, val_loss=4.04684, val_acc=0.40888, time=0.74099
Epoch:0171, train_loss=2.66018, train_acc=1.00000, val_loss=4.04683, val_acc=0.40888, time=0.86002
Epoch:0172, train_loss=2.66018, train_acc=1.00000, val_loss=4.04682, val_acc=0.40888, time=0.86402
Epoch:0173, train_loss=2.66018, train_acc=1.00000, val_loss=4.04681, val_acc=0.40888, time=0.77400
Epoch:0174, train_loss=2.66018, train_acc=1.00000, val_loss=4.04679, val_acc=0.40888, time=0.89901
Epoch:0175, train_loss=2.66018, train_acc=1.00000, val_loss=4.04678, val_acc=0.40888, time=1.02801
Epoch:0176, train_loss=2.66018, train_acc=1.00000, val_loss=4.04677, val_acc=0.40888, time=1.01701
Epoch:0177, train_loss=2.66018, train_acc=1.00000, val_loss=4.04676, val_acc=0.40888, time=0.95101
Epoch:0178, train_loss=2.66018, train_acc=1.00000, val_loss=4.04675, val_acc=0.40888, time=0.84501
Epoch:0179, train_loss=2.66018, train_acc=1.00000, val_loss=4.04674, val_acc=0.40888, time=0.87802
Epoch:0180, train_loss=2.66018, train_acc=1.00000, val_loss=4.04673, val_acc=0.40888, time=0.77800
Epoch:0181, train_loss=2.66018, train_acc=1.00000, val_loss=4.04672, val_acc=0.40888, time=0.83500
Epoch:0182, train_loss=2.66017, train_acc=1.00000, val_loss=4.04671, val_acc=0.40888, time=0.77801
Epoch:0183, train_loss=2.66017, train_acc=1.00000, val_loss=4.04670, val_acc=0.40888, time=0.73101
Epoch:0184, train_loss=2.66017, train_acc=1.00000, val_loss=4.04669, val_acc=0.40888, time=1.13803
Epoch:0185, train_loss=2.66017, train_acc=1.00000, val_loss=4.04668, val_acc=0.40888, time=0.75302
Epoch:0186, train_loss=2.66017, train_acc=1.00000, val_loss=4.04667, val_acc=0.40888, time=1.01102
Epoch:0187, train_loss=2.66017, train_acc=1.00000, val_loss=4.04666, val_acc=0.40888, time=0.87202
Epoch:0188, train_loss=2.66017, train_acc=1.00000, val_loss=4.04665, val_acc=0.40888, time=0.78401
Epoch:0189, train_loss=2.66017, train_acc=1.00000, val_loss=4.04664, val_acc=0.41041, time=0.82800
Epoch:0190, train_loss=2.66017, train_acc=1.00000, val_loss=4.04663, val_acc=0.41041, time=0.80501
Epoch:0191, train_loss=2.66017, train_acc=1.00000, val_loss=4.04662, val_acc=0.41041, time=0.73500
Epoch:0192, train_loss=2.66017, train_acc=1.00000, val_loss=4.04661, val_acc=0.41041, time=0.85701
Epoch:0193, train_loss=2.66017, train_acc=1.00000, val_loss=4.04660, val_acc=0.41041, time=0.78402
Epoch:0194, train_loss=2.66017, train_acc=1.00000, val_loss=4.04659, val_acc=0.41041, time=0.80002
Epoch:0195, train_loss=2.66017, train_acc=1.00000, val_loss=4.04658, val_acc=0.41041, time=0.79101
Epoch:0196, train_loss=2.66017, train_acc=1.00000, val_loss=4.04657, val_acc=0.41041, time=1.17202
Epoch:0197, train_loss=2.66017, train_acc=1.00000, val_loss=4.04656, val_acc=0.41041, time=0.72502
Epoch:0198, train_loss=2.66017, train_acc=1.00000, val_loss=4.04655, val_acc=0.41041, time=0.92200
Epoch:0199, train_loss=2.66017, train_acc=1.00000, val_loss=4.04654, val_acc=0.41041, time=0.88600
Epoch:0200, train_loss=2.66017, train_acc=1.00000, val_loss=4.04653, val_acc=0.41041, time=0.89302

Optimization Finished!

Test set results: loss= 4.35425, accuracy= 0.39603, time= 0.23000

Test Precision, Recall and F1-Score...
              precision    recall  f1-score   support

           0     0.6196    0.6962    0.6557      1083
           1     0.0417    0.0357    0.0385        28
           2     0.6108    0.3089    0.4103       696
           3     0.0270    0.0400    0.0323        25
           4     0.1210    0.1724    0.1422        87
           5     0.0000    0.0000    0.0000        15
           6     0.0231    0.0400    0.0293        75
           7     0.0000    0.0000    0.0000        36
           8     0.0000    0.0000    0.0000         9
           9     0.0000    0.0000    0.0000        12
          10     0.0000    0.0000    0.0000        11
          11     0.0000    0.0000    0.0000        11
          12     0.0227    0.0247    0.0237        81
          13     0.1639    0.1653    0.1646       121
          14     0.0000    0.0000    0.0000        20
          15     0.0000    0.0000    0.0000         1
          16     0.1250    0.1111    0.1176         9
          17     0.0000    0.0000    0.0000        22
          18     0.0000    0.0000    0.0000         5
          19     0.0408    0.1176    0.0606        17
          20     0.0370    0.0667    0.0476        15
          21     0.0000    0.0000    0.0000         9
          22     0.0000    0.0000    0.0000        10
          23     0.0000    0.0000    0.0000        19
          24     0.0000    0.0000    0.0000        10
          25     0.0000    0.0000    0.0000        12
          26     0.1429    0.1111    0.1250         9
          27     0.0588    0.0769    0.0667        13
          28     0.0000    0.0000    0.0000        12
          29     0.0000    0.0000    0.0000         2
          30     0.0000    0.0000    0.0000         6
          31     0.0000    0.0000    0.0000         4
          32     0.0000    0.0000    0.0000         7
          33     0.0000    0.0000    0.0000         2
          34     0.0000    0.0000    0.0000         9
          35     0.0000    0.0000    0.0000         3
          36     0.0000    0.0000    0.0000         1
          37     0.0000    0.0000    0.0000         8
          38     0.0000    0.0000    0.0000        12
          39     0.0000    0.0000    0.0000         3
          40     0.0000    0.0000    0.0000         5
          41     0.0000    0.0000    0.0000         6
          42     0.0000    0.0000    0.0000         1
          43     0.0000    0.0000    0.0000         3
          44     0.0000    0.0000    0.0000         4
          45     0.0000    0.0000    0.0000         1
          46     0.0000    0.0000    0.0000         5
          47     0.0000    0.0000    0.0000         1
          48     0.0000    0.0000    0.0000         4
          49     0.0000    0.0000    0.0000         3
          50     0.0000    0.0000    0.0000         4
          51     0.0000    0.0000    0.0000         1

    accuracy                         0.3960      2568
   macro avg     0.0391    0.0378    0.0368      2568
weighted avg     0.4425    0.3960    0.4045      2568


Macro average Test Precision, Recall and F1-Score...
(0.039120881422960545, 0.037820961219484715, 0.03680657757958306, None)

Micro average Test Precision, Recall and F1-Score...
(0.39602803738317754, 0.39602803738317754, 0.39602803738317754, None)

Embeddings:
Word_embeddings:8892
Train_doc_embeddings:6532
Test_doc_embeddings:2568
