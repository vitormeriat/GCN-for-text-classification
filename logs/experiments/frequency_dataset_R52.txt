
==================== Torch Seed: 1214545773200
Epoch:0001, train_loss=4.01187, train_acc=0.01208, val_loss=3.93492, val_acc=0.41960, time=0.92101
Epoch:0002, train_loss=3.80759, train_acc=0.42014, val_loss=3.91465, val_acc=0.59418, time=0.78502
Epoch:0003, train_loss=3.62784, train_acc=0.59058, val_loss=3.89726, val_acc=0.64319, time=1.00200
Epoch:0004, train_loss=3.47343, train_acc=0.62868, val_loss=3.88399, val_acc=0.65697, time=0.95801
Epoch:0005, train_loss=3.35638, train_acc=0.64569, val_loss=3.87509, val_acc=0.66309, time=0.89701
Epoch:0006, train_loss=3.27861, train_acc=0.65385, val_loss=3.86936, val_acc=0.66922, time=0.94101
Epoch:0007, train_loss=3.22860, train_acc=0.65811, val_loss=3.86524, val_acc=0.68147, time=0.76000
Epoch:0008, train_loss=3.19211, train_acc=0.66644, val_loss=3.86176, val_acc=0.70444, time=1.00101
Epoch:0009, train_loss=3.16043, train_acc=0.68158, val_loss=3.85858, val_acc=0.72741, time=0.77602
Epoch:0010, train_loss=3.13052, train_acc=0.70318, val_loss=3.85567, val_acc=0.74885, time=1.00700
Epoch:0011, train_loss=3.10214, train_acc=0.72274, val_loss=3.85304, val_acc=0.76723, time=0.89601
Epoch:0012, train_loss=3.07570, train_acc=0.73975, val_loss=3.85065, val_acc=0.77489, time=0.77403
Epoch:0013, train_loss=3.05110, train_acc=0.75557, val_loss=3.84847, val_acc=0.79632, time=0.87800
Epoch:0014, train_loss=3.02842, train_acc=0.77496, val_loss=3.84651, val_acc=0.81164, time=0.83401
Epoch:0015, train_loss=3.00805, train_acc=0.79775, val_loss=3.84480, val_acc=0.83461, time=0.86801
Epoch:0016, train_loss=2.99029, train_acc=0.81136, val_loss=3.84330, val_acc=0.84074, time=0.94100
Epoch:0017, train_loss=2.97476, train_acc=0.82531, val_loss=3.84193, val_acc=0.85299, time=0.90001
Epoch:0018, train_loss=2.96065, train_acc=0.83637, val_loss=3.84061, val_acc=0.86371, time=0.91700
Epoch:0019, train_loss=2.94735, train_acc=0.84742, val_loss=3.83934, val_acc=0.86524, time=0.78501
Epoch:0020, train_loss=2.93465, train_acc=0.85287, val_loss=3.83812, val_acc=0.87136, time=0.94900
Epoch:0021, train_loss=2.92256, train_acc=0.85882, val_loss=3.83696, val_acc=0.87596, time=0.71900
Epoch:0022, train_loss=2.91112, train_acc=0.86358, val_loss=3.83586, val_acc=0.87596, time=0.82102
Epoch:0023, train_loss=2.90034, train_acc=0.86903, val_loss=3.83481, val_acc=0.87902, time=0.85002
Epoch:0024, train_loss=2.89013, train_acc=0.87447, val_loss=3.83380, val_acc=0.88208, time=0.80700
Epoch:0025, train_loss=2.88041, train_acc=0.88025, val_loss=3.83284, val_acc=0.88821, time=0.87401
Epoch:0026, train_loss=2.87116, train_acc=0.88348, val_loss=3.83193, val_acc=0.89127, time=0.80801
Epoch:0027, train_loss=2.86239, train_acc=0.88808, val_loss=3.83107, val_acc=0.89433, time=0.84201
Epoch:0028, train_loss=2.85413, train_acc=0.89216, val_loss=3.83027, val_acc=0.89433, time=0.96803
Epoch:0029, train_loss=2.84634, train_acc=0.89471, val_loss=3.82953, val_acc=0.89740, time=0.93800
Epoch:0030, train_loss=2.83895, train_acc=0.89896, val_loss=3.82883, val_acc=0.89587, time=0.92402
Epoch:0031, train_loss=2.83186, train_acc=0.90373, val_loss=3.82816, val_acc=0.89587, time=1.15302
Epoch:0032, train_loss=2.82500, train_acc=0.90560, val_loss=3.82754, val_acc=0.89587, time=0.98201
Epoch:0033, train_loss=2.81838, train_acc=0.90934, val_loss=3.82695, val_acc=0.89433, time=1.07302
Epoch:0034, train_loss=2.81206, train_acc=0.91274, val_loss=3.82642, val_acc=0.90199, time=0.94601
Epoch:0035, train_loss=2.80604, train_acc=0.91580, val_loss=3.82593, val_acc=0.90658, time=1.00201
Epoch:0036, train_loss=2.80028, train_acc=0.91937, val_loss=3.82548, val_acc=0.90965, time=1.13302
Epoch:0037, train_loss=2.79472, train_acc=0.91988, val_loss=3.82505, val_acc=0.91118, time=0.88101
Epoch:0038, train_loss=2.78934, train_acc=0.92295, val_loss=3.82465, val_acc=0.91118, time=1.03601
Epoch:0039, train_loss=2.78421, train_acc=0.92584, val_loss=3.82427, val_acc=0.91118, time=0.94501
Epoch:0040, train_loss=2.77937, train_acc=0.92856, val_loss=3.82392, val_acc=0.91271, time=0.93801
Epoch:0041, train_loss=2.77481, train_acc=0.93111, val_loss=3.82357, val_acc=0.91271, time=0.78602
Epoch:0042, train_loss=2.77046, train_acc=0.93400, val_loss=3.82323, val_acc=0.91271, time=0.92202
Epoch:0043, train_loss=2.76622, train_acc=0.93655, val_loss=3.82288, val_acc=0.91271, time=0.83701
Epoch:0044, train_loss=2.76206, train_acc=0.93962, val_loss=3.82253, val_acc=0.91577, time=0.92303
Epoch:0045, train_loss=2.75801, train_acc=0.94472, val_loss=3.82219, val_acc=0.92037, time=1.02201
Epoch:0046, train_loss=2.75415, train_acc=0.94676, val_loss=3.82186, val_acc=0.92037, time=0.90400
Epoch:0047, train_loss=2.75055, train_acc=0.94965, val_loss=3.82156, val_acc=0.92802, time=0.87902
Epoch:0048, train_loss=2.74722, train_acc=0.95254, val_loss=3.82127, val_acc=0.93262, time=0.96101
Epoch:0049, train_loss=2.74409, train_acc=0.95475, val_loss=3.82100, val_acc=0.93262, time=0.94201
Epoch:0050, train_loss=2.74111, train_acc=0.95697, val_loss=3.82074, val_acc=0.93262, time=0.86102
Epoch:0051, train_loss=2.73822, train_acc=0.95799, val_loss=3.82049, val_acc=0.93262, time=0.93300
Epoch:0052, train_loss=2.73539, train_acc=0.95969, val_loss=3.82026, val_acc=0.93415, time=0.96302
Epoch:0053, train_loss=2.73264, train_acc=0.96207, val_loss=3.82003, val_acc=0.93262, time=0.84902
Epoch:0054, train_loss=2.72999, train_acc=0.96343, val_loss=3.81982, val_acc=0.93262, time=1.03802
Epoch:0055, train_loss=2.72747, train_acc=0.96411, val_loss=3.81962, val_acc=0.93415, time=0.92701
Epoch:0056, train_loss=2.72510, train_acc=0.96479, val_loss=3.81944, val_acc=0.93721, time=0.84301
Epoch:0057, train_loss=2.72287, train_acc=0.96649, val_loss=3.81928, val_acc=0.93874, time=0.96801
Epoch:0058, train_loss=2.72077, train_acc=0.96836, val_loss=3.81913, val_acc=0.94028, time=0.95602
Epoch:0059, train_loss=2.71877, train_acc=0.96972, val_loss=3.81899, val_acc=0.94028, time=1.01601
Epoch:0060, train_loss=2.71685, train_acc=0.97159, val_loss=3.81886, val_acc=0.94181, time=1.00902
Epoch:0061, train_loss=2.71500, train_acc=0.97295, val_loss=3.81874, val_acc=0.94181, time=0.93904
Epoch:0062, train_loss=2.71322, train_acc=0.97449, val_loss=3.81863, val_acc=0.94334, time=0.87801
Epoch:0063, train_loss=2.71154, train_acc=0.97602, val_loss=3.81852, val_acc=0.94334, time=0.84501
Epoch:0064, train_loss=2.70993, train_acc=0.97721, val_loss=3.81841, val_acc=0.94334, time=0.83202
Epoch:0065, train_loss=2.70838, train_acc=0.97755, val_loss=3.81830, val_acc=0.94334, time=0.89100
Epoch:0066, train_loss=2.70687, train_acc=0.97840, val_loss=3.81818, val_acc=0.94487, time=0.71801
Epoch:0067, train_loss=2.70541, train_acc=0.97891, val_loss=3.81807, val_acc=0.94487, time=0.87502
Epoch:0068, train_loss=2.70401, train_acc=0.97908, val_loss=3.81795, val_acc=0.94487, time=0.84003
Epoch:0069, train_loss=2.70267, train_acc=0.97942, val_loss=3.81784, val_acc=0.94640, time=0.84901
Epoch:0070, train_loss=2.70139, train_acc=0.97993, val_loss=3.81774, val_acc=0.94793, time=0.87600
Epoch:0071, train_loss=2.70017, train_acc=0.98095, val_loss=3.81764, val_acc=0.94793, time=0.73700
Epoch:0072, train_loss=2.69899, train_acc=0.98129, val_loss=3.81755, val_acc=0.94793, time=0.75802
Epoch:0073, train_loss=2.69787, train_acc=0.98248, val_loss=3.81747, val_acc=0.94793, time=0.81101
Epoch:0074, train_loss=2.69678, train_acc=0.98282, val_loss=3.81739, val_acc=0.94793, time=0.83701
Epoch:0075, train_loss=2.69574, train_acc=0.98350, val_loss=3.81732, val_acc=0.94793, time=0.78602
Epoch:0076, train_loss=2.69474, train_acc=0.98418, val_loss=3.81725, val_acc=0.94793, time=0.98901
Epoch:0077, train_loss=2.69377, train_acc=0.98435, val_loss=3.81719, val_acc=0.94793, time=0.95601
Epoch:0078, train_loss=2.69284, train_acc=0.98503, val_loss=3.81713, val_acc=0.94793, time=0.79502
Epoch:0079, train_loss=2.69195, train_acc=0.98588, val_loss=3.81708, val_acc=0.94793, time=0.93600
Epoch:0080, train_loss=2.69108, train_acc=0.98588, val_loss=3.81703, val_acc=0.94793, time=0.82900
Epoch:0081, train_loss=2.69024, train_acc=0.98673, val_loss=3.81699, val_acc=0.94793, time=0.81301
Epoch:0082, train_loss=2.68943, train_acc=0.98724, val_loss=3.81694, val_acc=0.94793, time=0.96402
Epoch:0083, train_loss=2.68866, train_acc=0.98741, val_loss=3.81690, val_acc=0.94793, time=1.11002
Epoch:0084, train_loss=2.68791, train_acc=0.98758, val_loss=3.81685, val_acc=0.94793, time=1.08501
Epoch:0085, train_loss=2.68719, train_acc=0.98775, val_loss=3.81681, val_acc=0.94793, time=1.14501
Epoch:0086, train_loss=2.68649, train_acc=0.98809, val_loss=3.81676, val_acc=0.94793, time=0.95202
Epoch:0087, train_loss=2.68582, train_acc=0.98860, val_loss=3.81671, val_acc=0.94946, time=0.78401
Epoch:0088, train_loss=2.68517, train_acc=0.98945, val_loss=3.81666, val_acc=0.94946, time=0.96201
Epoch:0089, train_loss=2.68454, train_acc=0.98945, val_loss=3.81662, val_acc=0.94946, time=0.87801
Epoch:0090, train_loss=2.68394, train_acc=0.98979, val_loss=3.81658, val_acc=0.94946, time=0.89803
Epoch:0091, train_loss=2.68336, train_acc=0.99064, val_loss=3.81654, val_acc=0.94946, time=0.83401
Epoch:0092, train_loss=2.68280, train_acc=0.99081, val_loss=3.81650, val_acc=0.94946, time=0.78000
Epoch:0093, train_loss=2.68225, train_acc=0.99115, val_loss=3.81646, val_acc=0.94946, time=0.95501
Epoch:0094, train_loss=2.68173, train_acc=0.99133, val_loss=3.81642, val_acc=0.94946, time=0.95601
Epoch:0095, train_loss=2.68122, train_acc=0.99167, val_loss=3.81639, val_acc=0.94946, time=0.94501
Epoch:0096, train_loss=2.68073, train_acc=0.99201, val_loss=3.81635, val_acc=0.94946, time=0.93201
Epoch:0097, train_loss=2.68025, train_acc=0.99201, val_loss=3.81632, val_acc=0.94946, time=0.81200
Epoch:0098, train_loss=2.67979, train_acc=0.99201, val_loss=3.81629, val_acc=0.94946, time=0.94704
Epoch:0099, train_loss=2.67935, train_acc=0.99218, val_loss=3.81626, val_acc=0.94946, time=0.81000
Epoch:0100, train_loss=2.67892, train_acc=0.99235, val_loss=3.81623, val_acc=0.94946, time=0.95500
Epoch:0101, train_loss=2.67850, train_acc=0.99269, val_loss=3.81620, val_acc=0.94946, time=0.95001
Epoch:0102, train_loss=2.67810, train_acc=0.99269, val_loss=3.81618, val_acc=0.94946, time=0.79502
Epoch:0103, train_loss=2.67771, train_acc=0.99269, val_loss=3.81615, val_acc=0.94946, time=1.02000
Epoch:0104, train_loss=2.67733, train_acc=0.99320, val_loss=3.81613, val_acc=0.94946, time=0.99802
Epoch:0105, train_loss=2.67696, train_acc=0.99320, val_loss=3.81611, val_acc=0.94946, time=0.94001
Epoch:0106, train_loss=2.67660, train_acc=0.99320, val_loss=3.81608, val_acc=0.94946, time=0.94000
Epoch:0107, train_loss=2.67626, train_acc=0.99337, val_loss=3.81606, val_acc=0.94946, time=0.99302
Epoch:0108, train_loss=2.67593, train_acc=0.99337, val_loss=3.81604, val_acc=0.94946, time=0.95700
Epoch:0109, train_loss=2.67560, train_acc=0.99337, val_loss=3.81602, val_acc=0.94946, time=0.92502
Epoch:0110, train_loss=2.67529, train_acc=0.99354, val_loss=3.81601, val_acc=0.94946, time=0.96100
Epoch:0111, train_loss=2.67498, train_acc=0.99354, val_loss=3.81599, val_acc=0.94946, time=0.79801
Epoch:0112, train_loss=2.67468, train_acc=0.99354, val_loss=3.81597, val_acc=0.94946, time=0.98703
Epoch:0113, train_loss=2.67440, train_acc=0.99354, val_loss=3.81596, val_acc=0.94946, time=0.96702
Epoch:0114, train_loss=2.67412, train_acc=0.99388, val_loss=3.81594, val_acc=0.94946, time=0.87200
Epoch:0115, train_loss=2.67384, train_acc=0.99405, val_loss=3.81593, val_acc=0.94946, time=0.77901
Epoch:0116, train_loss=2.67358, train_acc=0.99422, val_loss=3.81591, val_acc=0.94946, time=0.91102
Epoch:0117, train_loss=2.67332, train_acc=0.99456, val_loss=3.81590, val_acc=0.94946, time=0.85501
Epoch:0118, train_loss=2.67307, train_acc=0.99456, val_loss=3.81588, val_acc=0.94946, time=0.86601
Epoch:0119, train_loss=2.67283, train_acc=0.99490, val_loss=3.81587, val_acc=0.95100, time=0.72999
Epoch:0120, train_loss=2.67259, train_acc=0.99490, val_loss=3.81586, val_acc=0.95100, time=0.84400
Epoch:0121, train_loss=2.67236, train_acc=0.99490, val_loss=3.81584, val_acc=0.95100, time=0.85600
Epoch:0122, train_loss=2.67214, train_acc=0.99490, val_loss=3.81583, val_acc=0.95100, time=0.80702
Epoch:0123, train_loss=2.67192, train_acc=0.99524, val_loss=3.81582, val_acc=0.95100, time=0.80200
Epoch:0124, train_loss=2.67171, train_acc=0.99524, val_loss=3.81581, val_acc=0.95100, time=0.85301
Epoch:0125, train_loss=2.67150, train_acc=0.99541, val_loss=3.81580, val_acc=0.95100, time=0.90201
Epoch:0126, train_loss=2.67130, train_acc=0.99541, val_loss=3.81578, val_acc=0.95100, time=0.95802
Epoch:0127, train_loss=2.67111, train_acc=0.99541, val_loss=3.81577, val_acc=0.95100, time=0.91499
Epoch:0128, train_loss=2.67092, train_acc=0.99541, val_loss=3.81576, val_acc=0.95100, time=0.82601
Epoch:0129, train_loss=2.67074, train_acc=0.99575, val_loss=3.81575, val_acc=0.95100, time=0.96900
Epoch:0130, train_loss=2.67056, train_acc=0.99575, val_loss=3.81574, val_acc=0.95100, time=0.78101
Epoch:0131, train_loss=2.67038, train_acc=0.99592, val_loss=3.81573, val_acc=0.95100, time=0.85102
Epoch:0132, train_loss=2.67021, train_acc=0.99592, val_loss=3.81572, val_acc=0.95100, time=0.76402
Epoch:0133, train_loss=2.67005, train_acc=0.99592, val_loss=3.81572, val_acc=0.95100, time=0.89900
Epoch:0134, train_loss=2.66989, train_acc=0.99592, val_loss=3.81571, val_acc=0.95100, time=0.96101
Epoch:0135, train_loss=2.66973, train_acc=0.99626, val_loss=3.81570, val_acc=0.95100, time=0.78902
Epoch:0136, train_loss=2.66958, train_acc=0.99660, val_loss=3.81569, val_acc=0.95100, time=0.78400
Epoch:0137, train_loss=2.66943, train_acc=0.99660, val_loss=3.81569, val_acc=0.95100, time=0.74403
Epoch:0138, train_loss=2.66928, train_acc=0.99694, val_loss=3.81568, val_acc=0.95100, time=0.86400
Epoch:0139, train_loss=2.66914, train_acc=0.99694, val_loss=3.81567, val_acc=0.95100, time=0.83401
Epoch:0140, train_loss=2.66900, train_acc=0.99711, val_loss=3.81567, val_acc=0.95100, time=0.80001
Epoch:0141, train_loss=2.66887, train_acc=0.99728, val_loss=3.81566, val_acc=0.94946, time=0.77101
Epoch:0142, train_loss=2.66874, train_acc=0.99728, val_loss=3.81565, val_acc=0.94946, time=0.89202
Epoch:0143, train_loss=2.66861, train_acc=0.99728, val_loss=3.81565, val_acc=0.94946, time=0.88701
Epoch:0144, train_loss=2.66849, train_acc=0.99728, val_loss=3.81564, val_acc=0.94946, time=0.99699
Epoch:0145, train_loss=2.66837, train_acc=0.99728, val_loss=3.81564, val_acc=0.94946, time=0.84700
Epoch:0146, train_loss=2.66825, train_acc=0.99745, val_loss=3.81563, val_acc=0.94793, time=0.93302
Epoch:0147, train_loss=2.66813, train_acc=0.99745, val_loss=3.81563, val_acc=0.94793, time=0.99499
Epoch:0148, train_loss=2.66802, train_acc=0.99745, val_loss=3.81562, val_acc=0.94793, time=0.90901
Epoch:0149, train_loss=2.66791, train_acc=0.99762, val_loss=3.81562, val_acc=0.94793, time=0.83002
Epoch:0150, train_loss=2.66780, train_acc=0.99779, val_loss=3.81562, val_acc=0.94793, time=1.14602
Epoch:0151, train_loss=2.66770, train_acc=0.99779, val_loss=3.81561, val_acc=0.94793, time=0.95800
Epoch:0152, train_loss=2.66759, train_acc=0.99813, val_loss=3.81561, val_acc=0.94793, time=0.97302
Epoch:0153, train_loss=2.66749, train_acc=0.99813, val_loss=3.81561, val_acc=0.94793, time=0.86601
Epoch:0154, train_loss=2.66740, train_acc=0.99813, val_loss=3.81561, val_acc=0.94793, time=0.80702
Epoch:0155, train_loss=2.66730, train_acc=0.99813, val_loss=3.81561, val_acc=0.94793, time=1.08403
Epoch:0156, train_loss=2.66721, train_acc=0.99813, val_loss=3.81560, val_acc=0.94946, time=1.01301
Epoch:0157, train_loss=2.66711, train_acc=0.99813, val_loss=3.81560, val_acc=0.94946, time=0.95801
Epoch:0158, train_loss=2.66702, train_acc=0.99813, val_loss=3.81560, val_acc=0.94946, time=0.90401
Epoch:0159, train_loss=2.66694, train_acc=0.99813, val_loss=3.81560, val_acc=0.94946, time=0.88301
Epoch:0160, train_loss=2.66685, train_acc=0.99830, val_loss=3.81560, val_acc=0.95100, time=0.85001
Epoch:0161, train_loss=2.66677, train_acc=0.99830, val_loss=3.81560, val_acc=0.95100, time=0.84999
Epoch:0162, train_loss=2.66668, train_acc=0.99830, val_loss=3.81560, val_acc=0.95100, time=0.86102
Epoch:0163, train_loss=2.66660, train_acc=0.99830, val_loss=3.81561, val_acc=0.95100, time=0.89902
Epoch:0164, train_loss=2.66652, train_acc=0.99830, val_loss=3.81561, val_acc=0.95100, time=0.86200
Early stopping...

Optimization Finished!

Test set results: loss= 3.42487, accuracy= 0.93497, time= 0.21300

Test Precision, Recall and F1-Score...
              precision    recall  f1-score   support

           0     0.9530    0.9612    0.9571       696
           1     0.8621    0.8929    0.8772        28
           2     0.9763    0.9908    0.9835      1083
           3     0.9167    0.9167    0.9167        12
           4     0.6923    0.9000    0.7826        10
           5     0.8519    0.9200    0.8846        25
           6     0.9286    0.8667    0.8966        15
           7     0.7889    0.9467    0.8606        75
           8     1.0000    0.8182    0.9000        11
           9     0.8636    0.9500    0.9048        20
          10     1.0000    1.0000    1.0000         9
          11     1.0000    0.9167    0.9565        12
          12     0.8529    0.8056    0.8286        36
          13     1.0000    1.0000    1.0000        22
          14     0.8855    0.9587    0.9206       121
          15     0.8462    0.8462    0.8462        13
          16     0.8172    0.8736    0.8444        87
          17     0.8025    0.8025    0.8025        81
          18     0.6667    0.3333    0.4444         6
          19     0.8333    1.0000    0.9091         5
          20     1.0000    0.8667    0.9286        15
          21     1.0000    0.7500    0.8571         4
          22     1.0000    0.4167    0.5882        12
          23     0.8889    0.6667    0.7619        12
          24     0.0000    0.0000    0.0000         6
          25     0.9375    0.7895    0.8571        19
          26     1.0000    0.7778    0.8750         9
          27     1.0000    1.0000    1.0000         1
          28     0.9333    0.8235    0.8750        17
          29     0.8750    0.7778    0.8235         9
          30     1.0000    1.0000    1.0000         3
          31     1.0000    0.9000    0.9474        10
          32     0.8462    1.0000    0.9167        11
          33     1.0000    0.8889    0.9412         9
          34     1.0000    1.0000    1.0000         2
          35     1.0000    0.2000    0.3333         5
          36     0.0000    0.0000    0.0000         4
          37     0.7143    0.5556    0.6250         9
          38     0.7500    0.6000    0.6667         5
          39     1.0000    1.0000    1.0000         1
          40     1.0000    0.7500    0.8571         4
          41     1.0000    0.8750    0.9333         8
          42     0.7500    0.7500    0.7500         4
          43     0.0000    0.0000    0.0000         1
          44     0.0000    0.0000    0.0000         3
          45     0.5000    1.0000    0.6667         1
          46     1.0000    0.6667    0.8000         3
          47     0.0000    0.0000    0.0000         1
          48     1.0000    0.1429    0.2500         7
          49     0.0000    0.0000    0.0000         1
          50     0.0000    0.0000    0.0000         3
          51     0.0000    0.0000    0.0000         2

    accuracy                         0.9350      2568
   macro avg     0.7641    0.6903    0.7071      2568
weighted avg     0.9306    0.9350    0.9297      2568


Macro average Test Precision, Recall and F1-Score...
(0.7640913820194234, 0.6903261268192533, 0.7071115044848771, None)

Micro average Test Precision, Recall and F1-Score...
(0.9349688473520249, 0.9349688473520249, 0.9349688473520249, None)

Embeddings:
Word_embeddings:8892
Train_doc_embeddings:6532
Test_doc_embeddings:2568
