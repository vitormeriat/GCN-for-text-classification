
==================== Torch Seed: 7602820925200

Model parameters

Layer: layer1.W0 | Size: torch.Size([20169, 200])
Layer: layer2.W0 | Size: torch.Size([200, 3])

Data statistics

  Edges    Classes    Train samples    Val samples    Test samples
-------  ---------  ---------------  -------------  --------------
  20169          3            12422           1380            5915

Epoch:0001, train_loss=1.13493, train_acc=0.20850, val_loss=1.09893, val_acc=0.47391, time=0.53501
Epoch:0002, train_loss=1.08939, train_acc=0.49501, val_loss=1.09433, val_acc=0.54493, time=0.47000
Epoch:0003, train_loss=1.04845, train_acc=0.55708, val_loss=1.08738, val_acc=0.54928, time=0.39700
Epoch:0004, train_loss=0.98787, train_acc=0.57173, val_loss=1.08223, val_acc=0.57101, time=0.43100
Epoch:0005, train_loss=0.94301, train_acc=0.57406, val_loss=1.07919, val_acc=0.64420, time=0.39102
Epoch:0006, train_loss=0.91692, train_acc=0.65062, val_loss=1.07693, val_acc=0.70797, time=0.39600
Epoch:0007, train_loss=0.89773, train_acc=0.71518, val_loss=1.07473, val_acc=0.73116, time=0.51399
Epoch:0008, train_loss=0.87898, train_acc=0.73064, val_loss=1.07225, val_acc=0.74420, time=0.48900
Epoch:0009, train_loss=0.85754, train_acc=0.74102, val_loss=1.06942, val_acc=0.75290, time=0.43100
Epoch:0010, train_loss=0.83248, train_acc=0.75237, val_loss=1.06663, val_acc=0.76232, time=0.39101
Epoch:0011, train_loss=0.80753, train_acc=0.76316, val_loss=1.06423, val_acc=0.77174, time=0.46200
Epoch:0012, train_loss=0.78637, train_acc=0.77226, val_loss=1.06223, val_acc=0.78188, time=0.46401
Epoch:0013, train_loss=0.76931, train_acc=0.77902, val_loss=1.06050, val_acc=0.79420, time=0.40100
Epoch:0014, train_loss=0.75514, train_acc=0.78916, val_loss=1.05888, val_acc=0.80072, time=0.52101
Epoch:0015, train_loss=0.74188, train_acc=0.79858, val_loss=1.05744, val_acc=0.80000, time=0.41199
Epoch:0016, train_loss=0.73026, train_acc=0.80696, val_loss=1.05636, val_acc=0.81014, time=0.44900
Epoch:0017, train_loss=0.72212, train_acc=0.80800, val_loss=1.05572, val_acc=0.81087, time=0.38800
Epoch:0018, train_loss=0.71791, train_acc=0.80679, val_loss=1.05533, val_acc=0.81087, time=0.48101
Epoch:0019, train_loss=0.71530, train_acc=0.80671, val_loss=1.05492, val_acc=0.81304, time=0.41501
Epoch:0020, train_loss=0.71151, train_acc=0.80848, val_loss=1.05453, val_acc=0.81667, time=0.41500
Epoch:0021, train_loss=0.70761, train_acc=0.81001, val_loss=1.05427, val_acc=0.81377, time=0.46900
Epoch:0022, train_loss=0.70487, train_acc=0.81227, val_loss=1.05410, val_acc=0.81884, time=0.45500
Epoch:0023, train_loss=0.70290, train_acc=0.81388, val_loss=1.05384, val_acc=0.81957, time=0.43100
Epoch:0024, train_loss=0.70000, train_acc=0.81694, val_loss=1.05348, val_acc=0.82391, time=0.38403
Epoch:0025, train_loss=0.69655, train_acc=0.82016, val_loss=1.05315, val_acc=0.82609, time=0.45399
Epoch:0026, train_loss=0.69378, train_acc=0.82145, val_loss=1.05295, val_acc=0.83116, time=0.38400
Epoch:0027, train_loss=0.69207, train_acc=0.82257, val_loss=1.05277, val_acc=0.83116, time=0.38501
Epoch:0028, train_loss=0.69014, train_acc=0.82595, val_loss=1.05250, val_acc=0.83261, time=0.42099
Epoch:0029, train_loss=0.68754, train_acc=0.82885, val_loss=1.05221, val_acc=0.83043, time=0.38401
Epoch:0030, train_loss=0.68498, train_acc=0.83078, val_loss=1.05199, val_acc=0.83116, time=0.42099
Epoch:0031, train_loss=0.68295, train_acc=0.83207, val_loss=1.05176, val_acc=0.83406, time=0.51501
Epoch:0032, train_loss=0.68087, train_acc=0.83433, val_loss=1.05145, val_acc=0.83551, time=0.47200
Epoch:0033, train_loss=0.67833, train_acc=0.83561, val_loss=1.05112, val_acc=0.83406, time=0.38100
Epoch:0034, train_loss=0.67580, train_acc=0.83658, val_loss=1.05087, val_acc=0.83551, time=0.38001
Epoch:0035, train_loss=0.67375, train_acc=0.83714, val_loss=1.05067, val_acc=0.83986, time=0.42299
Epoch:0036, train_loss=0.67197, train_acc=0.83779, val_loss=1.05045, val_acc=0.84275, time=0.38500
Epoch:0037, train_loss=0.66995, train_acc=0.83835, val_loss=1.05025, val_acc=0.84420, time=0.38600
Epoch:0038, train_loss=0.66791, train_acc=0.84004, val_loss=1.05011, val_acc=0.84928, time=0.38700
Epoch:0039, train_loss=0.66616, train_acc=0.84238, val_loss=1.05000, val_acc=0.85072, time=0.38201
Epoch:0040, train_loss=0.66461, train_acc=0.84415, val_loss=1.04985, val_acc=0.84928, time=0.38499
Epoch:0041, train_loss=0.66294, train_acc=0.84544, val_loss=1.04971, val_acc=0.84855, time=0.38100
Epoch:0042, train_loss=0.66127, train_acc=0.84664, val_loss=1.04961, val_acc=0.85145, time=0.38101
Epoch:0043, train_loss=0.65986, train_acc=0.84866, val_loss=1.04952, val_acc=0.85217, time=0.40199
Epoch:0044, train_loss=0.65864, train_acc=0.84858, val_loss=1.04943, val_acc=0.85290, time=0.49700
Epoch:0045, train_loss=0.65740, train_acc=0.84962, val_loss=1.04935, val_acc=0.85362, time=0.41800
Epoch:0046, train_loss=0.65612, train_acc=0.85019, val_loss=1.04928, val_acc=0.85507, time=0.38801
Epoch:0047, train_loss=0.65499, train_acc=0.85067, val_loss=1.04919, val_acc=0.85217, time=0.38500
Epoch:0048, train_loss=0.65394, train_acc=0.85228, val_loss=1.04910, val_acc=0.85145, time=0.49001
Epoch:0049, train_loss=0.65285, train_acc=0.85341, val_loss=1.04900, val_acc=0.85362, time=0.45700
Epoch:0050, train_loss=0.65176, train_acc=0.85349, val_loss=1.04889, val_acc=0.85580, time=0.55500
Epoch:0051, train_loss=0.65078, train_acc=0.85453, val_loss=1.04881, val_acc=0.85725, time=0.41500
Epoch:0052, train_loss=0.64990, train_acc=0.85477, val_loss=1.04874, val_acc=0.85797, time=0.46399
Epoch:0053, train_loss=0.64900, train_acc=0.85614, val_loss=1.04866, val_acc=0.85725, time=0.38001
Epoch:0054, train_loss=0.64810, train_acc=0.85687, val_loss=1.04860, val_acc=0.85725, time=0.38999
Epoch:0055, train_loss=0.64726, train_acc=0.85848, val_loss=1.04855, val_acc=0.85942, time=0.43000
Epoch:0056, train_loss=0.64645, train_acc=0.85840, val_loss=1.04848, val_acc=0.86014, time=0.56300
Epoch:0057, train_loss=0.64559, train_acc=0.85928, val_loss=1.04842, val_acc=0.85870, time=0.51401
Epoch:0058, train_loss=0.64475, train_acc=0.86065, val_loss=1.04837, val_acc=0.86014, time=0.41100
Epoch:0059, train_loss=0.64398, train_acc=0.86033, val_loss=1.04832, val_acc=0.86014, time=0.38201
Epoch:0060, train_loss=0.64324, train_acc=0.86162, val_loss=1.04827, val_acc=0.86232, time=0.38000
Epoch:0061, train_loss=0.64250, train_acc=0.86210, val_loss=1.04823, val_acc=0.86159, time=0.38800
Epoch:0062, train_loss=0.64179, train_acc=0.86250, val_loss=1.04818, val_acc=0.86087, time=0.40402
Epoch:0063, train_loss=0.64112, train_acc=0.86307, val_loss=1.04813, val_acc=0.86087, time=0.38500
Epoch:0064, train_loss=0.64043, train_acc=0.86363, val_loss=1.04807, val_acc=0.86232, time=0.42999
Epoch:0065, train_loss=0.63974, train_acc=0.86363, val_loss=1.04801, val_acc=0.86304, time=0.51000
Epoch:0066, train_loss=0.63909, train_acc=0.86347, val_loss=1.04796, val_acc=0.86232, time=0.42801
Epoch:0067, train_loss=0.63846, train_acc=0.86403, val_loss=1.04790, val_acc=0.86232, time=0.38400
Epoch:0068, train_loss=0.63783, train_acc=0.86435, val_loss=1.04786, val_acc=0.86304, time=0.53502
Epoch:0069, train_loss=0.63723, train_acc=0.86556, val_loss=1.04782, val_acc=0.86522, time=0.52000
Epoch:0070, train_loss=0.63665, train_acc=0.86524, val_loss=1.04778, val_acc=0.86377, time=0.38000
Epoch:0071, train_loss=0.63606, train_acc=0.86516, val_loss=1.04775, val_acc=0.86522, time=0.46002
Epoch:0072, train_loss=0.63548, train_acc=0.86580, val_loss=1.04771, val_acc=0.86449, time=0.39500
Epoch:0073, train_loss=0.63492, train_acc=0.86653, val_loss=1.04768, val_acc=0.86304, time=0.38100
Epoch:0074, train_loss=0.63436, train_acc=0.86717, val_loss=1.04766, val_acc=0.86304, time=0.38900
Epoch:0075, train_loss=0.63382, train_acc=0.86773, val_loss=1.04763, val_acc=0.86304, time=0.41000
Epoch:0076, train_loss=0.63330, train_acc=0.86790, val_loss=1.04761, val_acc=0.86522, time=0.48001
Epoch:0077, train_loss=0.63278, train_acc=0.86838, val_loss=1.04757, val_acc=0.86594, time=0.45303
Epoch:0078, train_loss=0.63227, train_acc=0.86814, val_loss=1.04755, val_acc=0.86667, time=0.47699
Epoch:0079, train_loss=0.63178, train_acc=0.86862, val_loss=1.04750, val_acc=0.86522, time=0.37800
Epoch:0080, train_loss=0.63129, train_acc=0.86862, val_loss=1.04749, val_acc=0.86884, time=0.44400
Epoch:0081, train_loss=0.63082, train_acc=0.86959, val_loss=1.04743, val_acc=0.86522, time=0.41701
Epoch:0082, train_loss=0.63038, train_acc=0.86983, val_loss=1.04746, val_acc=0.86884, time=0.38100
Epoch:0083, train_loss=0.63002, train_acc=0.86959, val_loss=1.04738, val_acc=0.86159, time=0.48901
Epoch:0084, train_loss=0.62986, train_acc=0.86943, val_loss=1.04758, val_acc=0.86522, time=0.42000
Early stopping...

Optimization Finished!

Test set results: loss= 0.88727, accuracy= 0.85630, time= 0.11300

Test Precision, Recall and F1-Score...
              precision    recall  f1-score   support

           0     0.8547    0.8419    0.8483      1202
           1     0.8773    0.8222    0.8489      2357
           2     0.8386    0.8977    0.8672      2356

    accuracy                         0.8563      5915
   macro avg     0.8569    0.8540    0.8548      5915
weighted avg     0.8573    0.8563    0.8560      5915


Macro average Test Precision, Recall and F1-Score...
(0.8568899755989356, 0.8539565821673619, 0.854774454374656, None)

Micro average Test Precision, Recall and F1-Score...
(0.8562975486052409, 0.8562975486052409, 0.8562975486052409, None)

Embeddings:
Word_embeddings: 452
Train_doc_embeddings: 13802
Test_doc_embeddings: 5915

Elapsed time is 38.237930 seconds.
