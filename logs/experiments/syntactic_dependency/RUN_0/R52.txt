
==================== Torch Seed: 5993981017800

Model parameters

Layer: layer1.W0 | Size: torch.Size([17992, 200])
Layer: layer2.W0 | Size: torch.Size([200, 52])

Data statistics

  Edges    Classes    Train samples    Val samples    Test samples
-------  ---------  ---------------  -------------  --------------
  17992         52             5879            653            2568

Epoch:0001, train_loss=4.00690, train_acc=0.01718, val_loss=3.92005, val_acc=0.43032, time=0.51400
Epoch:0002, train_loss=3.68813, train_acc=0.42303, val_loss=3.89372, val_acc=0.57274, time=0.39200
Epoch:0003, train_loss=3.44891, train_acc=0.57697, val_loss=3.87718, val_acc=0.61256, time=0.36600
Epoch:0004, train_loss=3.29618, train_acc=0.62460, val_loss=3.86722, val_acc=0.63093, time=0.36500
Epoch:0005, train_loss=3.19978, train_acc=0.64841, val_loss=3.86044, val_acc=0.66003, time=0.37899
Epoch:0006, train_loss=3.13006, train_acc=0.67086, val_loss=3.85506, val_acc=0.68913, time=0.43400
Epoch:0007, train_loss=3.07298, train_acc=0.70335, val_loss=3.85068, val_acc=0.72282, time=0.45400
Epoch:0008, train_loss=3.02699, train_acc=0.74928, val_loss=3.84729, val_acc=0.75345, time=0.44099
Epoch:0009, train_loss=2.99246, train_acc=0.78057, val_loss=3.84459, val_acc=0.77489, time=0.36200
Epoch:0010, train_loss=2.96585, train_acc=0.79639, val_loss=3.84217, val_acc=0.78714, time=0.38001
Epoch:0011, train_loss=2.94230, train_acc=0.81340, val_loss=3.83983, val_acc=0.80398, time=0.46400
Epoch:0012, train_loss=2.91946, train_acc=0.83075, val_loss=3.83761, val_acc=0.83002, time=0.40500
Epoch:0013, train_loss=2.89740, train_acc=0.84929, val_loss=3.83561, val_acc=0.85145, time=0.35300
Epoch:0014, train_loss=2.87710, train_acc=0.86988, val_loss=3.83389, val_acc=0.86677, time=0.35700
Epoch:0015, train_loss=2.85938, train_acc=0.88706, val_loss=3.83246, val_acc=0.86677, time=0.38800
Epoch:0016, train_loss=2.84425, train_acc=0.89590, val_loss=3.83124, val_acc=0.87596, time=0.35500
Epoch:0017, train_loss=2.83108, train_acc=0.90356, val_loss=3.83014, val_acc=0.88055, time=0.36700
Epoch:0018, train_loss=2.81920, train_acc=0.91087, val_loss=3.82914, val_acc=0.88974, time=0.46800
Epoch:0019, train_loss=2.80828, train_acc=0.91716, val_loss=3.82824, val_acc=0.89127, time=0.38800
Epoch:0020, train_loss=2.79828, train_acc=0.92329, val_loss=3.82745, val_acc=0.89893, time=0.39400
Epoch:0021, train_loss=2.78928, train_acc=0.93213, val_loss=3.82676, val_acc=0.90046, time=0.35499
Epoch:0022, train_loss=2.78131, train_acc=0.93859, val_loss=3.82618, val_acc=0.90352, time=0.35600
Epoch:0023, train_loss=2.77426, train_acc=0.94234, val_loss=3.82566, val_acc=0.90352, time=0.38100
Epoch:0024, train_loss=2.76793, train_acc=0.94761, val_loss=3.82519, val_acc=0.90199, time=0.43101
Epoch:0025, train_loss=2.76208, train_acc=0.95237, val_loss=3.82474, val_acc=0.90199, time=0.40900
Epoch:0026, train_loss=2.75649, train_acc=0.95424, val_loss=3.82429, val_acc=0.90046, time=0.39200
Epoch:0027, train_loss=2.75106, train_acc=0.95697, val_loss=3.82384, val_acc=0.90199, time=0.45299
Epoch:0028, train_loss=2.74575, train_acc=0.95816, val_loss=3.82338, val_acc=0.90965, time=0.54400
Epoch:0029, train_loss=2.74057, train_acc=0.95952, val_loss=3.82293, val_acc=0.91118, time=0.41201
Epoch:0030, train_loss=2.73556, train_acc=0.96173, val_loss=3.82249, val_acc=0.91424, time=0.47499
Epoch:0031, train_loss=2.73081, train_acc=0.96360, val_loss=3.82208, val_acc=0.90812, time=0.43000
Epoch:0032, train_loss=2.72638, train_acc=0.96564, val_loss=3.82171, val_acc=0.91424, time=0.43000
Epoch:0033, train_loss=2.72233, train_acc=0.96887, val_loss=3.82137, val_acc=0.91884, time=0.40800
Epoch:0034, train_loss=2.71869, train_acc=0.97023, val_loss=3.82107, val_acc=0.92343, time=0.35301
Epoch:0035, train_loss=2.71543, train_acc=0.97142, val_loss=3.82082, val_acc=0.92343, time=0.43399
Epoch:0036, train_loss=2.71254, train_acc=0.97227, val_loss=3.82059, val_acc=0.92496, time=0.35201
Epoch:0037, train_loss=2.70993, train_acc=0.97363, val_loss=3.82038, val_acc=0.92802, time=0.42700
Epoch:0038, train_loss=2.70755, train_acc=0.97619, val_loss=3.82018, val_acc=0.92956, time=0.54401
Epoch:0039, train_loss=2.70533, train_acc=0.97891, val_loss=3.81998, val_acc=0.93109, time=0.35800
Epoch:0040, train_loss=2.70322, train_acc=0.97993, val_loss=3.81979, val_acc=0.93262, time=0.35100
Epoch:0041, train_loss=2.70119, train_acc=0.98129, val_loss=3.81959, val_acc=0.93262, time=0.44700
Epoch:0042, train_loss=2.69924, train_acc=0.98282, val_loss=3.81940, val_acc=0.93415, time=0.45501
Epoch:0043, train_loss=2.69737, train_acc=0.98316, val_loss=3.81921, val_acc=0.93262, time=0.42900
Epoch:0044, train_loss=2.69561, train_acc=0.98418, val_loss=3.81903, val_acc=0.93109, time=0.36499
Epoch:0045, train_loss=2.69395, train_acc=0.98418, val_loss=3.81886, val_acc=0.92956, time=0.51400
Epoch:0046, train_loss=2.69240, train_acc=0.98554, val_loss=3.81871, val_acc=0.93262, time=0.41399
Epoch:0047, train_loss=2.69095, train_acc=0.98605, val_loss=3.81857, val_acc=0.93415, time=0.37000
Epoch:0048, train_loss=2.68961, train_acc=0.98656, val_loss=3.81844, val_acc=0.93415, time=0.35100
Epoch:0049, train_loss=2.68835, train_acc=0.98826, val_loss=3.81832, val_acc=0.93262, time=0.35700
Epoch:0050, train_loss=2.68718, train_acc=0.98911, val_loss=3.81821, val_acc=0.93262, time=0.35201
Epoch:0051, train_loss=2.68607, train_acc=0.98979, val_loss=3.81811, val_acc=0.93568, time=0.42100
Epoch:0052, train_loss=2.68504, train_acc=0.98996, val_loss=3.81801, val_acc=0.93568, time=0.43000
Epoch:0053, train_loss=2.68406, train_acc=0.99047, val_loss=3.81791, val_acc=0.93568, time=0.56300
Epoch:0054, train_loss=2.68315, train_acc=0.99064, val_loss=3.81782, val_acc=0.93568, time=0.54700
Epoch:0055, train_loss=2.68228, train_acc=0.99064, val_loss=3.81773, val_acc=0.93568, time=0.35299
Epoch:0056, train_loss=2.68146, train_acc=0.99133, val_loss=3.81764, val_acc=0.93415, time=0.39801
Epoch:0057, train_loss=2.68068, train_acc=0.99150, val_loss=3.81754, val_acc=0.93721, time=0.47100
Epoch:0058, train_loss=2.67994, train_acc=0.99184, val_loss=3.81745, val_acc=0.93568, time=0.41099
Epoch:0059, train_loss=2.67924, train_acc=0.99286, val_loss=3.81736, val_acc=0.93721, time=0.46901
Epoch:0060, train_loss=2.67857, train_acc=0.99337, val_loss=3.81727, val_acc=0.93721, time=0.35201
Epoch:0061, train_loss=2.67793, train_acc=0.99371, val_loss=3.81719, val_acc=0.93568, time=0.43600
Epoch:0062, train_loss=2.67734, train_acc=0.99371, val_loss=3.81711, val_acc=0.93721, time=0.43800
Epoch:0063, train_loss=2.67677, train_acc=0.99439, val_loss=3.81703, val_acc=0.93721, time=0.42800
Epoch:0064, train_loss=2.67624, train_acc=0.99456, val_loss=3.81696, val_acc=0.93721, time=0.53700
Epoch:0065, train_loss=2.67574, train_acc=0.99507, val_loss=3.81690, val_acc=0.93721, time=0.35199
Epoch:0066, train_loss=2.67526, train_acc=0.99592, val_loss=3.81685, val_acc=0.93721, time=0.46802
Epoch:0067, train_loss=2.67481, train_acc=0.99592, val_loss=3.81680, val_acc=0.93721, time=0.35400
Epoch:0068, train_loss=2.67437, train_acc=0.99592, val_loss=3.81675, val_acc=0.93721, time=0.37499
Epoch:0069, train_loss=2.67395, train_acc=0.99609, val_loss=3.81671, val_acc=0.93568, time=0.38101
Epoch:0070, train_loss=2.67355, train_acc=0.99575, val_loss=3.81667, val_acc=0.93721, time=0.39300
Epoch:0071, train_loss=2.67317, train_acc=0.99592, val_loss=3.81664, val_acc=0.93721, time=0.41600
Epoch:0072, train_loss=2.67280, train_acc=0.99592, val_loss=3.81662, val_acc=0.93721, time=0.35200
Epoch:0073, train_loss=2.67245, train_acc=0.99609, val_loss=3.81659, val_acc=0.93874, time=0.40099
Epoch:0074, train_loss=2.67212, train_acc=0.99592, val_loss=3.81656, val_acc=0.93874, time=0.40401
Epoch:0075, train_loss=2.67180, train_acc=0.99592, val_loss=3.81654, val_acc=0.93874, time=0.38600
Epoch:0076, train_loss=2.67150, train_acc=0.99592, val_loss=3.81652, val_acc=0.93874, time=0.37600
Epoch:0077, train_loss=2.67120, train_acc=0.99592, val_loss=3.81649, val_acc=0.93874, time=0.38800
Epoch:0078, train_loss=2.67092, train_acc=0.99609, val_loss=3.81647, val_acc=0.93874, time=0.38100
Epoch:0079, train_loss=2.67065, train_acc=0.99609, val_loss=3.81644, val_acc=0.93874, time=0.40200
Epoch:0080, train_loss=2.67039, train_acc=0.99609, val_loss=3.81642, val_acc=0.93874, time=0.35200
Epoch:0081, train_loss=2.67015, train_acc=0.99626, val_loss=3.81640, val_acc=0.94028, time=0.35500
Epoch:0082, train_loss=2.66991, train_acc=0.99643, val_loss=3.81637, val_acc=0.93874, time=0.49200
Epoch:0083, train_loss=2.66968, train_acc=0.99643, val_loss=3.81635, val_acc=0.93568, time=0.35100
Epoch:0084, train_loss=2.66945, train_acc=0.99677, val_loss=3.81633, val_acc=0.93568, time=0.35900
Epoch:0085, train_loss=2.66924, train_acc=0.99677, val_loss=3.81631, val_acc=0.93568, time=0.35100
Epoch:0086, train_loss=2.66903, train_acc=0.99694, val_loss=3.81629, val_acc=0.93568, time=0.35600
Epoch:0087, train_loss=2.66883, train_acc=0.99694, val_loss=3.81627, val_acc=0.93568, time=0.45299
Epoch:0088, train_loss=2.66864, train_acc=0.99694, val_loss=3.81625, val_acc=0.93568, time=0.55899
Epoch:0089, train_loss=2.66846, train_acc=0.99711, val_loss=3.81623, val_acc=0.93568, time=0.36601
Epoch:0090, train_loss=2.66828, train_acc=0.99745, val_loss=3.81621, val_acc=0.93568, time=0.35300
Epoch:0091, train_loss=2.66811, train_acc=0.99728, val_loss=3.81619, val_acc=0.93568, time=0.38300
Epoch:0092, train_loss=2.66795, train_acc=0.99762, val_loss=3.81617, val_acc=0.93568, time=0.43801
Epoch:0093, train_loss=2.66779, train_acc=0.99796, val_loss=3.81616, val_acc=0.93568, time=0.45901
Epoch:0094, train_loss=2.66764, train_acc=0.99779, val_loss=3.81614, val_acc=0.93568, time=0.37999
Epoch:0095, train_loss=2.66750, train_acc=0.99779, val_loss=3.81612, val_acc=0.93568, time=0.35300
Epoch:0096, train_loss=2.66737, train_acc=0.99796, val_loss=3.81611, val_acc=0.93568, time=0.43301
Epoch:0097, train_loss=2.66723, train_acc=0.99796, val_loss=3.81609, val_acc=0.93568, time=0.45101
Epoch:0098, train_loss=2.66711, train_acc=0.99796, val_loss=3.81608, val_acc=0.93568, time=0.38800
Epoch:0099, train_loss=2.66699, train_acc=0.99796, val_loss=3.81607, val_acc=0.93568, time=0.42700
Epoch:0100, train_loss=2.66687, train_acc=0.99796, val_loss=3.81606, val_acc=0.93568, time=0.52000
Epoch:0101, train_loss=2.66676, train_acc=0.99779, val_loss=3.81605, val_acc=0.93568, time=0.38900
Epoch:0102, train_loss=2.66665, train_acc=0.99779, val_loss=3.81604, val_acc=0.93568, time=0.43500
Epoch:0103, train_loss=2.66654, train_acc=0.99779, val_loss=3.81603, val_acc=0.93568, time=0.35300
Epoch:0104, train_loss=2.66644, train_acc=0.99779, val_loss=3.81602, val_acc=0.93568, time=0.51600
Epoch:0105, train_loss=2.66634, train_acc=0.99779, val_loss=3.81602, val_acc=0.93568, time=0.35200
Epoch:0106, train_loss=2.66625, train_acc=0.99779, val_loss=3.81601, val_acc=0.93568, time=0.42800
Epoch:0107, train_loss=2.66615, train_acc=0.99779, val_loss=3.81601, val_acc=0.93568, time=0.39900
Epoch:0108, train_loss=2.66606, train_acc=0.99796, val_loss=3.81600, val_acc=0.93568, time=0.45500
Epoch:0109, train_loss=2.66598, train_acc=0.99796, val_loss=3.81600, val_acc=0.93568, time=0.46400
Epoch:0110, train_loss=2.66589, train_acc=0.99796, val_loss=3.81599, val_acc=0.93568, time=0.41200
Epoch:0111, train_loss=2.66581, train_acc=0.99796, val_loss=3.81599, val_acc=0.93568, time=0.38600
Epoch:0112, train_loss=2.66573, train_acc=0.99796, val_loss=3.81599, val_acc=0.93568, time=0.39300
Epoch:0113, train_loss=2.66565, train_acc=0.99796, val_loss=3.81598, val_acc=0.93568, time=0.43299
Epoch:0114, train_loss=2.66557, train_acc=0.99796, val_loss=3.81598, val_acc=0.93568, time=0.35400
Epoch:0115, train_loss=2.66550, train_acc=0.99796, val_loss=3.81598, val_acc=0.93568, time=0.40700
Epoch:0116, train_loss=2.66542, train_acc=0.99796, val_loss=3.81597, val_acc=0.93568, time=0.45002
Epoch:0117, train_loss=2.66535, train_acc=0.99796, val_loss=3.81597, val_acc=0.93568, time=0.47099
Epoch:0118, train_loss=2.66528, train_acc=0.99813, val_loss=3.81597, val_acc=0.93568, time=0.43800
Epoch:0119, train_loss=2.66522, train_acc=0.99813, val_loss=3.81597, val_acc=0.93568, time=0.35000
Epoch:0120, train_loss=2.66515, train_acc=0.99813, val_loss=3.81597, val_acc=0.93568, time=0.38900
Epoch:0121, train_loss=2.66508, train_acc=0.99813, val_loss=3.81597, val_acc=0.93568, time=0.40801
Epoch:0122, train_loss=2.66502, train_acc=0.99813, val_loss=3.81597, val_acc=0.93568, time=0.44900
Epoch:0123, train_loss=2.66496, train_acc=0.99813, val_loss=3.81596, val_acc=0.93568, time=0.40302
Epoch:0124, train_loss=2.66490, train_acc=0.99813, val_loss=3.81596, val_acc=0.93568, time=0.53901
Epoch:0125, train_loss=2.66484, train_acc=0.99813, val_loss=3.81596, val_acc=0.93568, time=0.47499
Epoch:0126, train_loss=2.66478, train_acc=0.99813, val_loss=3.81596, val_acc=0.93568, time=0.35400
Epoch:0127, train_loss=2.66473, train_acc=0.99813, val_loss=3.81596, val_acc=0.93568, time=0.44400
Epoch:0128, train_loss=2.66467, train_acc=0.99813, val_loss=3.81596, val_acc=0.93568, time=0.37800
Epoch:0129, train_loss=2.66462, train_acc=0.99813, val_loss=3.81595, val_acc=0.93568, time=0.43101
Epoch:0130, train_loss=2.66457, train_acc=0.99813, val_loss=3.81595, val_acc=0.93568, time=0.37300
Epoch:0131, train_loss=2.66452, train_acc=0.99813, val_loss=3.81595, val_acc=0.93568, time=0.50500
Epoch:0132, train_loss=2.66447, train_acc=0.99830, val_loss=3.81595, val_acc=0.93568, time=0.35399
Epoch:0133, train_loss=2.66442, train_acc=0.99830, val_loss=3.81595, val_acc=0.93568, time=0.47000
Epoch:0134, train_loss=2.66437, train_acc=0.99830, val_loss=3.81595, val_acc=0.93568, time=0.38001
Epoch:0135, train_loss=2.66432, train_acc=0.99830, val_loss=3.81595, val_acc=0.93721, time=0.43900
Epoch:0136, train_loss=2.66427, train_acc=0.99830, val_loss=3.81595, val_acc=0.93721, time=0.52299
Epoch:0137, train_loss=2.66423, train_acc=0.99830, val_loss=3.81595, val_acc=0.93721, time=0.39001
Epoch:0138, train_loss=2.66418, train_acc=0.99830, val_loss=3.81595, val_acc=0.93721, time=0.47401
Epoch:0139, train_loss=2.66414, train_acc=0.99830, val_loss=3.81594, val_acc=0.93721, time=0.35300
Epoch:0140, train_loss=2.66410, train_acc=0.99830, val_loss=3.81594, val_acc=0.93721, time=0.45300
Epoch:0141, train_loss=2.66406, train_acc=0.99830, val_loss=3.81594, val_acc=0.93721, time=0.38400
Epoch:0142, train_loss=2.66402, train_acc=0.99830, val_loss=3.81594, val_acc=0.93721, time=0.37400
Epoch:0143, train_loss=2.66398, train_acc=0.99830, val_loss=3.81594, val_acc=0.93721, time=0.41100
Epoch:0144, train_loss=2.66394, train_acc=0.99847, val_loss=3.81594, val_acc=0.93721, time=0.35399
Epoch:0145, train_loss=2.66390, train_acc=0.99847, val_loss=3.81594, val_acc=0.93721, time=0.43699
Epoch:0146, train_loss=2.66386, train_acc=0.99847, val_loss=3.81594, val_acc=0.93721, time=0.43200
Epoch:0147, train_loss=2.66382, train_acc=0.99847, val_loss=3.81594, val_acc=0.93721, time=0.53000
Epoch:0148, train_loss=2.66378, train_acc=0.99847, val_loss=3.81594, val_acc=0.93721, time=0.37801
Epoch:0149, train_loss=2.66375, train_acc=0.99847, val_loss=3.81594, val_acc=0.93721, time=0.43199
Epoch:0150, train_loss=2.66371, train_acc=0.99847, val_loss=3.81594, val_acc=0.93721, time=0.54200
Epoch:0151, train_loss=2.66368, train_acc=0.99847, val_loss=3.81594, val_acc=0.93721, time=0.36701
Epoch:0152, train_loss=2.66364, train_acc=0.99847, val_loss=3.81594, val_acc=0.93721, time=0.35499
Epoch:0153, train_loss=2.66361, train_acc=0.99847, val_loss=3.81594, val_acc=0.93721, time=0.42900
Epoch:0154, train_loss=2.66358, train_acc=0.99847, val_loss=3.81594, val_acc=0.93721, time=0.38900
Epoch:0155, train_loss=2.66355, train_acc=0.99847, val_loss=3.81594, val_acc=0.93721, time=0.46400
Epoch:0156, train_loss=2.66351, train_acc=0.99864, val_loss=3.81594, val_acc=0.93721, time=0.42600
Epoch:0157, train_loss=2.66348, train_acc=0.99864, val_loss=3.81594, val_acc=0.93721, time=0.46400
Epoch:0158, train_loss=2.66345, train_acc=0.99864, val_loss=3.81594, val_acc=0.93721, time=0.35101
Epoch:0159, train_loss=2.66342, train_acc=0.99864, val_loss=3.81594, val_acc=0.93721, time=0.41000
Epoch:0160, train_loss=2.66339, train_acc=0.99864, val_loss=3.81594, val_acc=0.93721, time=0.43200
Early stopping...

Optimization Finished!

Test set results: loss= 3.44274, accuracy= 0.91394, time= 0.10900

Test Precision, Recall and F1-Score...
              precision    recall  f1-score   support

           0     0.9496    0.9917    0.9702      1083
           1     0.8433    0.9339    0.8863       121
           2     0.9549    0.9124    0.9331       696
           3     1.0000    0.8667    0.9286        15
           4     0.8235    0.9333    0.8750        15
           5     1.0000    0.8235    0.9032        17
           6     0.8065    0.6944    0.7463        36
           7     0.8519    0.9200    0.8846        25
           8     0.9286    0.6842    0.7879        19
           9     0.7692    0.7692    0.7692        13
          10     0.7895    0.8621    0.8242        87
          11     0.9412    0.8000    0.8649        20
          12     0.7071    0.9333    0.8046        75
          13     0.8387    0.9286    0.8814        28
          14     1.0000    0.8889    0.9412         9
          15     0.9167    1.0000    0.9565        22
          16     1.0000    0.8000    0.8889         5
          17     0.9000    0.7500    0.8182        12
          18     0.7922    0.7531    0.7722        81
          19     0.7500    0.9000    0.8182        10
          20     1.0000    1.0000    1.0000         2
          21     0.9167    0.9167    0.9167        12
          22     1.0000    1.0000    1.0000         1
          23     1.0000    0.7778    0.8750         9
          24     0.8000    0.3333    0.4706        12
          25     1.0000    0.6000    0.7500         5
          26     1.0000    0.9000    0.9474        10
          27     1.0000    0.9167    0.9565        12
          28     0.0000    0.0000    0.0000         3
          29     1.0000    1.0000    1.0000         3
          30     0.5714    0.4444    0.5000         9
          31     1.0000    1.0000    1.0000         9
          32     0.7778    0.8750    0.8235         8
          33     0.8462    1.0000    0.9167        11
          34     1.0000    0.2000    0.3333         5
          35     1.0000    0.7500    0.8571         4
          36     0.6000    0.7500    0.6667         4
          37     1.0000    0.3333    0.5000         3
          38     0.8000    1.0000    0.8889         4
          39     0.0000    0.0000    0.0000         1
          40     0.2500    0.1667    0.2000         6
          41     1.0000    0.7273    0.8421        11
          42     1.0000    0.8889    0.9412         9
          43     0.0000    0.0000    0.0000         6
          44     1.0000    1.0000    1.0000         1
          45     0.0000    0.0000    0.0000         1
          46     0.0000    0.0000    0.0000         1
          47     1.0000    0.1429    0.2500         7
          48     0.0000    0.0000    0.0000         1
          49     0.0000    0.0000    0.0000         2
          50     0.0000    0.0000    0.0000         4
          51     0.0000    0.0000    0.0000         3

    accuracy                         0.9139      2568
   macro avg     0.7332    0.6513    0.6710      2568
weighted avg     0.9102    0.9139    0.9082      2568


Macro average Test Precision, Recall and F1-Score...
(0.7331680741665905, 0.6513115808803769, 0.6709643080882436, None)

Micro average Test Precision, Recall and F1-Score...
(0.9139408099688473, 0.9139408099688473, 0.9139408099688472, None)

Embeddings:
Word_embeddings: 8892
Train_doc_embeddings: 6532
Test_doc_embeddings: 2568

Elapsed time is 68.085837 seconds.
