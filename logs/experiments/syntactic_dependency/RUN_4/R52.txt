
==================== Torch Seed: 6274037515800

Model parameters

Layer: layer1.W0 | Size: torch.Size([17992, 200])
Layer: layer2.W0 | Size: torch.Size([200, 52])

Data statistics

  Edges    Classes    Train samples    Val samples    Test samples
-------  ---------  ---------------  -------------  --------------
  17992         52             5879            653            2568

Epoch:0001, train_loss=4.25538, train_acc=0.00697, val_loss=3.95074, val_acc=0.22205, time=0.51500
Epoch:0002, train_loss=3.92146, train_acc=0.21075, val_loss=3.91818, val_acc=0.44564, time=0.52499
Epoch:0003, train_loss=3.63041, train_acc=0.45824, val_loss=3.89181, val_acc=0.56662, time=0.40300
Epoch:0004, train_loss=3.39923, train_acc=0.59602, val_loss=3.87425, val_acc=0.62328, time=0.47601
Epoch:0005, train_loss=3.24999, train_acc=0.64042, val_loss=3.86416, val_acc=0.65237, time=0.39600
Epoch:0006, train_loss=3.16562, train_acc=0.66321, val_loss=3.85835, val_acc=0.67534, time=0.44699
Epoch:0007, train_loss=3.11503, train_acc=0.68090, val_loss=3.85421, val_acc=0.69525, time=0.48799
Epoch:0008, train_loss=3.07559, train_acc=0.69893, val_loss=3.85042, val_acc=0.71975, time=0.57801
Epoch:0009, train_loss=3.03702, train_acc=0.72733, val_loss=3.84673, val_acc=0.74885, time=0.56199
Epoch:0010, train_loss=2.99876, train_acc=0.76305, val_loss=3.84336, val_acc=0.77335, time=0.37800
Epoch:0011, train_loss=2.96397, train_acc=0.79299, val_loss=3.84050, val_acc=0.79020, time=0.49601
Epoch:0012, train_loss=2.93474, train_acc=0.81715, val_loss=3.83814, val_acc=0.80704, time=0.49100
Epoch:0013, train_loss=2.91076, train_acc=0.83773, val_loss=3.83618, val_acc=0.81930, time=0.40699
Epoch:0014, train_loss=2.89087, train_acc=0.85899, val_loss=3.83453, val_acc=0.83461, time=0.48301
Epoch:0015, train_loss=2.87401, train_acc=0.87192, val_loss=3.83311, val_acc=0.84839, time=0.44799
Epoch:0016, train_loss=2.85939, train_acc=0.88229, val_loss=3.83187, val_acc=0.85145, time=0.39300
Epoch:0017, train_loss=2.84643, train_acc=0.88978, val_loss=3.83079, val_acc=0.86524, time=0.38601
Epoch:0018, train_loss=2.83475, train_acc=0.89913, val_loss=3.82982, val_acc=0.87902, time=0.37701
Epoch:0019, train_loss=2.82407, train_acc=0.90900, val_loss=3.82895, val_acc=0.88361, time=0.41200
Epoch:0020, train_loss=2.81417, train_acc=0.91852, val_loss=3.82813, val_acc=0.89280, time=0.50001
Epoch:0021, train_loss=2.80483, train_acc=0.92482, val_loss=3.82736, val_acc=0.90505, time=0.48299
Epoch:0022, train_loss=2.79594, train_acc=0.93094, val_loss=3.82663, val_acc=0.90505, time=0.54400
Epoch:0023, train_loss=2.78749, train_acc=0.93502, val_loss=3.82595, val_acc=0.90965, time=0.49400
Epoch:0024, train_loss=2.77957, train_acc=0.93945, val_loss=3.82534, val_acc=0.91271, time=0.53500
Epoch:0025, train_loss=2.77221, train_acc=0.94336, val_loss=3.82479, val_acc=0.91884, time=0.51201
Epoch:0026, train_loss=2.76542, train_acc=0.94642, val_loss=3.82429, val_acc=0.92037, time=0.43600
Epoch:0027, train_loss=2.75917, train_acc=0.94880, val_loss=3.82383, val_acc=0.92649, time=0.45700
Epoch:0028, train_loss=2.75340, train_acc=0.95118, val_loss=3.82341, val_acc=0.92649, time=0.50599
Epoch:0029, train_loss=2.74806, train_acc=0.95373, val_loss=3.82301, val_acc=0.92649, time=0.43400
Epoch:0030, train_loss=2.74311, train_acc=0.95697, val_loss=3.82264, val_acc=0.91884, time=0.46702
Epoch:0031, train_loss=2.73851, train_acc=0.95833, val_loss=3.82230, val_acc=0.92190, time=0.49599
Epoch:0032, train_loss=2.73424, train_acc=0.96071, val_loss=3.82198, val_acc=0.92190, time=0.54000
Epoch:0033, train_loss=2.73026, train_acc=0.96309, val_loss=3.82167, val_acc=0.92343, time=0.39000
Epoch:0034, train_loss=2.72655, train_acc=0.96462, val_loss=3.82138, val_acc=0.92343, time=0.51701
Epoch:0035, train_loss=2.72309, train_acc=0.96564, val_loss=3.82110, val_acc=0.92649, time=0.42499
Epoch:0036, train_loss=2.71986, train_acc=0.96802, val_loss=3.82083, val_acc=0.92649, time=0.48799
Epoch:0037, train_loss=2.71684, train_acc=0.97023, val_loss=3.82058, val_acc=0.92649, time=0.49401
Epoch:0038, train_loss=2.71402, train_acc=0.97227, val_loss=3.82034, val_acc=0.92649, time=0.40800
Epoch:0039, train_loss=2.71137, train_acc=0.97381, val_loss=3.82011, val_acc=0.92496, time=0.38999
Epoch:0040, train_loss=2.70888, train_acc=0.97466, val_loss=3.81989, val_acc=0.92496, time=0.37900
Epoch:0041, train_loss=2.70652, train_acc=0.97721, val_loss=3.81968, val_acc=0.92649, time=0.35301
Epoch:0042, train_loss=2.70429, train_acc=0.97857, val_loss=3.81947, val_acc=0.92802, time=0.35700
Epoch:0043, train_loss=2.70217, train_acc=0.97993, val_loss=3.81927, val_acc=0.92802, time=0.41699
Epoch:0044, train_loss=2.70016, train_acc=0.98095, val_loss=3.81907, val_acc=0.92802, time=0.39001
Epoch:0045, train_loss=2.69826, train_acc=0.98214, val_loss=3.81888, val_acc=0.92802, time=0.37599
Epoch:0046, train_loss=2.69648, train_acc=0.98350, val_loss=3.81870, val_acc=0.92802, time=0.35400
Epoch:0047, train_loss=2.69482, train_acc=0.98452, val_loss=3.81853, val_acc=0.92802, time=0.48501
Epoch:0048, train_loss=2.69328, train_acc=0.98622, val_loss=3.81838, val_acc=0.93109, time=0.37500
Epoch:0049, train_loss=2.69185, train_acc=0.98639, val_loss=3.81823, val_acc=0.93262, time=0.36700
Epoch:0050, train_loss=2.69053, train_acc=0.98673, val_loss=3.81809, val_acc=0.93415, time=0.43201
Epoch:0051, train_loss=2.68931, train_acc=0.98741, val_loss=3.81797, val_acc=0.93568, time=0.43400
Epoch:0052, train_loss=2.68817, train_acc=0.98707, val_loss=3.81785, val_acc=0.93721, time=0.36499
Epoch:0053, train_loss=2.68711, train_acc=0.98758, val_loss=3.81774, val_acc=0.93415, time=0.35101
Epoch:0054, train_loss=2.68611, train_acc=0.98826, val_loss=3.81764, val_acc=0.93568, time=0.38000
Epoch:0055, train_loss=2.68517, train_acc=0.98877, val_loss=3.81755, val_acc=0.93568, time=0.35900
Epoch:0056, train_loss=2.68428, train_acc=0.98962, val_loss=3.81746, val_acc=0.93721, time=0.40799
Epoch:0057, train_loss=2.68344, train_acc=0.99064, val_loss=3.81738, val_acc=0.93721, time=0.43801
Epoch:0058, train_loss=2.68264, train_acc=0.99081, val_loss=3.81730, val_acc=0.93721, time=0.59900
Epoch:0059, train_loss=2.68187, train_acc=0.99115, val_loss=3.81723, val_acc=0.93721, time=0.38600
Epoch:0060, train_loss=2.68115, train_acc=0.99167, val_loss=3.81716, val_acc=0.93874, time=0.44699
Epoch:0061, train_loss=2.68045, train_acc=0.99201, val_loss=3.81710, val_acc=0.93874, time=0.42600
Epoch:0062, train_loss=2.67978, train_acc=0.99252, val_loss=3.81703, val_acc=0.93874, time=0.38801
Epoch:0063, train_loss=2.67915, train_acc=0.99252, val_loss=3.81698, val_acc=0.93874, time=0.38300
Epoch:0064, train_loss=2.67854, train_acc=0.99286, val_loss=3.81692, val_acc=0.93874, time=0.41500
Epoch:0065, train_loss=2.67796, train_acc=0.99354, val_loss=3.81686, val_acc=0.93874, time=0.35100
Epoch:0066, train_loss=2.67740, train_acc=0.99371, val_loss=3.81681, val_acc=0.93874, time=0.38399
Epoch:0067, train_loss=2.67687, train_acc=0.99371, val_loss=3.81676, val_acc=0.93874, time=0.50200
Epoch:0068, train_loss=2.67637, train_acc=0.99371, val_loss=3.81671, val_acc=0.93874, time=0.38801
Epoch:0069, train_loss=2.67589, train_acc=0.99388, val_loss=3.81666, val_acc=0.93721, time=0.40700
Epoch:0070, train_loss=2.67543, train_acc=0.99422, val_loss=3.81661, val_acc=0.93721, time=0.42800
Epoch:0071, train_loss=2.67498, train_acc=0.99439, val_loss=3.81656, val_acc=0.93874, time=0.35500
Epoch:0072, train_loss=2.67456, train_acc=0.99439, val_loss=3.81652, val_acc=0.93874, time=0.37399
Epoch:0073, train_loss=2.67416, train_acc=0.99439, val_loss=3.81648, val_acc=0.93874, time=0.40400
Epoch:0074, train_loss=2.67378, train_acc=0.99473, val_loss=3.81644, val_acc=0.93874, time=0.48301
Epoch:0075, train_loss=2.67341, train_acc=0.99473, val_loss=3.81640, val_acc=0.93874, time=0.47001
Epoch:0076, train_loss=2.67305, train_acc=0.99490, val_loss=3.81636, val_acc=0.93874, time=0.46900
Epoch:0077, train_loss=2.67271, train_acc=0.99507, val_loss=3.81633, val_acc=0.93874, time=0.40300
Epoch:0078, train_loss=2.67239, train_acc=0.99524, val_loss=3.81630, val_acc=0.93874, time=0.35100
Epoch:0079, train_loss=2.67207, train_acc=0.99524, val_loss=3.81626, val_acc=0.94028, time=0.35400
Epoch:0080, train_loss=2.67177, train_acc=0.99541, val_loss=3.81623, val_acc=0.94028, time=0.36200
Epoch:0081, train_loss=2.67148, train_acc=0.99592, val_loss=3.81620, val_acc=0.94181, time=0.38200
Epoch:0082, train_loss=2.67121, train_acc=0.99592, val_loss=3.81617, val_acc=0.94181, time=0.35600
Epoch:0083, train_loss=2.67094, train_acc=0.99592, val_loss=3.81614, val_acc=0.94181, time=0.42700
Epoch:0084, train_loss=2.67068, train_acc=0.99592, val_loss=3.81611, val_acc=0.94181, time=0.35700
Epoch:0085, train_loss=2.67043, train_acc=0.99609, val_loss=3.81608, val_acc=0.94334, time=0.49900
Epoch:0086, train_loss=2.67020, train_acc=0.99609, val_loss=3.81605, val_acc=0.94334, time=0.41700
Epoch:0087, train_loss=2.66997, train_acc=0.99643, val_loss=3.81602, val_acc=0.94334, time=0.37199
Epoch:0088, train_loss=2.66974, train_acc=0.99643, val_loss=3.81599, val_acc=0.94181, time=0.35201
Epoch:0089, train_loss=2.66953, train_acc=0.99643, val_loss=3.81596, val_acc=0.94334, time=0.35400
Epoch:0090, train_loss=2.66933, train_acc=0.99677, val_loss=3.81593, val_acc=0.94334, time=0.37000
Epoch:0091, train_loss=2.66913, train_acc=0.99711, val_loss=3.81590, val_acc=0.94334, time=0.35200
Epoch:0092, train_loss=2.66895, train_acc=0.99728, val_loss=3.81588, val_acc=0.94334, time=0.37400
Epoch:0093, train_loss=2.66877, train_acc=0.99728, val_loss=3.81585, val_acc=0.94334, time=0.49200
Epoch:0094, train_loss=2.66859, train_acc=0.99728, val_loss=3.81583, val_acc=0.94334, time=0.40200
Epoch:0095, train_loss=2.66843, train_acc=0.99745, val_loss=3.81581, val_acc=0.94334, time=0.45102
Epoch:0096, train_loss=2.66827, train_acc=0.99745, val_loss=3.81579, val_acc=0.94334, time=0.38399
Epoch:0097, train_loss=2.66812, train_acc=0.99779, val_loss=3.81577, val_acc=0.94334, time=0.50501
Epoch:0098, train_loss=2.66797, train_acc=0.99796, val_loss=3.81575, val_acc=0.94334, time=0.43899
Epoch:0099, train_loss=2.66783, train_acc=0.99796, val_loss=3.81573, val_acc=0.94334, time=0.43801
Epoch:0100, train_loss=2.66769, train_acc=0.99796, val_loss=3.81572, val_acc=0.94334, time=0.43400
Epoch:0101, train_loss=2.66756, train_acc=0.99796, val_loss=3.81570, val_acc=0.94334, time=0.35500
Epoch:0102, train_loss=2.66743, train_acc=0.99796, val_loss=3.81569, val_acc=0.94334, time=0.41200
Epoch:0103, train_loss=2.66731, train_acc=0.99796, val_loss=3.81568, val_acc=0.94334, time=0.35000
Epoch:0104, train_loss=2.66719, train_acc=0.99813, val_loss=3.81567, val_acc=0.94334, time=0.35600
Epoch:0105, train_loss=2.66708, train_acc=0.99813, val_loss=3.81566, val_acc=0.94334, time=0.35100
Epoch:0106, train_loss=2.66696, train_acc=0.99813, val_loss=3.81565, val_acc=0.94334, time=0.35100
Epoch:0107, train_loss=2.66686, train_acc=0.99813, val_loss=3.81565, val_acc=0.94334, time=0.38499
Epoch:0108, train_loss=2.66675, train_acc=0.99813, val_loss=3.81564, val_acc=0.94181, time=0.45900
Epoch:0109, train_loss=2.66665, train_acc=0.99813, val_loss=3.81564, val_acc=0.94181, time=0.35700
Epoch:0110, train_loss=2.66655, train_acc=0.99813, val_loss=3.81563, val_acc=0.94181, time=0.37000
Epoch:0111, train_loss=2.66645, train_acc=0.99813, val_loss=3.81563, val_acc=0.94028, time=0.37300
Epoch:0112, train_loss=2.66636, train_acc=0.99813, val_loss=3.81562, val_acc=0.94028, time=0.35401
Epoch:0113, train_loss=2.66627, train_acc=0.99813, val_loss=3.81562, val_acc=0.94028, time=0.44199
Epoch:0114, train_loss=2.66618, train_acc=0.99813, val_loss=3.81561, val_acc=0.94028, time=0.50000
Epoch:0115, train_loss=2.66609, train_acc=0.99813, val_loss=3.81561, val_acc=0.94028, time=0.48000
Epoch:0116, train_loss=2.66601, train_acc=0.99796, val_loss=3.81561, val_acc=0.94028, time=0.41300
Epoch:0117, train_loss=2.66593, train_acc=0.99796, val_loss=3.81560, val_acc=0.94028, time=0.37401
Epoch:0118, train_loss=2.66585, train_acc=0.99813, val_loss=3.81560, val_acc=0.94028, time=0.50999
Epoch:0119, train_loss=2.66577, train_acc=0.99813, val_loss=3.81560, val_acc=0.94028, time=0.41701
Epoch:0120, train_loss=2.66569, train_acc=0.99813, val_loss=3.81559, val_acc=0.94028, time=0.43999
Epoch:0121, train_loss=2.66562, train_acc=0.99813, val_loss=3.81559, val_acc=0.94028, time=0.39500
Epoch:0122, train_loss=2.66555, train_acc=0.99813, val_loss=3.81559, val_acc=0.94028, time=0.43900
Epoch:0123, train_loss=2.66548, train_acc=0.99813, val_loss=3.81559, val_acc=0.94028, time=0.36700
Epoch:0124, train_loss=2.66541, train_acc=0.99813, val_loss=3.81558, val_acc=0.94028, time=0.37301
Epoch:0125, train_loss=2.66534, train_acc=0.99796, val_loss=3.81558, val_acc=0.94028, time=0.47899
Epoch:0126, train_loss=2.66528, train_acc=0.99796, val_loss=3.81558, val_acc=0.94028, time=0.35200
Epoch:0127, train_loss=2.66521, train_acc=0.99796, val_loss=3.81558, val_acc=0.94028, time=0.35900
Epoch:0128, train_loss=2.66515, train_acc=0.99796, val_loss=3.81557, val_acc=0.94028, time=0.36100
Epoch:0129, train_loss=2.66509, train_acc=0.99796, val_loss=3.81557, val_acc=0.94181, time=0.35100
Epoch:0130, train_loss=2.66503, train_acc=0.99796, val_loss=3.81557, val_acc=0.94181, time=0.40200
Epoch:0131, train_loss=2.66497, train_acc=0.99796, val_loss=3.81557, val_acc=0.94181, time=0.36500
Epoch:0132, train_loss=2.66491, train_acc=0.99796, val_loss=3.81557, val_acc=0.94181, time=0.36101
Epoch:0133, train_loss=2.66486, train_acc=0.99796, val_loss=3.81557, val_acc=0.94181, time=0.46700
Epoch:0134, train_loss=2.66480, train_acc=0.99813, val_loss=3.81557, val_acc=0.94181, time=0.45600
Epoch:0135, train_loss=2.66475, train_acc=0.99830, val_loss=3.81557, val_acc=0.94181, time=0.38200
Epoch:0136, train_loss=2.66470, train_acc=0.99830, val_loss=3.81556, val_acc=0.94181, time=0.37300
Epoch:0137, train_loss=2.66464, train_acc=0.99830, val_loss=3.81556, val_acc=0.94181, time=0.40000
Epoch:0138, train_loss=2.66459, train_acc=0.99830, val_loss=3.81556, val_acc=0.94181, time=0.44699
Epoch:0139, train_loss=2.66454, train_acc=0.99830, val_loss=3.81556, val_acc=0.94181, time=0.35300
Epoch:0140, train_loss=2.66450, train_acc=0.99830, val_loss=3.81556, val_acc=0.94181, time=0.43201
Epoch:0141, train_loss=2.66445, train_acc=0.99830, val_loss=3.81556, val_acc=0.94181, time=0.35399
Epoch:0142, train_loss=2.66440, train_acc=0.99830, val_loss=3.81556, val_acc=0.94181, time=0.35401
Epoch:0143, train_loss=2.66436, train_acc=0.99830, val_loss=3.81556, val_acc=0.94181, time=0.35700
Epoch:0144, train_loss=2.66431, train_acc=0.99830, val_loss=3.81556, val_acc=0.94181, time=0.35400
Epoch:0145, train_loss=2.66427, train_acc=0.99830, val_loss=3.81556, val_acc=0.94181, time=0.35299
Epoch:0146, train_loss=2.66423, train_acc=0.99830, val_loss=3.81556, val_acc=0.94181, time=0.35400
Epoch:0147, train_loss=2.66418, train_acc=0.99830, val_loss=3.81556, val_acc=0.94181, time=0.37300
Epoch:0148, train_loss=2.66414, train_acc=0.99830, val_loss=3.81556, val_acc=0.94181, time=0.42101
Epoch:0149, train_loss=2.66410, train_acc=0.99830, val_loss=3.81556, val_acc=0.94181, time=0.43899
Epoch:0150, train_loss=2.66406, train_acc=0.99830, val_loss=3.81556, val_acc=0.94181, time=0.49901
Epoch:0151, train_loss=2.66402, train_acc=0.99830, val_loss=3.81556, val_acc=0.94181, time=0.44300
Epoch:0152, train_loss=2.66398, train_acc=0.99830, val_loss=3.81556, val_acc=0.94181, time=0.35300
Epoch:0153, train_loss=2.66394, train_acc=0.99830, val_loss=3.81556, val_acc=0.94181, time=0.39200
Epoch:0154, train_loss=2.66391, train_acc=0.99830, val_loss=3.81556, val_acc=0.94181, time=0.39400
Epoch:0155, train_loss=2.66387, train_acc=0.99830, val_loss=3.81556, val_acc=0.94181, time=0.38398
Epoch:0156, train_loss=2.66384, train_acc=0.99830, val_loss=3.81556, val_acc=0.94181, time=0.38200
Epoch:0157, train_loss=2.66380, train_acc=0.99830, val_loss=3.81556, val_acc=0.94181, time=0.35300
Epoch:0158, train_loss=2.66376, train_acc=0.99830, val_loss=3.81556, val_acc=0.94181, time=0.37400
Epoch:0159, train_loss=2.66373, train_acc=0.99830, val_loss=3.81555, val_acc=0.94181, time=0.45400
Epoch:0160, train_loss=2.66370, train_acc=0.99830, val_loss=3.81555, val_acc=0.94181, time=0.46100
Epoch:0161, train_loss=2.66366, train_acc=0.99830, val_loss=3.81555, val_acc=0.94181, time=0.37200
Epoch:0162, train_loss=2.66363, train_acc=0.99830, val_loss=3.81555, val_acc=0.94181, time=0.41000
Epoch:0163, train_loss=2.66360, train_acc=0.99830, val_loss=3.81555, val_acc=0.94181, time=0.38099
Epoch:0164, train_loss=2.66357, train_acc=0.99830, val_loss=3.81555, val_acc=0.94181, time=0.48700
Epoch:0165, train_loss=2.66354, train_acc=0.99830, val_loss=3.81555, val_acc=0.94181, time=0.35201
Epoch:0166, train_loss=2.66351, train_acc=0.99830, val_loss=3.81555, val_acc=0.94181, time=0.44299
Epoch:0167, train_loss=2.66348, train_acc=0.99830, val_loss=3.81555, val_acc=0.94334, time=0.44201
Epoch:0168, train_loss=2.66345, train_acc=0.99830, val_loss=3.81555, val_acc=0.94334, time=0.44600
Epoch:0169, train_loss=2.66342, train_acc=0.99830, val_loss=3.81555, val_acc=0.94334, time=0.41499
Epoch:0170, train_loss=2.66339, train_acc=0.99830, val_loss=3.81555, val_acc=0.94334, time=0.43301
Epoch:0171, train_loss=2.66336, train_acc=0.99830, val_loss=3.81555, val_acc=0.94334, time=0.44198
Epoch:0172, train_loss=2.66333, train_acc=0.99830, val_loss=3.81555, val_acc=0.94334, time=0.39301
Epoch:0173, train_loss=2.66331, train_acc=0.99830, val_loss=3.81555, val_acc=0.94334, time=0.51899
Epoch:0174, train_loss=2.66328, train_acc=0.99847, val_loss=3.81555, val_acc=0.94334, time=0.36801
Epoch:0175, train_loss=2.66325, train_acc=0.99847, val_loss=3.81555, val_acc=0.94334, time=0.42899
Early stopping...

Optimization Finished!

Test set results: loss= 3.44405, accuracy= 0.91433, time= 0.12900

Test Precision, Recall and F1-Score...
              precision    recall  f1-score   support

           0     0.9529    0.9898    0.9710      1083
           1     0.8248    0.9339    0.8760       121
           2     0.9540    0.9239    0.9387       696
           3     1.0000    0.8667    0.9286        15
           4     0.9333    0.9333    0.9333        15
           5     0.9333    0.8235    0.8750        17
           6     0.8000    0.6667    0.7273        36
           7     0.8519    0.9200    0.8846        25
           8     0.9286    0.6842    0.7879        19
           9     0.8462    0.8462    0.8462        13
          10     0.7849    0.8391    0.8111        87
          11     0.8125    0.6500    0.7222        20
          12     0.7010    0.9067    0.7907        75
          13     0.8387    0.9286    0.8814        28
          14     1.0000    0.8889    0.9412         9
          15     0.9565    1.0000    0.9778        22
          16     1.0000    1.0000    1.0000         5
          17     0.9000    0.7500    0.8182        12
          18     0.7895    0.7407    0.7643        81
          19     0.7692    1.0000    0.8696        10
          20     0.6667    1.0000    0.8000         2
          21     0.8571    1.0000    0.9231        12
          22     1.0000    1.0000    1.0000         1
          23     1.0000    0.7778    0.8750         9
          24     0.8000    0.3333    0.4706        12
          25     1.0000    0.6000    0.7500         5
          26     1.0000    0.8000    0.8889        10
          27     1.0000    0.9167    0.9565        12
          28     0.0000    0.0000    0.0000         3
          29     1.0000    1.0000    1.0000         3
          30     0.6667    0.4444    0.5333         9
          31     1.0000    1.0000    1.0000         9
          32     0.7778    0.8750    0.8235         8
          33     0.9167    1.0000    0.9565        11
          34     1.0000    0.2000    0.3333         5
          35     1.0000    0.5000    0.6667         4
          36     0.7500    0.7500    0.7500         4
          37     1.0000    0.3333    0.5000         3
          38     1.0000    1.0000    1.0000         4
          39     0.0000    0.0000    0.0000         1
          40     0.3333    0.3333    0.3333         6
          41     1.0000    0.7273    0.8421        11
          42     1.0000    0.8889    0.9412         9
          43     1.0000    0.1667    0.2857         6
          44     1.0000    1.0000    1.0000         1
          45     0.0000    0.0000    0.0000         1
          46     0.0000    0.0000    0.0000         1
          47     1.0000    0.1429    0.2500         7
          48     0.0000    0.0000    0.0000         1
          49     0.0000    0.0000    0.0000         2
          50     0.0000    0.0000    0.0000         4
          51     0.0000    0.0000    0.0000         3

    accuracy                         0.9143      2568
   macro avg     0.7566    0.6554    0.6774      2568
weighted avg     0.9132    0.9143    0.9091      2568


Macro average Test Precision, Recall and F1-Score...
(0.7566465988026577, 0.655416554826359, 0.6773981573406189, None)

Micro average Test Precision, Recall and F1-Score...
(0.9143302180685359, 0.9143302180685359, 0.9143302180685359, None)

Embeddings:
Word_embeddings: 8892
Train_doc_embeddings: 6532
Test_doc_embeddings: 2568

Elapsed time is 74.882845 seconds.
