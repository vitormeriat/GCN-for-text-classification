
==================== Torch Seed: 6066139576400

Model parameters

Layer: layer1.W0 | Size: torch.Size([17992, 200])
Layer: layer2.W0 | Size: torch.Size([200, 52])

Data statistics

  Edges    Classes    Train samples    Val samples    Test samples
-------  ---------  ---------------  -------------  --------------
  17992         52             5879            653            2568

Epoch:0001, train_loss=4.26417, train_acc=0.00987, val_loss=3.95031, val_acc=0.23583, time=0.43501
Epoch:0002, train_loss=3.93267, train_acc=0.23882, val_loss=3.91804, val_acc=0.44257, time=0.36800
Epoch:0003, train_loss=3.63983, train_acc=0.46283, val_loss=3.89120, val_acc=0.57274, time=0.45100
Epoch:0004, train_loss=3.39905, train_acc=0.59840, val_loss=3.87338, val_acc=0.61562, time=0.36300
Epoch:0005, train_loss=3.24103, train_acc=0.64246, val_loss=3.86373, val_acc=0.63706, time=0.38000
Epoch:0006, train_loss=3.15713, train_acc=0.65964, val_loss=3.85823, val_acc=0.65544, time=0.43200
Epoch:0007, train_loss=3.10948, train_acc=0.67563, val_loss=3.85412, val_acc=0.67075, time=0.56800
Epoch:0008, train_loss=3.07179, train_acc=0.69706, val_loss=3.85036, val_acc=0.70597, time=0.38900
Epoch:0009, train_loss=3.03490, train_acc=0.72767, val_loss=3.84681, val_acc=0.73660, time=0.35600
Epoch:0010, train_loss=2.99888, train_acc=0.76152, val_loss=3.84364, val_acc=0.77182, time=0.38199
Epoch:0011, train_loss=2.96646, train_acc=0.78874, val_loss=3.84097, val_acc=0.79479, time=0.38301
Epoch:0012, train_loss=2.93902, train_acc=0.81749, val_loss=3.83873, val_acc=0.80858, time=0.35500
Epoch:0013, train_loss=2.91599, train_acc=0.83654, val_loss=3.83683, val_acc=0.82695, time=0.38099
Epoch:0014, train_loss=2.89619, train_acc=0.85763, val_loss=3.83518, val_acc=0.84227, time=0.35801
Epoch:0015, train_loss=2.87882, train_acc=0.87141, val_loss=3.83375, val_acc=0.85605, time=0.35698
Epoch:0016, train_loss=2.86339, train_acc=0.88229, val_loss=3.83250, val_acc=0.86371, time=0.49000
Epoch:0017, train_loss=2.84948, train_acc=0.89114, val_loss=3.83139, val_acc=0.86983, time=0.40600
Epoch:0018, train_loss=2.83677, train_acc=0.89998, val_loss=3.83039, val_acc=0.87749, time=0.44699
Epoch:0019, train_loss=2.82504, train_acc=0.90662, val_loss=3.82947, val_acc=0.88668, time=0.46200
Epoch:0020, train_loss=2.81422, train_acc=0.91546, val_loss=3.82863, val_acc=0.89127, time=0.38601
Epoch:0021, train_loss=2.80424, train_acc=0.92244, val_loss=3.82785, val_acc=0.89280, time=0.43500
Epoch:0022, train_loss=2.79497, train_acc=0.92618, val_loss=3.82711, val_acc=0.89740, time=0.35599
Epoch:0023, train_loss=2.78633, train_acc=0.92975, val_loss=3.82642, val_acc=0.90046, time=0.39001
Epoch:0024, train_loss=2.77825, train_acc=0.93553, val_loss=3.82576, val_acc=0.90046, time=0.41200
Epoch:0025, train_loss=2.77069, train_acc=0.93928, val_loss=3.82514, val_acc=0.90352, time=0.37900
Epoch:0026, train_loss=2.76368, train_acc=0.94234, val_loss=3.82457, val_acc=0.90505, time=0.45600
Epoch:0027, train_loss=2.75725, train_acc=0.94710, val_loss=3.82405, val_acc=0.90812, time=0.35400
Epoch:0028, train_loss=2.75142, train_acc=0.95118, val_loss=3.82358, val_acc=0.91118, time=0.43700
Epoch:0029, train_loss=2.74618, train_acc=0.95373, val_loss=3.82317, val_acc=0.91271, time=0.38701
Epoch:0030, train_loss=2.74143, train_acc=0.95663, val_loss=3.82280, val_acc=0.91271, time=0.35800
Epoch:0031, train_loss=2.73707, train_acc=0.95833, val_loss=3.82246, val_acc=0.90965, time=0.47499
Epoch:0032, train_loss=2.73298, train_acc=0.96088, val_loss=3.82214, val_acc=0.90965, time=0.41201
Epoch:0033, train_loss=2.72907, train_acc=0.96139, val_loss=3.82183, val_acc=0.91118, time=0.41400
Epoch:0034, train_loss=2.72531, train_acc=0.96462, val_loss=3.82153, val_acc=0.91730, time=0.37800
Epoch:0035, train_loss=2.72172, train_acc=0.96649, val_loss=3.82126, val_acc=0.91884, time=0.39900
Epoch:0036, train_loss=2.71833, train_acc=0.97040, val_loss=3.82100, val_acc=0.91884, time=0.41600
Epoch:0037, train_loss=2.71518, train_acc=0.97176, val_loss=3.82076, val_acc=0.91730, time=0.41100
Epoch:0038, train_loss=2.71229, train_acc=0.97329, val_loss=3.82055, val_acc=0.91730, time=0.41500
Epoch:0039, train_loss=2.70965, train_acc=0.97483, val_loss=3.82035, val_acc=0.91884, time=0.45399
Epoch:0040, train_loss=2.70725, train_acc=0.97602, val_loss=3.82016, val_acc=0.92343, time=0.40503
Epoch:0041, train_loss=2.70506, train_acc=0.97755, val_loss=3.81998, val_acc=0.92190, time=0.44701
Epoch:0042, train_loss=2.70303, train_acc=0.97789, val_loss=3.81980, val_acc=0.92343, time=0.39801
Epoch:0043, train_loss=2.70114, train_acc=0.97874, val_loss=3.81963, val_acc=0.92649, time=0.36801
Epoch:0044, train_loss=2.69937, train_acc=0.98010, val_loss=3.81946, val_acc=0.92649, time=0.38700
Epoch:0045, train_loss=2.69770, train_acc=0.98146, val_loss=3.81929, val_acc=0.92802, time=0.37701
Epoch:0046, train_loss=2.69611, train_acc=0.98248, val_loss=3.81913, val_acc=0.92956, time=0.36000
Epoch:0047, train_loss=2.69460, train_acc=0.98520, val_loss=3.81897, val_acc=0.93262, time=0.35200
Epoch:0048, train_loss=2.69316, train_acc=0.98707, val_loss=3.81881, val_acc=0.93415, time=0.40399
Epoch:0049, train_loss=2.69180, train_acc=0.98758, val_loss=3.81867, val_acc=0.93721, time=0.44601
Epoch:0050, train_loss=2.69050, train_acc=0.98826, val_loss=3.81853, val_acc=0.93721, time=0.35400
Epoch:0051, train_loss=2.68926, train_acc=0.98843, val_loss=3.81840, val_acc=0.93721, time=0.54500
Epoch:0052, train_loss=2.68809, train_acc=0.98877, val_loss=3.81828, val_acc=0.93874, time=0.45000
Epoch:0053, train_loss=2.68698, train_acc=0.98894, val_loss=3.81817, val_acc=0.93874, time=0.39900
Epoch:0054, train_loss=2.68593, train_acc=0.98996, val_loss=3.81806, val_acc=0.93874, time=0.40100
Epoch:0055, train_loss=2.68493, train_acc=0.99013, val_loss=3.81797, val_acc=0.93874, time=0.36200
Epoch:0056, train_loss=2.68399, train_acc=0.99064, val_loss=3.81787, val_acc=0.93874, time=0.41799
Epoch:0057, train_loss=2.68310, train_acc=0.99133, val_loss=3.81779, val_acc=0.93874, time=0.39901
Epoch:0058, train_loss=2.68226, train_acc=0.99150, val_loss=3.81771, val_acc=0.93874, time=0.41600
Epoch:0059, train_loss=2.68146, train_acc=0.99218, val_loss=3.81763, val_acc=0.93721, time=0.45300
Epoch:0060, train_loss=2.68070, train_acc=0.99218, val_loss=3.81756, val_acc=0.93874, time=0.37000
Epoch:0061, train_loss=2.67999, train_acc=0.99218, val_loss=3.81749, val_acc=0.93874, time=0.37300
Epoch:0062, train_loss=2.67931, train_acc=0.99252, val_loss=3.81742, val_acc=0.93874, time=0.39400
Epoch:0063, train_loss=2.67866, train_acc=0.99269, val_loss=3.81736, val_acc=0.93874, time=0.48000
Epoch:0064, train_loss=2.67805, train_acc=0.99303, val_loss=3.81730, val_acc=0.93874, time=0.47900
Epoch:0065, train_loss=2.67746, train_acc=0.99303, val_loss=3.81724, val_acc=0.93874, time=0.38200
Epoch:0066, train_loss=2.67691, train_acc=0.99354, val_loss=3.81718, val_acc=0.93874, time=0.47500
Epoch:0067, train_loss=2.67638, train_acc=0.99337, val_loss=3.81713, val_acc=0.94028, time=0.35300
Epoch:0068, train_loss=2.67588, train_acc=0.99354, val_loss=3.81707, val_acc=0.94028, time=0.42699
Epoch:0069, train_loss=2.67539, train_acc=0.99371, val_loss=3.81702, val_acc=0.94181, time=0.35301
Epoch:0070, train_loss=2.67493, train_acc=0.99405, val_loss=3.81697, val_acc=0.94181, time=0.40000
Epoch:0071, train_loss=2.67449, train_acc=0.99439, val_loss=3.81692, val_acc=0.94334, time=0.46000
Epoch:0072, train_loss=2.67407, train_acc=0.99439, val_loss=3.81687, val_acc=0.94334, time=0.53099
Epoch:0073, train_loss=2.67366, train_acc=0.99456, val_loss=3.81682, val_acc=0.94181, time=0.35500
Epoch:0074, train_loss=2.67327, train_acc=0.99456, val_loss=3.81677, val_acc=0.94181, time=0.40201
Epoch:0075, train_loss=2.67290, train_acc=0.99490, val_loss=3.81673, val_acc=0.94181, time=0.40800
Epoch:0076, train_loss=2.67254, train_acc=0.99524, val_loss=3.81669, val_acc=0.94181, time=0.50400
Epoch:0077, train_loss=2.67220, train_acc=0.99507, val_loss=3.81664, val_acc=0.94181, time=0.39100
Epoch:0078, train_loss=2.67187, train_acc=0.99524, val_loss=3.81660, val_acc=0.94181, time=0.48800
Epoch:0079, train_loss=2.67156, train_acc=0.99524, val_loss=3.81657, val_acc=0.94181, time=0.35100
Epoch:0080, train_loss=2.67126, train_acc=0.99558, val_loss=3.81653, val_acc=0.94181, time=0.41599
Epoch:0081, train_loss=2.67097, train_acc=0.99558, val_loss=3.81650, val_acc=0.94334, time=0.40999
Epoch:0082, train_loss=2.67069, train_acc=0.99575, val_loss=3.81646, val_acc=0.94334, time=0.37300
Epoch:0083, train_loss=2.67043, train_acc=0.99626, val_loss=3.81643, val_acc=0.94334, time=0.36201
Epoch:0084, train_loss=2.67017, train_acc=0.99626, val_loss=3.81641, val_acc=0.94334, time=0.35300
Epoch:0085, train_loss=2.66993, train_acc=0.99643, val_loss=3.81638, val_acc=0.94334, time=0.49700
Epoch:0086, train_loss=2.66970, train_acc=0.99660, val_loss=3.81635, val_acc=0.94334, time=0.35200
Epoch:0087, train_loss=2.66948, train_acc=0.99677, val_loss=3.81633, val_acc=0.94334, time=0.36600
Epoch:0088, train_loss=2.66926, train_acc=0.99694, val_loss=3.81631, val_acc=0.94334, time=0.35900
Epoch:0089, train_loss=2.66906, train_acc=0.99711, val_loss=3.81629, val_acc=0.94334, time=0.37500
Epoch:0090, train_loss=2.66887, train_acc=0.99711, val_loss=3.81626, val_acc=0.94334, time=0.41199
Epoch:0091, train_loss=2.66868, train_acc=0.99762, val_loss=3.81625, val_acc=0.94181, time=0.50600
Epoch:0092, train_loss=2.66850, train_acc=0.99779, val_loss=3.81623, val_acc=0.94181, time=0.38000
Epoch:0093, train_loss=2.66833, train_acc=0.99762, val_loss=3.81621, val_acc=0.94181, time=0.49301
Epoch:0094, train_loss=2.66817, train_acc=0.99762, val_loss=3.81620, val_acc=0.94181, time=0.37700
Epoch:0095, train_loss=2.66801, train_acc=0.99762, val_loss=3.81619, val_acc=0.94181, time=0.44700
Epoch:0096, train_loss=2.66786, train_acc=0.99762, val_loss=3.81617, val_acc=0.94181, time=0.44100
Epoch:0097, train_loss=2.66772, train_acc=0.99762, val_loss=3.81616, val_acc=0.94181, time=0.49799
Epoch:0098, train_loss=2.66758, train_acc=0.99762, val_loss=3.81615, val_acc=0.94181, time=0.38601
Epoch:0099, train_loss=2.66745, train_acc=0.99779, val_loss=3.81615, val_acc=0.94181, time=0.36700
Epoch:0100, train_loss=2.66732, train_acc=0.99779, val_loss=3.81614, val_acc=0.94181, time=0.53100
Epoch:0101, train_loss=2.66719, train_acc=0.99813, val_loss=3.81614, val_acc=0.94181, time=0.35099
Epoch:0102, train_loss=2.66707, train_acc=0.99813, val_loss=3.81613, val_acc=0.94181, time=0.45901
Epoch:0103, train_loss=2.66695, train_acc=0.99813, val_loss=3.81613, val_acc=0.94181, time=0.42800
Epoch:0104, train_loss=2.66684, train_acc=0.99813, val_loss=3.81613, val_acc=0.94028, time=0.40599
Epoch:0105, train_loss=2.66672, train_acc=0.99813, val_loss=3.81612, val_acc=0.94028, time=0.45601
Epoch:0106, train_loss=2.66662, train_acc=0.99813, val_loss=3.81612, val_acc=0.94028, time=0.40600
Epoch:0107, train_loss=2.66651, train_acc=0.99813, val_loss=3.81612, val_acc=0.94028, time=0.45700
Epoch:0108, train_loss=2.66641, train_acc=0.99830, val_loss=3.81612, val_acc=0.94028, time=0.36800
Epoch:0109, train_loss=2.66631, train_acc=0.99847, val_loss=3.81612, val_acc=0.94028, time=0.37700
Epoch:0110, train_loss=2.66622, train_acc=0.99847, val_loss=3.81612, val_acc=0.94028, time=0.50600
Epoch:0111, train_loss=2.66612, train_acc=0.99847, val_loss=3.81612, val_acc=0.94028, time=0.46100
Epoch:0112, train_loss=2.66603, train_acc=0.99847, val_loss=3.81612, val_acc=0.94028, time=0.50500
Epoch:0113, train_loss=2.66594, train_acc=0.99847, val_loss=3.81612, val_acc=0.94181, time=0.45000
Epoch:0114, train_loss=2.66586, train_acc=0.99847, val_loss=3.81612, val_acc=0.94181, time=0.45100
Epoch:0115, train_loss=2.66578, train_acc=0.99830, val_loss=3.81612, val_acc=0.94181, time=0.40100
Epoch:0116, train_loss=2.66569, train_acc=0.99830, val_loss=3.81612, val_acc=0.94181, time=0.50001
Epoch:0117, train_loss=2.66562, train_acc=0.99830, val_loss=3.81612, val_acc=0.94181, time=0.46300
Epoch:0118, train_loss=2.66554, train_acc=0.99830, val_loss=3.81612, val_acc=0.94181, time=0.39800
Early stopping...

Optimization Finished!

Test set results: loss= 3.44143, accuracy= 0.91550, time= 0.14300

Test Precision, Recall and F1-Score...
              precision    recall  f1-score   support

           0     0.9504    0.9898    0.9697      1083
           1     0.8333    0.9504    0.8880       121
           2     0.9582    0.9224    0.9400       696
           3     1.0000    0.8000    0.8889        15
           4     0.8235    0.9333    0.8750        15
           5     1.0000    0.8235    0.9032        17
           6     0.8276    0.6667    0.7385        36
           7     0.8519    0.9200    0.8846        25
           8     0.9286    0.6842    0.7879        19
           9     0.8462    0.8462    0.8462        13
          10     0.8041    0.8966    0.8478        87
          11     0.8333    0.7500    0.7895        20
          12     0.7216    0.9333    0.8140        75
          13     0.8333    0.8929    0.8621        28
          14     1.0000    0.7778    0.8750         9
          15     0.9167    1.0000    0.9565        22
          16     1.0000    1.0000    1.0000         5
          17     0.9000    0.7500    0.8182        12
          18     0.7895    0.7407    0.7643        81
          19     0.6429    0.9000    0.7500        10
          20     1.0000    1.0000    1.0000         2
          21     0.9167    0.9167    0.9167        12
          22     1.0000    1.0000    1.0000         1
          23     0.8750    0.7778    0.8235         9
          24     1.0000    0.3333    0.5000        12
          25     0.6000    0.6000    0.6000         5
          26     1.0000    0.7000    0.8235        10
          27     1.0000    0.9167    0.9565        12
          28     0.0000    0.0000    0.0000         3
          29     1.0000    1.0000    1.0000         3
          30     0.7143    0.5556    0.6250         9
          31     1.0000    1.0000    1.0000         9
          32     0.8750    0.8750    0.8750         8
          33     0.7857    1.0000    0.8800        11
          34     1.0000    0.2000    0.3333         5
          35     1.0000    0.5000    0.6667         4
          36     0.6000    0.7500    0.6667         4
          37     1.0000    0.3333    0.5000         3
          38     1.0000    1.0000    1.0000         4
          39     0.0000    0.0000    0.0000         1
          40     0.3333    0.1667    0.2222         6
          41     1.0000    0.6364    0.7778        11
          42     1.0000    0.8889    0.9412         9
          43     1.0000    0.1667    0.2857         6
          44     1.0000    1.0000    1.0000         1
          45     0.0000    0.0000    0.0000         1
          46     0.0000    0.0000    0.0000         1
          47     1.0000    0.1429    0.2500         7
          48     0.0000    0.0000    0.0000         1
          49     0.0000    0.0000    0.0000         2
          50     0.0000    0.0000    0.0000         4
          51     0.0000    0.0000    0.0000         3

    accuracy                         0.9155      2568
   macro avg     0.7531    0.6469    0.6701      2568
weighted avg     0.9143    0.9155    0.9096      2568


Macro average Test Precision, Recall and F1-Score...
(0.7530966730769608, 0.646876940052418, 0.670059316778009, None)

Micro average Test Precision, Recall and F1-Score...
(0.9154984423676013, 0.9154984423676013, 0.9154984423676013, None)

Embeddings:
Word_embeddings: 8892
Train_doc_embeddings: 6532
Test_doc_embeddings: 2568

Elapsed time is 50.722881 seconds.
