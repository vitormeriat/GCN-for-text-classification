
==================== Torch Seed: 5886621750700

Model parameters

Layer: layer1.W0 | Size: torch.Size([15362, 200])
Layer: layer2.W0 | Size: torch.Size([200, 8])

Data statistics

  Edges    Classes    Train samples    Val samples    Test samples
-------  ---------  ---------------  -------------  --------------
  15362          8             4937            548            2189

Epoch:0001, train_loss=2.30310, train_acc=0.08730, val_loss=2.07426, val_acc=0.50730, time=0.38001
Epoch:0002, train_loss=2.02065, train_acc=0.50658, val_loss=2.05391, val_acc=0.59854, time=0.33201
Epoch:0003, train_loss=1.83365, train_acc=0.61029, val_loss=2.04123, val_acc=0.69161, time=0.32800
Epoch:0004, train_loss=1.72050, train_acc=0.71238, val_loss=2.03473, val_acc=0.73540, time=0.31300
Epoch:0005, train_loss=1.66389, train_acc=0.74985, val_loss=2.03036, val_acc=0.76277, time=0.39303
Epoch:0006, train_loss=1.62457, train_acc=0.78610, val_loss=2.02663, val_acc=0.79562, time=0.41202
Epoch:0007, train_loss=1.58877, train_acc=0.83208, val_loss=2.02371, val_acc=0.84124, time=0.38501
Epoch:0008, train_loss=1.55892, train_acc=0.87057, val_loss=2.02175, val_acc=0.85766, time=0.38499
Epoch:0009, train_loss=1.53666, train_acc=0.89852, val_loss=2.02048, val_acc=0.87226, time=0.43801
Epoch:0010, train_loss=1.52013, train_acc=0.91797, val_loss=2.01959, val_acc=0.87409, time=0.39801
Epoch:0011, train_loss=1.50717, train_acc=0.92789, val_loss=2.01887, val_acc=0.87956, time=0.39901
Epoch:0012, train_loss=1.49633, train_acc=0.93944, val_loss=2.01816, val_acc=0.88869, time=0.34401
Epoch:0013, train_loss=1.48694, train_acc=0.94531, val_loss=2.01744, val_acc=0.89781, time=0.34100
Epoch:0014, train_loss=1.47881, train_acc=0.95078, val_loss=2.01673, val_acc=0.89964, time=0.36901
Epoch:0015, train_loss=1.47184, train_acc=0.95625, val_loss=2.01604, val_acc=0.90146, time=0.35100
Epoch:0016, train_loss=1.46587, train_acc=0.96070, val_loss=2.01539, val_acc=0.89964, time=0.33199
Epoch:0017, train_loss=1.46071, train_acc=0.96455, val_loss=2.01480, val_acc=0.91058, time=0.38800
Epoch:0018, train_loss=1.45620, train_acc=0.96800, val_loss=2.01428, val_acc=0.91058, time=0.42601
Epoch:0019, train_loss=1.45228, train_acc=0.97043, val_loss=2.01385, val_acc=0.93066, time=0.38301
Epoch:0020, train_loss=1.44897, train_acc=0.97367, val_loss=2.01352, val_acc=0.93431, time=0.31700
Epoch:0021, train_loss=1.44629, train_acc=0.97610, val_loss=2.01328, val_acc=0.93796, time=0.32800
Epoch:0022, train_loss=1.44414, train_acc=0.97752, val_loss=2.01311, val_acc=0.93796, time=0.33203
Epoch:0023, train_loss=1.44237, train_acc=0.97731, val_loss=2.01298, val_acc=0.93613, time=0.39100
Epoch:0024, train_loss=1.44075, train_acc=0.97853, val_loss=2.01287, val_acc=0.93613, time=0.35200
Epoch:0025, train_loss=1.43910, train_acc=0.97995, val_loss=2.01277, val_acc=0.93613, time=0.36300
Epoch:0026, train_loss=1.43735, train_acc=0.98055, val_loss=2.01267, val_acc=0.93431, time=0.40294
Epoch:0027, train_loss=1.43550, train_acc=0.98218, val_loss=2.01259, val_acc=0.93613, time=0.42899
Epoch:0028, train_loss=1.43364, train_acc=0.98400, val_loss=2.01252, val_acc=0.93613, time=0.41300
Epoch:0029, train_loss=1.43188, train_acc=0.98562, val_loss=2.01248, val_acc=0.93978, time=0.33901
Epoch:0030, train_loss=1.43032, train_acc=0.98744, val_loss=2.01245, val_acc=0.94161, time=0.43200
Epoch:0031, train_loss=1.42901, train_acc=0.98886, val_loss=2.01244, val_acc=0.93978, time=0.39300
Epoch:0032, train_loss=1.42795, train_acc=0.98947, val_loss=2.01244, val_acc=0.93613, time=0.37499
Epoch:0033, train_loss=1.42711, train_acc=0.98926, val_loss=2.01245, val_acc=0.93796, time=0.36901
Epoch:0034, train_loss=1.42642, train_acc=0.98926, val_loss=2.01246, val_acc=0.93978, time=0.41501
Epoch:0035, train_loss=1.42582, train_acc=0.98926, val_loss=2.01246, val_acc=0.93796, time=0.39300
Epoch:0036, train_loss=1.42526, train_acc=0.99048, val_loss=2.01245, val_acc=0.93796, time=0.35899
Epoch:0037, train_loss=1.42471, train_acc=0.99068, val_loss=2.01243, val_acc=0.93796, time=0.40201
Epoch:0038, train_loss=1.42415, train_acc=0.99089, val_loss=2.01241, val_acc=0.93978, time=0.40600
Epoch:0039, train_loss=1.42358, train_acc=0.99129, val_loss=2.01237, val_acc=0.94161, time=0.38100
Epoch:0040, train_loss=1.42302, train_acc=0.99251, val_loss=2.01234, val_acc=0.94161, time=0.38300
Epoch:0041, train_loss=1.42249, train_acc=0.99332, val_loss=2.01230, val_acc=0.94161, time=0.41202
Epoch:0042, train_loss=1.42200, train_acc=0.99392, val_loss=2.01226, val_acc=0.94161, time=0.42098
Epoch:0043, train_loss=1.42156, train_acc=0.99453, val_loss=2.01223, val_acc=0.94161, time=0.37099
Epoch:0044, train_loss=1.42117, train_acc=0.99534, val_loss=2.01220, val_acc=0.94161, time=0.29603
Epoch:0045, train_loss=1.42083, train_acc=0.99514, val_loss=2.01217, val_acc=0.94161, time=0.34900
Epoch:0046, train_loss=1.42053, train_acc=0.99595, val_loss=2.01215, val_acc=0.94161, time=0.31901
Epoch:0047, train_loss=1.42025, train_acc=0.99554, val_loss=2.01214, val_acc=0.94161, time=0.37301
Epoch:0048, train_loss=1.41999, train_acc=0.99575, val_loss=2.01212, val_acc=0.94343, time=0.33399
Epoch:0049, train_loss=1.41972, train_acc=0.99575, val_loss=2.01211, val_acc=0.94343, time=0.30100
Epoch:0050, train_loss=1.41946, train_acc=0.99575, val_loss=2.01210, val_acc=0.94343, time=0.44700
Epoch:0051, train_loss=1.41920, train_acc=0.99575, val_loss=2.01210, val_acc=0.94526, time=0.28900
Epoch:0052, train_loss=1.41894, train_acc=0.99595, val_loss=2.01210, val_acc=0.94526, time=0.30801
Epoch:0053, train_loss=1.41869, train_acc=0.99635, val_loss=2.01210, val_acc=0.94526, time=0.29801
Epoch:0054, train_loss=1.41845, train_acc=0.99656, val_loss=2.01210, val_acc=0.94526, time=0.36601
Epoch:0055, train_loss=1.41822, train_acc=0.99635, val_loss=2.01210, val_acc=0.94526, time=0.33299
Epoch:0056, train_loss=1.41802, train_acc=0.99615, val_loss=2.01211, val_acc=0.94526, time=0.39401
Epoch:0057, train_loss=1.41783, train_acc=0.99676, val_loss=2.01211, val_acc=0.94526, time=0.31300
Early stopping...

Optimization Finished!

Test set results: loss= 1.80497, accuracy= 0.95203, time= 0.12999

Test Precision, Recall and F1-Score...
              precision    recall  f1-score   support

           0     0.9732    0.9397    0.9561       696
           1     0.9639    0.9852    0.9744      1083
           2     0.8659    0.9467    0.9045        75
           3     0.9000    0.9669    0.9323       121
           4     0.8387    0.8966    0.8667        87
           5     0.8986    0.7654    0.8267        81
           6     1.0000    0.6944    0.8197        36
           7     0.9091    1.0000    0.9524        10

    accuracy                         0.9520      2189
   macro avg     0.9187    0.8994    0.9041      2189
weighted avg     0.9529    0.9520    0.9515      2189


Macro average Test Precision, Recall and F1-Score...
(0.918660695091067, 0.899364809830246, 0.904085688318011, None)

Micro average Test Precision, Recall and F1-Score...
(0.9520328917313842, 0.9520328917313842, 0.9520328917313842, None)

Embeddings:
Word_embeddings: 7688
Train_doc_embeddings: 5485
Test_doc_embeddings: 2189

Elapsed time is 22.280943 seconds.
