
==================== Torch Seed: 6776062646500

Model parameters

Layer: layer1.W0 | Size: torch.Size([61603, 200])
Layer: layer2.W0 | Size: torch.Size([200, 20])

Data statistics

  Edges    Classes    Train samples    Val samples    Test samples
-------  ---------  ---------------  -------------  --------------
  61603         20            10183           1131            7532

Epoch:0001, train_loss=3.10346, train_acc=0.04301, val_loss=2.99733, val_acc=0.08665, time=3.70998
Epoch:0002, train_loss=3.01001, train_acc=0.08671, val_loss=2.99109, val_acc=0.27233, time=3.82000
Epoch:0003, train_loss=2.95084, train_acc=0.25926, val_loss=2.98597, val_acc=0.41468, time=3.54998
Epoch:0004, train_loss=2.90162, train_acc=0.43170, val_loss=2.98133, val_acc=0.55349, time=3.63099
Epoch:0005, train_loss=2.85701, train_acc=0.58843, val_loss=2.97711, val_acc=0.64810, time=3.54797
Epoch:0006, train_loss=2.81629, train_acc=0.70097, val_loss=2.97334, val_acc=0.70999, time=3.91799
Epoch:0007, train_loss=2.77968, train_acc=0.77266, val_loss=2.96997, val_acc=0.75508, time=3.88301
Epoch:0008, train_loss=2.74687, train_acc=0.81548, val_loss=2.96690, val_acc=0.79222, time=3.81097
Epoch:0009, train_loss=2.71719, train_acc=0.84896, val_loss=2.96408, val_acc=0.81874, time=3.55199
Epoch:0010, train_loss=2.69030, train_acc=0.87499, val_loss=2.96153, val_acc=0.84350, time=4.09900
Epoch:0011, train_loss=2.66627, train_acc=0.89973, val_loss=2.95927, val_acc=0.86295, time=3.75499
Epoch:0012, train_loss=2.64528, train_acc=0.91476, val_loss=2.95732, val_acc=0.87445, time=3.81100
Epoch:0013, train_loss=2.62727, train_acc=0.92625, val_loss=2.95566, val_acc=0.88506, time=3.64299
Epoch:0014, train_loss=2.61194, train_acc=0.93322, val_loss=2.95424, val_acc=0.89036, time=3.56797
Epoch:0015, train_loss=2.59889, train_acc=0.93843, val_loss=2.95302, val_acc=0.89036, time=3.72600
Epoch:0016, train_loss=2.58771, train_acc=0.94353, val_loss=2.95198, val_acc=0.89478, time=3.81699
Epoch:0017, train_loss=2.57811, train_acc=0.94785, val_loss=2.95108, val_acc=0.89655, time=3.81599
Epoch:0018, train_loss=2.56983, train_acc=0.95178, val_loss=2.95031, val_acc=0.90186, time=3.64199
Epoch:0019, train_loss=2.56272, train_acc=0.95620, val_loss=2.94966, val_acc=0.90363, time=3.65901
Epoch:0020, train_loss=2.55664, train_acc=0.96013, val_loss=2.94912, val_acc=0.90451, time=3.60097
Epoch:0021, train_loss=2.55144, train_acc=0.96357, val_loss=2.94867, val_acc=0.90539, time=3.65600
Epoch:0022, train_loss=2.54699, train_acc=0.96651, val_loss=2.94829, val_acc=0.90805, time=3.63599
Epoch:0023, train_loss=2.54315, train_acc=0.96956, val_loss=2.94798, val_acc=0.91247, time=3.82800
Epoch:0024, train_loss=2.53978, train_acc=0.97172, val_loss=2.94770, val_acc=0.91247, time=3.69199
Epoch:0025, train_loss=2.53680, train_acc=0.97299, val_loss=2.94746, val_acc=0.91158, time=3.78698
Epoch:0026, train_loss=2.53413, train_acc=0.97437, val_loss=2.94725, val_acc=0.91070, time=3.55000
Epoch:0027, train_loss=2.53173, train_acc=0.97574, val_loss=2.94707, val_acc=0.91335, time=3.87899
Epoch:0028, train_loss=2.52956, train_acc=0.97761, val_loss=2.94690, val_acc=0.91689, time=3.60599
Epoch:0029, train_loss=2.52761, train_acc=0.97859, val_loss=2.94675, val_acc=0.91777, time=3.67800
Epoch:0030, train_loss=2.52585, train_acc=0.97987, val_loss=2.94662, val_acc=0.91954, time=3.64899
Epoch:0031, train_loss=2.52426, train_acc=0.98056, val_loss=2.94651, val_acc=0.92042, time=3.76299
Epoch:0032, train_loss=2.52280, train_acc=0.98154, val_loss=2.94640, val_acc=0.92042, time=3.94999
Epoch:0033, train_loss=2.52146, train_acc=0.98223, val_loss=2.94631, val_acc=0.92219, time=3.80499
Epoch:0034, train_loss=2.52023, train_acc=0.98311, val_loss=2.94622, val_acc=0.92396, time=3.72900
Epoch:0035, train_loss=2.51910, train_acc=0.98419, val_loss=2.94615, val_acc=0.92308, time=3.83398
Epoch:0036, train_loss=2.51805, train_acc=0.98537, val_loss=2.94608, val_acc=0.92396, time=3.58500
Epoch:0037, train_loss=2.51709, train_acc=0.98625, val_loss=2.94602, val_acc=0.92308, time=4.02100
Epoch:0038, train_loss=2.51621, train_acc=0.98753, val_loss=2.94597, val_acc=0.92219, time=3.95298
Epoch:0039, train_loss=2.51539, train_acc=0.98861, val_loss=2.94593, val_acc=0.92219, time=3.70601
Epoch:0040, train_loss=2.51464, train_acc=0.98939, val_loss=2.94590, val_acc=0.92131, time=3.72498
Epoch:0041, train_loss=2.51394, train_acc=0.99018, val_loss=2.94586, val_acc=0.92131, time=3.62900
Epoch:0042, train_loss=2.51329, train_acc=0.99067, val_loss=2.94584, val_acc=0.92131, time=3.66599
Epoch:0043, train_loss=2.51268, train_acc=0.99136, val_loss=2.94581, val_acc=0.92131, time=3.86099
Epoch:0044, train_loss=2.51211, train_acc=0.99244, val_loss=2.94580, val_acc=0.92219, time=3.74100
Epoch:0045, train_loss=2.51158, train_acc=0.99313, val_loss=2.94578, val_acc=0.92219, time=3.77598
Epoch:0046, train_loss=2.51108, train_acc=0.99372, val_loss=2.94577, val_acc=0.92219, time=3.83198
Epoch:0047, train_loss=2.51062, train_acc=0.99381, val_loss=2.94576, val_acc=0.92219, time=4.05400
Epoch:0048, train_loss=2.51019, train_acc=0.99381, val_loss=2.94575, val_acc=0.92308, time=4.12598
Epoch:0049, train_loss=2.50978, train_acc=0.99430, val_loss=2.94575, val_acc=0.92219, time=4.12000
Epoch:0050, train_loss=2.50940, train_acc=0.99470, val_loss=2.94574, val_acc=0.92308, time=3.93400
Epoch:0051, train_loss=2.50904, train_acc=0.99519, val_loss=2.94574, val_acc=0.92219, time=3.76999
Epoch:0052, train_loss=2.50870, train_acc=0.99548, val_loss=2.94574, val_acc=0.92219, time=3.87300
Epoch:0053, train_loss=2.50839, train_acc=0.99597, val_loss=2.94574, val_acc=0.92308, time=3.79699
Epoch:0054, train_loss=2.50809, train_acc=0.99637, val_loss=2.94573, val_acc=0.92308, time=3.75498
Epoch:0055, train_loss=2.50781, train_acc=0.99656, val_loss=2.94573, val_acc=0.92396, time=3.81699
Epoch:0056, train_loss=2.50755, train_acc=0.99676, val_loss=2.94573, val_acc=0.92485, time=3.66900
Epoch:0057, train_loss=2.50730, train_acc=0.99686, val_loss=2.94573, val_acc=0.92485, time=3.64600
Epoch:0058, train_loss=2.50706, train_acc=0.99705, val_loss=2.94572, val_acc=0.92573, time=3.78900
Epoch:0059, train_loss=2.50684, train_acc=0.99715, val_loss=2.94572, val_acc=0.92485, time=3.75899
Epoch:0060, train_loss=2.50662, train_acc=0.99725, val_loss=2.94573, val_acc=0.92485, time=3.73902
Epoch:0061, train_loss=2.50642, train_acc=0.99735, val_loss=2.94573, val_acc=0.92573, time=3.78200
Epoch:0062, train_loss=2.50623, train_acc=0.99754, val_loss=2.94573, val_acc=0.92573, time=3.67099
Early stopping...

Optimization Finished!

Test set results: loss= 2.70074, accuracy= 0.83935, time= 1.15800

Test Precision, Recall and F1-Score...
              precision    recall  f1-score   support

           0     0.8862    0.9196    0.9026       398
           1     0.6942    0.7352    0.7141       389
           2     0.8586    0.7994    0.8279       319
           3     0.9440    0.8939    0.9183       396
           4     0.8000    0.6710    0.7298       310
           5     0.7850    0.6396    0.7049       394
           6     0.9421    0.9421    0.9421       397
           7     0.9084    0.9061    0.9072       394
           8     0.8889    0.9293    0.9086       396
           9     0.9622    0.9574    0.9598       399
          10     0.9804    0.9335    0.9564       376
          11     0.8068    0.7823    0.7943       395
          12     0.7312    0.8231    0.7744       390
          13     0.7933    0.7812    0.7872       393
          14     0.6834    0.7653    0.7220       392
          15     0.7848    0.8819    0.8305       364
          16     0.8625    0.8712    0.8668       396
          17     0.7899    0.8104    0.8000       385
          18     0.9366    0.9648    0.9505       398
          19     0.7296    0.6773    0.7025       251

    accuracy                         0.8394      7532
   macro avg     0.8384    0.8342    0.8350      7532
weighted avg     0.8411    0.8394    0.8389      7532


Macro average Test Precision, Recall and F1-Score...
(0.838404155861139, 0.8342230663985782, 0.8350045235070527, None)

Micro average Test Precision, Recall and F1-Score...
(0.83935209771641, 0.83935209771641, 0.8393520977164098, None)

Embeddings:
Word_embeddings: 42757
Train_doc_embeddings: 11314
Test_doc_embeddings: 7532

Elapsed time is 247.023328 seconds.
