
==================== Torch Seed: 5935502380200

Model parameters

Layer: layer1.W0 | Size: torch.Size([15362, 200])
Layer: layer2.W0 | Size: torch.Size([200, 8])

Data statistics

  Edges    Classes    Train samples    Val samples    Test samples
-------  ---------  ---------------  -------------  --------------
  15362          8             4937            548            2189

Epoch:0001, train_loss=2.44098, train_acc=0.04152, val_loss=2.08152, val_acc=0.45803, time=0.38600
Epoch:0002, train_loss=2.09336, train_acc=0.45412, val_loss=2.05860, val_acc=0.55109, time=0.29999
Epoch:0003, train_loss=1.88059, train_acc=0.55439, val_loss=2.04468, val_acc=0.65876, time=0.30401
Epoch:0004, train_loss=1.75447, train_acc=0.67025, val_loss=2.03685, val_acc=0.74635, time=0.35803
Epoch:0005, train_loss=1.68790, train_acc=0.73891, val_loss=2.03321, val_acc=0.77190, time=0.30098
Epoch:0006, train_loss=1.65866, train_acc=0.76808, val_loss=2.03066, val_acc=0.78650, time=0.29900
Epoch:0007, train_loss=1.63565, train_acc=0.78448, val_loss=2.02782, val_acc=0.79380, time=0.32299
Epoch:0008, train_loss=1.60761, train_acc=0.80332, val_loss=2.02484, val_acc=0.81752, time=0.31903
Epoch:0009, train_loss=1.57737, train_acc=0.83533, val_loss=2.02229, val_acc=0.84672, time=0.29500
Epoch:0010, train_loss=1.55091, train_acc=0.87158, val_loss=2.02045, val_acc=0.86314, time=0.29599
Epoch:0011, train_loss=1.53128, train_acc=0.90014, val_loss=2.01919, val_acc=0.87409, time=0.33601
Epoch:0012, train_loss=1.51725, train_acc=0.91351, val_loss=2.01829, val_acc=0.88869, time=0.28902
Epoch:0013, train_loss=1.50654, train_acc=0.92809, val_loss=2.01754, val_acc=0.89416, time=0.29800
Epoch:0014, train_loss=1.49765, train_acc=0.93782, val_loss=2.01687, val_acc=0.90693, time=0.34699
Epoch:0015, train_loss=1.48994, train_acc=0.94369, val_loss=2.01623, val_acc=0.91241, time=0.31401
Epoch:0016, train_loss=1.48306, train_acc=0.94916, val_loss=2.01561, val_acc=0.91971, time=0.29101
Epoch:0017, train_loss=1.47687, train_acc=0.95524, val_loss=2.01501, val_acc=0.92701, time=0.32000
Epoch:0018, train_loss=1.47123, train_acc=0.95726, val_loss=2.01443, val_acc=0.92883, time=0.35002
Epoch:0019, train_loss=1.46604, train_acc=0.95969, val_loss=2.01389, val_acc=0.93248, time=0.29000
Epoch:0020, train_loss=1.46121, train_acc=0.96476, val_loss=2.01339, val_acc=0.93248, time=0.31399
Epoch:0021, train_loss=1.45672, train_acc=0.96779, val_loss=2.01293, val_acc=0.92883, time=0.44199
Epoch:0022, train_loss=1.45260, train_acc=0.97083, val_loss=2.01253, val_acc=0.93066, time=0.30201
Epoch:0023, train_loss=1.44889, train_acc=0.97326, val_loss=2.01220, val_acc=0.93613, time=0.28701
Epoch:0024, train_loss=1.44563, train_acc=0.97590, val_loss=2.01192, val_acc=0.93613, time=0.29201
Epoch:0025, train_loss=1.44283, train_acc=0.97792, val_loss=2.01171, val_acc=0.93796, time=0.28700
Epoch:0026, train_loss=1.44045, train_acc=0.97893, val_loss=2.01155, val_acc=0.93978, time=0.31601
Epoch:0027, train_loss=1.43844, train_acc=0.98015, val_loss=2.01144, val_acc=0.94161, time=0.31500
Epoch:0028, train_loss=1.43673, train_acc=0.98177, val_loss=2.01135, val_acc=0.94343, time=0.38200
Epoch:0029, train_loss=1.43527, train_acc=0.98359, val_loss=2.01130, val_acc=0.94526, time=0.43200
Epoch:0030, train_loss=1.43400, train_acc=0.98420, val_loss=2.01126, val_acc=0.94526, time=0.31299
Epoch:0031, train_loss=1.43288, train_acc=0.98521, val_loss=2.01123, val_acc=0.94708, time=0.28500
Epoch:0032, train_loss=1.43187, train_acc=0.98663, val_loss=2.01121, val_acc=0.94526, time=0.30400
Epoch:0033, train_loss=1.43097, train_acc=0.98704, val_loss=2.01119, val_acc=0.95438, time=0.31601
Epoch:0034, train_loss=1.43013, train_acc=0.98785, val_loss=2.01118, val_acc=0.95438, time=0.36000
Epoch:0035, train_loss=1.42934, train_acc=0.98825, val_loss=2.01116, val_acc=0.95438, time=0.31199
Epoch:0036, train_loss=1.42859, train_acc=0.98866, val_loss=2.01114, val_acc=0.95803, time=0.32701
Epoch:0037, train_loss=1.42788, train_acc=0.98906, val_loss=2.01112, val_acc=0.95803, time=0.41701
Epoch:0038, train_loss=1.42720, train_acc=0.98947, val_loss=2.01109, val_acc=0.95803, time=0.34700
Epoch:0039, train_loss=1.42655, train_acc=0.99028, val_loss=2.01106, val_acc=0.95985, time=0.32501
Epoch:0040, train_loss=1.42593, train_acc=0.99089, val_loss=2.01102, val_acc=0.95985, time=0.36099
Epoch:0041, train_loss=1.42534, train_acc=0.99170, val_loss=2.01098, val_acc=0.95803, time=0.28499
Epoch:0042, train_loss=1.42477, train_acc=0.99230, val_loss=2.01094, val_acc=0.95803, time=0.32201
Epoch:0043, train_loss=1.42423, train_acc=0.99271, val_loss=2.01089, val_acc=0.95620, time=0.42999
Epoch:0044, train_loss=1.42372, train_acc=0.99271, val_loss=2.01085, val_acc=0.95620, time=0.40901
Epoch:0045, train_loss=1.42323, train_acc=0.99332, val_loss=2.01080, val_acc=0.95620, time=0.30702
Epoch:0046, train_loss=1.42277, train_acc=0.99352, val_loss=2.01076, val_acc=0.95620, time=0.28601
Epoch:0047, train_loss=1.42234, train_acc=0.99352, val_loss=2.01072, val_acc=0.95438, time=0.28512
Epoch:0048, train_loss=1.42193, train_acc=0.99413, val_loss=2.01068, val_acc=0.95438, time=0.35200
Epoch:0049, train_loss=1.42155, train_acc=0.99433, val_loss=2.01065, val_acc=0.95255, time=0.37200
Epoch:0050, train_loss=1.42120, train_acc=0.99453, val_loss=2.01062, val_acc=0.95620, time=0.32799
Epoch:0051, train_loss=1.42087, train_acc=0.99514, val_loss=2.01060, val_acc=0.95620, time=0.34501
Epoch:0052, train_loss=1.42056, train_acc=0.99534, val_loss=2.01058, val_acc=0.95803, time=0.28799
Epoch:0053, train_loss=1.42028, train_acc=0.99554, val_loss=2.01056, val_acc=0.95803, time=0.28500
Epoch:0054, train_loss=1.42002, train_acc=0.99595, val_loss=2.01054, val_acc=0.95985, time=0.33702
Epoch:0055, train_loss=1.41977, train_acc=0.99615, val_loss=2.01053, val_acc=0.95985, time=0.32700
Epoch:0056, train_loss=1.41954, train_acc=0.99615, val_loss=2.01052, val_acc=0.95985, time=0.36500
Epoch:0057, train_loss=1.41932, train_acc=0.99595, val_loss=2.01051, val_acc=0.95985, time=0.30002
Epoch:0058, train_loss=1.41911, train_acc=0.99635, val_loss=2.01050, val_acc=0.95985, time=0.30599
Epoch:0059, train_loss=1.41891, train_acc=0.99635, val_loss=2.01050, val_acc=0.95985, time=0.31502
Epoch:0060, train_loss=1.41872, train_acc=0.99635, val_loss=2.01049, val_acc=0.95985, time=0.29400
Epoch:0061, train_loss=1.41853, train_acc=0.99635, val_loss=2.01049, val_acc=0.95985, time=0.30399
Epoch:0062, train_loss=1.41836, train_acc=0.99656, val_loss=2.01049, val_acc=0.95985, time=0.29101
Epoch:0063, train_loss=1.41819, train_acc=0.99656, val_loss=2.01049, val_acc=0.95985, time=0.28700
Epoch:0064, train_loss=1.41802, train_acc=0.99696, val_loss=2.01049, val_acc=0.95985, time=0.28699
Epoch:0065, train_loss=1.41787, train_acc=0.99696, val_loss=2.01049, val_acc=0.95985, time=0.29000
Epoch:0066, train_loss=1.41772, train_acc=0.99696, val_loss=2.01049, val_acc=0.95985, time=0.28601
Epoch:0067, train_loss=1.41757, train_acc=0.99696, val_loss=2.01049, val_acc=0.95985, time=0.28498
Epoch:0068, train_loss=1.41743, train_acc=0.99716, val_loss=2.01049, val_acc=0.95985, time=0.37402
Epoch:0069, train_loss=1.41729, train_acc=0.99716, val_loss=2.01049, val_acc=0.95985, time=0.28402
Epoch:0070, train_loss=1.41717, train_acc=0.99737, val_loss=2.01049, val_acc=0.95985, time=0.30196
Epoch:0071, train_loss=1.41704, train_acc=0.99737, val_loss=2.01049, val_acc=0.95985, time=0.33601
Epoch:0072, train_loss=1.41692, train_acc=0.99737, val_loss=2.01049, val_acc=0.95985, time=0.31800
Epoch:0073, train_loss=1.41681, train_acc=0.99737, val_loss=2.01049, val_acc=0.95985, time=0.28600
Epoch:0074, train_loss=1.41670, train_acc=0.99737, val_loss=2.01049, val_acc=0.95985, time=0.34600
Epoch:0075, train_loss=1.41660, train_acc=0.99737, val_loss=2.01049, val_acc=0.95985, time=0.30101
Epoch:0076, train_loss=1.41650, train_acc=0.99757, val_loss=2.01048, val_acc=0.95985, time=0.28500
Epoch:0077, train_loss=1.41641, train_acc=0.99777, val_loss=2.01048, val_acc=0.96168, time=0.29200
Epoch:0078, train_loss=1.41632, train_acc=0.99777, val_loss=2.01048, val_acc=0.96168, time=0.37300
Epoch:0079, train_loss=1.41623, train_acc=0.99797, val_loss=2.01048, val_acc=0.96168, time=0.28400
Epoch:0080, train_loss=1.41614, train_acc=0.99797, val_loss=2.01048, val_acc=0.96168, time=0.28700
Epoch:0081, train_loss=1.41606, train_acc=0.99797, val_loss=2.01047, val_acc=0.96168, time=0.30500
Epoch:0082, train_loss=1.41598, train_acc=0.99797, val_loss=2.01047, val_acc=0.96168, time=0.28500
Epoch:0083, train_loss=1.41590, train_acc=0.99797, val_loss=2.01047, val_acc=0.96168, time=0.28400
Epoch:0084, train_loss=1.41583, train_acc=0.99797, val_loss=2.01047, val_acc=0.96168, time=0.31300
Epoch:0085, train_loss=1.41576, train_acc=0.99797, val_loss=2.01047, val_acc=0.95985, time=0.30800
Epoch:0086, train_loss=1.41569, train_acc=0.99797, val_loss=2.01047, val_acc=0.95985, time=0.28400
Epoch:0087, train_loss=1.41562, train_acc=0.99797, val_loss=2.01046, val_acc=0.96168, time=0.44000
Epoch:0088, train_loss=1.41555, train_acc=0.99818, val_loss=2.01046, val_acc=0.96168, time=0.28901
Epoch:0089, train_loss=1.41548, train_acc=0.99838, val_loss=2.01046, val_acc=0.96168, time=0.28300
Epoch:0090, train_loss=1.41542, train_acc=0.99838, val_loss=2.01046, val_acc=0.96168, time=0.28800
Epoch:0091, train_loss=1.41536, train_acc=0.99838, val_loss=2.01046, val_acc=0.96168, time=0.39302
Epoch:0092, train_loss=1.41530, train_acc=0.99838, val_loss=2.01046, val_acc=0.96168, time=0.34400
Epoch:0093, train_loss=1.41524, train_acc=0.99838, val_loss=2.01046, val_acc=0.96168, time=0.29100
Epoch:0094, train_loss=1.41519, train_acc=0.99838, val_loss=2.01046, val_acc=0.96168, time=0.35300
Epoch:0095, train_loss=1.41513, train_acc=0.99838, val_loss=2.01046, val_acc=0.96168, time=0.28500
Epoch:0096, train_loss=1.41508, train_acc=0.99838, val_loss=2.01046, val_acc=0.96168, time=0.28700
Epoch:0097, train_loss=1.41503, train_acc=0.99838, val_loss=2.01046, val_acc=0.96168, time=0.33100
Early stopping...

Optimization Finished!

Test set results: loss= 1.80668, accuracy= 0.94838, time= 0.09800

Test Precision, Recall and F1-Score...
              precision    recall  f1-score   support

           0     0.9817    0.9267    0.9534       696
           1     0.9563    0.9908    0.9732      1083
           2     0.8659    0.9467    0.9045        75
           3     0.9000    0.9669    0.9323       121
           4     0.8172    0.8736    0.8444        87
           5     0.8714    0.7531    0.8079        81
           6     0.9259    0.6944    0.7937        36
           7     1.0000    0.8000    0.8889        10

    accuracy                         0.9484      2189
   macro avg     0.9148    0.8690    0.8873      2189
weighted avg     0.9492    0.9484    0.9477      2189


Macro average Test Precision, Recall and F1-Score...
(0.9148094503154317, 0.8690241782005904, 0.8872925124241605, None)

Micro average Test Precision, Recall and F1-Score...
(0.9483782549109182, 0.9483782549109182, 0.9483782549109182, None)

Embeddings:
Word_embeddings: 7688
Train_doc_embeddings: 5485
Test_doc_embeddings: 2189

Elapsed time is 32.411900 seconds.
