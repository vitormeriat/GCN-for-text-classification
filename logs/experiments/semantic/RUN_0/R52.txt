
==================== Torch Seed: 404883823500

Model parameters

Layer: layer1.W0 | Size: torch.Size([17992, 200])
Layer: layer2.W0 | Size: torch.Size([200, 52])

Data statistics

  Edges    Classes    Train samples    Val samples    Test samples
-------  ---------  ---------------  -------------  --------------
  17992         52             5879            653            2568

Epoch:0001, train_loss=4.13609, train_acc=0.00816, val_loss=3.93648, val_acc=0.26187, time=0.51001
Epoch:0002, train_loss=3.80639, train_acc=0.27181, val_loss=3.90765, val_acc=0.47014, time=0.40200
Epoch:0003, train_loss=3.54945, train_acc=0.48920, val_loss=3.88584, val_acc=0.55743, time=0.49100
Epoch:0004, train_loss=3.35727, train_acc=0.58003, val_loss=3.87051, val_acc=0.64625, time=0.37900
Epoch:0005, train_loss=3.22172, train_acc=0.64501, val_loss=3.86141, val_acc=0.68147, time=0.44899
Epoch:0006, train_loss=3.13986, train_acc=0.67937, val_loss=3.85656, val_acc=0.68453, time=0.44602
Epoch:0007, train_loss=3.09434, train_acc=0.69025, val_loss=3.85302, val_acc=0.70750, time=0.45199
Epoch:0008, train_loss=3.05900, train_acc=0.71135, val_loss=3.84945, val_acc=0.73660, time=0.39899
Epoch:0009, train_loss=3.02274, train_acc=0.74264, val_loss=3.84591, val_acc=0.76876, time=0.36701
Epoch:0010, train_loss=2.98746, train_acc=0.78023, val_loss=3.84282, val_acc=0.79632, time=0.36901
Epoch:0011, train_loss=2.95694, train_acc=0.81204, val_loss=3.84030, val_acc=0.81470, time=0.40100
Epoch:0012, train_loss=2.93207, train_acc=0.83058, val_loss=3.83824, val_acc=0.82848, time=0.38400
Epoch:0013, train_loss=2.91154, train_acc=0.84028, val_loss=3.83650, val_acc=0.83155, time=0.45400
Epoch:0014, train_loss=2.89378, train_acc=0.85270, val_loss=3.83493, val_acc=0.84074, time=0.34900
Epoch:0015, train_loss=2.87754, train_acc=0.86358, val_loss=3.83346, val_acc=0.84839, time=0.42100
Epoch:0016, train_loss=2.86210, train_acc=0.87481, val_loss=3.83205, val_acc=0.86217, time=0.43400
Epoch:0017, train_loss=2.84719, train_acc=0.88621, val_loss=3.83073, val_acc=0.86677, time=0.51399
Epoch:0018, train_loss=2.83304, train_acc=0.89913, val_loss=3.82953, val_acc=0.86983, time=0.50701
Epoch:0019, train_loss=2.82006, train_acc=0.91206, val_loss=3.82850, val_acc=0.88055, time=0.49899
Epoch:0020, train_loss=2.80858, train_acc=0.92005, val_loss=3.82764, val_acc=0.89127, time=0.45400
Epoch:0021, train_loss=2.79864, train_acc=0.92856, val_loss=3.82693, val_acc=0.89433, time=0.40901
Epoch:0022, train_loss=2.79001, train_acc=0.93553, val_loss=3.82632, val_acc=0.89280, time=0.38200
Epoch:0023, train_loss=2.78233, train_acc=0.93911, val_loss=3.82577, val_acc=0.89433, time=0.46600
Epoch:0024, train_loss=2.77531, train_acc=0.94353, val_loss=3.82526, val_acc=0.89587, time=0.37900
Epoch:0025, train_loss=2.76878, train_acc=0.94574, val_loss=3.82478, val_acc=0.90046, time=0.43299
Epoch:0026, train_loss=2.76266, train_acc=0.94846, val_loss=3.82433, val_acc=0.90046, time=0.40201
Epoch:0027, train_loss=2.75693, train_acc=0.95152, val_loss=3.82390, val_acc=0.90046, time=0.52001
Epoch:0028, train_loss=2.75156, train_acc=0.95492, val_loss=3.82350, val_acc=0.90352, time=0.35000
Epoch:0029, train_loss=2.74651, train_acc=0.95594, val_loss=3.82313, val_acc=0.90812, time=0.38400
Epoch:0030, train_loss=2.74177, train_acc=0.95969, val_loss=3.82278, val_acc=0.90658, time=0.43100
Epoch:0031, train_loss=2.73730, train_acc=0.96071, val_loss=3.82244, val_acc=0.90965, time=0.44900
Epoch:0032, train_loss=2.73308, train_acc=0.96275, val_loss=3.82211, val_acc=0.91118, time=0.46700
Epoch:0033, train_loss=2.72910, train_acc=0.96411, val_loss=3.82179, val_acc=0.90965, time=0.38101
Epoch:0034, train_loss=2.72535, train_acc=0.96632, val_loss=3.82149, val_acc=0.91118, time=0.39301
Epoch:0035, train_loss=2.72183, train_acc=0.96904, val_loss=3.82119, val_acc=0.91271, time=0.47100
Epoch:0036, train_loss=2.71855, train_acc=0.97040, val_loss=3.82091, val_acc=0.91118, time=0.49100
Epoch:0037, train_loss=2.71550, train_acc=0.97227, val_loss=3.82064, val_acc=0.91118, time=0.43599
Epoch:0038, train_loss=2.71268, train_acc=0.97244, val_loss=3.82039, val_acc=0.91118, time=0.46401
Epoch:0039, train_loss=2.71006, train_acc=0.97381, val_loss=3.82014, val_acc=0.91271, time=0.49800
Epoch:0040, train_loss=2.70762, train_acc=0.97483, val_loss=3.81990, val_acc=0.91424, time=0.55900
Epoch:0041, train_loss=2.70532, train_acc=0.97721, val_loss=3.81967, val_acc=0.91730, time=0.51199
Epoch:0042, train_loss=2.70314, train_acc=0.97840, val_loss=3.81945, val_acc=0.92037, time=0.44603
Epoch:0043, train_loss=2.70106, train_acc=0.97942, val_loss=3.81922, val_acc=0.92343, time=0.50500
Epoch:0044, train_loss=2.69908, train_acc=0.98163, val_loss=3.81901, val_acc=0.92343, time=0.35999
Epoch:0045, train_loss=2.69719, train_acc=0.98265, val_loss=3.81881, val_acc=0.92343, time=0.43901
Epoch:0046, train_loss=2.69541, train_acc=0.98401, val_loss=3.81862, val_acc=0.92802, time=0.44000
Epoch:0047, train_loss=2.69374, train_acc=0.98486, val_loss=3.81844, val_acc=0.93109, time=0.52600
Epoch:0048, train_loss=2.69219, train_acc=0.98622, val_loss=3.81828, val_acc=0.93109, time=0.37399
Epoch:0049, train_loss=2.69074, train_acc=0.98707, val_loss=3.81813, val_acc=0.93109, time=0.48801
Epoch:0050, train_loss=2.68940, train_acc=0.98775, val_loss=3.81799, val_acc=0.93262, time=0.54799
Epoch:0051, train_loss=2.68814, train_acc=0.98826, val_loss=3.81787, val_acc=0.93109, time=0.49401
Epoch:0052, train_loss=2.68697, train_acc=0.98826, val_loss=3.81775, val_acc=0.93109, time=0.43699
Epoch:0053, train_loss=2.68588, train_acc=0.98894, val_loss=3.81765, val_acc=0.93109, time=0.40900
Epoch:0054, train_loss=2.68485, train_acc=0.98945, val_loss=3.81754, val_acc=0.93109, time=0.53599
Epoch:0055, train_loss=2.68387, train_acc=0.98979, val_loss=3.81745, val_acc=0.93415, time=0.42800
Epoch:0056, train_loss=2.68296, train_acc=0.99047, val_loss=3.81735, val_acc=0.93415, time=0.51800
Epoch:0057, train_loss=2.68209, train_acc=0.99115, val_loss=3.81726, val_acc=0.93415, time=0.43999
Epoch:0058, train_loss=2.68126, train_acc=0.99252, val_loss=3.81716, val_acc=0.93415, time=0.38100
Epoch:0059, train_loss=2.68048, train_acc=0.99303, val_loss=3.81707, val_acc=0.93415, time=0.46201
Epoch:0060, train_loss=2.67973, train_acc=0.99303, val_loss=3.81698, val_acc=0.93415, time=0.51700
Epoch:0061, train_loss=2.67902, train_acc=0.99320, val_loss=3.81689, val_acc=0.93415, time=0.37200
Epoch:0062, train_loss=2.67835, train_acc=0.99354, val_loss=3.81680, val_acc=0.93415, time=0.43400
Epoch:0063, train_loss=2.67771, train_acc=0.99371, val_loss=3.81672, val_acc=0.93415, time=0.38800
Epoch:0064, train_loss=2.67710, train_acc=0.99388, val_loss=3.81664, val_acc=0.93415, time=0.40099
Epoch:0065, train_loss=2.67652, train_acc=0.99371, val_loss=3.81656, val_acc=0.93415, time=0.42101
Epoch:0066, train_loss=2.67597, train_acc=0.99405, val_loss=3.81649, val_acc=0.93568, time=0.42799
Epoch:0067, train_loss=2.67545, train_acc=0.99439, val_loss=3.81643, val_acc=0.93568, time=0.46500
Epoch:0068, train_loss=2.67496, train_acc=0.99456, val_loss=3.81636, val_acc=0.93568, time=0.52599
Epoch:0069, train_loss=2.67449, train_acc=0.99456, val_loss=3.81631, val_acc=0.93568, time=0.43699
Epoch:0070, train_loss=2.67404, train_acc=0.99473, val_loss=3.81625, val_acc=0.93568, time=0.42200
Epoch:0071, train_loss=2.67361, train_acc=0.99473, val_loss=3.81620, val_acc=0.94028, time=0.41800
Epoch:0072, train_loss=2.67321, train_acc=0.99524, val_loss=3.81616, val_acc=0.94028, time=0.36100
Epoch:0073, train_loss=2.67283, train_acc=0.99524, val_loss=3.81611, val_acc=0.94028, time=0.36800
Epoch:0074, train_loss=2.67247, train_acc=0.99524, val_loss=3.81607, val_acc=0.94028, time=0.44301
Epoch:0075, train_loss=2.67213, train_acc=0.99558, val_loss=3.81604, val_acc=0.94028, time=0.41399
Epoch:0076, train_loss=2.67180, train_acc=0.99558, val_loss=3.81600, val_acc=0.93874, time=0.51001
Epoch:0077, train_loss=2.67149, train_acc=0.99575, val_loss=3.81597, val_acc=0.94028, time=0.44000
Epoch:0078, train_loss=2.67119, train_acc=0.99592, val_loss=3.81594, val_acc=0.94028, time=0.44500
Epoch:0079, train_loss=2.67091, train_acc=0.99609, val_loss=3.81591, val_acc=0.94028, time=0.46399
Epoch:0080, train_loss=2.67064, train_acc=0.99626, val_loss=3.81588, val_acc=0.94181, time=0.34901
Epoch:0081, train_loss=2.67038, train_acc=0.99626, val_loss=3.81585, val_acc=0.94181, time=0.47000
Epoch:0082, train_loss=2.67014, train_acc=0.99643, val_loss=3.81583, val_acc=0.94334, time=0.50000
Epoch:0083, train_loss=2.66990, train_acc=0.99677, val_loss=3.81580, val_acc=0.94334, time=0.42400
Epoch:0084, train_loss=2.66967, train_acc=0.99711, val_loss=3.81578, val_acc=0.94334, time=0.41799
Epoch:0085, train_loss=2.66945, train_acc=0.99711, val_loss=3.81575, val_acc=0.94334, time=0.40500
Epoch:0086, train_loss=2.66924, train_acc=0.99745, val_loss=3.81573, val_acc=0.94487, time=0.51499
Epoch:0087, train_loss=2.66904, train_acc=0.99779, val_loss=3.81571, val_acc=0.94181, time=0.35000
Epoch:0088, train_loss=2.66884, train_acc=0.99779, val_loss=3.81569, val_acc=0.94181, time=0.39601
Epoch:0089, train_loss=2.66865, train_acc=0.99779, val_loss=3.81567, val_acc=0.94181, time=0.48699
Epoch:0090, train_loss=2.66848, train_acc=0.99762, val_loss=3.81565, val_acc=0.94334, time=0.42701
Epoch:0091, train_loss=2.66830, train_acc=0.99762, val_loss=3.81563, val_acc=0.94334, time=0.50199
Epoch:0092, train_loss=2.66814, train_acc=0.99762, val_loss=3.81562, val_acc=0.94334, time=0.43001
Epoch:0093, train_loss=2.66798, train_acc=0.99762, val_loss=3.81560, val_acc=0.94334, time=0.46900
Epoch:0094, train_loss=2.66782, train_acc=0.99779, val_loss=3.81559, val_acc=0.94334, time=0.34800
Epoch:0095, train_loss=2.66768, train_acc=0.99779, val_loss=3.81558, val_acc=0.94334, time=0.42000
Epoch:0096, train_loss=2.66754, train_acc=0.99779, val_loss=3.81557, val_acc=0.94487, time=0.37299
Epoch:0097, train_loss=2.66740, train_acc=0.99762, val_loss=3.81556, val_acc=0.94487, time=0.38000
Epoch:0098, train_loss=2.66727, train_acc=0.99762, val_loss=3.81555, val_acc=0.94487, time=0.51901
Epoch:0099, train_loss=2.66714, train_acc=0.99762, val_loss=3.81554, val_acc=0.94487, time=0.42599
Epoch:0100, train_loss=2.66702, train_acc=0.99762, val_loss=3.81553, val_acc=0.94487, time=0.45899
Epoch:0101, train_loss=2.66690, train_acc=0.99779, val_loss=3.81552, val_acc=0.94487, time=0.43701
Epoch:0102, train_loss=2.66678, train_acc=0.99779, val_loss=3.81551, val_acc=0.94487, time=0.41200
Epoch:0103, train_loss=2.66667, train_acc=0.99779, val_loss=3.81550, val_acc=0.94487, time=0.38500
Epoch:0104, train_loss=2.66656, train_acc=0.99779, val_loss=3.81550, val_acc=0.94487, time=0.52499
Epoch:0105, train_loss=2.66646, train_acc=0.99779, val_loss=3.81549, val_acc=0.94487, time=0.52802
Epoch:0106, train_loss=2.66636, train_acc=0.99779, val_loss=3.81548, val_acc=0.94487, time=0.51100
Epoch:0107, train_loss=2.66626, train_acc=0.99779, val_loss=3.81547, val_acc=0.94487, time=0.50899
Epoch:0108, train_loss=2.66616, train_acc=0.99796, val_loss=3.81547, val_acc=0.94487, time=0.53400
Epoch:0109, train_loss=2.66607, train_acc=0.99779, val_loss=3.81546, val_acc=0.94487, time=0.53800
Epoch:0110, train_loss=2.66598, train_acc=0.99779, val_loss=3.81546, val_acc=0.94487, time=0.43500
Epoch:0111, train_loss=2.66589, train_acc=0.99779, val_loss=3.81545, val_acc=0.94487, time=0.51599
Epoch:0112, train_loss=2.66581, train_acc=0.99779, val_loss=3.81545, val_acc=0.94487, time=0.48000
Epoch:0113, train_loss=2.66572, train_acc=0.99779, val_loss=3.81544, val_acc=0.94487, time=0.53299
Epoch:0114, train_loss=2.66564, train_acc=0.99796, val_loss=3.81544, val_acc=0.94487, time=0.52001
Epoch:0115, train_loss=2.66556, train_acc=0.99796, val_loss=3.81543, val_acc=0.94487, time=0.42100
Epoch:0116, train_loss=2.66549, train_acc=0.99796, val_loss=3.81543, val_acc=0.94487, time=0.45599
Epoch:0117, train_loss=2.66541, train_acc=0.99796, val_loss=3.81543, val_acc=0.94487, time=0.40901
Epoch:0118, train_loss=2.66534, train_acc=0.99813, val_loss=3.81542, val_acc=0.94487, time=0.46400
Epoch:0119, train_loss=2.66527, train_acc=0.99830, val_loss=3.81542, val_acc=0.94487, time=0.47100
Epoch:0120, train_loss=2.66520, train_acc=0.99830, val_loss=3.81542, val_acc=0.94640, time=0.43000
Epoch:0121, train_loss=2.66513, train_acc=0.99830, val_loss=3.81542, val_acc=0.94640, time=0.46999
Epoch:0122, train_loss=2.66507, train_acc=0.99830, val_loss=3.81541, val_acc=0.94640, time=0.37201
Epoch:0123, train_loss=2.66500, train_acc=0.99830, val_loss=3.81541, val_acc=0.94640, time=0.36999
Epoch:0124, train_loss=2.66494, train_acc=0.99830, val_loss=3.81541, val_acc=0.94640, time=0.44001
Epoch:0125, train_loss=2.66488, train_acc=0.99830, val_loss=3.81541, val_acc=0.94640, time=0.44199
Epoch:0126, train_loss=2.66482, train_acc=0.99830, val_loss=3.81541, val_acc=0.94640, time=0.39001
Epoch:0127, train_loss=2.66476, train_acc=0.99830, val_loss=3.81540, val_acc=0.94640, time=0.46699
Epoch:0128, train_loss=2.66471, train_acc=0.99830, val_loss=3.81540, val_acc=0.94640, time=0.34900
Epoch:0129, train_loss=2.66465, train_acc=0.99830, val_loss=3.81540, val_acc=0.94640, time=0.43801
Epoch:0130, train_loss=2.66459, train_acc=0.99830, val_loss=3.81540, val_acc=0.94640, time=0.41200
Epoch:0131, train_loss=2.66454, train_acc=0.99830, val_loss=3.81540, val_acc=0.94640, time=0.42600
Epoch:0132, train_loss=2.66449, train_acc=0.99830, val_loss=3.81540, val_acc=0.94640, time=0.48199
Epoch:0133, train_loss=2.66444, train_acc=0.99847, val_loss=3.81540, val_acc=0.94640, time=0.49100
Epoch:0134, train_loss=2.66439, train_acc=0.99847, val_loss=3.81540, val_acc=0.94640, time=0.49000
Epoch:0135, train_loss=2.66434, train_acc=0.99847, val_loss=3.81540, val_acc=0.94640, time=0.46702
Epoch:0136, train_loss=2.66429, train_acc=0.99847, val_loss=3.81540, val_acc=0.94640, time=0.45601
Epoch:0137, train_loss=2.66424, train_acc=0.99847, val_loss=3.81539, val_acc=0.94640, time=0.37201
Epoch:0138, train_loss=2.66420, train_acc=0.99847, val_loss=3.81539, val_acc=0.94640, time=0.43800
Epoch:0139, train_loss=2.66415, train_acc=0.99847, val_loss=3.81539, val_acc=0.94640, time=0.43699
Epoch:0140, train_loss=2.66411, train_acc=0.99847, val_loss=3.81539, val_acc=0.94640, time=0.44901
Epoch:0141, train_loss=2.66407, train_acc=0.99847, val_loss=3.81539, val_acc=0.94640, time=0.40399
Epoch:0142, train_loss=2.66402, train_acc=0.99847, val_loss=3.81539, val_acc=0.94640, time=0.42801
Epoch:0143, train_loss=2.66398, train_acc=0.99847, val_loss=3.81539, val_acc=0.94640, time=0.43100
Epoch:0144, train_loss=2.66394, train_acc=0.99847, val_loss=3.81539, val_acc=0.94640, time=0.48400
Epoch:0145, train_loss=2.66390, train_acc=0.99847, val_loss=3.81539, val_acc=0.94640, time=0.46599
Epoch:0146, train_loss=2.66386, train_acc=0.99847, val_loss=3.81539, val_acc=0.94640, time=0.36701
Early stopping...

Optimization Finished!

Test set results: loss= 3.44021, accuracy= 0.92017, time= 0.10799

Test Precision, Recall and F1-Score...
              precision    recall  f1-score   support

           0     0.9546    0.9908    0.9724      1083
           1     0.8529    0.9587    0.9027       121
           2     0.9629    0.9310    0.9467       696
           3     1.0000    0.9333    0.9655        15
           4     0.9286    0.8667    0.8966        15
           5     1.0000    0.8235    0.9032        17
           6     0.8276    0.6667    0.7385        36
           7     0.8889    0.9600    0.9231        25
           8     0.8462    0.5789    0.6875        19
           9     0.8333    0.7692    0.8000        13
          10     0.7812    0.8621    0.8197        87
          11     0.8235    0.7000    0.7568        20
          12     0.7423    0.9600    0.8372        75
          13     0.8065    0.8929    0.8475        28
          14     1.0000    0.7778    0.8750         9
          15     0.9167    1.0000    0.9565        22
          16     0.8000    0.8000    0.8000         5
          17     0.9000    0.7500    0.8182        12
          18     0.7975    0.7778    0.7875        81
          19     0.7500    0.9000    0.8182        10
          20     1.0000    1.0000    1.0000         2
          21     0.9167    0.9167    0.9167        12
          22     0.5000    1.0000    0.6667         1
          23     0.8750    0.7778    0.8235         9
          24     0.8000    0.3333    0.4706        12
          25     1.0000    0.6000    0.7500         5
          26     1.0000    0.9000    0.9474        10
          27     1.0000    0.9167    0.9565        12
          28     0.0000    0.0000    0.0000         3
          29     1.0000    1.0000    1.0000         3
          30     0.7143    0.5556    0.6250         9
          31     1.0000    1.0000    1.0000         9
          32     0.8571    0.7500    0.8000         8
          33     0.9167    1.0000    0.9565        11
          34     1.0000    0.2000    0.3333         5
          35     1.0000    0.5000    0.6667         4
          36     0.7500    0.7500    0.7500         4
          37     1.0000    0.3333    0.5000         3
          38     1.0000    1.0000    1.0000         4
          39     1.0000    1.0000    1.0000         1
          40     0.3333    0.3333    0.3333         6
          41     1.0000    0.8182    0.9000        11
          42     1.0000    0.8889    0.9412         9
          43     1.0000    0.1667    0.2857         6
          44     1.0000    1.0000    1.0000         1
          45     0.0000    0.0000    0.0000         1
          46     0.0000    0.0000    0.0000         1
          47     1.0000    0.1429    0.2500         7
          48     0.0000    0.0000    0.0000         1
          49     0.0000    0.0000    0.0000         2
          50     0.0000    0.0000    0.0000         4
          51     0.0000    0.0000    0.0000         3

    accuracy                         0.9202      2568
   macro avg     0.7707    0.6689    0.6909      2568
weighted avg     0.9191    0.9202    0.9151      2568


Macro average Test Precision, Recall and F1-Score...
(0.7706862199501063, 0.6688960696469558, 0.6908781193813348, None)

Micro average Test Precision, Recall and F1-Score...
(0.9201713395638629, 0.9201713395638629, 0.9201713395638629, None)

Embeddings:
Word_embeddings: 8892
Train_doc_embeddings: 6532
Test_doc_embeddings: 2568

Elapsed time is 66.702868 seconds.
