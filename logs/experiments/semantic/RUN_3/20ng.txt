
==================== Torch Seed: 1509815755300

Model parameters

Layer: layer1.W0 | Size: torch.Size([61603, 200])
Layer: layer2.W0 | Size: torch.Size([200, 20])

Data statistics

  Edges    Classes    Train samples    Val samples    Test samples
-------  ---------  ---------------  -------------  --------------
  61603         20            10183           1131            7532

Epoch:0001, train_loss=3.06188, train_acc=0.04949, val_loss=2.99544, val_acc=0.13439, time=3.72800
Epoch:0002, train_loss=2.98489, train_acc=0.14043, val_loss=2.98936, val_acc=0.29885, time=3.69499
Epoch:0003, train_loss=2.92765, train_acc=0.32819, val_loss=2.98416, val_acc=0.47038, time=3.74899
Epoch:0004, train_loss=2.87818, train_acc=0.51399, val_loss=2.97936, val_acc=0.60566, time=3.70501
Epoch:0005, train_loss=2.83273, train_acc=0.65894, val_loss=2.97497, val_acc=0.69761, time=3.81000
Epoch:0006, train_loss=2.79136, train_acc=0.75852, val_loss=2.97105, val_acc=0.74536, time=3.69000
Epoch:0007, train_loss=2.75444, train_acc=0.80958, val_loss=2.96753, val_acc=0.78249, time=3.86300
Epoch:0008, train_loss=2.72131, train_acc=0.84592, val_loss=2.96433, val_acc=0.82317, time=3.79700
Epoch:0009, train_loss=2.69148, train_acc=0.87626, val_loss=2.96149, val_acc=0.84792, time=3.81001
Epoch:0010, train_loss=2.66504, train_acc=0.90170, val_loss=2.95901, val_acc=0.87356, time=3.72001
Epoch:0011, train_loss=2.64215, train_acc=0.92330, val_loss=2.95689, val_acc=0.88948, time=3.72900
Epoch:0012, train_loss=2.62271, train_acc=0.93607, val_loss=2.95511, val_acc=0.89567, time=3.64898
Epoch:0013, train_loss=2.60646, train_acc=0.94402, val_loss=2.95364, val_acc=0.90186, time=3.66099
Epoch:0014, train_loss=2.59296, train_acc=0.95011, val_loss=2.95242, val_acc=0.90539, time=3.60499
Epoch:0015, train_loss=2.58174, train_acc=0.95375, val_loss=2.95140, val_acc=0.90981, time=3.71100
Epoch:0016, train_loss=2.57236, train_acc=0.95699, val_loss=2.95056, val_acc=0.90893, time=3.67999
Epoch:0017, train_loss=2.56444, train_acc=0.95944, val_loss=2.94985, val_acc=0.90981, time=3.78499
Epoch:0018, train_loss=2.55769, train_acc=0.96219, val_loss=2.94924, val_acc=0.91335, time=3.75501
Epoch:0019, train_loss=2.55190, train_acc=0.96376, val_loss=2.94873, val_acc=0.91335, time=3.62600
Epoch:0020, train_loss=2.54689, train_acc=0.96612, val_loss=2.94829, val_acc=0.91335, time=3.87400
Epoch:0021, train_loss=2.54255, train_acc=0.96789, val_loss=2.94791, val_acc=0.91689, time=3.63499
Epoch:0022, train_loss=2.53877, train_acc=0.96995, val_loss=2.94759, val_acc=0.91866, time=3.53000
Epoch:0023, train_loss=2.53548, train_acc=0.97299, val_loss=2.94732, val_acc=0.92131, time=3.88000
Epoch:0024, train_loss=2.53261, train_acc=0.97486, val_loss=2.94709, val_acc=0.92396, time=3.76499
Epoch:0025, train_loss=2.53009, train_acc=0.97663, val_loss=2.94688, val_acc=0.92308, time=3.64899
Epoch:0026, train_loss=2.52786, train_acc=0.97800, val_loss=2.94671, val_acc=0.92573, time=3.71201
Epoch:0027, train_loss=2.52587, train_acc=0.97987, val_loss=2.94655, val_acc=0.92750, time=3.86600
Epoch:0028, train_loss=2.52408, train_acc=0.98105, val_loss=2.94642, val_acc=0.92661, time=3.63100
Epoch:0029, train_loss=2.52247, train_acc=0.98223, val_loss=2.94631, val_acc=0.92750, time=3.65000
Epoch:0030, train_loss=2.52100, train_acc=0.98360, val_loss=2.94621, val_acc=0.92750, time=3.60100
Epoch:0031, train_loss=2.51968, train_acc=0.98497, val_loss=2.94613, val_acc=0.92573, time=3.67797
Epoch:0032, train_loss=2.51848, train_acc=0.98596, val_loss=2.94606, val_acc=0.92396, time=3.56801
Epoch:0033, train_loss=2.51738, train_acc=0.98694, val_loss=2.94600, val_acc=0.92573, time=3.57599
Epoch:0034, train_loss=2.51638, train_acc=0.98782, val_loss=2.94595, val_acc=0.92661, time=3.71300
Epoch:0035, train_loss=2.51546, train_acc=0.98802, val_loss=2.94591, val_acc=0.92750, time=3.79698
Epoch:0036, train_loss=2.51460, train_acc=0.98880, val_loss=2.94587, val_acc=0.92661, time=3.92200
Epoch:0037, train_loss=2.51381, train_acc=0.98930, val_loss=2.94584, val_acc=0.92750, time=3.69199
Epoch:0038, train_loss=2.51309, train_acc=0.98998, val_loss=2.94581, val_acc=0.92661, time=3.63301
Epoch:0039, train_loss=2.51241, train_acc=0.99028, val_loss=2.94579, val_acc=0.92661, time=3.79597
Epoch:0040, train_loss=2.51179, train_acc=0.99175, val_loss=2.94577, val_acc=0.92661, time=3.71100
Epoch:0041, train_loss=2.51122, train_acc=0.99224, val_loss=2.94576, val_acc=0.92661, time=3.60499
Epoch:0042, train_loss=2.51069, train_acc=0.99303, val_loss=2.94575, val_acc=0.92573, time=3.60800
Epoch:0043, train_loss=2.51020, train_acc=0.99332, val_loss=2.94574, val_acc=0.92661, time=3.68099
Epoch:0044, train_loss=2.50974, train_acc=0.99391, val_loss=2.94573, val_acc=0.92661, time=3.61098
Epoch:0045, train_loss=2.50930, train_acc=0.99411, val_loss=2.94572, val_acc=0.92573, time=3.51899
Epoch:0046, train_loss=2.50890, train_acc=0.99509, val_loss=2.94571, val_acc=0.92573, time=4.05000
Epoch:0047, train_loss=2.50852, train_acc=0.99538, val_loss=2.94570, val_acc=0.92661, time=3.73399
Epoch:0048, train_loss=2.50816, train_acc=0.99568, val_loss=2.94570, val_acc=0.92750, time=3.73798
Epoch:0049, train_loss=2.50783, train_acc=0.99617, val_loss=2.94569, val_acc=0.92750, time=3.54599
Epoch:0050, train_loss=2.50752, train_acc=0.99627, val_loss=2.94569, val_acc=0.92750, time=3.65999
Epoch:0051, train_loss=2.50723, train_acc=0.99656, val_loss=2.94568, val_acc=0.92750, time=3.62098
Epoch:0052, train_loss=2.50696, train_acc=0.99656, val_loss=2.94568, val_acc=0.92750, time=3.71899
Epoch:0053, train_loss=2.50671, train_acc=0.99666, val_loss=2.94568, val_acc=0.92927, time=3.62100
Epoch:0054, train_loss=2.50647, train_acc=0.99676, val_loss=2.94568, val_acc=0.92838, time=3.77500
Epoch:0055, train_loss=2.50625, train_acc=0.99715, val_loss=2.94569, val_acc=0.92750, time=3.81999
Epoch:0056, train_loss=2.50604, train_acc=0.99754, val_loss=2.94570, val_acc=0.92661, time=3.53698
Early stopping...

Optimization Finished!

Test set results: loss= 2.70110, accuracy= 0.84426, time= 1.08701

Test Precision, Recall and F1-Score...
              precision    recall  f1-score   support

           0     0.8801    0.9221    0.9006       398
           1     0.7143    0.7712    0.7417       389
           2     0.8562    0.8213    0.8384       319
           3     0.9339    0.8914    0.9121       396
           4     0.8247    0.6677    0.7380       310
           5     0.7861    0.6624    0.7190       394
           6     0.9496    0.9496    0.9496       397
           7     0.8970    0.9061    0.9015       394
           8     0.9024    0.9343    0.9181       396
           9     0.9719    0.9524    0.9620       399
          10     0.9916    0.9388    0.9645       376
          11     0.7911    0.7671    0.7789       395
          12     0.7506    0.8103    0.7793       390
          13     0.8042    0.7735    0.7886       393
          14     0.6741    0.7755    0.7212       392
          15     0.7923    0.9011    0.8432       364
          16     0.8706    0.8662    0.8684       396
          17     0.8112    0.8260    0.8185       385
          18     0.9531    0.9698    0.9614       398
          19     0.7143    0.6773    0.6953       251

    accuracy                         0.8443      7532
   macro avg     0.8435    0.8392    0.8400      7532
weighted avg     0.8463    0.8443    0.8440      7532


Macro average Test Precision, Recall and F1-Score...
(0.8434608838156283, 0.8392131236508173, 0.840016984128807, None)

Micro average Test Precision, Recall and F1-Score...
(0.8442644715878916, 0.8442644715878916, 0.8442644715878916, None)

Embeddings:
Word_embeddings: 42757
Train_doc_embeddings: 11314
Test_doc_embeddings: 7532

Elapsed time is 220.896508 seconds.
