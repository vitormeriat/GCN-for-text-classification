
==================== Torch Seed: 1266874041600

Model parameters

Layer: layer1.W0 | Size: torch.Size([61603, 200])
Layer: layer2.W0 | Size: torch.Size([200, 20])

Data statistics

  Edges    Classes    Train samples    Val samples    Test samples
-------  ---------  ---------------  -------------  --------------
  61603         20            10183           1131            7532

Epoch:0001, train_loss=3.07889, train_acc=0.05391, val_loss=2.99526, val_acc=0.12025, time=3.76798
Epoch:0002, train_loss=2.99453, train_acc=0.12305, val_loss=2.98932, val_acc=0.27940, time=3.67600
Epoch:0003, train_loss=2.93691, train_acc=0.30698, val_loss=2.98446, val_acc=0.43855, time=3.78699
Epoch:0004, train_loss=2.88844, train_acc=0.46725, val_loss=2.98006, val_acc=0.57560, time=3.63400
Epoch:0005, train_loss=2.84431, train_acc=0.61141, val_loss=2.97601, val_acc=0.66225, time=3.78998
Epoch:0006, train_loss=2.80385, train_acc=0.71806, val_loss=2.97226, val_acc=0.73210, time=3.67401
Epoch:0007, train_loss=2.76691, train_acc=0.78317, val_loss=2.96877, val_acc=0.75950, time=3.62000
Epoch:0008, train_loss=2.73321, train_acc=0.83001, val_loss=2.96554, val_acc=0.80460, time=4.16699
Epoch:0009, train_loss=2.70264, train_acc=0.86605, val_loss=2.96260, val_acc=0.83378, time=3.79198
Epoch:0010, train_loss=2.67532, train_acc=0.89630, val_loss=2.96001, val_acc=0.86118, time=3.71301
Epoch:0011, train_loss=2.65145, train_acc=0.91682, val_loss=2.95779, val_acc=0.87445, time=3.59800
Epoch:0012, train_loss=2.63106, train_acc=0.93008, val_loss=2.95591, val_acc=0.88771, time=3.75099
Epoch:0013, train_loss=2.61392, train_acc=0.93744, val_loss=2.95433, val_acc=0.89478, time=3.54598
Epoch:0014, train_loss=2.59960, train_acc=0.94294, val_loss=2.95301, val_acc=0.90363, time=3.85000
Epoch:0015, train_loss=2.58763, train_acc=0.94776, val_loss=2.95190, val_acc=0.90716, time=3.66800
Epoch:0016, train_loss=2.57759, train_acc=0.95100, val_loss=2.95098, val_acc=0.91070, time=3.69600
Epoch:0017, train_loss=2.56912, train_acc=0.95384, val_loss=2.95020, val_acc=0.90981, time=3.71098
Epoch:0018, train_loss=2.56194, train_acc=0.95699, val_loss=2.94955, val_acc=0.91600, time=3.64401
Epoch:0019, train_loss=2.55583, train_acc=0.96042, val_loss=2.94901, val_acc=0.91954, time=3.56199
Epoch:0020, train_loss=2.55059, train_acc=0.96298, val_loss=2.94854, val_acc=0.91954, time=3.75400
Epoch:0021, train_loss=2.54606, train_acc=0.96524, val_loss=2.94815, val_acc=0.92219, time=3.73699
Epoch:0022, train_loss=2.54212, train_acc=0.96691, val_loss=2.94781, val_acc=0.92485, time=3.70100
Epoch:0023, train_loss=2.53865, train_acc=0.96946, val_loss=2.94752, val_acc=0.92661, time=3.63297
Epoch:0024, train_loss=2.53558, train_acc=0.97123, val_loss=2.94726, val_acc=0.92838, time=3.66301
Epoch:0025, train_loss=2.53285, train_acc=0.97299, val_loss=2.94704, val_acc=0.93192, time=3.61399
Epoch:0026, train_loss=2.53040, train_acc=0.97476, val_loss=2.94685, val_acc=0.93103, time=3.61800
Epoch:0027, train_loss=2.52821, train_acc=0.97682, val_loss=2.94668, val_acc=0.92838, time=3.67300
Epoch:0028, train_loss=2.52625, train_acc=0.97830, val_loss=2.94654, val_acc=0.92927, time=3.60200
Epoch:0029, train_loss=2.52449, train_acc=0.97967, val_loss=2.94643, val_acc=0.92927, time=3.53000
Epoch:0030, train_loss=2.52291, train_acc=0.98115, val_loss=2.94633, val_acc=0.93192, time=3.61299
Epoch:0031, train_loss=2.52150, train_acc=0.98203, val_loss=2.94625, val_acc=0.93369, time=3.52499
Epoch:0032, train_loss=2.52022, train_acc=0.98350, val_loss=2.94618, val_acc=0.93546, time=3.66400
Epoch:0033, train_loss=2.51906, train_acc=0.98448, val_loss=2.94612, val_acc=0.93457, time=3.72699
Epoch:0034, train_loss=2.51800, train_acc=0.98527, val_loss=2.94608, val_acc=0.93546, time=3.70200
Epoch:0035, train_loss=2.51703, train_acc=0.98566, val_loss=2.94603, val_acc=0.93457, time=3.54099
Epoch:0036, train_loss=2.51613, train_acc=0.98674, val_loss=2.94599, val_acc=0.93457, time=3.68499
Epoch:0037, train_loss=2.51530, train_acc=0.98772, val_loss=2.94595, val_acc=0.93457, time=3.44600
Epoch:0038, train_loss=2.51452, train_acc=0.98851, val_loss=2.94592, val_acc=0.93280, time=3.75502
Epoch:0039, train_loss=2.51379, train_acc=0.98930, val_loss=2.94588, val_acc=0.93280, time=3.71500
Epoch:0040, train_loss=2.51312, train_acc=0.99028, val_loss=2.94585, val_acc=0.93192, time=3.76398
Epoch:0041, train_loss=2.51249, train_acc=0.99097, val_loss=2.94582, val_acc=0.93280, time=4.19998
Epoch:0042, train_loss=2.51190, train_acc=0.99155, val_loss=2.94580, val_acc=0.93280, time=3.78298
Epoch:0043, train_loss=2.51136, train_acc=0.99234, val_loss=2.94578, val_acc=0.93280, time=3.67701
Epoch:0044, train_loss=2.51086, train_acc=0.99313, val_loss=2.94576, val_acc=0.93192, time=3.63898
Epoch:0045, train_loss=2.51039, train_acc=0.99362, val_loss=2.94574, val_acc=0.93192, time=3.74701
Epoch:0046, train_loss=2.50995, train_acc=0.99421, val_loss=2.94573, val_acc=0.93192, time=3.59599
Epoch:0047, train_loss=2.50954, train_acc=0.99489, val_loss=2.94572, val_acc=0.93192, time=3.67999
Epoch:0048, train_loss=2.50916, train_acc=0.99509, val_loss=2.94570, val_acc=0.93192, time=3.71301
Epoch:0049, train_loss=2.50880, train_acc=0.99548, val_loss=2.94570, val_acc=0.93192, time=3.50497
Epoch:0050, train_loss=2.50846, train_acc=0.99558, val_loss=2.94569, val_acc=0.93192, time=3.71299
Epoch:0051, train_loss=2.50814, train_acc=0.99597, val_loss=2.94569, val_acc=0.93103, time=3.72302
Epoch:0052, train_loss=2.50785, train_acc=0.99627, val_loss=2.94568, val_acc=0.93103, time=3.76900
Epoch:0053, train_loss=2.50756, train_acc=0.99656, val_loss=2.94568, val_acc=0.93103, time=3.62498
Epoch:0054, train_loss=2.50730, train_acc=0.99676, val_loss=2.94568, val_acc=0.93103, time=3.72601
Epoch:0055, train_loss=2.50705, train_acc=0.99715, val_loss=2.94568, val_acc=0.93280, time=3.64799
Epoch:0056, train_loss=2.50682, train_acc=0.99725, val_loss=2.94568, val_acc=0.93192, time=3.96401
Epoch:0057, train_loss=2.50660, train_acc=0.99754, val_loss=2.94569, val_acc=0.93192, time=3.62200
Epoch:0058, train_loss=2.50639, train_acc=0.99774, val_loss=2.94569, val_acc=0.93192, time=3.57100
Early stopping...

Optimization Finished!

Test set results: loss= 2.69914, accuracy= 0.84599, time= 1.09100

Test Precision, Recall and F1-Score...
              precision    recall  f1-score   support

           0     0.8849    0.9271    0.9055       398
           1     0.7170    0.7815    0.7478       389
           2     0.8700    0.8182    0.8433       319
           3     0.9357    0.8813    0.9077       396
           4     0.8171    0.6774    0.7407       310
           5     0.7952    0.6701    0.7273       394
           6     0.9495    0.9471    0.9483       397
           7     0.8809    0.9010    0.8908       394
           8     0.9138    0.9369    0.9252       396
           9     0.9698    0.9649    0.9673       399
          10     0.9889    0.9441    0.9660       376
          11     0.8137    0.7519    0.7816       395
          12     0.7535    0.8231    0.7868       390
          13     0.7964    0.7863    0.7913       393
          14     0.7070    0.7755    0.7397       392
          15     0.7873    0.8846    0.8331       364
          16     0.8675    0.8763    0.8719       396
          17     0.7955    0.8286    0.8117       385
          18     0.9294    0.9598    0.9444       398
          19     0.7227    0.6853    0.7035       251

    accuracy                         0.8460      7532
   macro avg     0.8448    0.8410    0.8417      7532
weighted avg     0.8475    0.8460    0.8456      7532


Macro average Test Precision, Recall and F1-Score...
(0.844782949804286, 0.841044647766606, 0.8416914534734943, None)

Micro average Test Precision, Recall and F1-Score...
(0.8459904407859798, 0.8459904407859798, 0.8459904407859798, None)

Embeddings:
Word_embeddings: 42757
Train_doc_embeddings: 11314
Test_doc_embeddings: 7532

Elapsed time is 227.342587 seconds.
