
==================== Torch Seed: 556494330700

Model parameters

Layer: layer1.W0 | Size: torch.Size([17992, 200])
Layer: layer2.W0 | Size: torch.Size([200, 52])

Data statistics

  Edges    Classes    Train samples    Val samples    Test samples
-------  ---------  ---------------  -------------  --------------
  17992         52             5879            653            2568

Epoch:0001, train_loss=4.23425, train_acc=0.00527, val_loss=3.94337, val_acc=0.24502, time=0.48599
Epoch:0002, train_loss=3.88394, train_acc=0.24681, val_loss=3.91000, val_acc=0.49311, time=0.39901
Epoch:0003, train_loss=3.58243, train_acc=0.48376, val_loss=3.88504, val_acc=0.58499, time=0.45000
Epoch:0004, train_loss=3.35584, train_acc=0.60044, val_loss=3.87006, val_acc=0.63553, time=0.42800
Epoch:0005, train_loss=3.22054, train_acc=0.64246, val_loss=3.86198, val_acc=0.64778, time=0.38299
Epoch:0006, train_loss=3.14753, train_acc=0.66559, val_loss=3.85705, val_acc=0.67228, time=0.44300
Epoch:0007, train_loss=3.10151, train_acc=0.68872, val_loss=3.85325, val_acc=0.70138, time=0.46801
Epoch:0008, train_loss=3.06376, train_acc=0.71696, val_loss=3.84986, val_acc=0.72588, time=0.45300
Epoch:0009, train_loss=3.02868, train_acc=0.74094, val_loss=3.84674, val_acc=0.75498, time=0.42999
Epoch:0010, train_loss=2.99569, train_acc=0.76476, val_loss=3.84389, val_acc=0.77029, time=0.38900
Epoch:0011, train_loss=2.96509, train_acc=0.79265, val_loss=3.84134, val_acc=0.79939, time=0.46901
Epoch:0012, train_loss=2.93753, train_acc=0.81868, val_loss=3.83913, val_acc=0.81776, time=0.35400
Epoch:0013, train_loss=2.91358, train_acc=0.84470, val_loss=3.83726, val_acc=0.83614, time=0.48600
Epoch:0014, train_loss=2.89324, train_acc=0.86664, val_loss=3.83568, val_acc=0.84686, time=0.41100
Epoch:0015, train_loss=2.87608, train_acc=0.88297, val_loss=3.83434, val_acc=0.85758, time=0.49599
Epoch:0016, train_loss=2.86140, train_acc=0.89386, val_loss=3.83316, val_acc=0.87443, time=0.41300
Epoch:0017, train_loss=2.84851, train_acc=0.90321, val_loss=3.83207, val_acc=0.87596, time=0.36301
Epoch:0018, train_loss=2.83680, train_acc=0.90985, val_loss=3.83104, val_acc=0.88361, time=0.53700
Epoch:0019, train_loss=2.82584, train_acc=0.91580, val_loss=3.83003, val_acc=0.88821, time=0.40801
Epoch:0020, train_loss=2.81537, train_acc=0.92005, val_loss=3.82906, val_acc=0.89280, time=0.36700
Epoch:0021, train_loss=2.80533, train_acc=0.92652, val_loss=3.82812, val_acc=0.89587, time=0.37800
Epoch:0022, train_loss=2.79578, train_acc=0.93043, val_loss=3.82724, val_acc=0.89893, time=0.39200
Epoch:0023, train_loss=2.78684, train_acc=0.93400, val_loss=3.82642, val_acc=0.89893, time=0.38200
Epoch:0024, train_loss=2.77860, train_acc=0.93979, val_loss=3.82568, val_acc=0.90505, time=0.38200
Epoch:0025, train_loss=2.77108, train_acc=0.94302, val_loss=3.82501, val_acc=0.90965, time=0.42399
Epoch:0026, train_loss=2.76425, train_acc=0.94421, val_loss=3.82440, val_acc=0.91271, time=0.42100
Epoch:0027, train_loss=2.75806, train_acc=0.94523, val_loss=3.82387, val_acc=0.91271, time=0.35600
Epoch:0028, train_loss=2.75243, train_acc=0.94931, val_loss=3.82339, val_acc=0.91884, time=0.48301
Epoch:0029, train_loss=2.74729, train_acc=0.95305, val_loss=3.82297, val_acc=0.91884, time=0.42100
Epoch:0030, train_loss=2.74259, train_acc=0.95629, val_loss=3.82259, val_acc=0.92037, time=0.39900
Epoch:0031, train_loss=2.73826, train_acc=0.95850, val_loss=3.82226, val_acc=0.92037, time=0.42200
Epoch:0032, train_loss=2.73423, train_acc=0.96037, val_loss=3.82195, val_acc=0.92190, time=0.37099
Epoch:0033, train_loss=2.73044, train_acc=0.96292, val_loss=3.82167, val_acc=0.92190, time=0.50501
Epoch:0034, train_loss=2.72685, train_acc=0.96564, val_loss=3.82140, val_acc=0.92037, time=0.43700
Epoch:0035, train_loss=2.72343, train_acc=0.96700, val_loss=3.82115, val_acc=0.92343, time=0.41499
Epoch:0036, train_loss=2.72016, train_acc=0.96870, val_loss=3.82091, val_acc=0.92343, time=0.43201
Epoch:0037, train_loss=2.71706, train_acc=0.97210, val_loss=3.82069, val_acc=0.92190, time=0.41300
Epoch:0038, train_loss=2.71413, train_acc=0.97329, val_loss=3.82048, val_acc=0.92190, time=0.44000
Epoch:0039, train_loss=2.71137, train_acc=0.97483, val_loss=3.82028, val_acc=0.92037, time=0.42600
Epoch:0040, train_loss=2.70880, train_acc=0.97568, val_loss=3.82009, val_acc=0.92343, time=0.35401
Epoch:0041, train_loss=2.70640, train_acc=0.97721, val_loss=3.81992, val_acc=0.92343, time=0.39999
Epoch:0042, train_loss=2.70417, train_acc=0.97857, val_loss=3.81975, val_acc=0.92496, time=0.41401
Epoch:0043, train_loss=2.70210, train_acc=0.97908, val_loss=3.81958, val_acc=0.92649, time=0.35300
Epoch:0044, train_loss=2.70017, train_acc=0.97976, val_loss=3.81942, val_acc=0.92649, time=0.37499
Epoch:0045, train_loss=2.69837, train_acc=0.98146, val_loss=3.81926, val_acc=0.92649, time=0.41001
Epoch:0046, train_loss=2.69668, train_acc=0.98282, val_loss=3.81910, val_acc=0.92649, time=0.38400
Epoch:0047, train_loss=2.69510, train_acc=0.98452, val_loss=3.81894, val_acc=0.92649, time=0.44400
Epoch:0048, train_loss=2.69360, train_acc=0.98520, val_loss=3.81877, val_acc=0.92802, time=0.40400
Epoch:0049, train_loss=2.69218, train_acc=0.98571, val_loss=3.81861, val_acc=0.93109, time=0.43099
Epoch:0050, train_loss=2.69083, train_acc=0.98639, val_loss=3.81844, val_acc=0.93262, time=0.46501
Epoch:0051, train_loss=2.68955, train_acc=0.98758, val_loss=3.81828, val_acc=0.93262, time=0.42099
Epoch:0052, train_loss=2.68833, train_acc=0.98826, val_loss=3.81812, val_acc=0.93262, time=0.43101
Epoch:0053, train_loss=2.68718, train_acc=0.98877, val_loss=3.81796, val_acc=0.93415, time=0.46100
Epoch:0054, train_loss=2.68610, train_acc=0.98945, val_loss=3.81782, val_acc=0.93568, time=0.49500
Epoch:0055, train_loss=2.68508, train_acc=0.98996, val_loss=3.81767, val_acc=0.93568, time=0.53199
Epoch:0056, train_loss=2.68411, train_acc=0.99013, val_loss=3.81754, val_acc=0.93568, time=0.39199
Epoch:0057, train_loss=2.68321, train_acc=0.99064, val_loss=3.81741, val_acc=0.93874, time=0.38701
Epoch:0058, train_loss=2.68235, train_acc=0.99115, val_loss=3.81729, val_acc=0.93874, time=0.42400
Epoch:0059, train_loss=2.68154, train_acc=0.99167, val_loss=3.81717, val_acc=0.93874, time=0.43799
Epoch:0060, train_loss=2.68078, train_acc=0.99235, val_loss=3.81707, val_acc=0.93874, time=0.41001
Epoch:0061, train_loss=2.68006, train_acc=0.99235, val_loss=3.81697, val_acc=0.94028, time=0.42799
Epoch:0062, train_loss=2.67937, train_acc=0.99252, val_loss=3.81688, val_acc=0.94181, time=0.38501
Epoch:0063, train_loss=2.67873, train_acc=0.99269, val_loss=3.81680, val_acc=0.94181, time=0.47600
Epoch:0064, train_loss=2.67811, train_acc=0.99286, val_loss=3.81672, val_acc=0.94028, time=0.41600
Epoch:0065, train_loss=2.67753, train_acc=0.99354, val_loss=3.81665, val_acc=0.94181, time=0.41900
Epoch:0066, train_loss=2.67698, train_acc=0.99371, val_loss=3.81659, val_acc=0.94028, time=0.34800
Epoch:0067, train_loss=2.67646, train_acc=0.99422, val_loss=3.81653, val_acc=0.94028, time=0.40400
Epoch:0068, train_loss=2.67596, train_acc=0.99439, val_loss=3.81647, val_acc=0.94028, time=0.36999
Epoch:0069, train_loss=2.67549, train_acc=0.99456, val_loss=3.81642, val_acc=0.94028, time=0.39201
Epoch:0070, train_loss=2.67503, train_acc=0.99456, val_loss=3.81637, val_acc=0.94028, time=0.34899
Epoch:0071, train_loss=2.67460, train_acc=0.99456, val_loss=3.81632, val_acc=0.94028, time=0.37501
Epoch:0072, train_loss=2.67419, train_acc=0.99456, val_loss=3.81627, val_acc=0.94028, time=0.43800
Epoch:0073, train_loss=2.67379, train_acc=0.99473, val_loss=3.81623, val_acc=0.94028, time=0.39800
Epoch:0074, train_loss=2.67341, train_acc=0.99490, val_loss=3.81618, val_acc=0.94028, time=0.51700
Epoch:0075, train_loss=2.67304, train_acc=0.99507, val_loss=3.81614, val_acc=0.94028, time=0.36700
Epoch:0076, train_loss=2.67269, train_acc=0.99541, val_loss=3.81610, val_acc=0.94028, time=0.45300
Epoch:0077, train_loss=2.67235, train_acc=0.99541, val_loss=3.81606, val_acc=0.94028, time=0.47801
Epoch:0078, train_loss=2.67203, train_acc=0.99541, val_loss=3.81603, val_acc=0.94028, time=0.38499
Epoch:0079, train_loss=2.67172, train_acc=0.99541, val_loss=3.81599, val_acc=0.94028, time=0.45401
Epoch:0080, train_loss=2.67142, train_acc=0.99541, val_loss=3.81596, val_acc=0.94028, time=0.45400
Epoch:0081, train_loss=2.67114, train_acc=0.99541, val_loss=3.81593, val_acc=0.94028, time=0.36700
Epoch:0082, train_loss=2.67086, train_acc=0.99558, val_loss=3.81590, val_acc=0.94028, time=0.36900
Epoch:0083, train_loss=2.67060, train_acc=0.99575, val_loss=3.81587, val_acc=0.94028, time=0.53199
Epoch:0084, train_loss=2.67035, train_acc=0.99592, val_loss=3.81585, val_acc=0.94028, time=0.50001
Epoch:0085, train_loss=2.67010, train_acc=0.99643, val_loss=3.81582, val_acc=0.94028, time=0.44200
Epoch:0086, train_loss=2.66987, train_acc=0.99711, val_loss=3.81580, val_acc=0.94181, time=0.39000
Epoch:0087, train_loss=2.66965, train_acc=0.99728, val_loss=3.81577, val_acc=0.94181, time=0.35700
Epoch:0088, train_loss=2.66944, train_acc=0.99728, val_loss=3.81575, val_acc=0.94181, time=0.45400
Epoch:0089, train_loss=2.66923, train_acc=0.99745, val_loss=3.81573, val_acc=0.94181, time=0.36600
Epoch:0090, train_loss=2.66903, train_acc=0.99745, val_loss=3.81572, val_acc=0.94181, time=0.42899
Epoch:0091, train_loss=2.66885, train_acc=0.99745, val_loss=3.81570, val_acc=0.94181, time=0.52400
Epoch:0092, train_loss=2.66866, train_acc=0.99745, val_loss=3.81568, val_acc=0.94181, time=0.40501
Epoch:0093, train_loss=2.66849, train_acc=0.99762, val_loss=3.81567, val_acc=0.94181, time=0.36200
Epoch:0094, train_loss=2.66832, train_acc=0.99762, val_loss=3.81565, val_acc=0.94181, time=0.35400
Epoch:0095, train_loss=2.66816, train_acc=0.99745, val_loss=3.81564, val_acc=0.94181, time=0.44400
Epoch:0096, train_loss=2.66800, train_acc=0.99745, val_loss=3.81563, val_acc=0.94181, time=0.42500
Epoch:0097, train_loss=2.66784, train_acc=0.99745, val_loss=3.81561, val_acc=0.94181, time=0.49699
Epoch:0098, train_loss=2.66770, train_acc=0.99745, val_loss=3.81560, val_acc=0.94181, time=0.41800
Epoch:0099, train_loss=2.66755, train_acc=0.99762, val_loss=3.81559, val_acc=0.94028, time=0.43901
Epoch:0100, train_loss=2.66742, train_acc=0.99745, val_loss=3.81558, val_acc=0.94028, time=0.50901
Epoch:0101, train_loss=2.66728, train_acc=0.99745, val_loss=3.81557, val_acc=0.94028, time=0.40000
Epoch:0102, train_loss=2.66715, train_acc=0.99762, val_loss=3.81556, val_acc=0.94028, time=0.37900
Epoch:0103, train_loss=2.66703, train_acc=0.99762, val_loss=3.81555, val_acc=0.94028, time=0.40700
Epoch:0104, train_loss=2.66691, train_acc=0.99762, val_loss=3.81554, val_acc=0.94028, time=0.42800
Epoch:0105, train_loss=2.66679, train_acc=0.99762, val_loss=3.81554, val_acc=0.94028, time=0.40500
Epoch:0106, train_loss=2.66668, train_acc=0.99762, val_loss=3.81553, val_acc=0.94028, time=0.38400
Epoch:0107, train_loss=2.66657, train_acc=0.99762, val_loss=3.81552, val_acc=0.94028, time=0.47400
Epoch:0108, train_loss=2.66647, train_acc=0.99762, val_loss=3.81552, val_acc=0.94028, time=0.43300
Epoch:0109, train_loss=2.66637, train_acc=0.99762, val_loss=3.81551, val_acc=0.94028, time=0.46900
Epoch:0110, train_loss=2.66627, train_acc=0.99762, val_loss=3.81551, val_acc=0.94028, time=0.42100
Epoch:0111, train_loss=2.66617, train_acc=0.99762, val_loss=3.81550, val_acc=0.94028, time=0.48799
Epoch:0112, train_loss=2.66608, train_acc=0.99779, val_loss=3.81550, val_acc=0.94028, time=0.41601
Epoch:0113, train_loss=2.66599, train_acc=0.99796, val_loss=3.81549, val_acc=0.94028, time=0.38500
Epoch:0114, train_loss=2.66591, train_acc=0.99796, val_loss=3.81549, val_acc=0.94028, time=0.40400
Epoch:0115, train_loss=2.66582, train_acc=0.99796, val_loss=3.81549, val_acc=0.94028, time=0.38101
Epoch:0116, train_loss=2.66574, train_acc=0.99813, val_loss=3.81549, val_acc=0.94028, time=0.46301
Epoch:0117, train_loss=2.66566, train_acc=0.99813, val_loss=3.81548, val_acc=0.93874, time=0.49500
Epoch:0118, train_loss=2.66558, train_acc=0.99830, val_loss=3.81548, val_acc=0.93874, time=0.37400
Epoch:0119, train_loss=2.66551, train_acc=0.99830, val_loss=3.81548, val_acc=0.93874, time=0.40200
Epoch:0120, train_loss=2.66543, train_acc=0.99830, val_loss=3.81548, val_acc=0.93874, time=0.48000
Epoch:0121, train_loss=2.66536, train_acc=0.99830, val_loss=3.81547, val_acc=0.93874, time=0.49199
Epoch:0122, train_loss=2.66529, train_acc=0.99830, val_loss=3.81547, val_acc=0.93874, time=0.46101
Epoch:0123, train_loss=2.66522, train_acc=0.99830, val_loss=3.81547, val_acc=0.93874, time=0.49900
Epoch:0124, train_loss=2.66515, train_acc=0.99830, val_loss=3.81547, val_acc=0.93874, time=0.43099
Epoch:0125, train_loss=2.66509, train_acc=0.99830, val_loss=3.81547, val_acc=0.93874, time=0.46201
Epoch:0126, train_loss=2.66502, train_acc=0.99830, val_loss=3.81546, val_acc=0.93874, time=0.47599
Epoch:0127, train_loss=2.66496, train_acc=0.99830, val_loss=3.81546, val_acc=0.93874, time=0.39100
Epoch:0128, train_loss=2.66490, train_acc=0.99830, val_loss=3.81546, val_acc=0.93874, time=0.50501
Epoch:0129, train_loss=2.66484, train_acc=0.99830, val_loss=3.81546, val_acc=0.93874, time=0.49200
Epoch:0130, train_loss=2.66478, train_acc=0.99830, val_loss=3.81546, val_acc=0.93874, time=0.36100
Epoch:0131, train_loss=2.66473, train_acc=0.99830, val_loss=3.81545, val_acc=0.93874, time=0.38599
Epoch:0132, train_loss=2.66467, train_acc=0.99830, val_loss=3.81545, val_acc=0.93874, time=0.39301
Epoch:0133, train_loss=2.66462, train_acc=0.99847, val_loss=3.81545, val_acc=0.93874, time=0.44199
Epoch:0134, train_loss=2.66457, train_acc=0.99847, val_loss=3.81545, val_acc=0.93874, time=0.44501
Epoch:0135, train_loss=2.66451, train_acc=0.99847, val_loss=3.81545, val_acc=0.93874, time=0.39800
Epoch:0136, train_loss=2.66446, train_acc=0.99847, val_loss=3.81545, val_acc=0.93874, time=0.37599
Epoch:0137, train_loss=2.66441, train_acc=0.99847, val_loss=3.81544, val_acc=0.93874, time=0.38701
Epoch:0138, train_loss=2.66436, train_acc=0.99847, val_loss=3.81544, val_acc=0.93874, time=0.45799
Epoch:0139, train_loss=2.66432, train_acc=0.99847, val_loss=3.81544, val_acc=0.93874, time=0.41700
Epoch:0140, train_loss=2.66427, train_acc=0.99847, val_loss=3.81544, val_acc=0.93874, time=0.44001
Epoch:0141, train_loss=2.66422, train_acc=0.99847, val_loss=3.81544, val_acc=0.93874, time=0.46601
Epoch:0142, train_loss=2.66418, train_acc=0.99847, val_loss=3.81544, val_acc=0.93874, time=0.46100
Epoch:0143, train_loss=2.66414, train_acc=0.99847, val_loss=3.81544, val_acc=0.93874, time=0.52299
Epoch:0144, train_loss=2.66409, train_acc=0.99847, val_loss=3.81544, val_acc=0.93874, time=0.38899
Epoch:0145, train_loss=2.66405, train_acc=0.99847, val_loss=3.81544, val_acc=0.93874, time=0.49301
Epoch:0146, train_loss=2.66401, train_acc=0.99847, val_loss=3.81543, val_acc=0.93874, time=0.53699
Epoch:0147, train_loss=2.66397, train_acc=0.99847, val_loss=3.81543, val_acc=0.93874, time=0.45001
Epoch:0148, train_loss=2.66393, train_acc=0.99847, val_loss=3.81543, val_acc=0.93874, time=0.41600
Epoch:0149, train_loss=2.66389, train_acc=0.99847, val_loss=3.81543, val_acc=0.93874, time=0.47299
Epoch:0150, train_loss=2.66385, train_acc=0.99847, val_loss=3.81543, val_acc=0.93874, time=0.37701
Epoch:0151, train_loss=2.66381, train_acc=0.99847, val_loss=3.81543, val_acc=0.93874, time=0.46399
Epoch:0152, train_loss=2.66378, train_acc=0.99847, val_loss=3.81543, val_acc=0.93874, time=0.51901
Epoch:0153, train_loss=2.66374, train_acc=0.99847, val_loss=3.81543, val_acc=0.93874, time=0.42500
Epoch:0154, train_loss=2.66371, train_acc=0.99847, val_loss=3.81543, val_acc=0.93874, time=0.38500
Epoch:0155, train_loss=2.66367, train_acc=0.99847, val_loss=3.81543, val_acc=0.93874, time=0.44900
Epoch:0156, train_loss=2.66364, train_acc=0.99847, val_loss=3.81543, val_acc=0.93874, time=0.42900
Epoch:0157, train_loss=2.66360, train_acc=0.99847, val_loss=3.81543, val_acc=0.93874, time=0.41500
Epoch:0158, train_loss=2.66357, train_acc=0.99847, val_loss=3.81543, val_acc=0.93874, time=0.39200
Epoch:0159, train_loss=2.66354, train_acc=0.99847, val_loss=3.81543, val_acc=0.93874, time=0.44399
Epoch:0160, train_loss=2.66351, train_acc=0.99847, val_loss=3.81543, val_acc=0.93874, time=0.40301
Early stopping...

Optimization Finished!

Test set results: loss= 3.44128, accuracy= 0.91706, time= 0.13199

Test Precision, Recall and F1-Score...
              precision    recall  f1-score   support

           0     0.9589    0.9898    0.9741      1083
           1     0.8667    0.9669    0.9141       121
           2     0.9543    0.9310    0.9425       696
           3     1.0000    0.8667    0.9286        15
           4     0.7647    0.8667    0.8125        15
           5     0.9333    0.8235    0.8750        17
           6     0.8571    0.6667    0.7500        36
           7     0.8519    0.9200    0.8846        25
           8     1.0000    0.6842    0.8125        19
           9     0.8333    0.7692    0.8000        13
          10     0.7826    0.8276    0.8045        87
          11     0.8421    0.8000    0.8205        20
          12     0.7113    0.9200    0.8023        75
          13     0.7576    0.8929    0.8197        28
          14     1.0000    0.7778    0.8750         9
          15     0.9565    1.0000    0.9778        22
          16     1.0000    0.8000    0.8889         5
          17     0.8182    0.7500    0.7826        12
          18     0.7763    0.7284    0.7516        81
          19     0.5714    0.8000    0.6667        10
          20     1.0000    1.0000    1.0000         2
          21     0.9231    1.0000    0.9600        12
          22     1.0000    1.0000    1.0000         1
          23     1.0000    0.7778    0.8750         9
          24     1.0000    0.3333    0.5000        12
          25     1.0000    0.6000    0.7500         5
          26     0.9091    1.0000    0.9524        10
          27     1.0000    0.9167    0.9565        12
          28     1.0000    0.3333    0.5000         3
          29     1.0000    1.0000    1.0000         3
          30     0.6667    0.4444    0.5333         9
          31     1.0000    1.0000    1.0000         9
          32     0.8750    0.8750    0.8750         8
          33     0.7857    1.0000    0.8800        11
          34     1.0000    0.2000    0.3333         5
          35     1.0000    0.5000    0.6667         4
          36     0.6000    0.7500    0.6667         4
          37     1.0000    0.3333    0.5000         3
          38     1.0000    1.0000    1.0000         4
          39     0.0000    0.0000    0.0000         1
          40     0.2000    0.1667    0.1818         6
          41     1.0000    0.8182    0.9000        11
          42     1.0000    0.8889    0.9412         9
          43     0.0000    0.0000    0.0000         6
          44     1.0000    1.0000    1.0000         1
          45     0.5000    1.0000    0.6667         1
          46     0.0000    0.0000    0.0000         1
          47     1.0000    0.1429    0.2500         7
          48     0.0000    0.0000    0.0000         1
          49     0.0000    0.0000    0.0000         2
          50     0.0000    0.0000    0.0000         4
          51     0.0000    0.0000    0.0000         3

    accuracy                         0.9171      2568
   macro avg     0.7634    0.6704    0.6879      2568
weighted avg     0.9154    0.9171    0.9117      2568


Macro average Test Precision, Recall and F1-Score...
(0.7633819283241032, 0.6704209608114938, 0.68792260868055, None)

Micro average Test Precision, Recall and F1-Score...
(0.9170560747663551, 0.9170560747663551, 0.9170560747663551, None)

Embeddings:
Word_embeddings: 8892
Train_doc_embeddings: 6532
Test_doc_embeddings: 2568

Elapsed time is 70.114866 seconds.
