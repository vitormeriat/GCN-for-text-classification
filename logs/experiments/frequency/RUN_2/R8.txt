
==================== Torch Seed: 2597757202800

Model parameters

Layer: layer1.W0 | Size: torch.Size([15362, 200])
Layer: layer2.W0 | Size: torch.Size([200, 8])

Data statistics

  Edges    Classes    Train samples    Val samples    Test samples
-------  ---------  ---------------  -------------  --------------
  15362          8             4937            548            2189

Epoch:0001, train_loss=2.10067, train_acc=0.03119, val_loss=2.06205, val_acc=0.68248, time=0.31401
Epoch:0002, train_loss=1.91986, train_acc=0.68483, val_loss=2.04973, val_acc=0.70803, time=0.31799
Epoch:0003, train_loss=1.80798, train_acc=0.72453, val_loss=2.04247, val_acc=0.74453, time=0.36200
Epoch:0004, train_loss=1.74259, train_acc=0.76099, val_loss=2.03787, val_acc=0.76460, time=0.29801
Epoch:0005, train_loss=1.70136, train_acc=0.78428, val_loss=2.03439, val_acc=0.77190, time=0.35500
Epoch:0006, train_loss=1.67041, train_acc=0.78935, val_loss=2.03134, val_acc=0.78467, time=0.41202
Epoch:0007, train_loss=1.64296, train_acc=0.79178, val_loss=2.02851, val_acc=0.79562, time=0.36200
Epoch:0008, train_loss=1.61705, train_acc=0.80231, val_loss=2.02596, val_acc=0.82117, time=0.42800
Epoch:0009, train_loss=1.59346, train_acc=0.83026, val_loss=2.02383, val_acc=0.85219, time=0.39201
Epoch:0010, train_loss=1.57340, train_acc=0.86166, val_loss=2.02213, val_acc=0.87591, time=0.35300
Epoch:0011, train_loss=1.55715, train_acc=0.88292, val_loss=2.02076, val_acc=0.88504, time=0.35200
Epoch:0012, train_loss=1.54388, train_acc=0.89366, val_loss=2.01960, val_acc=0.89599, time=0.37001
Epoch:0013, train_loss=1.53259, train_acc=0.90926, val_loss=2.01859, val_acc=0.90876, time=0.31799
Epoch:0014, train_loss=1.52268, train_acc=0.92181, val_loss=2.01771, val_acc=0.91241, time=0.30901
Epoch:0015, train_loss=1.51393, train_acc=0.93154, val_loss=2.01694, val_acc=0.92518, time=0.32699
Epoch:0016, train_loss=1.50627, train_acc=0.93721, val_loss=2.01627, val_acc=0.92883, time=0.37800
Epoch:0017, train_loss=1.49953, train_acc=0.94187, val_loss=2.01568, val_acc=0.92883, time=0.29103
Epoch:0018, train_loss=1.49348, train_acc=0.94410, val_loss=2.01515, val_acc=0.93248, time=0.35500
Epoch:0019, train_loss=1.48801, train_acc=0.94754, val_loss=2.01470, val_acc=0.93796, time=0.33498
Epoch:0020, train_loss=1.48314, train_acc=0.95037, val_loss=2.01429, val_acc=0.93796, time=0.38000
Epoch:0021, train_loss=1.47882, train_acc=0.95422, val_loss=2.01391, val_acc=0.93613, time=0.36800
Epoch:0022, train_loss=1.47487, train_acc=0.95746, val_loss=2.01353, val_acc=0.93978, time=0.36200
Epoch:0023, train_loss=1.47106, train_acc=0.95827, val_loss=2.01315, val_acc=0.93978, time=0.30400
Epoch:0024, train_loss=1.46726, train_acc=0.96010, val_loss=2.01276, val_acc=0.93978, time=0.36100
Epoch:0025, train_loss=1.46351, train_acc=0.96152, val_loss=2.01240, val_acc=0.94343, time=0.34900
Epoch:0026, train_loss=1.45995, train_acc=0.96314, val_loss=2.01207, val_acc=0.94891, time=0.29400
Epoch:0027, train_loss=1.45673, train_acc=0.96557, val_loss=2.01178, val_acc=0.94891, time=0.40900
Epoch:0028, train_loss=1.45394, train_acc=0.96678, val_loss=2.01154, val_acc=0.95073, time=0.37603
Epoch:0029, train_loss=1.45157, train_acc=0.96779, val_loss=2.01135, val_acc=0.95438, time=0.36498
Epoch:0030, train_loss=1.44957, train_acc=0.96820, val_loss=2.01118, val_acc=0.95620, time=0.29200
Epoch:0031, train_loss=1.44784, train_acc=0.96921, val_loss=2.01103, val_acc=0.95620, time=0.36001
Epoch:0032, train_loss=1.44627, train_acc=0.97083, val_loss=2.01090, val_acc=0.95803, time=0.34700
Epoch:0033, train_loss=1.44477, train_acc=0.97245, val_loss=2.01079, val_acc=0.95803, time=0.32800
Epoch:0034, train_loss=1.44333, train_acc=0.97488, val_loss=2.01069, val_acc=0.95803, time=0.39001
Epoch:0035, train_loss=1.44195, train_acc=0.97812, val_loss=2.01059, val_acc=0.95985, time=0.32100
Epoch:0036, train_loss=1.44067, train_acc=0.98015, val_loss=2.01050, val_acc=0.95985, time=0.30701
Epoch:0037, train_loss=1.43948, train_acc=0.98116, val_loss=2.01041, val_acc=0.95985, time=0.29800
Epoch:0038, train_loss=1.43836, train_acc=0.98218, val_loss=2.01033, val_acc=0.96168, time=0.37800
Epoch:0039, train_loss=1.43729, train_acc=0.98278, val_loss=2.01025, val_acc=0.96168, time=0.35100
Epoch:0040, train_loss=1.43625, train_acc=0.98339, val_loss=2.01016, val_acc=0.96350, time=0.33600
Epoch:0041, train_loss=1.43525, train_acc=0.98380, val_loss=2.01008, val_acc=0.96533, time=0.37601
Epoch:0042, train_loss=1.43432, train_acc=0.98501, val_loss=2.01000, val_acc=0.96898, time=0.39200
Epoch:0043, train_loss=1.43348, train_acc=0.98440, val_loss=2.00993, val_acc=0.96898, time=0.32001
Epoch:0044, train_loss=1.43272, train_acc=0.98501, val_loss=2.00986, val_acc=0.97080, time=0.37200
Epoch:0045, train_loss=1.43202, train_acc=0.98582, val_loss=2.00979, val_acc=0.97080, time=0.30901
Epoch:0046, train_loss=1.43134, train_acc=0.98582, val_loss=2.00973, val_acc=0.97080, time=0.33101
Epoch:0047, train_loss=1.43068, train_acc=0.98602, val_loss=2.00968, val_acc=0.97080, time=0.41200
Epoch:0048, train_loss=1.43005, train_acc=0.98683, val_loss=2.00964, val_acc=0.97080, time=0.35801
Epoch:0049, train_loss=1.42947, train_acc=0.98724, val_loss=2.00960, val_acc=0.97080, time=0.33302
Epoch:0050, train_loss=1.42893, train_acc=0.98825, val_loss=2.00957, val_acc=0.96898, time=0.40100
Epoch:0051, train_loss=1.42843, train_acc=0.98866, val_loss=2.00954, val_acc=0.96898, time=0.42001
Epoch:0052, train_loss=1.42794, train_acc=0.98906, val_loss=2.00951, val_acc=0.97080, time=0.37499
Epoch:0053, train_loss=1.42746, train_acc=0.98906, val_loss=2.00949, val_acc=0.97080, time=0.37900
Epoch:0054, train_loss=1.42700, train_acc=0.98906, val_loss=2.00946, val_acc=0.97080, time=0.40400
Epoch:0055, train_loss=1.42655, train_acc=0.99007, val_loss=2.00944, val_acc=0.97080, time=0.36203
Epoch:0056, train_loss=1.42613, train_acc=0.99048, val_loss=2.00941, val_acc=0.97080, time=0.28301
Epoch:0057, train_loss=1.42573, train_acc=0.99028, val_loss=2.00939, val_acc=0.96898, time=0.34199
Epoch:0058, train_loss=1.42534, train_acc=0.99089, val_loss=2.00937, val_acc=0.97080, time=0.40100
Epoch:0059, train_loss=1.42497, train_acc=0.99170, val_loss=2.00934, val_acc=0.97080, time=0.38700
Epoch:0060, train_loss=1.42460, train_acc=0.99170, val_loss=2.00932, val_acc=0.97080, time=0.38300
Epoch:0061, train_loss=1.42425, train_acc=0.99230, val_loss=2.00929, val_acc=0.97080, time=0.35801
Epoch:0062, train_loss=1.42392, train_acc=0.99230, val_loss=2.00927, val_acc=0.97080, time=0.38800
Epoch:0063, train_loss=1.42361, train_acc=0.99271, val_loss=2.00925, val_acc=0.97080, time=0.29901
Epoch:0064, train_loss=1.42331, train_acc=0.99291, val_loss=2.00923, val_acc=0.97080, time=0.38100
Epoch:0065, train_loss=1.42302, train_acc=0.99332, val_loss=2.00921, val_acc=0.97080, time=0.44500
Epoch:0066, train_loss=1.42273, train_acc=0.99352, val_loss=2.00920, val_acc=0.97080, time=0.37102
Epoch:0067, train_loss=1.42245, train_acc=0.99372, val_loss=2.00918, val_acc=0.97080, time=0.29200
Epoch:0068, train_loss=1.42218, train_acc=0.99372, val_loss=2.00917, val_acc=0.97080, time=0.29900
Epoch:0069, train_loss=1.42193, train_acc=0.99433, val_loss=2.00915, val_acc=0.97263, time=0.33800
Epoch:0070, train_loss=1.42168, train_acc=0.99433, val_loss=2.00914, val_acc=0.97080, time=0.33501
Epoch:0071, train_loss=1.42144, train_acc=0.99453, val_loss=2.00913, val_acc=0.97080, time=0.30501
Epoch:0072, train_loss=1.42121, train_acc=0.99534, val_loss=2.00912, val_acc=0.97080, time=0.35599
Epoch:0073, train_loss=1.42098, train_acc=0.99534, val_loss=2.00910, val_acc=0.97080, time=0.32301
Epoch:0074, train_loss=1.42076, train_acc=0.99534, val_loss=2.00909, val_acc=0.97080, time=0.28700
Epoch:0075, train_loss=1.42055, train_acc=0.99554, val_loss=2.00908, val_acc=0.97080, time=0.33700
Epoch:0076, train_loss=1.42035, train_acc=0.99575, val_loss=2.00907, val_acc=0.97080, time=0.32102
Epoch:0077, train_loss=1.42015, train_acc=0.99615, val_loss=2.00907, val_acc=0.97080, time=0.33400
Epoch:0078, train_loss=1.41996, train_acc=0.99635, val_loss=2.00906, val_acc=0.97080, time=0.29000
Epoch:0079, train_loss=1.41977, train_acc=0.99635, val_loss=2.00905, val_acc=0.97080, time=0.28702
Epoch:0080, train_loss=1.41959, train_acc=0.99615, val_loss=2.00904, val_acc=0.97080, time=0.33699
Epoch:0081, train_loss=1.41941, train_acc=0.99615, val_loss=2.00904, val_acc=0.97080, time=0.29599
Epoch:0082, train_loss=1.41925, train_acc=0.99615, val_loss=2.00903, val_acc=0.97080, time=0.28700
Epoch:0083, train_loss=1.41908, train_acc=0.99635, val_loss=2.00903, val_acc=0.97080, time=0.28301
Epoch:0084, train_loss=1.41892, train_acc=0.99635, val_loss=2.00902, val_acc=0.97080, time=0.28700
Epoch:0085, train_loss=1.41877, train_acc=0.99656, val_loss=2.00901, val_acc=0.97080, time=0.29101
Epoch:0086, train_loss=1.41862, train_acc=0.99656, val_loss=2.00901, val_acc=0.97080, time=0.28800
Epoch:0087, train_loss=1.41848, train_acc=0.99676, val_loss=2.00900, val_acc=0.97080, time=0.33000
Epoch:0088, train_loss=1.41834, train_acc=0.99676, val_loss=2.00900, val_acc=0.97080, time=0.32700
Epoch:0089, train_loss=1.41820, train_acc=0.99696, val_loss=2.00899, val_acc=0.97080, time=0.30300
Epoch:0090, train_loss=1.41807, train_acc=0.99696, val_loss=2.00898, val_acc=0.97080, time=0.29299
Epoch:0091, train_loss=1.41794, train_acc=0.99696, val_loss=2.00898, val_acc=0.97080, time=0.28900
Epoch:0092, train_loss=1.41782, train_acc=0.99696, val_loss=2.00897, val_acc=0.96898, time=0.37900
Epoch:0093, train_loss=1.41770, train_acc=0.99696, val_loss=2.00897, val_acc=0.96898, time=0.35601
Epoch:0094, train_loss=1.41758, train_acc=0.99696, val_loss=2.00896, val_acc=0.96898, time=0.28501
Epoch:0095, train_loss=1.41747, train_acc=0.99696, val_loss=2.00896, val_acc=0.96898, time=0.28800
Epoch:0096, train_loss=1.41736, train_acc=0.99696, val_loss=2.00895, val_acc=0.96898, time=0.28899
Epoch:0097, train_loss=1.41725, train_acc=0.99696, val_loss=2.00895, val_acc=0.96898, time=0.28801
Epoch:0098, train_loss=1.41715, train_acc=0.99696, val_loss=2.00894, val_acc=0.96898, time=0.28901
Epoch:0099, train_loss=1.41705, train_acc=0.99696, val_loss=2.00894, val_acc=0.96898, time=0.32902
Epoch:0100, train_loss=1.41695, train_acc=0.99737, val_loss=2.00894, val_acc=0.96898, time=0.32800
Epoch:0101, train_loss=1.41685, train_acc=0.99737, val_loss=2.00894, val_acc=0.96898, time=0.29005
Epoch:0102, train_loss=1.41676, train_acc=0.99757, val_loss=2.00894, val_acc=0.96898, time=0.34500
Epoch:0103, train_loss=1.41667, train_acc=0.99757, val_loss=2.00893, val_acc=0.96898, time=0.34499
Epoch:0104, train_loss=1.41658, train_acc=0.99757, val_loss=2.00893, val_acc=0.96898, time=0.29501
Epoch:0105, train_loss=1.41650, train_acc=0.99757, val_loss=2.00893, val_acc=0.96898, time=0.36300
Epoch:0106, train_loss=1.41641, train_acc=0.99757, val_loss=2.00893, val_acc=0.96898, time=0.38900
Epoch:0107, train_loss=1.41633, train_acc=0.99757, val_loss=2.00893, val_acc=0.96898, time=0.28900
Epoch:0108, train_loss=1.41625, train_acc=0.99757, val_loss=2.00893, val_acc=0.96898, time=0.28800
Epoch:0109, train_loss=1.41618, train_acc=0.99757, val_loss=2.00893, val_acc=0.96898, time=0.28600
Epoch:0110, train_loss=1.41610, train_acc=0.99777, val_loss=2.00893, val_acc=0.96898, time=0.28600
Epoch:0111, train_loss=1.41603, train_acc=0.99777, val_loss=2.00893, val_acc=0.96898, time=0.29100
Epoch:0112, train_loss=1.41596, train_acc=0.99777, val_loss=2.00893, val_acc=0.96898, time=0.36198
Epoch:0113, train_loss=1.41589, train_acc=0.99777, val_loss=2.00893, val_acc=0.96898, time=0.35400
Epoch:0114, train_loss=1.41582, train_acc=0.99797, val_loss=2.00893, val_acc=0.96898, time=0.30600
Epoch:0115, train_loss=1.41576, train_acc=0.99797, val_loss=2.00893, val_acc=0.96898, time=0.36201
Epoch:0116, train_loss=1.41569, train_acc=0.99797, val_loss=2.00893, val_acc=0.96898, time=0.28500
Epoch:0117, train_loss=1.41563, train_acc=0.99797, val_loss=2.00893, val_acc=0.96898, time=0.29000
Epoch:0118, train_loss=1.41557, train_acc=0.99818, val_loss=2.00893, val_acc=0.97080, time=0.30100
Epoch:0119, train_loss=1.41551, train_acc=0.99818, val_loss=2.00893, val_acc=0.97080, time=0.29400
Early stopping...

Optimization Finished!

Test set results: loss= 1.79918, accuracy= 0.96711, time= 0.10301

Test Precision, Recall and F1-Score...
              precision    recall  f1-score   support

           0     0.9853    0.9655    0.9753       696
           1     0.9781    0.9917    0.9849      1083
           2     0.9024    0.9867    0.9427        75
           3     0.9365    0.9752    0.9555       121
           4     0.8478    0.8966    0.8715        87
           5     0.9041    0.8148    0.8571        81
           6     0.9615    0.6944    0.8065        36
           7     1.0000    1.0000    1.0000        10

    accuracy                         0.9671      2189
   macro avg     0.9395    0.9156    0.9242      2189
weighted avg     0.9674    0.9671    0.9667      2189


Macro average Test Precision, Recall and F1-Score...
(0.9394875522923439, 0.9156114067132419, 0.9241799353055029, None)

Micro average Test Precision, Recall and F1-Score...
(0.9671082686158063, 0.9671082686158063, 0.9671082686158063, None)

Embeddings:
Word_embeddings: 7688
Train_doc_embeddings: 5485
Test_doc_embeddings: 2189

Elapsed time is 41.855925 seconds.
