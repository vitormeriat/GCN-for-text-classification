
==================== Torch Seed: 3795218346600

Model parameters

Layer: layer1.W0 | Size: torch.Size([61603, 200])
Layer: layer2.W0 | Size: torch.Size([200, 20])

Data statistics

  Edges    Classes    Train samples    Val samples    Test samples
-------  ---------  ---------------  -------------  --------------
  61603         20            10183           1131            7532

Epoch:0001, train_loss=3.00249, train_acc=0.04920, val_loss=2.99303, val_acc=0.31034, time=3.99000
Epoch:0002, train_loss=2.97198, train_acc=0.30963, val_loss=2.99010, val_acc=0.52166, time=3.65200
Epoch:0003, train_loss=2.94514, train_acc=0.53265, val_loss=2.98727, val_acc=0.66313, time=3.70298
Epoch:0004, train_loss=2.91897, train_acc=0.67171, val_loss=2.98439, val_acc=0.72679, time=4.11999
Epoch:0005, train_loss=2.89222, train_acc=0.75145, val_loss=2.98141, val_acc=0.77277, time=3.71601
Epoch:0006, train_loss=2.86466, train_acc=0.79937, val_loss=2.97837, val_acc=0.80018, time=3.64800
Epoch:0007, train_loss=2.83650, train_acc=0.82844, val_loss=2.97532, val_acc=0.82140, time=3.79699
Epoch:0008, train_loss=2.80821, train_acc=0.85250, val_loss=2.97232, val_acc=0.84350, time=3.74798
Epoch:0009, train_loss=2.78040, train_acc=0.87499, val_loss=2.96945, val_acc=0.85500, time=3.83300
Epoch:0010, train_loss=2.75370, train_acc=0.88982, val_loss=2.96675, val_acc=0.86118, time=3.57598
Epoch:0011, train_loss=2.72871, train_acc=0.90091, val_loss=2.96428, val_acc=0.87179, time=3.68799
Epoch:0012, train_loss=2.70582, train_acc=0.90946, val_loss=2.96204, val_acc=0.88329, time=3.66200
Epoch:0013, train_loss=2.68521, train_acc=0.91456, val_loss=2.96004, val_acc=0.88859, time=3.59798
Epoch:0014, train_loss=2.66671, train_acc=0.91888, val_loss=2.95824, val_acc=0.89744, time=3.75999
Epoch:0015, train_loss=2.65011, train_acc=0.92271, val_loss=2.95664, val_acc=0.89920, time=3.78999
Epoch:0016, train_loss=2.63538, train_acc=0.92605, val_loss=2.95525, val_acc=0.90186, time=4.03900
Epoch:0017, train_loss=2.62250, train_acc=0.92841, val_loss=2.95406, val_acc=0.90274, time=3.69499
Epoch:0018, train_loss=2.61132, train_acc=0.93028, val_loss=2.95303, val_acc=0.90274, time=3.87799
Epoch:0019, train_loss=2.60149, train_acc=0.93214, val_loss=2.95213, val_acc=0.90451, time=3.80100
Epoch:0020, train_loss=2.59280, train_acc=0.93470, val_loss=2.95135, val_acc=0.90805, time=3.75198
Epoch:0021, train_loss=2.58515, train_acc=0.93715, val_loss=2.95067, val_acc=0.90981, time=3.49300
Epoch:0022, train_loss=2.57843, train_acc=0.94108, val_loss=2.95007, val_acc=0.91247, time=3.80500
Epoch:0023, train_loss=2.57246, train_acc=0.94285, val_loss=2.94953, val_acc=0.91247, time=3.77500
Epoch:0024, train_loss=2.56714, train_acc=0.94520, val_loss=2.94907, val_acc=0.91335, time=3.72198
Epoch:0025, train_loss=2.56240, train_acc=0.94658, val_loss=2.94867, val_acc=0.91335, time=3.83399
Epoch:0026, train_loss=2.55816, train_acc=0.94884, val_loss=2.94832, val_acc=0.91512, time=3.65299
Epoch:0027, train_loss=2.55435, train_acc=0.95168, val_loss=2.94801, val_acc=0.91600, time=3.74699
Epoch:0028, train_loss=2.55091, train_acc=0.95384, val_loss=2.94773, val_acc=0.91954, time=3.61701
Epoch:0029, train_loss=2.54781, train_acc=0.95522, val_loss=2.94749, val_acc=0.91954, time=3.67800
Epoch:0030, train_loss=2.54499, train_acc=0.95748, val_loss=2.94726, val_acc=0.92042, time=3.76497
Epoch:0031, train_loss=2.54239, train_acc=0.95925, val_loss=2.94706, val_acc=0.92396, time=3.67400
Epoch:0032, train_loss=2.54003, train_acc=0.96121, val_loss=2.94689, val_acc=0.92485, time=4.32399
Epoch:0033, train_loss=2.53789, train_acc=0.96249, val_loss=2.94673, val_acc=0.92485, time=3.55399
Epoch:0034, train_loss=2.53591, train_acc=0.96376, val_loss=2.94658, val_acc=0.92485, time=3.75499
Epoch:0035, train_loss=2.53408, train_acc=0.96612, val_loss=2.94646, val_acc=0.92308, time=3.61399
Epoch:0036, train_loss=2.53239, train_acc=0.96848, val_loss=2.94636, val_acc=0.92308, time=3.71300
Epoch:0037, train_loss=2.53083, train_acc=0.96975, val_loss=2.94626, val_acc=0.92485, time=3.58198
Epoch:0038, train_loss=2.52938, train_acc=0.97172, val_loss=2.94616, val_acc=0.92573, time=3.97408
Epoch:0039, train_loss=2.52802, train_acc=0.97407, val_loss=2.94607, val_acc=0.92661, time=3.65300
Epoch:0040, train_loss=2.52676, train_acc=0.97555, val_loss=2.94599, val_acc=0.92750, time=3.56598
Epoch:0041, train_loss=2.52558, train_acc=0.97732, val_loss=2.94592, val_acc=0.92750, time=3.81399
Epoch:0042, train_loss=2.52448, train_acc=0.97810, val_loss=2.94585, val_acc=0.92927, time=3.63399
Epoch:0043, train_loss=2.52345, train_acc=0.97938, val_loss=2.94580, val_acc=0.93015, time=3.83001
Epoch:0044, train_loss=2.52248, train_acc=0.98036, val_loss=2.94575, val_acc=0.92838, time=3.62999
Epoch:0045, train_loss=2.52157, train_acc=0.98115, val_loss=2.94571, val_acc=0.92750, time=3.67101
Epoch:0046, train_loss=2.52071, train_acc=0.98193, val_loss=2.94568, val_acc=0.92485, time=3.62100
Epoch:0047, train_loss=2.51990, train_acc=0.98281, val_loss=2.94564, val_acc=0.92485, time=3.68300
Epoch:0048, train_loss=2.51914, train_acc=0.98340, val_loss=2.94561, val_acc=0.92396, time=3.56899
Epoch:0049, train_loss=2.51842, train_acc=0.98478, val_loss=2.94558, val_acc=0.92750, time=3.75801
Epoch:0050, train_loss=2.51774, train_acc=0.98537, val_loss=2.94556, val_acc=0.92927, time=3.57399
Epoch:0051, train_loss=2.51710, train_acc=0.98596, val_loss=2.94553, val_acc=0.92927, time=3.75198
Epoch:0052, train_loss=2.51649, train_acc=0.98733, val_loss=2.94552, val_acc=0.93015, time=3.69898
Epoch:0053, train_loss=2.51591, train_acc=0.98802, val_loss=2.94550, val_acc=0.93015, time=3.64901
Epoch:0054, train_loss=2.51537, train_acc=0.98831, val_loss=2.94549, val_acc=0.93015, time=3.81198
Epoch:0055, train_loss=2.51485, train_acc=0.98930, val_loss=2.94548, val_acc=0.93015, time=3.83100
Epoch:0056, train_loss=2.51435, train_acc=0.99008, val_loss=2.94547, val_acc=0.93015, time=3.84899
Epoch:0057, train_loss=2.51389, train_acc=0.99047, val_loss=2.94545, val_acc=0.93015, time=3.89299
Epoch:0058, train_loss=2.51344, train_acc=0.99146, val_loss=2.94545, val_acc=0.93015, time=3.64900
Epoch:0059, train_loss=2.51302, train_acc=0.99185, val_loss=2.94544, val_acc=0.93103, time=3.60398
Epoch:0060, train_loss=2.51261, train_acc=0.99254, val_loss=2.94544, val_acc=0.93103, time=3.63899
Epoch:0061, train_loss=2.51223, train_acc=0.99293, val_loss=2.94543, val_acc=0.93103, time=3.96800
Epoch:0062, train_loss=2.51186, train_acc=0.99342, val_loss=2.94543, val_acc=0.93103, time=4.33498
Epoch:0063, train_loss=2.51152, train_acc=0.99342, val_loss=2.94543, val_acc=0.93103, time=3.66700
Epoch:0064, train_loss=2.51118, train_acc=0.99391, val_loss=2.94543, val_acc=0.93015, time=3.66899
Epoch:0065, train_loss=2.51086, train_acc=0.99421, val_loss=2.94542, val_acc=0.92927, time=3.82700
Epoch:0066, train_loss=2.51056, train_acc=0.99470, val_loss=2.94542, val_acc=0.92927, time=3.88498
Epoch:0067, train_loss=2.51027, train_acc=0.99480, val_loss=2.94541, val_acc=0.93015, time=3.83798
Epoch:0068, train_loss=2.50999, train_acc=0.99499, val_loss=2.94541, val_acc=0.93015, time=3.85399
Epoch:0069, train_loss=2.50972, train_acc=0.99499, val_loss=2.94541, val_acc=0.93015, time=3.63001
Epoch:0070, train_loss=2.50946, train_acc=0.99519, val_loss=2.94542, val_acc=0.93103, time=3.67298
Epoch:0071, train_loss=2.50921, train_acc=0.99568, val_loss=2.94542, val_acc=0.93192, time=3.82900
Epoch:0072, train_loss=2.50897, train_acc=0.99588, val_loss=2.94542, val_acc=0.93103, time=3.69399
Early stopping...

Optimization Finished!

Test set results: loss= 2.69021, accuracy= 0.86192, time= 1.16199

Test Precision, Recall and F1-Score...
              precision    recall  f1-score   support

           0     0.8913    0.9472    0.9184       398
           1     0.7506    0.8046    0.7767       389
           2     0.8694    0.7931    0.8295       319
           3     0.9351    0.9091    0.9219       396
           4     0.8347    0.6677    0.7419       310
           5     0.8006    0.7030    0.7486       394
           6     0.9545    0.9521    0.9533       397
           7     0.8980    0.9162    0.9070       394
           8     0.9118    0.9394    0.9254       396
           9     0.9650    0.9674    0.9662       399
          10     0.9972    0.9548    0.9755       376
          11     0.8447    0.8127    0.8284       395
          12     0.7788    0.8667    0.8204       390
          13     0.8247    0.8142    0.8195       393
          14     0.7210    0.7781    0.7485       392
          15     0.7904    0.9011    0.8421       364
          16     0.9110    0.8788    0.8946       396
          17     0.8244    0.8416    0.8329       385
          18     0.9579    0.9724    0.9651       398
          19     0.7511    0.7092    0.7295       251

    accuracy                         0.8619      7532
   macro avg     0.8606    0.8565    0.8573      7532
weighted avg     0.8633    0.8619    0.8614      7532


Macro average Test Precision, Recall and F1-Score...
(0.8606105741556966, 0.8564717850463588, 0.8572733433456156, None)

Micro average Test Precision, Recall and F1-Score...
(0.8619224641529474, 0.8619224641529474, 0.8619224641529474, None)

Embeddings:
Word_embeddings: 42757
Train_doc_embeddings: 11314
Test_doc_embeddings: 7532

Elapsed time is 288.306306 seconds.
