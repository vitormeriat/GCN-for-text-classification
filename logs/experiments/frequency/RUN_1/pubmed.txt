
==================== Torch Seed: 4899719632800

Model parameters

Layer: layer1.W0 | Size: torch.Size([20169, 200])
Layer: layer2.W0 | Size: torch.Size([200, 3])

Data statistics

  Edges    Classes    Train samples    Val samples    Test samples
-------  ---------  ---------------  -------------  --------------
  20169          3            12422           1380            5915

Epoch:0001, train_loss=1.11134, train_acc=0.38504, val_loss=1.09744, val_acc=0.38913, time=0.43702
Epoch:0002, train_loss=1.07757, train_acc=0.39760, val_loss=1.09104, val_acc=0.58913, time=0.44300
Epoch:0003, train_loss=1.02227, train_acc=0.58654, val_loss=1.08503, val_acc=0.58116, time=0.38901
Epoch:0004, train_loss=0.97090, train_acc=0.58090, val_loss=1.08031, val_acc=0.65942, time=0.39201
Epoch:0005, train_loss=0.93011, train_acc=0.65521, val_loss=1.07778, val_acc=0.71594, time=0.39001
Epoch:0006, train_loss=0.90837, train_acc=0.71059, val_loss=1.07565, val_acc=0.72174, time=0.43700
Epoch:0007, train_loss=0.88982, train_acc=0.71446, val_loss=1.07265, val_acc=0.72971, time=0.39002
Epoch:0008, train_loss=0.86306, train_acc=0.72339, val_loss=1.06956, val_acc=0.74928, time=0.38802
Epoch:0009, train_loss=0.83534, train_acc=0.74328, val_loss=1.06721, val_acc=0.74710, time=0.44401
Epoch:0010, train_loss=0.81410, train_acc=0.75374, val_loss=1.06523, val_acc=0.75652, time=0.38999
Epoch:0011, train_loss=0.79575, train_acc=0.75777, val_loss=1.06365, val_acc=0.76667, time=0.40901
Epoch:0012, train_loss=0.78100, train_acc=0.75914, val_loss=1.06229, val_acc=0.77609, time=0.39200
Epoch:0013, train_loss=0.76869, train_acc=0.76461, val_loss=1.06053, val_acc=0.78333, time=0.39800
Epoch:0014, train_loss=0.75364, train_acc=0.78248, val_loss=1.05912, val_acc=0.79348, time=0.45400
Epoch:0015, train_loss=0.74218, train_acc=0.79762, val_loss=1.05837, val_acc=0.79638, time=0.39300
Epoch:0016, train_loss=0.73628, train_acc=0.79979, val_loss=1.05759, val_acc=0.80507, time=0.48000
Epoch:0017, train_loss=0.72938, train_acc=0.80406, val_loss=1.05696, val_acc=0.80435, time=0.38900
Epoch:0018, train_loss=0.72341, train_acc=0.80349, val_loss=1.05635, val_acc=0.80507, time=0.44299
Epoch:0019, train_loss=0.71789, train_acc=0.80543, val_loss=1.05596, val_acc=0.80507, time=0.57500
Epoch:0020, train_loss=0.71465, train_acc=0.80776, val_loss=1.05581, val_acc=0.80507, time=0.52299
Epoch:0021, train_loss=0.71361, train_acc=0.80744, val_loss=1.05540, val_acc=0.80797, time=0.38800
Epoch:0022, train_loss=0.70979, train_acc=0.81090, val_loss=1.05502, val_acc=0.80507, time=0.38900
Epoch:0023, train_loss=0.70634, train_acc=0.81154, val_loss=1.05459, val_acc=0.81087, time=0.43400
Epoch:0024, train_loss=0.70313, train_acc=0.81227, val_loss=1.05427, val_acc=0.81884, time=0.42700
Epoch:0025, train_loss=0.70095, train_acc=0.81372, val_loss=1.05392, val_acc=0.82174, time=0.47701
Epoch:0026, train_loss=0.69805, train_acc=0.81654, val_loss=1.05351, val_acc=0.82391, time=0.46900
Epoch:0027, train_loss=0.69400, train_acc=0.82000, val_loss=1.05326, val_acc=0.82971, time=0.40199
Epoch:0028, train_loss=0.69141, train_acc=0.82459, val_loss=1.05295, val_acc=0.83333, time=0.46000
Epoch:0029, train_loss=0.68898, train_acc=0.82716, val_loss=1.05267, val_acc=0.83333, time=0.43001
Epoch:0030, train_loss=0.68703, train_acc=0.82917, val_loss=1.05233, val_acc=0.83261, time=0.38801
Epoch:0031, train_loss=0.68419, train_acc=0.83159, val_loss=1.05206, val_acc=0.83478, time=0.38301
Epoch:0032, train_loss=0.68191, train_acc=0.83207, val_loss=1.05182, val_acc=0.83333, time=0.38701
Epoch:0033, train_loss=0.68016, train_acc=0.83344, val_loss=1.05153, val_acc=0.83696, time=0.38400
Epoch:0034, train_loss=0.67822, train_acc=0.83537, val_loss=1.05126, val_acc=0.83841, time=0.38300
Epoch:0035, train_loss=0.67607, train_acc=0.83674, val_loss=1.05103, val_acc=0.84058, time=0.42299
Epoch:0036, train_loss=0.67371, train_acc=0.83747, val_loss=1.05087, val_acc=0.84058, time=0.38703
Epoch:0037, train_loss=0.67209, train_acc=0.83924, val_loss=1.05064, val_acc=0.84203, time=0.38400
Epoch:0038, train_loss=0.67024, train_acc=0.84149, val_loss=1.05042, val_acc=0.84348, time=0.43399
Epoch:0039, train_loss=0.66849, train_acc=0.84069, val_loss=1.05020, val_acc=0.84348, time=0.38301
Epoch:0040, train_loss=0.66656, train_acc=0.84141, val_loss=1.05006, val_acc=0.84710, time=0.46699
Epoch:0041, train_loss=0.66518, train_acc=0.84254, val_loss=1.04990, val_acc=0.84783, time=0.38901
Epoch:0042, train_loss=0.66375, train_acc=0.84350, val_loss=1.04974, val_acc=0.85000, time=0.43400
Epoch:0043, train_loss=0.66232, train_acc=0.84487, val_loss=1.04962, val_acc=0.85290, time=0.46499
Epoch:0044, train_loss=0.66081, train_acc=0.84560, val_loss=1.04956, val_acc=0.85000, time=0.38301
Epoch:0045, train_loss=0.65961, train_acc=0.84672, val_loss=1.04946, val_acc=0.85000, time=0.40900
Epoch:0046, train_loss=0.65842, train_acc=0.84672, val_loss=1.04933, val_acc=0.85362, time=0.38400
Epoch:0047, train_loss=0.65721, train_acc=0.84769, val_loss=1.04921, val_acc=0.85435, time=0.38200
Epoch:0048, train_loss=0.65599, train_acc=0.84930, val_loss=1.04913, val_acc=0.85290, time=0.41000
Epoch:0049, train_loss=0.65496, train_acc=0.85002, val_loss=1.04904, val_acc=0.85725, time=0.42100
Epoch:0050, train_loss=0.65396, train_acc=0.85123, val_loss=1.04894, val_acc=0.85870, time=0.45399
Epoch:0051, train_loss=0.65294, train_acc=0.85276, val_loss=1.04886, val_acc=0.85725, time=0.41600
Epoch:0052, train_loss=0.65192, train_acc=0.85300, val_loss=1.04881, val_acc=0.85725, time=0.39599
Epoch:0053, train_loss=0.65104, train_acc=0.85373, val_loss=1.04872, val_acc=0.85870, time=0.57101
Epoch:0054, train_loss=0.65015, train_acc=0.85485, val_loss=1.04862, val_acc=0.85725, time=0.38599
Epoch:0055, train_loss=0.64928, train_acc=0.85542, val_loss=1.04854, val_acc=0.86014, time=0.45200
Epoch:0056, train_loss=0.64841, train_acc=0.85663, val_loss=1.04848, val_acc=0.86087, time=0.38001
Epoch:0057, train_loss=0.64766, train_acc=0.85751, val_loss=1.04840, val_acc=0.86087, time=0.49999
Epoch:0058, train_loss=0.64688, train_acc=0.85856, val_loss=1.04833, val_acc=0.86087, time=0.38102
Epoch:0059, train_loss=0.64611, train_acc=0.85968, val_loss=1.04829, val_acc=0.86232, time=0.42600
Epoch:0060, train_loss=0.64535, train_acc=0.85912, val_loss=1.04824, val_acc=0.86304, time=0.38301
Epoch:0061, train_loss=0.64464, train_acc=0.85952, val_loss=1.04817, val_acc=0.86232, time=0.37900
Epoch:0062, train_loss=0.64391, train_acc=0.85993, val_loss=1.04811, val_acc=0.86522, time=0.43000
Epoch:0063, train_loss=0.64317, train_acc=0.86017, val_loss=1.04807, val_acc=0.86594, time=0.38000
Epoch:0064, train_loss=0.64251, train_acc=0.86089, val_loss=1.04801, val_acc=0.86667, time=0.39199
Epoch:0065, train_loss=0.64185, train_acc=0.86105, val_loss=1.04797, val_acc=0.86667, time=0.38800
Epoch:0066, train_loss=0.64119, train_acc=0.86162, val_loss=1.04794, val_acc=0.86667, time=0.38401
Epoch:0067, train_loss=0.64056, train_acc=0.86250, val_loss=1.04789, val_acc=0.86594, time=0.43900
Epoch:0068, train_loss=0.63995, train_acc=0.86339, val_loss=1.04784, val_acc=0.86594, time=0.38000
Epoch:0069, train_loss=0.63935, train_acc=0.86315, val_loss=1.04780, val_acc=0.86739, time=0.38100
Epoch:0070, train_loss=0.63874, train_acc=0.86395, val_loss=1.04774, val_acc=0.86667, time=0.38499
Epoch:0071, train_loss=0.63816, train_acc=0.86347, val_loss=1.04769, val_acc=0.86377, time=0.38000
Epoch:0072, train_loss=0.63760, train_acc=0.86371, val_loss=1.04765, val_acc=0.86594, time=0.42801
Epoch:0073, train_loss=0.63704, train_acc=0.86371, val_loss=1.04761, val_acc=0.86667, time=0.47600
Epoch:0074, train_loss=0.63650, train_acc=0.86403, val_loss=1.04756, val_acc=0.86739, time=0.38399
Epoch:0075, train_loss=0.63598, train_acc=0.86443, val_loss=1.04753, val_acc=0.86812, time=0.44200
Epoch:0076, train_loss=0.63546, train_acc=0.86476, val_loss=1.04749, val_acc=0.86739, time=0.39201
Epoch:0077, train_loss=0.63494, train_acc=0.86532, val_loss=1.04746, val_acc=0.86957, time=0.45600
Epoch:0078, train_loss=0.63444, train_acc=0.86540, val_loss=1.04744, val_acc=0.86667, time=0.50801
Epoch:0079, train_loss=0.63395, train_acc=0.86556, val_loss=1.04741, val_acc=0.86884, time=0.39900
Epoch:0080, train_loss=0.63345, train_acc=0.86588, val_loss=1.04739, val_acc=0.86957, time=0.43100
Epoch:0081, train_loss=0.63298, train_acc=0.86596, val_loss=1.04737, val_acc=0.87029, time=0.38199
Epoch:0082, train_loss=0.63252, train_acc=0.86588, val_loss=1.04733, val_acc=0.86957, time=0.42000
Epoch:0083, train_loss=0.63206, train_acc=0.86629, val_loss=1.04731, val_acc=0.86884, time=0.39701
Epoch:0084, train_loss=0.63161, train_acc=0.86669, val_loss=1.04729, val_acc=0.86812, time=0.38299
Epoch:0085, train_loss=0.63116, train_acc=0.86685, val_loss=1.04726, val_acc=0.86884, time=0.37902
Epoch:0086, train_loss=0.63073, train_acc=0.86741, val_loss=1.04725, val_acc=0.86957, time=0.38100
Epoch:0087, train_loss=0.63030, train_acc=0.86709, val_loss=1.04721, val_acc=0.86957, time=0.39299
Epoch:0088, train_loss=0.62988, train_acc=0.86773, val_loss=1.04719, val_acc=0.87029, time=0.44001
Epoch:0089, train_loss=0.62946, train_acc=0.86773, val_loss=1.04716, val_acc=0.86884, time=0.39700
Epoch:0090, train_loss=0.62905, train_acc=0.86854, val_loss=1.04715, val_acc=0.86884, time=0.39101
Epoch:0091, train_loss=0.62865, train_acc=0.86894, val_loss=1.04713, val_acc=0.86957, time=0.38000
Epoch:0092, train_loss=0.62825, train_acc=0.86918, val_loss=1.04711, val_acc=0.86957, time=0.40399
Epoch:0093, train_loss=0.62787, train_acc=0.86934, val_loss=1.04709, val_acc=0.86957, time=0.38002
Epoch:0094, train_loss=0.62748, train_acc=0.86910, val_loss=1.04707, val_acc=0.86884, time=0.37900
Epoch:0095, train_loss=0.62711, train_acc=0.86991, val_loss=1.04706, val_acc=0.86957, time=0.43100
Epoch:0096, train_loss=0.62674, train_acc=0.87007, val_loss=1.04703, val_acc=0.86812, time=0.38000
Epoch:0097, train_loss=0.62638, train_acc=0.87087, val_loss=1.04704, val_acc=0.86884, time=0.42099
Epoch:0098, train_loss=0.62604, train_acc=0.87112, val_loss=1.04700, val_acc=0.86812, time=0.50300
Epoch:0099, train_loss=0.62572, train_acc=0.87095, val_loss=1.04703, val_acc=0.86667, time=0.50901
Epoch:0100, train_loss=0.62546, train_acc=0.87152, val_loss=1.04697, val_acc=0.86449, time=0.39400
Epoch:0101, train_loss=0.62537, train_acc=0.87232, val_loss=1.04715, val_acc=0.86739, time=0.45900
Early stopping...

Optimization Finished!

Test set results: loss= 0.88609, accuracy= 0.85816, time= 0.13300

Test Precision, Recall and F1-Score...
              precision    recall  f1-score   support

           0     0.8582    0.8511    0.8546      1202
           1     0.8798    0.8227    0.8503      2357
           2     0.8392    0.8973    0.8673      2356

    accuracy                         0.8582      5915
   macro avg     0.8591    0.8570    0.8574      5915
weighted avg     0.8592    0.8582    0.8579      5915


Macro average Test Precision, Recall and F1-Score...
(0.8590691517678472, 0.8570069935772385, 0.8573902601499505, None)

Micro average Test Precision, Recall and F1-Score...
(0.8581572273879966, 0.8581572273879966, 0.8581572273879965, None)

Embeddings:
Word_embeddings: 452
Train_doc_embeddings: 13802
Test_doc_embeddings: 5915

Elapsed time is 44.278881 seconds.
