
==================== Torch Seed: 3475858891000

Model parameters

Layer: layer1.W0 | Size: torch.Size([61603, 200])
Layer: layer2.W0 | Size: torch.Size([200, 20])

Data statistics

  Edges    Classes    Train samples    Val samples    Test samples
-------  ---------  ---------------  -------------  --------------
  61603         20            10183           1131            7532

Epoch:0001, train_loss=3.00552, train_acc=0.05008, val_loss=2.99331, val_acc=0.29973, time=3.61701
Epoch:0002, train_loss=2.97434, train_acc=0.29343, val_loss=2.99047, val_acc=0.55084, time=3.82198
Epoch:0003, train_loss=2.94788, train_acc=0.56280, val_loss=2.98781, val_acc=0.64810, time=3.71300
Epoch:0004, train_loss=2.92279, train_acc=0.68605, val_loss=2.98512, val_acc=0.70645, time=3.71801
Epoch:0005, train_loss=2.89751, train_acc=0.73819, val_loss=2.98233, val_acc=0.74094, time=3.61298
Epoch:0006, train_loss=2.87147, train_acc=0.77197, val_loss=2.97941, val_acc=0.76835, time=3.71000
Epoch:0007, train_loss=2.84462, train_acc=0.79947, val_loss=2.97642, val_acc=0.79841, time=3.60299
Epoch:0008, train_loss=2.81722, train_acc=0.82981, val_loss=2.97341, val_acc=0.83466, time=3.53398
Epoch:0009, train_loss=2.78980, train_acc=0.85878, val_loss=2.97047, val_acc=0.86472, time=3.67501
Epoch:0010, train_loss=2.76304, train_acc=0.88451, val_loss=2.96768, val_acc=0.87975, time=3.79999
Epoch:0011, train_loss=2.73760, train_acc=0.90140, val_loss=2.96511, val_acc=0.89213, time=3.73500
Epoch:0012, train_loss=2.71400, train_acc=0.91103, val_loss=2.96278, val_acc=0.89302, time=4.55400
Epoch:0013, train_loss=2.69250, train_acc=0.91613, val_loss=2.96070, val_acc=0.89567, time=3.69899
Epoch:0014, train_loss=2.67307, train_acc=0.92095, val_loss=2.95883, val_acc=0.90186, time=3.71800
Epoch:0015, train_loss=2.65557, train_acc=0.92350, val_loss=2.95716, val_acc=0.90451, time=3.68799
Epoch:0016, train_loss=2.63993, train_acc=0.92694, val_loss=2.95569, val_acc=0.90716, time=3.98701
Epoch:0017, train_loss=2.62613, train_acc=0.93096, val_loss=2.95441, val_acc=0.90893, time=3.67700
Epoch:0018, train_loss=2.61406, train_acc=0.93312, val_loss=2.95329, val_acc=0.90805, time=3.57898
Epoch:0019, train_loss=2.60357, train_acc=0.93411, val_loss=2.95233, val_acc=0.91070, time=3.63301
Epoch:0020, train_loss=2.59442, train_acc=0.93607, val_loss=2.95149, val_acc=0.91335, time=3.52299
Epoch:0021, train_loss=2.58641, train_acc=0.93813, val_loss=2.95077, val_acc=0.91158, time=3.67298
Epoch:0022, train_loss=2.57937, train_acc=0.94078, val_loss=2.95015, val_acc=0.91335, time=3.75401
Epoch:0023, train_loss=2.57315, train_acc=0.94353, val_loss=2.94960, val_acc=0.91512, time=3.82200
Epoch:0024, train_loss=2.56763, train_acc=0.94510, val_loss=2.94913, val_acc=0.91600, time=3.66899
Epoch:0025, train_loss=2.56274, train_acc=0.94756, val_loss=2.94871, val_acc=0.91424, time=3.49299
Epoch:0026, train_loss=2.55840, train_acc=0.94962, val_loss=2.94834, val_acc=0.91600, time=3.55800
Epoch:0027, train_loss=2.55455, train_acc=0.95247, val_loss=2.94801, val_acc=0.91777, time=3.47300
Epoch:0028, train_loss=2.55108, train_acc=0.95483, val_loss=2.94772, val_acc=0.92131, time=3.75698
Epoch:0029, train_loss=2.54794, train_acc=0.95699, val_loss=2.94747, val_acc=0.92042, time=3.60201
Epoch:0030, train_loss=2.54507, train_acc=0.95934, val_loss=2.94725, val_acc=0.92219, time=3.64800
Epoch:0031, train_loss=2.54247, train_acc=0.96101, val_loss=2.94706, val_acc=0.92131, time=3.56700
Epoch:0032, train_loss=2.54011, train_acc=0.96219, val_loss=2.94690, val_acc=0.92131, time=3.54100
Epoch:0033, train_loss=2.53796, train_acc=0.96317, val_loss=2.94674, val_acc=0.92219, time=3.75999
Epoch:0034, train_loss=2.53597, train_acc=0.96524, val_loss=2.94659, val_acc=0.92219, time=3.77600
Epoch:0035, train_loss=2.53412, train_acc=0.96740, val_loss=2.94647, val_acc=0.92308, time=3.52199
Epoch:0036, train_loss=2.53241, train_acc=0.96877, val_loss=2.94635, val_acc=0.92308, time=3.70800
Epoch:0037, train_loss=2.53084, train_acc=0.97034, val_loss=2.94625, val_acc=0.92219, time=3.74398
Epoch:0038, train_loss=2.52940, train_acc=0.97142, val_loss=2.94616, val_acc=0.92396, time=3.73299
Epoch:0039, train_loss=2.52804, train_acc=0.97329, val_loss=2.94608, val_acc=0.92308, time=3.83199
Epoch:0040, train_loss=2.52678, train_acc=0.97417, val_loss=2.94601, val_acc=0.92308, time=3.70999
Epoch:0041, train_loss=2.52561, train_acc=0.97604, val_loss=2.94594, val_acc=0.92219, time=4.02602
Epoch:0042, train_loss=2.52451, train_acc=0.97712, val_loss=2.94588, val_acc=0.92396, time=4.32898
Epoch:0043, train_loss=2.52349, train_acc=0.97840, val_loss=2.94582, val_acc=0.92485, time=3.67800
Epoch:0044, train_loss=2.52252, train_acc=0.97957, val_loss=2.94577, val_acc=0.92485, time=3.81399
Epoch:0045, train_loss=2.52162, train_acc=0.98046, val_loss=2.94572, val_acc=0.92573, time=4.33099
Epoch:0046, train_loss=2.52078, train_acc=0.98134, val_loss=2.94568, val_acc=0.92927, time=3.65801
Epoch:0047, train_loss=2.51998, train_acc=0.98252, val_loss=2.94565, val_acc=0.93015, time=3.78300
Epoch:0048, train_loss=2.51922, train_acc=0.98370, val_loss=2.94562, val_acc=0.92927, time=3.59101
Epoch:0049, train_loss=2.51850, train_acc=0.98389, val_loss=2.94560, val_acc=0.92838, time=3.56998
Epoch:0050, train_loss=2.51782, train_acc=0.98517, val_loss=2.94558, val_acc=0.92838, time=3.90901
Epoch:0051, train_loss=2.51718, train_acc=0.98606, val_loss=2.94557, val_acc=0.92750, time=3.98999
Epoch:0052, train_loss=2.51658, train_acc=0.98694, val_loss=2.94555, val_acc=0.92838, time=3.54801
Epoch:0053, train_loss=2.51601, train_acc=0.98763, val_loss=2.94553, val_acc=0.92838, time=3.71200
Epoch:0054, train_loss=2.51546, train_acc=0.98822, val_loss=2.94551, val_acc=0.92750, time=3.94598
Epoch:0055, train_loss=2.51495, train_acc=0.98890, val_loss=2.94550, val_acc=0.92661, time=3.66199
Epoch:0056, train_loss=2.51446, train_acc=0.98920, val_loss=2.94549, val_acc=0.92573, time=3.86301
Epoch:0057, train_loss=2.51400, train_acc=0.98969, val_loss=2.94548, val_acc=0.92485, time=3.61600
Epoch:0058, train_loss=2.51355, train_acc=0.99028, val_loss=2.94547, val_acc=0.92485, time=3.61199
Epoch:0059, train_loss=2.51313, train_acc=0.99077, val_loss=2.94547, val_acc=0.92485, time=3.74799
Epoch:0060, train_loss=2.51273, train_acc=0.99155, val_loss=2.94546, val_acc=0.92485, time=3.82399
Epoch:0061, train_loss=2.51235, train_acc=0.99224, val_loss=2.94546, val_acc=0.92485, time=3.67698
Epoch:0062, train_loss=2.51199, train_acc=0.99293, val_loss=2.94545, val_acc=0.92573, time=3.89600
Epoch:0063, train_loss=2.51164, train_acc=0.99322, val_loss=2.94544, val_acc=0.92573, time=3.95299
Epoch:0064, train_loss=2.51131, train_acc=0.99372, val_loss=2.94543, val_acc=0.92573, time=3.82399
Epoch:0065, train_loss=2.51099, train_acc=0.99401, val_loss=2.94543, val_acc=0.92573, time=3.64699
Epoch:0066, train_loss=2.51069, train_acc=0.99411, val_loss=2.94543, val_acc=0.92573, time=3.79299
Epoch:0067, train_loss=2.51040, train_acc=0.99450, val_loss=2.94543, val_acc=0.92573, time=3.71599
Epoch:0068, train_loss=2.51012, train_acc=0.99480, val_loss=2.94543, val_acc=0.92573, time=3.67399
Epoch:0069, train_loss=2.50985, train_acc=0.99519, val_loss=2.94543, val_acc=0.92573, time=3.62698
Epoch:0070, train_loss=2.50959, train_acc=0.99558, val_loss=2.94543, val_acc=0.92573, time=3.71000
Epoch:0071, train_loss=2.50934, train_acc=0.99529, val_loss=2.94543, val_acc=0.92573, time=3.86398
Epoch:0072, train_loss=2.50911, train_acc=0.99568, val_loss=2.94543, val_acc=0.92485, time=3.59100
Epoch:0073, train_loss=2.50888, train_acc=0.99617, val_loss=2.94543, val_acc=0.92485, time=3.55899
Epoch:0074, train_loss=2.50866, train_acc=0.99637, val_loss=2.94543, val_acc=0.92573, time=3.64100
Epoch:0075, train_loss=2.50845, train_acc=0.99656, val_loss=2.94543, val_acc=0.92573, time=3.70299
Epoch:0076, train_loss=2.50824, train_acc=0.99696, val_loss=2.94543, val_acc=0.92661, time=3.72399
Early stopping...

Optimization Finished!

Test set results: loss= 2.69152, accuracy= 0.86325, time= 1.25000

Test Precision, Recall and F1-Score...
              precision    recall  f1-score   support

           0     0.9026    0.9548    0.9280       398
           1     0.7482    0.8175    0.7813       389
           2     0.8737    0.8025    0.8366       319
           3     0.9372    0.9040    0.9203       396
           4     0.8320    0.6710    0.7429       310
           5     0.7989    0.7056    0.7493       394
           6     0.9450    0.9521    0.9486       397
           7     0.9066    0.9112    0.9089       394
           8     0.9138    0.9369    0.9252       396
           9     0.9698    0.9674    0.9686       399
          10     0.9916    0.9468    0.9687       376
          11     0.8484    0.8076    0.8275       395
          12     0.7780    0.8718    0.8222       390
          13     0.8205    0.8142    0.8174       393
          14     0.7245    0.7985    0.7597       392
          15     0.7961    0.9011    0.8454       364
          16     0.9089    0.8813    0.8949       396
          17     0.8466    0.8312    0.8388       385
          18     0.9578    0.9698    0.9638       398
          19     0.7397    0.7131    0.7262       251

    accuracy                         0.8633      7532
   macro avg     0.8620    0.8579    0.8587      7532
weighted avg     0.8649    0.8633    0.8629      7532


Macro average Test Precision, Recall and F1-Score...
(0.8619973266664281, 0.8579222361623804, 0.8587068821327479, None)

Micro average Test Precision, Recall and F1-Score...
(0.8632501327668614, 0.8632501327668614, 0.8632501327668612, None)

Embeddings:
Word_embeddings: 42757
Train_doc_embeddings: 11314
Test_doc_embeddings: 7532

Elapsed time is 302.284353 seconds.
