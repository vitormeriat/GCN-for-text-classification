
==================== Torch Seed: 4992340142900

Model parameters

Layer: layer1.W0 | Size: torch.Size([20169, 200])
Layer: layer2.W0 | Size: torch.Size([200, 3])

Data statistics

  Edges    Classes    Train samples    Val samples    Test samples
-------  ---------  ---------------  -------------  --------------
  20169          3            12422           1380            5915

Epoch:0001, train_loss=1.09452, train_acc=0.30800, val_loss=1.09425, val_acc=0.46449, time=0.45000
Epoch:0002, train_loss=1.04930, train_acc=0.47545, val_loss=1.08687, val_acc=0.58478, time=0.42799
Epoch:0003, train_loss=0.98570, train_acc=0.58348, val_loss=1.08092, val_acc=0.64348, time=0.43499
Epoch:0004, train_loss=0.93490, train_acc=0.64362, val_loss=1.07746, val_acc=0.70217, time=0.39101
Epoch:0005, train_loss=0.90529, train_acc=0.71059, val_loss=1.07467, val_acc=0.70217, time=0.41201
Epoch:0006, train_loss=0.88056, train_acc=0.70601, val_loss=1.07110, val_acc=0.72609, time=0.39700
Epoch:0007, train_loss=0.84800, train_acc=0.72186, val_loss=1.06792, val_acc=0.73986, time=0.42599
Epoch:0008, train_loss=0.81898, train_acc=0.73877, val_loss=1.06588, val_acc=0.74855, time=0.43900
Epoch:0009, train_loss=0.80050, train_acc=0.74795, val_loss=1.06403, val_acc=0.75580, time=0.49100
Epoch:0010, train_loss=0.78399, train_acc=0.75664, val_loss=1.06200, val_acc=0.77391, time=0.49900
Epoch:0011, train_loss=0.76657, train_acc=0.76976, val_loss=1.05999, val_acc=0.78478, time=0.40300
Epoch:0012, train_loss=0.75020, train_acc=0.78546, val_loss=1.05871, val_acc=0.79638, time=0.50102
Epoch:0013, train_loss=0.74061, train_acc=0.79520, val_loss=1.05773, val_acc=0.80580, time=0.38902
Epoch:0014, train_loss=0.73298, train_acc=0.79987, val_loss=1.05669, val_acc=0.81159, time=0.45499
Epoch:0015, train_loss=0.72388, train_acc=0.80446, val_loss=1.05604, val_acc=0.80797, time=0.50001
Epoch:0016, train_loss=0.71812, train_acc=0.80390, val_loss=1.05575, val_acc=0.80362, time=0.44600
Epoch:0017, train_loss=0.71604, train_acc=0.80583, val_loss=1.05539, val_acc=0.80725, time=0.39200
Epoch:0018, train_loss=0.71331, train_acc=0.80623, val_loss=1.05485, val_acc=0.81304, time=0.43600
Epoch:0019, train_loss=0.70883, train_acc=0.80873, val_loss=1.05443, val_acc=0.81739, time=0.47901
Epoch:0020, train_loss=0.70578, train_acc=0.80921, val_loss=1.05411, val_acc=0.81957, time=0.46199
Epoch:0021, train_loss=0.70371, train_acc=0.81066, val_loss=1.05369, val_acc=0.82609, time=0.40301
Epoch:0022, train_loss=0.69992, train_acc=0.81348, val_loss=1.05330, val_acc=0.82826, time=0.39101
Epoch:0023, train_loss=0.69582, train_acc=0.81871, val_loss=1.05303, val_acc=0.83116, time=0.42200
Epoch:0024, train_loss=0.69323, train_acc=0.82298, val_loss=1.05278, val_acc=0.83043, time=0.41599
Epoch:0025, train_loss=0.69117, train_acc=0.82539, val_loss=1.05242, val_acc=0.83188, time=0.38900
Epoch:0026, train_loss=0.68810, train_acc=0.82805, val_loss=1.05210, val_acc=0.83551, time=0.44200
Epoch:0027, train_loss=0.68544, train_acc=0.82966, val_loss=1.05182, val_acc=0.83768, time=0.38901
Epoch:0028, train_loss=0.68351, train_acc=0.83095, val_loss=1.05154, val_acc=0.83913, time=0.47099
Epoch:0029, train_loss=0.68138, train_acc=0.83070, val_loss=1.05123, val_acc=0.83841, time=0.38900
Epoch:0030, train_loss=0.67839, train_acc=0.83417, val_loss=1.05098, val_acc=0.83986, time=0.43100
Epoch:0031, train_loss=0.67587, train_acc=0.83650, val_loss=1.05073, val_acc=0.83696, time=0.38900
Epoch:0032, train_loss=0.67374, train_acc=0.83811, val_loss=1.05046, val_acc=0.83986, time=0.42501
Epoch:0033, train_loss=0.67138, train_acc=0.83932, val_loss=1.05018, val_acc=0.83986, time=0.47799
Epoch:0034, train_loss=0.66889, train_acc=0.84044, val_loss=1.04995, val_acc=0.84348, time=0.51700
Epoch:0035, train_loss=0.66696, train_acc=0.84044, val_loss=1.04976, val_acc=0.84493, time=0.42403
Epoch:0036, train_loss=0.66532, train_acc=0.84093, val_loss=1.04959, val_acc=0.84855, time=0.38201
Epoch:0037, train_loss=0.66335, train_acc=0.84222, val_loss=1.04947, val_acc=0.85290, time=0.39299
Epoch:0038, train_loss=0.66154, train_acc=0.84479, val_loss=1.04936, val_acc=0.84928, time=0.58001
Epoch:0039, train_loss=0.66006, train_acc=0.84560, val_loss=1.04925, val_acc=0.85000, time=0.41400
Epoch:0040, train_loss=0.65856, train_acc=0.84656, val_loss=1.04912, val_acc=0.85217, time=0.51399
Epoch:0041, train_loss=0.65696, train_acc=0.84769, val_loss=1.04900, val_acc=0.85507, time=0.43001
Epoch:0042, train_loss=0.65560, train_acc=0.84954, val_loss=1.04891, val_acc=0.85290, time=0.39099
Epoch:0043, train_loss=0.65451, train_acc=0.84930, val_loss=1.04884, val_acc=0.85507, time=0.47401
Epoch:0044, train_loss=0.65330, train_acc=0.85091, val_loss=1.04878, val_acc=0.85797, time=0.44601
Epoch:0045, train_loss=0.65215, train_acc=0.85300, val_loss=1.04871, val_acc=0.85507, time=0.44800
Epoch:0046, train_loss=0.65121, train_acc=0.85365, val_loss=1.04865, val_acc=0.85725, time=0.38900
Epoch:0047, train_loss=0.65026, train_acc=0.85526, val_loss=1.04856, val_acc=0.85580, time=0.52601
Epoch:0048, train_loss=0.64924, train_acc=0.85719, val_loss=1.04846, val_acc=0.85435, time=0.44299
Epoch:0049, train_loss=0.64834, train_acc=0.85783, val_loss=1.04840, val_acc=0.85580, time=0.53302
Epoch:0050, train_loss=0.64753, train_acc=0.85815, val_loss=1.04834, val_acc=0.85652, time=0.38500
Epoch:0051, train_loss=0.64668, train_acc=0.85783, val_loss=1.04827, val_acc=0.85725, time=0.58499
Epoch:0052, train_loss=0.64585, train_acc=0.85856, val_loss=1.04823, val_acc=0.86014, time=0.42100
Epoch:0053, train_loss=0.64509, train_acc=0.85848, val_loss=1.04818, val_acc=0.86159, time=0.38600
Epoch:0054, train_loss=0.64432, train_acc=0.85896, val_loss=1.04811, val_acc=0.86232, time=0.46601
Epoch:0055, train_loss=0.64350, train_acc=0.85920, val_loss=1.04806, val_acc=0.86159, time=0.42199
Epoch:0056, train_loss=0.64274, train_acc=0.86017, val_loss=1.04801, val_acc=0.86087, time=0.38401
Epoch:0057, train_loss=0.64202, train_acc=0.86057, val_loss=1.04796, val_acc=0.86159, time=0.38401
Epoch:0058, train_loss=0.64127, train_acc=0.86194, val_loss=1.04794, val_acc=0.86449, time=0.42400
Epoch:0059, train_loss=0.64059, train_acc=0.86218, val_loss=1.04789, val_acc=0.86522, time=0.38899
Epoch:0060, train_loss=0.63994, train_acc=0.86290, val_loss=1.04784, val_acc=0.86449, time=0.41900
Epoch:0061, train_loss=0.63927, train_acc=0.86347, val_loss=1.04779, val_acc=0.86522, time=0.38600
Epoch:0062, train_loss=0.63862, train_acc=0.86363, val_loss=1.04773, val_acc=0.86377, time=0.39402
Epoch:0063, train_loss=0.63801, train_acc=0.86339, val_loss=1.04769, val_acc=0.86522, time=0.45700
Epoch:0064, train_loss=0.63738, train_acc=0.86451, val_loss=1.04764, val_acc=0.86522, time=0.46800
Epoch:0065, train_loss=0.63677, train_acc=0.86427, val_loss=1.04760, val_acc=0.86522, time=0.44900
Epoch:0066, train_loss=0.63621, train_acc=0.86443, val_loss=1.04755, val_acc=0.86522, time=0.46400
Epoch:0067, train_loss=0.63564, train_acc=0.86484, val_loss=1.04751, val_acc=0.86522, time=0.43299
Epoch:0068, train_loss=0.63509, train_acc=0.86532, val_loss=1.04748, val_acc=0.86522, time=0.50399
Epoch:0069, train_loss=0.63456, train_acc=0.86580, val_loss=1.04744, val_acc=0.86594, time=0.37902
Epoch:0070, train_loss=0.63402, train_acc=0.86588, val_loss=1.04743, val_acc=0.86304, time=0.49299
Epoch:0071, train_loss=0.63350, train_acc=0.86701, val_loss=1.04740, val_acc=0.86449, time=0.38100
Epoch:0072, train_loss=0.63300, train_acc=0.86693, val_loss=1.04740, val_acc=0.86377, time=0.43100
Epoch:0073, train_loss=0.63250, train_acc=0.86773, val_loss=1.04734, val_acc=0.86522, time=0.54501
Epoch:0074, train_loss=0.63203, train_acc=0.86765, val_loss=1.04737, val_acc=0.86449, time=0.46700
Epoch:0075, train_loss=0.63161, train_acc=0.86798, val_loss=1.04730, val_acc=0.86087, time=0.47401
Epoch:0076, train_loss=0.63126, train_acc=0.86725, val_loss=1.04740, val_acc=0.86812, time=0.39499
Epoch:0077, train_loss=0.63113, train_acc=0.86790, val_loss=1.04732, val_acc=0.85652, time=0.43701
Epoch:0078, train_loss=0.63149, train_acc=0.86677, val_loss=1.04771, val_acc=0.86304, time=0.38300
Early stopping...

Optimization Finished!

Test set results: loss= 0.88820, accuracy= 0.85545, time= 0.11300

Test Precision, Recall and F1-Score...
              precision    recall  f1-score   support

           0     0.8550    0.8486    0.8518      1202
           1     0.8866    0.8091    0.8461      2357
           2     0.8296    0.9053    0.8658      2356

    accuracy                         0.8555      5915
   macro avg     0.8571    0.8543    0.8546      5915
weighted avg     0.8575    0.8555    0.8551      5915


Macro average Test Precision, Recall and F1-Score...
(0.8570633627858122, 0.8543376920652376, 0.8545557590210371, None)

Micro average Test Precision, Recall and F1-Score...
(0.8554522400676247, 0.8554522400676247, 0.8554522400676245, None)

Embeddings:
Word_embeddings: 452
Train_doc_embeddings: 13802
Test_doc_embeddings: 5915

Elapsed time is 36.264920 seconds.
