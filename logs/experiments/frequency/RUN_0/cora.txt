
==================== Torch Seed: 4810843948200

Model parameters

Layer: layer1.W0 | Size: torch.Size([4051, 200])
Layer: layer2.W0 | Size: torch.Size([200, 7])

Data statistics

  Edges    Classes    Train samples    Val samples    Test samples
-------  ---------  ---------------  -------------  --------------
   4051          7             1707            189             812

Epoch:0001, train_loss=1.93725, train_acc=0.25835, val_loss=1.94069, val_acc=0.25397, time=0.02900
Epoch:0002, train_loss=1.86252, train_acc=0.31693, val_loss=1.93755, val_acc=0.25397, time=0.02801
Epoch:0003, train_loss=1.82124, train_acc=0.32572, val_loss=1.93372, val_acc=0.28571, time=0.02801
Epoch:0004, train_loss=1.78260, train_acc=0.37727, val_loss=1.92960, val_acc=0.38095, time=0.02801
Epoch:0005, train_loss=1.74450, train_acc=0.48155, val_loss=1.92554, val_acc=0.47619, time=0.02701
Epoch:0006, train_loss=1.70703, train_acc=0.59344, val_loss=1.92159, val_acc=0.57143, time=0.02801
Epoch:0007, train_loss=1.66982, train_acc=0.64558, val_loss=1.91776, val_acc=0.59788, time=0.02800
Epoch:0008, train_loss=1.63332, train_acc=0.67780, val_loss=1.91410, val_acc=0.60847, time=0.02801
Epoch:0009, train_loss=1.59855, train_acc=0.69303, val_loss=1.91063, val_acc=0.66138, time=0.02802
Epoch:0010, train_loss=1.56633, train_acc=0.71295, val_loss=1.90737, val_acc=0.68783, time=0.02801
Epoch:0011, train_loss=1.53691, train_acc=0.75220, val_loss=1.90431, val_acc=0.70899, time=0.02801
Epoch:0012, train_loss=1.51014, train_acc=0.78676, val_loss=1.90152, val_acc=0.75661, time=0.02702
Epoch:0013, train_loss=1.48573, train_acc=0.80668, val_loss=1.89900, val_acc=0.76720, time=0.02701
Epoch:0014, train_loss=1.46317, train_acc=0.82015, val_loss=1.89676, val_acc=0.77249, time=0.03001
Epoch:0015, train_loss=1.44200, train_acc=0.82601, val_loss=1.89483, val_acc=0.76720, time=0.02999
Epoch:0016, train_loss=1.42229, train_acc=0.82894, val_loss=1.89323, val_acc=0.76190, time=0.03701
Epoch:0017, train_loss=1.40459, train_acc=0.83245, val_loss=1.89193, val_acc=0.76190, time=0.03601
Epoch:0018, train_loss=1.38918, train_acc=0.83363, val_loss=1.89077, val_acc=0.76190, time=0.03700
Epoch:0019, train_loss=1.37561, train_acc=0.83656, val_loss=1.88964, val_acc=0.78836, time=0.03701
Epoch:0020, train_loss=1.36313, train_acc=0.83948, val_loss=1.88852, val_acc=0.78307, time=0.03701
Epoch:0021, train_loss=1.35140, train_acc=0.84593, val_loss=1.88752, val_acc=0.78836, time=0.03700
Epoch:0022, train_loss=1.34046, train_acc=0.85062, val_loss=1.88674, val_acc=0.79365, time=0.03601
Epoch:0023, train_loss=1.33031, train_acc=0.85589, val_loss=1.88624, val_acc=0.78836, time=0.03700
Epoch:0024, train_loss=1.32095, train_acc=0.86585, val_loss=1.88597, val_acc=0.77778, time=0.03700
Epoch:0025, train_loss=1.31233, train_acc=0.87112, val_loss=1.88578, val_acc=0.78307, time=0.03601
Epoch:0026, train_loss=1.30418, train_acc=0.87522, val_loss=1.88554, val_acc=0.77778, time=0.03601
Epoch:0027, train_loss=1.29615, train_acc=0.87991, val_loss=1.88520, val_acc=0.76720, time=0.03700
Epoch:0028, train_loss=1.28829, train_acc=0.88518, val_loss=1.88487, val_acc=0.77778, time=0.03301
Epoch:0029, train_loss=1.28088, train_acc=0.88928, val_loss=1.88463, val_acc=0.77778, time=0.02702
Epoch:0030, train_loss=1.27397, train_acc=0.89631, val_loss=1.88451, val_acc=0.78307, time=0.02702
Epoch:0031, train_loss=1.26735, train_acc=0.89924, val_loss=1.88448, val_acc=0.78836, time=0.02701
Epoch:0032, train_loss=1.26087, train_acc=0.90275, val_loss=1.88448, val_acc=0.78836, time=0.02700
Epoch:0033, train_loss=1.25452, train_acc=0.90920, val_loss=1.88447, val_acc=0.79365, time=0.02800
Epoch:0034, train_loss=1.24840, train_acc=0.91681, val_loss=1.88446, val_acc=0.79365, time=0.02700
Epoch:0035, train_loss=1.24266, train_acc=0.92326, val_loss=1.88449, val_acc=0.79894, time=0.02701
Epoch:0036, train_loss=1.23723, train_acc=0.92794, val_loss=1.88453, val_acc=0.79894, time=0.02701
Epoch:0037, train_loss=1.23193, train_acc=0.93322, val_loss=1.88457, val_acc=0.79365, time=0.02702
Epoch:0038, train_loss=1.22676, train_acc=0.93790, val_loss=1.88461, val_acc=0.79894, time=0.02700
Early stopping...

Optimization Finished!

Test set results: loss= 1.70437, accuracy= 0.76478, time= 0.00702

Test Precision, Recall and F1-Score...
              precision    recall  f1-score   support

           0     0.7652    0.8112    0.7875       233
           1     0.8417    0.8357    0.8387       140
           2     0.8364    0.7077    0.7667        65
           3     0.7742    0.7934    0.7837       121
           4     0.6818    0.6466    0.6637       116
           5     0.7660    0.7826    0.7742        92
           6     0.6047    0.5778    0.5909        45

    accuracy                         0.7648       812
   macro avg     0.7528    0.7364    0.7436       812
weighted avg     0.7647    0.7648    0.7640       812


Macro average Test Precision, Recall and F1-Score...
(0.7528418258725681, 0.7364131455728291, 0.7436241809898937, None)

Micro average Test Precision, Recall and F1-Score...
(0.7647783251231527, 0.7647783251231527, 0.7647783251231527, None)

Embeddings:
Word_embeddings: 1343
Train_doc_embeddings: 1896
Test_doc_embeddings: 812

Elapsed time is 1.437998 seconds.
