
==================== Torch Seed: 1639453716000
Epoch:0001, train_loss=1.92390, train_acc=0.25132, val_loss=1.93690, val_acc=0.29630, time=0.04701
Epoch:0002, train_loss=1.86072, train_acc=0.31752, val_loss=1.93318, val_acc=0.32275, time=0.07000
Epoch:0003, train_loss=1.82393, train_acc=0.34564, val_loss=1.92855, val_acc=0.37566, time=0.06600
Epoch:0004, train_loss=1.78230, train_acc=0.41125, val_loss=1.92382, val_acc=0.46561, time=0.04801
Epoch:0005, train_loss=1.74045, train_acc=0.50732, val_loss=1.91948, val_acc=0.57143, time=0.04800
Epoch:0006, train_loss=1.70154, train_acc=0.59871, val_loss=1.91563, val_acc=0.64550, time=0.04600
Epoch:0007, train_loss=1.66558, train_acc=0.66608, val_loss=1.91214, val_acc=0.67196, time=0.04901
Epoch:0008, train_loss=1.63135, train_acc=0.69479, val_loss=1.90896, val_acc=0.67725, time=0.05500
Epoch:0009, train_loss=1.59826, train_acc=0.72349, val_loss=1.90608, val_acc=0.70370, time=0.04900
Epoch:0010, train_loss=1.56677, train_acc=0.74985, val_loss=1.90352, val_acc=0.74074, time=0.05300
Epoch:0011, train_loss=1.53763, train_acc=0.77270, val_loss=1.90119, val_acc=0.74603, time=0.04902
Epoch:0012, train_loss=1.51097, train_acc=0.78735, val_loss=1.89895, val_acc=0.75132, time=0.04701
Epoch:0013, train_loss=1.48606, train_acc=0.80199, val_loss=1.89671, val_acc=0.75661, time=0.04702
Epoch:0014, train_loss=1.46230, train_acc=0.81078, val_loss=1.89456, val_acc=0.77778, time=0.04902
Epoch:0015, train_loss=1.44035, train_acc=0.82074, val_loss=1.89272, val_acc=0.78836, time=0.04703
Epoch:0016, train_loss=1.42123, train_acc=0.82542, val_loss=1.89125, val_acc=0.78836, time=0.04601
Epoch:0017, train_loss=1.40456, train_acc=0.82777, val_loss=1.89009, val_acc=0.78836, time=0.04700
Epoch:0018, train_loss=1.38910, train_acc=0.83831, val_loss=1.88918, val_acc=0.78307, time=0.06200
Epoch:0019, train_loss=1.37439, train_acc=0.84007, val_loss=1.88852, val_acc=0.78836, time=0.06499
Epoch:0020, train_loss=1.36101, train_acc=0.84651, val_loss=1.88802, val_acc=0.78836, time=0.06700
Epoch:0021, train_loss=1.34921, train_acc=0.85296, val_loss=1.88754, val_acc=0.78307, time=0.06800
Epoch:0022, train_loss=1.33843, train_acc=0.86057, val_loss=1.88701, val_acc=0.78836, time=0.06700
Epoch:0023, train_loss=1.32810, train_acc=0.86526, val_loss=1.88646, val_acc=0.79365, time=0.06701
Epoch:0024, train_loss=1.31828, train_acc=0.86995, val_loss=1.88600, val_acc=0.78836, time=0.06800
Epoch:0025, train_loss=1.30918, train_acc=0.87405, val_loss=1.88569, val_acc=0.79365, time=0.06900
Epoch:0026, train_loss=1.30065, train_acc=0.88108, val_loss=1.88552, val_acc=0.79894, time=0.07001
Epoch:0027, train_loss=1.29243, train_acc=0.88459, val_loss=1.88545, val_acc=0.79365, time=0.06900
Epoch:0028, train_loss=1.28455, train_acc=0.89045, val_loss=1.88541, val_acc=0.79365, time=0.06900
Epoch:0029, train_loss=1.27713, train_acc=0.89631, val_loss=1.88534, val_acc=0.79365, time=0.04701
Epoch:0030, train_loss=1.27006, train_acc=0.90041, val_loss=1.88519, val_acc=0.79365, time=0.06302
Epoch:0031, train_loss=1.26318, train_acc=0.90803, val_loss=1.88503, val_acc=0.80423, time=0.04800
Epoch:0032, train_loss=1.25658, train_acc=0.91388, val_loss=1.88493, val_acc=0.79894, time=0.04699
Epoch:0033, train_loss=1.25038, train_acc=0.91916, val_loss=1.88492, val_acc=0.79894, time=0.04702
Epoch:0034, train_loss=1.24448, train_acc=0.92326, val_loss=1.88499, val_acc=0.79894, time=0.06699
Epoch:0035, train_loss=1.23875, train_acc=0.92677, val_loss=1.88507, val_acc=0.79365, time=0.05499
Epoch:0036, train_loss=1.23326, train_acc=0.93146, val_loss=1.88511, val_acc=0.78836, time=0.04901
Epoch:0037, train_loss=1.22804, train_acc=0.93615, val_loss=1.88508, val_acc=0.78836, time=0.04700
Epoch:0038, train_loss=1.22311, train_acc=0.93907, val_loss=1.88503, val_acc=0.78836, time=0.04701
Epoch:0039, train_loss=1.21843, train_acc=0.94493, val_loss=1.88503, val_acc=0.78836, time=0.05400
Epoch:0040, train_loss=1.21392, train_acc=0.95079, val_loss=1.88511, val_acc=0.78836, time=0.06700
Early stopping...

Optimization Finished!

Test set results: loss= 1.70552, accuracy= 0.75616, time= 0.01699

Test Precision, Recall and F1-Score...
              precision    recall  f1-score   support

           0     0.7931    0.7603    0.7764       121
           1     0.7273    0.7826    0.7539        92
           2     0.8036    0.6923    0.7438        65
           3     0.7530    0.8112    0.7810       233
           4     0.6525    0.6638    0.6581       116
           5     0.6216    0.5111    0.5610        45
           6     0.8593    0.8286    0.8436       140

    accuracy                         0.7562       812
   macro avg     0.7443    0.7214    0.7311       812
weighted avg     0.7568    0.7562    0.7553       812


Macro average Test Precision, Recall and F1-Score...
(0.7443369865272885, 0.7214116296980485, 0.7311175756470613, None)

Micro average Test Precision, Recall and F1-Score...
(0.7561576354679803, 0.7561576354679803, 0.7561576354679802, None)

Embeddings:
Word_embeddings:1343
Train_doc_embeddings:1896
Test_doc_embeddings:812
