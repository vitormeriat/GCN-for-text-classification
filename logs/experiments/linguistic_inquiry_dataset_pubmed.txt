
==================== Torch Seed: 149529739019100
Epoch:0001, train_loss=1.09338, train_acc=0.40179, val_loss=1.09264, val_acc=0.50870, time=0.83799
Epoch:0002, train_loss=1.04544, train_acc=0.51916, val_loss=1.08600, val_acc=0.55435, time=0.75398
Epoch:0003, train_loss=0.98799, train_acc=0.54347, val_loss=1.08036, val_acc=0.60725, time=0.82000
Epoch:0004, train_loss=0.93404, train_acc=0.60127, val_loss=1.07767, val_acc=0.70435, time=0.81201
Epoch:0005, train_loss=0.90599, train_acc=0.72025, val_loss=1.07524, val_acc=0.71812, time=0.81899
Epoch:0006, train_loss=0.88234, train_acc=0.73458, val_loss=1.07180, val_acc=0.72464, time=0.84998
Epoch:0007, train_loss=0.85149, train_acc=0.73635, val_loss=1.06849, val_acc=0.73116, time=0.76099
Epoch:0008, train_loss=0.82224, train_acc=0.74344, val_loss=1.06592, val_acc=0.74638, time=0.80598
Epoch:0009, train_loss=0.79789, train_acc=0.75616, val_loss=1.06435, val_acc=0.74928, time=0.91301
Epoch:0010, train_loss=0.78121, train_acc=0.76823, val_loss=1.06271, val_acc=0.75725, time=0.77296
Epoch:0011, train_loss=0.76539, train_acc=0.78015, val_loss=1.06071, val_acc=0.77029, time=0.84301
Epoch:0012, train_loss=0.74820, train_acc=0.79190, val_loss=1.05954, val_acc=0.77899, time=0.90697
Epoch:0013, train_loss=0.73756, train_acc=0.79746, val_loss=1.05892, val_acc=0.79058, time=0.73299
Epoch:0014, train_loss=0.72989, train_acc=0.80317, val_loss=1.05841, val_acc=0.78768, time=0.84199
Epoch:0015, train_loss=0.72416, train_acc=0.80486, val_loss=1.05739, val_acc=0.78768, time=0.70301
Epoch:0016, train_loss=0.71674, train_acc=0.80881, val_loss=1.05678, val_acc=0.79493, time=0.68502
Epoch:0017, train_loss=0.71385, train_acc=0.80655, val_loss=1.05650, val_acc=0.79420, time=0.80199
Epoch:0018, train_loss=0.71143, train_acc=0.80897, val_loss=1.05634, val_acc=0.78841, time=0.78298
Epoch:0019, train_loss=0.70846, train_acc=0.81195, val_loss=1.05589, val_acc=0.79275, time=0.83999
Epoch:0020, train_loss=0.70385, train_acc=0.81412, val_loss=1.05562, val_acc=0.80145, time=0.80898
Epoch:0021, train_loss=0.70132, train_acc=0.81573, val_loss=1.05545, val_acc=0.80000, time=0.82599
Epoch:0022, train_loss=0.69850, train_acc=0.81782, val_loss=1.05521, val_acc=0.80072, time=0.75798
Epoch:0023, train_loss=0.69495, train_acc=0.82080, val_loss=1.05476, val_acc=0.80435, time=0.74400
Epoch:0024, train_loss=0.69120, train_acc=0.82563, val_loss=1.05439, val_acc=0.81232, time=0.81499
Epoch:0025, train_loss=0.68900, train_acc=0.82950, val_loss=1.05420, val_acc=0.81159, time=0.79401
Epoch:0026, train_loss=0.68662, train_acc=0.83111, val_loss=1.05411, val_acc=0.81304, time=0.79999
Epoch:0027, train_loss=0.68381, train_acc=0.83264, val_loss=1.05390, val_acc=0.81304, time=0.76098
Epoch:0028, train_loss=0.68107, train_acc=0.83457, val_loss=1.05368, val_acc=0.81087, time=0.76499
Epoch:0029, train_loss=0.67910, train_acc=0.83521, val_loss=1.05346, val_acc=0.81159, time=0.93801
Epoch:0030, train_loss=0.67672, train_acc=0.83602, val_loss=1.05321, val_acc=0.81594, time=0.85997
Epoch:0031, train_loss=0.67397, train_acc=0.83924, val_loss=1.05286, val_acc=0.81812, time=0.79998
Epoch:0032, train_loss=0.67146, train_acc=0.84141, val_loss=1.05251, val_acc=0.82174, time=0.75498
Epoch:0033, train_loss=0.66947, train_acc=0.84270, val_loss=1.05227, val_acc=0.82029, time=0.81700
Epoch:0034, train_loss=0.66719, train_acc=0.84374, val_loss=1.05212, val_acc=0.81812, time=0.81398
Epoch:0035, train_loss=0.66500, train_acc=0.84350, val_loss=1.05192, val_acc=0.82101, time=0.82399
Epoch:0036, train_loss=0.66308, train_acc=0.84463, val_loss=1.05171, val_acc=0.82174, time=0.88598
Epoch:0037, train_loss=0.66156, train_acc=0.84592, val_loss=1.05152, val_acc=0.82319, time=0.79400
Epoch:0038, train_loss=0.65968, train_acc=0.84729, val_loss=1.05137, val_acc=0.82246, time=0.74298
Epoch:0039, train_loss=0.65806, train_acc=0.85035, val_loss=1.05115, val_acc=0.82391, time=0.76198
Epoch:0040, train_loss=0.65656, train_acc=0.85107, val_loss=1.05100, val_acc=0.82609, time=0.90699
Epoch:0041, train_loss=0.65526, train_acc=0.85180, val_loss=1.05094, val_acc=0.82681, time=0.80400
Epoch:0042, train_loss=0.65373, train_acc=0.85397, val_loss=1.05089, val_acc=0.82681, time=0.82096
Epoch:0043, train_loss=0.65250, train_acc=0.85453, val_loss=1.05080, val_acc=0.82971, time=0.79100
Epoch:0044, train_loss=0.65140, train_acc=0.85461, val_loss=1.05072, val_acc=0.83188, time=0.76101
Epoch:0045, train_loss=0.65029, train_acc=0.85526, val_loss=1.05067, val_acc=0.83261, time=0.80997
Epoch:0046, train_loss=0.64920, train_acc=0.85606, val_loss=1.05057, val_acc=0.83623, time=0.86499
Epoch:0047, train_loss=0.64822, train_acc=0.85638, val_loss=1.05048, val_acc=0.83696, time=0.85697
Epoch:0048, train_loss=0.64738, train_acc=0.85751, val_loss=1.05045, val_acc=0.83551, time=0.73599
Epoch:0049, train_loss=0.64639, train_acc=0.85807, val_loss=1.05043, val_acc=0.83406, time=0.84799
Epoch:0050, train_loss=0.64552, train_acc=0.85904, val_loss=1.05035, val_acc=0.83551, time=0.81099
Epoch:0051, train_loss=0.64473, train_acc=0.86001, val_loss=1.05030, val_acc=0.83551, time=0.83498
Epoch:0052, train_loss=0.64389, train_acc=0.86041, val_loss=1.05026, val_acc=0.83841, time=0.83100
Epoch:0053, train_loss=0.64307, train_acc=0.86089, val_loss=1.05017, val_acc=0.83986, time=0.79699
Epoch:0054, train_loss=0.64228, train_acc=0.86282, val_loss=1.05011, val_acc=0.83986, time=0.76298
Epoch:0055, train_loss=0.64153, train_acc=0.86307, val_loss=1.05010, val_acc=0.83986, time=0.94498
Epoch:0056, train_loss=0.64074, train_acc=0.86290, val_loss=1.05006, val_acc=0.84058, time=0.84899
Epoch:0057, train_loss=0.63997, train_acc=0.86387, val_loss=1.05003, val_acc=0.83696, time=0.77798
Epoch:0058, train_loss=0.63927, train_acc=0.86427, val_loss=1.05002, val_acc=0.83913, time=0.70799
Epoch:0059, train_loss=0.63855, train_acc=0.86476, val_loss=1.04997, val_acc=0.83986, time=0.85498
Epoch:0060, train_loss=0.63784, train_acc=0.86476, val_loss=1.04993, val_acc=0.83913, time=1.02901
Epoch:0061, train_loss=0.63720, train_acc=0.86548, val_loss=1.04993, val_acc=0.83986, time=0.82397
Epoch:0062, train_loss=0.63654, train_acc=0.86564, val_loss=1.04990, val_acc=0.84203, time=0.94600
Epoch:0063, train_loss=0.63588, train_acc=0.86621, val_loss=1.04988, val_acc=0.83913, time=0.89199
Epoch:0064, train_loss=0.63527, train_acc=0.86725, val_loss=1.04988, val_acc=0.83986, time=0.75698
Epoch:0065, train_loss=0.63466, train_acc=0.86765, val_loss=1.04983, val_acc=0.83768, time=0.72698
Epoch:0066, train_loss=0.63404, train_acc=0.86790, val_loss=1.04980, val_acc=0.83768, time=0.81598
Epoch:0067, train_loss=0.63347, train_acc=0.86886, val_loss=1.04980, val_acc=0.83768, time=0.73899
Epoch:0068, train_loss=0.63292, train_acc=0.86878, val_loss=1.04977, val_acc=0.83913, time=0.71999
Epoch:0069, train_loss=0.63235, train_acc=0.86934, val_loss=1.04977, val_acc=0.83986, time=0.80301
Epoch:0070, train_loss=0.63181, train_acc=0.86975, val_loss=1.04976, val_acc=0.83986, time=0.87100
Epoch:0071, train_loss=0.63128, train_acc=0.87055, val_loss=1.04972, val_acc=0.84058, time=0.74198
Epoch:0072, train_loss=0.63075, train_acc=0.87007, val_loss=1.04972, val_acc=0.83986, time=0.74498
Epoch:0073, train_loss=0.63024, train_acc=0.87120, val_loss=1.04970, val_acc=0.84058, time=0.77902
Epoch:0074, train_loss=0.62973, train_acc=0.87112, val_loss=1.04970, val_acc=0.84058, time=0.68597
Epoch:0075, train_loss=0.62923, train_acc=0.87136, val_loss=1.04970, val_acc=0.84203, time=0.84901
Epoch:0076, train_loss=0.62876, train_acc=0.87200, val_loss=1.04967, val_acc=0.84275, time=0.69900
Epoch:0077, train_loss=0.62828, train_acc=0.87256, val_loss=1.04966, val_acc=0.84275, time=0.80500
Epoch:0078, train_loss=0.62781, train_acc=0.87248, val_loss=1.04963, val_acc=0.84348, time=0.83296
Epoch:0079, train_loss=0.62736, train_acc=0.87305, val_loss=1.04963, val_acc=0.84275, time=0.83699
Epoch:0080, train_loss=0.62690, train_acc=0.87353, val_loss=1.04962, val_acc=0.84348, time=0.68700
Epoch:0081, train_loss=0.62646, train_acc=0.87353, val_loss=1.04961, val_acc=0.84203, time=0.72899
Epoch:0082, train_loss=0.62602, train_acc=0.87369, val_loss=1.04960, val_acc=0.84130, time=0.80598
Epoch:0083, train_loss=0.62560, train_acc=0.87377, val_loss=1.04958, val_acc=0.84203, time=0.75199
Epoch:0084, train_loss=0.62518, train_acc=0.87474, val_loss=1.04959, val_acc=0.84058, time=0.68799
Epoch:0085, train_loss=0.62477, train_acc=0.87554, val_loss=1.04957, val_acc=0.84203, time=0.71100
Epoch:0086, train_loss=0.62437, train_acc=0.87538, val_loss=1.04959, val_acc=0.84130, time=0.84900
Epoch:0087, train_loss=0.62398, train_acc=0.87619, val_loss=1.04956, val_acc=0.84275, time=0.84797
Epoch:0088, train_loss=0.62361, train_acc=0.87578, val_loss=1.04960, val_acc=0.83913, time=0.74499
Epoch:0089, train_loss=0.62326, train_acc=0.87667, val_loss=1.04954, val_acc=0.84275, time=0.81901
Epoch:0090, train_loss=0.62297, train_acc=0.87611, val_loss=1.04966, val_acc=0.84130, time=0.76399
Early stopping...

Optimization Finished!

Test set results: loss= 0.88611, accuracy= 0.85968, time= 0.26099

Test Precision, Recall and F1-Score...
              precision    recall  f1-score   support

           0     0.8557    0.8586    0.8571      1202
           1     0.8556    0.8524    0.8540      2357
           2     0.8657    0.8676    0.8667      2356

    accuracy                         0.8597      5915
   macro avg     0.8590    0.8595    0.8593      5915
weighted avg     0.8597    0.8597    0.8597      5915


Macro average Test Precision, Recall and F1-Score...
(0.8590260189793198, 0.8594986319801872, 0.8592601709143793, None)

Micro average Test Precision, Recall and F1-Score...
(0.8596787827557059, 0.8596787827557059, 0.8596787827557059, None)

Embeddings:
Word_embeddings:452
Train_doc_embeddings:13802
Test_doc_embeddings:5915

Elapsed time is 73.965643 seconds.
