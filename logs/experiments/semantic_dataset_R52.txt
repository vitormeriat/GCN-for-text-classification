
==================== Torch Seed: 2143940368000
Epoch:0001, train_loss=4.17528, train_acc=0.01038, val_loss=3.93809, val_acc=0.25574, time=1.15701
Epoch:0002, train_loss=3.83225, train_acc=0.26467, val_loss=3.90710, val_acc=0.49005, time=1.12802
Epoch:0003, train_loss=3.55586, train_acc=0.49226, val_loss=3.88468, val_acc=0.58346, time=0.96702
Epoch:0004, train_loss=3.35828, train_acc=0.58513, val_loss=3.86963, val_acc=0.63859, time=1.08901
Epoch:0005, train_loss=3.22543, train_acc=0.63531, val_loss=3.86084, val_acc=0.67841, time=1.16702
Epoch:0006, train_loss=3.14530, train_acc=0.67018, val_loss=3.85613, val_acc=0.69219, time=1.05902
Epoch:0007, train_loss=3.09972, train_acc=0.68362, val_loss=3.85262, val_acc=0.71057, time=1.25902
Epoch:0008, train_loss=3.06427, train_acc=0.70063, val_loss=3.84895, val_acc=0.73966, time=1.13902
Epoch:0009, train_loss=3.02767, train_acc=0.73414, val_loss=3.84527, val_acc=0.76570, time=1.20503
Epoch:0010, train_loss=2.99152, train_acc=0.77734, val_loss=3.84212, val_acc=0.80551, time=1.24801
Epoch:0011, train_loss=2.96052, train_acc=0.81034, val_loss=3.83977, val_acc=0.82542, time=1.05903
Epoch:0012, train_loss=2.93674, train_acc=0.83399, val_loss=3.83811, val_acc=0.83920, time=1.03201
Epoch:0013, train_loss=2.91890, train_acc=0.85048, val_loss=3.83680, val_acc=0.83920, time=1.16504
Epoch:0014, train_loss=2.90394, train_acc=0.85763, val_loss=3.83558, val_acc=0.84533, time=1.12702
Epoch:0015, train_loss=2.88947, train_acc=0.86307, val_loss=3.83435, val_acc=0.84839, time=1.08101
Epoch:0016, train_loss=2.87465, train_acc=0.87039, val_loss=3.83311, val_acc=0.85452, time=0.95602
Epoch:0017, train_loss=2.85968, train_acc=0.88008, val_loss=3.83188, val_acc=0.86677, time=1.04402
Epoch:0018, train_loss=2.84511, train_acc=0.88910, val_loss=3.83072, val_acc=0.88515, time=1.05203
Epoch:0019, train_loss=2.83140, train_acc=0.89709, val_loss=3.82964, val_acc=0.88515, time=1.03901
Epoch:0020, train_loss=2.81884, train_acc=0.90628, val_loss=3.82865, val_acc=0.89127, time=1.05301
Epoch:0021, train_loss=2.80753, train_acc=0.91342, val_loss=3.82775, val_acc=0.89740, time=0.94303
Epoch:0022, train_loss=2.79749, train_acc=0.92312, val_loss=3.82693, val_acc=0.90046, time=0.93300
Epoch:0023, train_loss=2.78865, train_acc=0.92975, val_loss=3.82620, val_acc=0.89893, time=0.98102
Epoch:0024, train_loss=2.78093, train_acc=0.93502, val_loss=3.82553, val_acc=0.90199, time=1.17800
Epoch:0025, train_loss=2.77419, train_acc=0.94030, val_loss=3.82492, val_acc=0.90199, time=1.01901
Epoch:0026, train_loss=2.76827, train_acc=0.94676, val_loss=3.82437, val_acc=0.90965, time=0.96801
Epoch:0027, train_loss=2.76295, train_acc=0.94931, val_loss=3.82384, val_acc=0.90658, time=1.10800
Epoch:0028, train_loss=2.75805, train_acc=0.95220, val_loss=3.82335, val_acc=0.91118, time=1.09301
Epoch:0029, train_loss=2.75336, train_acc=0.95373, val_loss=3.82287, val_acc=0.90965, time=1.00401
Epoch:0030, train_loss=2.74875, train_acc=0.95611, val_loss=3.82240, val_acc=0.91424, time=0.99601
Epoch:0031, train_loss=2.74417, train_acc=0.95782, val_loss=3.82196, val_acc=0.91424, time=0.96301
Epoch:0032, train_loss=2.73964, train_acc=0.96071, val_loss=3.82156, val_acc=0.91730, time=0.98601
Epoch:0033, train_loss=2.73521, train_acc=0.96207, val_loss=3.82119, val_acc=0.92343, time=1.10802
Epoch:0034, train_loss=2.73097, train_acc=0.96496, val_loss=3.82087, val_acc=0.92496, time=0.93500
Epoch:0035, train_loss=2.72701, train_acc=0.96547, val_loss=3.82060, val_acc=0.92649, time=0.82801
Epoch:0036, train_loss=2.72336, train_acc=0.96683, val_loss=3.82038, val_acc=0.92649, time=1.00901
Epoch:0037, train_loss=2.72004, train_acc=0.96853, val_loss=3.82020, val_acc=0.92802, time=0.91300
Epoch:0038, train_loss=2.71702, train_acc=0.97023, val_loss=3.82005, val_acc=0.92496, time=0.93801
Epoch:0039, train_loss=2.71427, train_acc=0.97176, val_loss=3.81991, val_acc=0.92802, time=1.04103
Epoch:0040, train_loss=2.71172, train_acc=0.97261, val_loss=3.81979, val_acc=0.92802, time=1.01003
Epoch:0041, train_loss=2.70935, train_acc=0.97466, val_loss=3.81966, val_acc=0.92649, time=0.94902
Epoch:0042, train_loss=2.70710, train_acc=0.97602, val_loss=3.81954, val_acc=0.93109, time=0.97300
Epoch:0043, train_loss=2.70496, train_acc=0.97772, val_loss=3.81941, val_acc=0.93109, time=0.98904
Epoch:0044, train_loss=2.70293, train_acc=0.97925, val_loss=3.81929, val_acc=0.93262, time=1.01101
Epoch:0045, train_loss=2.70101, train_acc=0.98095, val_loss=3.81916, val_acc=0.93109, time=0.99302
Epoch:0046, train_loss=2.69918, train_acc=0.98163, val_loss=3.81903, val_acc=0.93109, time=0.93904
Epoch:0047, train_loss=2.69747, train_acc=0.98350, val_loss=3.81890, val_acc=0.92956, time=0.97400
Epoch:0048, train_loss=2.69586, train_acc=0.98486, val_loss=3.81877, val_acc=0.93109, time=0.98402
Epoch:0049, train_loss=2.69436, train_acc=0.98571, val_loss=3.81866, val_acc=0.92956, time=0.89303
Epoch:0050, train_loss=2.69295, train_acc=0.98622, val_loss=3.81855, val_acc=0.93262, time=0.96701
Epoch:0051, train_loss=2.69164, train_acc=0.98690, val_loss=3.81845, val_acc=0.93415, time=0.86203
Epoch:0052, train_loss=2.69040, train_acc=0.98741, val_loss=3.81835, val_acc=0.93262, time=0.89500
Epoch:0053, train_loss=2.68923, train_acc=0.98843, val_loss=3.81827, val_acc=0.93415, time=0.91902
Epoch:0054, train_loss=2.68813, train_acc=0.98877, val_loss=3.81819, val_acc=0.93415, time=1.11802
Epoch:0055, train_loss=2.68708, train_acc=0.98945, val_loss=3.81812, val_acc=0.93415, time=0.94901
Epoch:0056, train_loss=2.68607, train_acc=0.98979, val_loss=3.81805, val_acc=0.93415, time=0.84903
Epoch:0057, train_loss=2.68512, train_acc=0.99047, val_loss=3.81799, val_acc=0.93415, time=0.91201
Epoch:0058, train_loss=2.68420, train_acc=0.99064, val_loss=3.81794, val_acc=0.93415, time=1.00103
Epoch:0059, train_loss=2.68332, train_acc=0.99081, val_loss=3.81788, val_acc=0.93568, time=0.90003
Epoch:0060, train_loss=2.68248, train_acc=0.99081, val_loss=3.81783, val_acc=0.93721, time=0.88001
Epoch:0061, train_loss=2.68168, train_acc=0.99098, val_loss=3.81777, val_acc=0.93874, time=0.90302
Epoch:0062, train_loss=2.68092, train_acc=0.99133, val_loss=3.81772, val_acc=0.93874, time=0.92703
Epoch:0063, train_loss=2.68020, train_acc=0.99150, val_loss=3.81767, val_acc=0.93874, time=0.88901
Epoch:0064, train_loss=2.67951, train_acc=0.99150, val_loss=3.81761, val_acc=0.93874, time=0.93303
Epoch:0065, train_loss=2.67886, train_acc=0.99184, val_loss=3.81756, val_acc=0.93874, time=0.99500
Epoch:0066, train_loss=2.67823, train_acc=0.99201, val_loss=3.81750, val_acc=0.93874, time=0.90502
Epoch:0067, train_loss=2.67764, train_acc=0.99269, val_loss=3.81745, val_acc=0.93874, time=0.86202
Epoch:0068, train_loss=2.67708, train_acc=0.99320, val_loss=3.81739, val_acc=0.93874, time=1.08001
Epoch:0069, train_loss=2.67655, train_acc=0.99371, val_loss=3.81734, val_acc=0.93874, time=1.03301
Epoch:0070, train_loss=2.67604, train_acc=0.99405, val_loss=3.81729, val_acc=0.93874, time=0.92402
Epoch:0071, train_loss=2.67556, train_acc=0.99456, val_loss=3.81724, val_acc=0.93874, time=0.83601
Epoch:0072, train_loss=2.67510, train_acc=0.99490, val_loss=3.81720, val_acc=0.93874, time=0.97900
Epoch:0073, train_loss=2.67466, train_acc=0.99490, val_loss=3.81716, val_acc=0.94028, time=1.01903
Epoch:0074, train_loss=2.67424, train_acc=0.99541, val_loss=3.81713, val_acc=0.94028, time=1.12003
Epoch:0075, train_loss=2.67383, train_acc=0.99558, val_loss=3.81709, val_acc=0.94028, time=0.98402
Epoch:0076, train_loss=2.67345, train_acc=0.99558, val_loss=3.81707, val_acc=0.94028, time=1.18101
Epoch:0077, train_loss=2.67309, train_acc=0.99558, val_loss=3.81705, val_acc=0.94028, time=1.02402
Epoch:0078, train_loss=2.67274, train_acc=0.99592, val_loss=3.81703, val_acc=0.93874, time=1.12504
Epoch:0079, train_loss=2.67240, train_acc=0.99609, val_loss=3.81701, val_acc=0.94028, time=1.09801
Epoch:0080, train_loss=2.67208, train_acc=0.99609, val_loss=3.81699, val_acc=0.94028, time=1.15301
Epoch:0081, train_loss=2.67178, train_acc=0.99626, val_loss=3.81698, val_acc=0.94028, time=1.02602
Epoch:0082, train_loss=2.67149, train_acc=0.99643, val_loss=3.81696, val_acc=0.94028, time=1.19201
Epoch:0083, train_loss=2.67121, train_acc=0.99660, val_loss=3.81695, val_acc=0.94028, time=1.19202
Epoch:0084, train_loss=2.67094, train_acc=0.99677, val_loss=3.81693, val_acc=0.94028, time=1.03902
Epoch:0085, train_loss=2.67069, train_acc=0.99677, val_loss=3.81691, val_acc=0.94028, time=1.07802
Epoch:0086, train_loss=2.67044, train_acc=0.99677, val_loss=3.81689, val_acc=0.94028, time=0.99000
Epoch:0087, train_loss=2.67021, train_acc=0.99677, val_loss=3.81687, val_acc=0.94028, time=0.93102
Epoch:0088, train_loss=2.66998, train_acc=0.99694, val_loss=3.81685, val_acc=0.94028, time=1.02401
Epoch:0089, train_loss=2.66976, train_acc=0.99694, val_loss=3.81683, val_acc=0.94028, time=0.96701
Epoch:0090, train_loss=2.66956, train_acc=0.99694, val_loss=3.81681, val_acc=0.94028, time=1.15802
Epoch:0091, train_loss=2.66936, train_acc=0.99694, val_loss=3.81679, val_acc=0.94028, time=1.02101
Epoch:0092, train_loss=2.66916, train_acc=0.99711, val_loss=3.81677, val_acc=0.94028, time=0.91402
Epoch:0093, train_loss=2.66898, train_acc=0.99711, val_loss=3.81675, val_acc=0.94028, time=0.84202
Epoch:0094, train_loss=2.66880, train_acc=0.99728, val_loss=3.81673, val_acc=0.94028, time=0.89501
Epoch:0095, train_loss=2.66863, train_acc=0.99728, val_loss=3.81671, val_acc=0.94028, time=0.93600
Epoch:0096, train_loss=2.66846, train_acc=0.99728, val_loss=3.81669, val_acc=0.94028, time=0.93402
Epoch:0097, train_loss=2.66830, train_acc=0.99745, val_loss=3.81667, val_acc=0.94181, time=1.01501
Epoch:0098, train_loss=2.66815, train_acc=0.99745, val_loss=3.81666, val_acc=0.94181, time=1.05803
Epoch:0099, train_loss=2.66800, train_acc=0.99745, val_loss=3.81665, val_acc=0.94181, time=1.08302
Epoch:0100, train_loss=2.66786, train_acc=0.99745, val_loss=3.81664, val_acc=0.94181, time=1.14599
Epoch:0101, train_loss=2.66772, train_acc=0.99762, val_loss=3.81663, val_acc=0.94181, time=1.14002
Epoch:0102, train_loss=2.66759, train_acc=0.99762, val_loss=3.81662, val_acc=0.94181, time=0.99701
Epoch:0103, train_loss=2.66746, train_acc=0.99762, val_loss=3.81661, val_acc=0.94181, time=1.12001
Epoch:0104, train_loss=2.66733, train_acc=0.99762, val_loss=3.81660, val_acc=0.94181, time=1.20902
Epoch:0105, train_loss=2.66721, train_acc=0.99762, val_loss=3.81660, val_acc=0.94181, time=0.93502
Epoch:0106, train_loss=2.66709, train_acc=0.99779, val_loss=3.81659, val_acc=0.94181, time=0.94701
Epoch:0107, train_loss=2.66698, train_acc=0.99779, val_loss=3.81659, val_acc=0.94181, time=0.92503
Epoch:0108, train_loss=2.66686, train_acc=0.99779, val_loss=3.81658, val_acc=0.94181, time=0.87802
Epoch:0109, train_loss=2.66676, train_acc=0.99779, val_loss=3.81658, val_acc=0.94181, time=0.94700
Epoch:0110, train_loss=2.66665, train_acc=0.99779, val_loss=3.81657, val_acc=0.94181, time=0.91001
Epoch:0111, train_loss=2.66655, train_acc=0.99779, val_loss=3.81657, val_acc=0.94181, time=0.86201
Epoch:0112, train_loss=2.66645, train_acc=0.99779, val_loss=3.81657, val_acc=0.94181, time=0.97401
Epoch:0113, train_loss=2.66635, train_acc=0.99779, val_loss=3.81656, val_acc=0.94181, time=0.96403
Epoch:0114, train_loss=2.66626, train_acc=0.99779, val_loss=3.81656, val_acc=0.94181, time=0.98001
Epoch:0115, train_loss=2.66617, train_acc=0.99779, val_loss=3.81655, val_acc=0.94028, time=0.97102
Epoch:0116, train_loss=2.66608, train_acc=0.99779, val_loss=3.81655, val_acc=0.94028, time=0.98100
Epoch:0117, train_loss=2.66599, train_acc=0.99762, val_loss=3.81654, val_acc=0.94028, time=0.92402
Epoch:0118, train_loss=2.66591, train_acc=0.99762, val_loss=3.81654, val_acc=0.94028, time=1.03401
Epoch:0119, train_loss=2.66583, train_acc=0.99762, val_loss=3.81653, val_acc=0.94028, time=0.84201
Epoch:0120, train_loss=2.66575, train_acc=0.99779, val_loss=3.81653, val_acc=0.94028, time=0.84501
Epoch:0121, train_loss=2.66567, train_acc=0.99779, val_loss=3.81653, val_acc=0.94028, time=0.96002
Epoch:0122, train_loss=2.66560, train_acc=0.99779, val_loss=3.81652, val_acc=0.94028, time=0.92800
Epoch:0123, train_loss=2.66552, train_acc=0.99779, val_loss=3.81652, val_acc=0.94028, time=0.92103
Epoch:0124, train_loss=2.66545, train_acc=0.99779, val_loss=3.81652, val_acc=0.94028, time=0.86101
Epoch:0125, train_loss=2.66538, train_acc=0.99779, val_loss=3.81652, val_acc=0.94028, time=0.99703
Epoch:0126, train_loss=2.66531, train_acc=0.99779, val_loss=3.81652, val_acc=0.94028, time=0.97000
Epoch:0127, train_loss=2.66524, train_acc=0.99779, val_loss=3.81651, val_acc=0.94028, time=0.91302
Epoch:0128, train_loss=2.66518, train_acc=0.99779, val_loss=3.81651, val_acc=0.94028, time=0.91701
Epoch:0129, train_loss=2.66511, train_acc=0.99779, val_loss=3.81651, val_acc=0.93874, time=0.95002
Epoch:0130, train_loss=2.66505, train_acc=0.99779, val_loss=3.81651, val_acc=0.93874, time=0.93800
Epoch:0131, train_loss=2.66499, train_acc=0.99779, val_loss=3.81651, val_acc=0.93874, time=0.88201
Epoch:0132, train_loss=2.66493, train_acc=0.99779, val_loss=3.81651, val_acc=0.93874, time=0.86900
Epoch:0133, train_loss=2.66487, train_acc=0.99779, val_loss=3.81650, val_acc=0.93874, time=0.89602
Epoch:0134, train_loss=2.66481, train_acc=0.99796, val_loss=3.81650, val_acc=0.93874, time=1.01901
Epoch:0135, train_loss=2.66476, train_acc=0.99796, val_loss=3.81650, val_acc=0.93874, time=0.82201
Epoch:0136, train_loss=2.66470, train_acc=0.99796, val_loss=3.81650, val_acc=0.93874, time=0.83703
Epoch:0137, train_loss=2.66465, train_acc=0.99796, val_loss=3.81650, val_acc=0.93874, time=0.99700
Epoch:0138, train_loss=2.66460, train_acc=0.99796, val_loss=3.81650, val_acc=0.93874, time=0.90901
Epoch:0139, train_loss=2.66454, train_acc=0.99796, val_loss=3.81650, val_acc=0.93874, time=1.09801
Epoch:0140, train_loss=2.66449, train_acc=0.99813, val_loss=3.81650, val_acc=0.93874, time=0.86902
Epoch:0141, train_loss=2.66445, train_acc=0.99813, val_loss=3.81650, val_acc=0.93874, time=0.93100
Epoch:0142, train_loss=2.66440, train_acc=0.99813, val_loss=3.81649, val_acc=0.93874, time=0.80500
Epoch:0143, train_loss=2.66435, train_acc=0.99813, val_loss=3.81649, val_acc=0.93874, time=0.79501
Epoch:0144, train_loss=2.66430, train_acc=0.99813, val_loss=3.81649, val_acc=0.94028, time=0.93801
Epoch:0145, train_loss=2.66426, train_acc=0.99813, val_loss=3.81649, val_acc=0.94028, time=0.78901
Epoch:0146, train_loss=2.66421, train_acc=0.99813, val_loss=3.81649, val_acc=0.94028, time=0.86301
Epoch:0147, train_loss=2.66417, train_acc=0.99813, val_loss=3.81649, val_acc=0.94028, time=0.78803
Epoch:0148, train_loss=2.66413, train_acc=0.99813, val_loss=3.81649, val_acc=0.94028, time=0.98201
Epoch:0149, train_loss=2.66408, train_acc=0.99813, val_loss=3.81649, val_acc=0.94028, time=0.80601
Epoch:0150, train_loss=2.66404, train_acc=0.99813, val_loss=3.81649, val_acc=0.94028, time=1.02601
Epoch:0151, train_loss=2.66400, train_acc=0.99813, val_loss=3.81649, val_acc=0.94028, time=0.84901
Epoch:0152, train_loss=2.66396, train_acc=0.99813, val_loss=3.81649, val_acc=0.94028, time=0.83701
Epoch:0153, train_loss=2.66392, train_acc=0.99813, val_loss=3.81649, val_acc=0.94028, time=0.93901
Epoch:0154, train_loss=2.66389, train_acc=0.99813, val_loss=3.81649, val_acc=0.94028, time=1.04101
Early stopping...

Optimization Finished!

Test set results: loss= 3.43816, accuracy= 0.92445, time= 0.24701

Test Precision, Recall and F1-Score...
              precision    recall  f1-score   support

           0     0.8406    0.9587    0.8958       121
           1     0.9632    0.9908    0.9768      1083
           2     0.9618    0.9411    0.9513       696
           3     1.0000    0.8235    0.9032        17
           4     0.8846    0.9200    0.9020        25
           5     1.0000    0.3333    0.5000         3
           6     0.7030    0.9467    0.8068        75
           7     0.9000    0.9000    0.9000        10
           8     0.9167    0.9167    0.9167        12
           9     0.8194    0.7284    0.7712        81
          10     0.7917    0.8736    0.8306        87
          11     1.0000    0.3333    0.5000        12
          12     0.7500    0.9000    0.8182        10
          13     0.8929    0.6944    0.7812        36
          14     0.7857    1.0000    0.8800        11
          15     0.8387    0.9286    0.8814        28
          16     1.0000    0.7273    0.8421        11
          17     0.8235    0.9333    0.8750        15
          18     1.0000    1.0000    1.0000         4
          19     1.0000    0.7778    0.8750         9
          20     0.9565    1.0000    0.9778        22
          21     1.0000    0.8889    0.9412         9
          22     1.0000    0.8000    0.8889        15
          23     0.0000    0.0000    0.0000         1
          24     0.8500    0.8500    0.8500        20
          25     0.8333    0.7692    0.8000        13
          26     0.9000    0.7500    0.8182        12
          27     0.8571    0.7500    0.8000         8
          28     0.9333    0.7368    0.8235        19
          29     1.0000    0.9167    0.9565        12
          30     1.0000    0.5000    0.6667         4
          31     0.5000    0.6000    0.5455         5
          32     1.0000    0.2857    0.4444         7
          33     1.0000    0.8889    0.9412         9
          34     1.0000    1.0000    1.0000         9
          35     1.0000    1.0000    1.0000         1
          36     0.0000    0.0000    0.0000         1
          37     1.0000    1.0000    1.0000         3
          38     1.0000    0.1667    0.2857         6
          39     0.0000    0.0000    0.0000         4
          40     1.0000    0.2000    0.3333         5
          41     0.8333    1.0000    0.9091         5
          42     1.0000    1.0000    1.0000         2
          43     1.0000    0.3333    0.5000         3
          44     1.0000    1.0000    1.0000         1
          45     0.7143    0.5556    0.6250         9
          46     0.0000    0.0000    0.0000         1
          47     0.5000    0.1667    0.2500         6
          48     1.0000    0.3333    0.5000         3
          49     0.7500    0.7500    0.7500         4
          50     0.0000    0.0000    0.0000         1
          51     0.0000    0.0000    0.0000         2

    accuracy                         0.9245      2568
   macro avg     0.7981    0.6706    0.7003      2568
weighted avg     0.9256    0.9245    0.9198      2568


Macro average Test Precision, Recall and F1-Score...
(0.7980715488610356, 0.6705617421754536, 0.7002739819341826, None)

Micro average Test Precision, Recall and F1-Score...
(0.9244548286604362, 0.9244548286604362, 0.9244548286604362, None)

Embeddings:
Word_embeddings:8892
Train_doc_embeddings:6532
Test_doc_embeddings:2568

Elapsed time is 153.903425 seconds.
