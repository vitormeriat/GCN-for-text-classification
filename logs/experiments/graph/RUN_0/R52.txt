
==================== Torch Seed: 10684127502300

Model parameters

Layer: layer1.W0 | Size: torch.Size([17992, 200])
Layer: layer2.W0 | Size: torch.Size([200, 52])

Data statistics

  Edges    Classes    Train samples    Val samples    Test samples
-------  ---------  ---------------  -------------  --------------
  17992         52             5879            653            2568

Epoch:0001, train_loss=8.79137, train_acc=0.02245, val_loss=4.44243, val_acc=0.03369, time=0.49399
Epoch:0002, train_loss=7.78999, train_acc=0.05307, val_loss=4.38280, val_acc=0.05054, time=0.47000
Epoch:0003, train_loss=6.94356, train_acc=0.10240, val_loss=4.33270, val_acc=0.07044, time=0.39800
Epoch:0004, train_loss=6.25430, train_acc=0.15853, val_loss=4.29036, val_acc=0.08729, time=0.44600
Epoch:0005, train_loss=5.67806, train_acc=0.20208, val_loss=4.25221, val_acc=0.09342, time=0.35400
Epoch:0006, train_loss=5.15356, train_acc=0.24749, val_loss=4.21632, val_acc=0.11179, time=0.35601
Epoch:0007, train_loss=4.65971, train_acc=0.30175, val_loss=4.18208, val_acc=0.13323, time=0.44200
Epoch:0008, train_loss=4.20377, train_acc=0.37710, val_loss=4.15008, val_acc=0.14855, time=0.35399
Epoch:0009, train_loss=3.80786, train_acc=0.47661, val_loss=4.12149, val_acc=0.18377, time=0.35801
Epoch:0010, train_loss=3.49155, train_acc=0.58564, val_loss=4.09762, val_acc=0.21899, time=0.40200
Epoch:0011, train_loss=3.26293, train_acc=0.67903, val_loss=4.07933, val_acc=0.26493, time=0.39700
Epoch:0012, train_loss=3.11387, train_acc=0.74809, val_loss=4.06687, val_acc=0.30168, time=0.47799
Epoch:0013, train_loss=3.02142, train_acc=0.78874, val_loss=4.05933, val_acc=0.32159, time=0.38801
Epoch:0014, train_loss=2.96191, train_acc=0.81919, val_loss=4.05533, val_acc=0.35375, time=0.43001
Epoch:0015, train_loss=2.91833, train_acc=0.83382, val_loss=4.05357, val_acc=0.37213, time=0.35700
Epoch:0016, train_loss=2.88017, train_acc=0.85048, val_loss=4.05300, val_acc=0.38438, time=0.35800
Epoch:0017, train_loss=2.84300, train_acc=0.87294, val_loss=4.05292, val_acc=0.39816, time=0.40499
Epoch:0018, train_loss=2.80654, train_acc=0.90304, val_loss=4.05296, val_acc=0.41041, time=0.39601
Epoch:0019, train_loss=2.77276, train_acc=0.92363, val_loss=4.05296, val_acc=0.42266, time=0.37900
Epoch:0020, train_loss=2.74365, train_acc=0.94098, val_loss=4.05284, val_acc=0.42266, time=0.44201
Epoch:0021, train_loss=2.72017, train_acc=0.95543, val_loss=4.05264, val_acc=0.42420, time=0.43100
Epoch:0022, train_loss=2.70222, train_acc=0.96853, val_loss=4.05236, val_acc=0.42726, time=0.43599
Epoch:0023, train_loss=2.68914, train_acc=0.97857, val_loss=4.05206, val_acc=0.42879, time=0.46501
Epoch:0024, train_loss=2.68001, train_acc=0.98690, val_loss=4.05174, val_acc=0.42879, time=0.43099
Epoch:0025, train_loss=2.67388, train_acc=0.99115, val_loss=4.05143, val_acc=0.42726, time=0.46300
Epoch:0026, train_loss=2.66982, train_acc=0.99337, val_loss=4.05114, val_acc=0.42726, time=0.37900
Epoch:0027, train_loss=2.66704, train_acc=0.99558, val_loss=4.05086, val_acc=0.42726, time=0.35700
Epoch:0028, train_loss=2.66508, train_acc=0.99660, val_loss=4.05061, val_acc=0.42420, time=0.42501
Epoch:0029, train_loss=2.66369, train_acc=0.99762, val_loss=4.05038, val_acc=0.42726, time=0.51799
Epoch:0030, train_loss=2.66271, train_acc=0.99830, val_loss=4.05019, val_acc=0.42573, time=0.44701
Epoch:0031, train_loss=2.66201, train_acc=0.99966, val_loss=4.05001, val_acc=0.42879, time=0.46499
Epoch:0032, train_loss=2.66153, train_acc=0.99983, val_loss=4.04985, val_acc=0.42879, time=0.53101
Epoch:0033, train_loss=2.66119, train_acc=0.99983, val_loss=4.04972, val_acc=0.43185, time=0.39399
Epoch:0034, train_loss=2.66096, train_acc=0.99983, val_loss=4.04959, val_acc=0.43185, time=0.39900
Epoch:0035, train_loss=2.66079, train_acc=0.99983, val_loss=4.04948, val_acc=0.43338, time=0.35501
Epoch:0036, train_loss=2.66066, train_acc=0.99983, val_loss=4.04938, val_acc=0.43338, time=0.35900
Epoch:0037, train_loss=2.66056, train_acc=0.99983, val_loss=4.04929, val_acc=0.43338, time=0.40200
Epoch:0038, train_loss=2.66048, train_acc=0.99983, val_loss=4.04920, val_acc=0.43338, time=0.43699
Epoch:0039, train_loss=2.66041, train_acc=1.00000, val_loss=4.04912, val_acc=0.43645, time=0.39000
Epoch:0040, train_loss=2.66036, train_acc=1.00000, val_loss=4.04905, val_acc=0.43645, time=0.40901
Epoch:0041, train_loss=2.66033, train_acc=1.00000, val_loss=4.04898, val_acc=0.43645, time=0.45799
Epoch:0042, train_loss=2.66031, train_acc=1.00000, val_loss=4.04892, val_acc=0.43798, time=0.43101
Epoch:0043, train_loss=2.66030, train_acc=1.00000, val_loss=4.04886, val_acc=0.43798, time=0.38699
Epoch:0044, train_loss=2.66029, train_acc=1.00000, val_loss=4.04880, val_acc=0.43798, time=0.43901
Epoch:0045, train_loss=2.66028, train_acc=1.00000, val_loss=4.04875, val_acc=0.43798, time=0.41800
Epoch:0046, train_loss=2.66027, train_acc=1.00000, val_loss=4.04869, val_acc=0.43798, time=0.49499
Epoch:0047, train_loss=2.66026, train_acc=1.00000, val_loss=4.04865, val_acc=0.43798, time=0.43901
Epoch:0048, train_loss=2.66026, train_acc=1.00000, val_loss=4.04860, val_acc=0.43798, time=0.38100
Epoch:0049, train_loss=2.66025, train_acc=1.00000, val_loss=4.04855, val_acc=0.43798, time=0.48799
Epoch:0050, train_loss=2.66025, train_acc=1.00000, val_loss=4.04851, val_acc=0.43798, time=0.36700
Epoch:0051, train_loss=2.66024, train_acc=1.00000, val_loss=4.04847, val_acc=0.43645, time=0.36000
Epoch:0052, train_loss=2.66024, train_acc=1.00000, val_loss=4.04843, val_acc=0.43645, time=0.37701
Epoch:0053, train_loss=2.66024, train_acc=1.00000, val_loss=4.04839, val_acc=0.43798, time=0.37602
Epoch:0054, train_loss=2.66023, train_acc=1.00000, val_loss=4.04835, val_acc=0.43798, time=0.43099
Epoch:0055, train_loss=2.66023, train_acc=1.00000, val_loss=4.04832, val_acc=0.43798, time=0.35500
Epoch:0056, train_loss=2.66023, train_acc=1.00000, val_loss=4.04828, val_acc=0.43798, time=0.45501
Epoch:0057, train_loss=2.66023, train_acc=1.00000, val_loss=4.04825, val_acc=0.43951, time=0.35700
Epoch:0058, train_loss=2.66023, train_acc=1.00000, val_loss=4.04821, val_acc=0.43951, time=0.40399
Epoch:0059, train_loss=2.66022, train_acc=1.00000, val_loss=4.04818, val_acc=0.43951, time=0.49201
Epoch:0060, train_loss=2.66022, train_acc=1.00000, val_loss=4.04815, val_acc=0.44104, time=0.37999
Epoch:0061, train_loss=2.66022, train_acc=1.00000, val_loss=4.04812, val_acc=0.44104, time=0.39500
Epoch:0062, train_loss=2.66022, train_acc=1.00000, val_loss=4.04809, val_acc=0.44104, time=0.39401
Epoch:0063, train_loss=2.66022, train_acc=1.00000, val_loss=4.04806, val_acc=0.44104, time=0.38000
Epoch:0064, train_loss=2.66022, train_acc=1.00000, val_loss=4.04804, val_acc=0.44104, time=0.35800
Epoch:0065, train_loss=2.66021, train_acc=1.00000, val_loss=4.04801, val_acc=0.44104, time=0.36400
Epoch:0066, train_loss=2.66021, train_acc=1.00000, val_loss=4.04798, val_acc=0.44104, time=0.44099
Epoch:0067, train_loss=2.66021, train_acc=1.00000, val_loss=4.04796, val_acc=0.44257, time=0.43401
Epoch:0068, train_loss=2.66021, train_acc=1.00000, val_loss=4.04793, val_acc=0.44257, time=0.46200
Epoch:0069, train_loss=2.66021, train_acc=1.00000, val_loss=4.04791, val_acc=0.44257, time=0.38700
Epoch:0070, train_loss=2.66021, train_acc=1.00000, val_loss=4.04788, val_acc=0.44257, time=0.35300
Epoch:0071, train_loss=2.66021, train_acc=1.00000, val_loss=4.04786, val_acc=0.44257, time=0.37700
Epoch:0072, train_loss=2.66021, train_acc=1.00000, val_loss=4.04783, val_acc=0.44257, time=0.39200
Epoch:0073, train_loss=2.66021, train_acc=1.00000, val_loss=4.04781, val_acc=0.44257, time=0.39299
Epoch:0074, train_loss=2.66021, train_acc=1.00000, val_loss=4.04779, val_acc=0.44257, time=0.41701
Epoch:0075, train_loss=2.66020, train_acc=1.00000, val_loss=4.04777, val_acc=0.44257, time=0.40200
Epoch:0076, train_loss=2.66020, train_acc=1.00000, val_loss=4.04775, val_acc=0.44257, time=0.40200
Epoch:0077, train_loss=2.66020, train_acc=1.00000, val_loss=4.04773, val_acc=0.44410, time=0.38799
Epoch:0078, train_loss=2.66020, train_acc=1.00000, val_loss=4.04771, val_acc=0.44410, time=0.37300
Epoch:0079, train_loss=2.66020, train_acc=1.00000, val_loss=4.04769, val_acc=0.44410, time=0.42401
Epoch:0080, train_loss=2.66020, train_acc=1.00000, val_loss=4.04767, val_acc=0.44410, time=0.35800
Epoch:0081, train_loss=2.66020, train_acc=1.00000, val_loss=4.04765, val_acc=0.44410, time=0.39299
Epoch:0082, train_loss=2.66020, train_acc=1.00000, val_loss=4.04763, val_acc=0.44410, time=0.37801
Epoch:0083, train_loss=2.66020, train_acc=1.00000, val_loss=4.04761, val_acc=0.44410, time=0.35600
Epoch:0084, train_loss=2.66020, train_acc=1.00000, val_loss=4.04759, val_acc=0.44410, time=0.37600
Epoch:0085, train_loss=2.66020, train_acc=1.00000, val_loss=4.04757, val_acc=0.44410, time=0.41900
Epoch:0086, train_loss=2.66020, train_acc=1.00000, val_loss=4.04755, val_acc=0.44410, time=0.37199
Epoch:0087, train_loss=2.66020, train_acc=1.00000, val_loss=4.04753, val_acc=0.44410, time=0.41101
Epoch:0088, train_loss=2.66020, train_acc=1.00000, val_loss=4.04752, val_acc=0.44410, time=0.35400
Epoch:0089, train_loss=2.66020, train_acc=1.00000, val_loss=4.04750, val_acc=0.44410, time=0.41300
Epoch:0090, train_loss=2.66020, train_acc=1.00000, val_loss=4.04748, val_acc=0.44410, time=0.35600
Epoch:0091, train_loss=2.66020, train_acc=1.00000, val_loss=4.04747, val_acc=0.44410, time=0.35600
Epoch:0092, train_loss=2.66020, train_acc=1.00000, val_loss=4.04745, val_acc=0.44410, time=0.47200
Epoch:0093, train_loss=2.66020, train_acc=1.00000, val_loss=4.04743, val_acc=0.44410, time=0.48399
Epoch:0094, train_loss=2.66019, train_acc=1.00000, val_loss=4.04742, val_acc=0.44410, time=0.45101
Epoch:0095, train_loss=2.66019, train_acc=1.00000, val_loss=4.04740, val_acc=0.44410, time=0.40400
Epoch:0096, train_loss=2.66019, train_acc=1.00000, val_loss=4.04738, val_acc=0.44410, time=0.35599
Epoch:0097, train_loss=2.66019, train_acc=1.00000, val_loss=4.04737, val_acc=0.44564, time=0.38100
Epoch:0098, train_loss=2.66019, train_acc=1.00000, val_loss=4.04735, val_acc=0.44564, time=0.41300
Epoch:0099, train_loss=2.66019, train_acc=1.00000, val_loss=4.04734, val_acc=0.44564, time=0.40001
Epoch:0100, train_loss=2.66019, train_acc=1.00000, val_loss=4.04732, val_acc=0.44564, time=0.53900
Epoch:0101, train_loss=2.66019, train_acc=1.00000, val_loss=4.04731, val_acc=0.44564, time=0.42599
Epoch:0102, train_loss=2.66019, train_acc=1.00000, val_loss=4.04729, val_acc=0.44564, time=0.60000
Epoch:0103, train_loss=2.66019, train_acc=1.00000, val_loss=4.04728, val_acc=0.44564, time=0.53901
Epoch:0104, train_loss=2.66019, train_acc=1.00000, val_loss=4.04726, val_acc=0.44564, time=0.41100
Epoch:0105, train_loss=2.66019, train_acc=1.00000, val_loss=4.04725, val_acc=0.44564, time=0.50899
Epoch:0106, train_loss=2.66019, train_acc=1.00000, val_loss=4.04724, val_acc=0.44564, time=0.41000
Epoch:0107, train_loss=2.66019, train_acc=1.00000, val_loss=4.04722, val_acc=0.44564, time=0.44201
Epoch:0108, train_loss=2.66019, train_acc=1.00000, val_loss=4.04721, val_acc=0.44564, time=0.48500
Epoch:0109, train_loss=2.66019, train_acc=1.00000, val_loss=4.04719, val_acc=0.44564, time=0.38999
Epoch:0110, train_loss=2.66019, train_acc=1.00000, val_loss=4.04718, val_acc=0.44564, time=0.43400
Epoch:0111, train_loss=2.66019, train_acc=1.00000, val_loss=4.04717, val_acc=0.44564, time=0.43601
Epoch:0112, train_loss=2.66019, train_acc=1.00000, val_loss=4.04715, val_acc=0.44564, time=0.44199
Epoch:0113, train_loss=2.66019, train_acc=1.00000, val_loss=4.04714, val_acc=0.44564, time=0.43300
Epoch:0114, train_loss=2.66019, train_acc=1.00000, val_loss=4.04713, val_acc=0.44564, time=0.35600
Epoch:0115, train_loss=2.66019, train_acc=1.00000, val_loss=4.04711, val_acc=0.44564, time=0.52001
Epoch:0116, train_loss=2.66019, train_acc=1.00000, val_loss=4.04710, val_acc=0.44564, time=0.35400
Epoch:0117, train_loss=2.66019, train_acc=1.00000, val_loss=4.04709, val_acc=0.44564, time=0.44700
Epoch:0118, train_loss=2.66019, train_acc=1.00000, val_loss=4.04707, val_acc=0.44564, time=0.35200
Epoch:0119, train_loss=2.66019, train_acc=1.00000, val_loss=4.04706, val_acc=0.44564, time=0.36100
Epoch:0120, train_loss=2.66019, train_acc=1.00000, val_loss=4.04705, val_acc=0.44564, time=0.45901
Epoch:0121, train_loss=2.66019, train_acc=1.00000, val_loss=4.04703, val_acc=0.44564, time=0.42300
Epoch:0122, train_loss=2.66019, train_acc=1.00000, val_loss=4.04702, val_acc=0.44564, time=0.50800
Epoch:0123, train_loss=2.66019, train_acc=1.00000, val_loss=4.04701, val_acc=0.44564, time=0.35600
Epoch:0124, train_loss=2.66019, train_acc=1.00000, val_loss=4.04700, val_acc=0.44564, time=0.37499
Epoch:0125, train_loss=2.66019, train_acc=1.00000, val_loss=4.04698, val_acc=0.44564, time=0.36700
Epoch:0126, train_loss=2.66019, train_acc=1.00000, val_loss=4.04697, val_acc=0.44564, time=0.40900
Epoch:0127, train_loss=2.66019, train_acc=1.00000, val_loss=4.04696, val_acc=0.44564, time=0.43500
Epoch:0128, train_loss=2.66018, train_acc=1.00000, val_loss=4.04695, val_acc=0.44564, time=0.35600
Epoch:0129, train_loss=2.66018, train_acc=1.00000, val_loss=4.04694, val_acc=0.44564, time=0.40800
Epoch:0130, train_loss=2.66018, train_acc=1.00000, val_loss=4.04692, val_acc=0.44564, time=0.40600
Epoch:0131, train_loss=2.66018, train_acc=1.00000, val_loss=4.04691, val_acc=0.44564, time=0.35300
Epoch:0132, train_loss=2.66018, train_acc=1.00000, val_loss=4.04690, val_acc=0.44564, time=0.42700
Epoch:0133, train_loss=2.66018, train_acc=1.00000, val_loss=4.04689, val_acc=0.44564, time=0.44000
Epoch:0134, train_loss=2.66018, train_acc=1.00000, val_loss=4.04688, val_acc=0.44564, time=0.44600
Epoch:0135, train_loss=2.66018, train_acc=1.00000, val_loss=4.04687, val_acc=0.44564, time=0.39400
Epoch:0136, train_loss=2.66018, train_acc=1.00000, val_loss=4.04685, val_acc=0.44564, time=0.35500
Epoch:0137, train_loss=2.66018, train_acc=1.00000, val_loss=4.04684, val_acc=0.44564, time=0.39800
Epoch:0138, train_loss=2.66018, train_acc=1.00000, val_loss=4.04683, val_acc=0.44564, time=0.53000
Epoch:0139, train_loss=2.66018, train_acc=1.00000, val_loss=4.04682, val_acc=0.44564, time=0.50900
Epoch:0140, train_loss=2.66018, train_acc=1.00000, val_loss=4.04681, val_acc=0.44564, time=0.35400
Epoch:0141, train_loss=2.66018, train_acc=1.00000, val_loss=4.04680, val_acc=0.44564, time=0.42501
Epoch:0142, train_loss=2.66018, train_acc=1.00000, val_loss=4.04679, val_acc=0.44564, time=0.49799
Epoch:0143, train_loss=2.66018, train_acc=1.00000, val_loss=4.04678, val_acc=0.44564, time=0.49000
Epoch:0144, train_loss=2.66018, train_acc=1.00000, val_loss=4.04677, val_acc=0.44564, time=0.38501
Epoch:0145, train_loss=2.66018, train_acc=1.00000, val_loss=4.04676, val_acc=0.44564, time=0.40500
Epoch:0146, train_loss=2.66018, train_acc=1.00000, val_loss=4.04675, val_acc=0.44564, time=0.44000
Epoch:0147, train_loss=2.66018, train_acc=1.00000, val_loss=4.04673, val_acc=0.44564, time=0.47000
Epoch:0148, train_loss=2.66018, train_acc=1.00000, val_loss=4.04672, val_acc=0.44564, time=0.43900
Epoch:0149, train_loss=2.66018, train_acc=1.00000, val_loss=4.04671, val_acc=0.44564, time=0.45099
Epoch:0150, train_loss=2.66018, train_acc=1.00000, val_loss=4.04670, val_acc=0.44564, time=0.35201
Epoch:0151, train_loss=2.66018, train_acc=1.00000, val_loss=4.04669, val_acc=0.44564, time=0.45199
Epoch:0152, train_loss=2.66018, train_acc=1.00000, val_loss=4.04668, val_acc=0.44564, time=0.49400
Epoch:0153, train_loss=2.66018, train_acc=1.00000, val_loss=4.04667, val_acc=0.44564, time=0.50600
Epoch:0154, train_loss=2.66018, train_acc=1.00000, val_loss=4.04666, val_acc=0.44564, time=0.35400
Epoch:0155, train_loss=2.66018, train_acc=1.00000, val_loss=4.04665, val_acc=0.44564, time=0.35700
Epoch:0156, train_loss=2.66018, train_acc=1.00000, val_loss=4.04664, val_acc=0.44564, time=0.49400
Epoch:0157, train_loss=2.66018, train_acc=1.00000, val_loss=4.04663, val_acc=0.44564, time=0.44800
Epoch:0158, train_loss=2.66018, train_acc=1.00000, val_loss=4.04662, val_acc=0.44564, time=0.40501
Epoch:0159, train_loss=2.66018, train_acc=1.00000, val_loss=4.04661, val_acc=0.44564, time=0.35500
Epoch:0160, train_loss=2.66018, train_acc=1.00000, val_loss=4.04660, val_acc=0.44564, time=0.38800
Epoch:0161, train_loss=2.66018, train_acc=1.00000, val_loss=4.04659, val_acc=0.44564, time=0.36699
Epoch:0162, train_loss=2.66018, train_acc=1.00000, val_loss=4.04658, val_acc=0.44564, time=0.46899
Epoch:0163, train_loss=2.66018, train_acc=1.00000, val_loss=4.04657, val_acc=0.44564, time=0.51700
Epoch:0164, train_loss=2.66018, train_acc=1.00000, val_loss=4.04656, val_acc=0.44564, time=0.44301
Epoch:0165, train_loss=2.66018, train_acc=1.00000, val_loss=4.04655, val_acc=0.44564, time=0.37700
Epoch:0166, train_loss=2.66018, train_acc=1.00000, val_loss=4.04654, val_acc=0.44564, time=0.39400
Epoch:0167, train_loss=2.66018, train_acc=1.00000, val_loss=4.04653, val_acc=0.44564, time=0.35599
Epoch:0168, train_loss=2.66018, train_acc=1.00000, val_loss=4.04653, val_acc=0.44564, time=0.36500
Epoch:0169, train_loss=2.66018, train_acc=1.00000, val_loss=4.04652, val_acc=0.44564, time=0.36401
Epoch:0170, train_loss=2.66018, train_acc=1.00000, val_loss=4.04651, val_acc=0.44564, time=0.35300
Epoch:0171, train_loss=2.66018, train_acc=1.00000, val_loss=4.04650, val_acc=0.44564, time=0.37899
Epoch:0172, train_loss=2.66018, train_acc=1.00000, val_loss=4.04649, val_acc=0.44564, time=0.35501
Epoch:0173, train_loss=2.66018, train_acc=1.00000, val_loss=4.04648, val_acc=0.44564, time=0.35599
Epoch:0174, train_loss=2.66018, train_acc=1.00000, val_loss=4.04647, val_acc=0.44564, time=0.36400
Epoch:0175, train_loss=2.66018, train_acc=1.00000, val_loss=4.04646, val_acc=0.44564, time=0.42501
Epoch:0176, train_loss=2.66018, train_acc=1.00000, val_loss=4.04645, val_acc=0.44564, time=0.39000
Epoch:0177, train_loss=2.66018, train_acc=1.00000, val_loss=4.04644, val_acc=0.44564, time=0.35300
Epoch:0178, train_loss=2.66018, train_acc=1.00000, val_loss=4.04643, val_acc=0.44564, time=0.41100
Epoch:0179, train_loss=2.66018, train_acc=1.00000, val_loss=4.04642, val_acc=0.44564, time=0.46100
Epoch:0180, train_loss=2.66018, train_acc=1.00000, val_loss=4.04642, val_acc=0.44564, time=0.41300
Epoch:0181, train_loss=2.66018, train_acc=1.00000, val_loss=4.04641, val_acc=0.44564, time=0.44099
Epoch:0182, train_loss=2.66018, train_acc=1.00000, val_loss=4.04640, val_acc=0.44564, time=0.45900
Epoch:0183, train_loss=2.66018, train_acc=1.00000, val_loss=4.04639, val_acc=0.44564, time=0.53200
Epoch:0184, train_loss=2.66018, train_acc=1.00000, val_loss=4.04638, val_acc=0.44564, time=0.44501
Epoch:0185, train_loss=2.66018, train_acc=1.00000, val_loss=4.04637, val_acc=0.44564, time=0.35500
Epoch:0186, train_loss=2.66018, train_acc=1.00000, val_loss=4.04636, val_acc=0.44564, time=0.50699
Epoch:0187, train_loss=2.66018, train_acc=1.00000, val_loss=4.04635, val_acc=0.44564, time=0.43200
Epoch:0188, train_loss=2.66018, train_acc=1.00000, val_loss=4.04635, val_acc=0.44564, time=0.44001
Epoch:0189, train_loss=2.66018, train_acc=1.00000, val_loss=4.04634, val_acc=0.44564, time=0.41500
Epoch:0190, train_loss=2.66018, train_acc=1.00000, val_loss=4.04633, val_acc=0.44564, time=0.39199
Epoch:0191, train_loss=2.66018, train_acc=1.00000, val_loss=4.04632, val_acc=0.44564, time=0.39301
Epoch:0192, train_loss=2.66018, train_acc=1.00000, val_loss=4.04631, val_acc=0.44564, time=0.35700
Epoch:0193, train_loss=2.66017, train_acc=1.00000, val_loss=4.04630, val_acc=0.44564, time=0.46800
Epoch:0194, train_loss=2.66017, train_acc=1.00000, val_loss=4.04630, val_acc=0.44564, time=0.37599
Epoch:0195, train_loss=2.66017, train_acc=1.00000, val_loss=4.04629, val_acc=0.44564, time=0.36900
Epoch:0196, train_loss=2.66017, train_acc=1.00000, val_loss=4.04628, val_acc=0.44564, time=0.39101
Epoch:0197, train_loss=2.66017, train_acc=1.00000, val_loss=4.04627, val_acc=0.44564, time=0.35600
Epoch:0198, train_loss=2.66017, train_acc=1.00000, val_loss=4.04626, val_acc=0.44564, time=0.36900
Epoch:0199, train_loss=2.66017, train_acc=1.00000, val_loss=4.04626, val_acc=0.44564, time=0.38699
Epoch:0200, train_loss=2.66017, train_acc=1.00000, val_loss=4.04625, val_acc=0.44564, time=0.56999

Optimization Finished!

Test set results: loss= 4.35906, accuracy= 0.43069, time= 0.15600

Test Precision, Recall and F1-Score...
              precision    recall  f1-score   support

           0     0.5805    0.7821    0.6664      1083
           1     0.2105    0.1983    0.2043       121
           2     0.5972    0.3046    0.4034       696
           3     0.0000    0.0000    0.0000        15
           4     0.0714    0.0667    0.0690        15
           5     0.0000    0.0000    0.0000        17
           6     0.0385    0.0556    0.0455        36
           7     0.0541    0.0800    0.0645        25
           8     0.1250    0.0526    0.0741        19
           9     0.2000    0.0769    0.1111        13
          10     0.1273    0.0805    0.0986        87
          11     0.0000    0.0000    0.0000        20
          12     0.0235    0.0267    0.0250        75
          13     0.1111    0.0357    0.0541        28
          14     0.0000    0.0000    0.0000         9
          15     0.0000    0.0000    0.0000        22
          16     0.0000    0.0000    0.0000         5
          17     0.1111    0.0833    0.0952        12
          18     0.0460    0.0494    0.0476        81
          19     0.0000    0.0000    0.0000        10
          20     0.0000    0.0000    0.0000         2
          21     0.0000    0.0000    0.0000        12
          22     0.0000    0.0000    0.0000         1
          23     0.0000    0.0000    0.0000         9
          24     0.0000    0.0000    0.0000        12
          25     0.0000    0.0000    0.0000         5
          26     0.0000    0.0000    0.0000        10
          27     0.0250    0.0833    0.0385        12
          28     0.0000    0.0000    0.0000         3
          29     0.0000    0.0000    0.0000         3
          30     0.0000    0.0000    0.0000         9
          31     0.0000    0.0000    0.0000         9
          32     0.0000    0.0000    0.0000         8
          33     0.0000    0.0000    0.0000        11
          34     0.0000    0.0000    0.0000         5
          35     0.0000    0.0000    0.0000         4
          36     0.0000    0.0000    0.0000         4
          37     0.0000    0.0000    0.0000         3
          38     0.0000    0.0000    0.0000         4
          39     0.0000    0.0000    0.0000         1
          40     0.0000    0.0000    0.0000         6
          41     0.0000    0.0000    0.0000        11
          42     0.0000    0.0000    0.0000         9
          43     0.0000    0.0000    0.0000         6
          44     0.0000    0.0000    0.0000         1
          45     0.0000    0.0000    0.0000         1
          46     0.0000    0.0000    0.0000         1
          47     0.0000    0.0000    0.0000         7
          48     0.0000    0.0000    0.0000         1
          49     0.0000    0.0000    0.0000         2
          50     0.0000    0.0000    0.0000         4
          51     0.0000    0.0000    0.0000         3

    accuracy                         0.4307      2568
   macro avg     0.0446    0.0380    0.0384      2568
weighted avg     0.4283    0.4307    0.4096      2568


Macro average Test Precision, Recall and F1-Score...
(0.044638260842837556, 0.037994203612101256, 0.038407128768070446, None)

Micro average Test Precision, Recall and F1-Score...
(0.43068535825545173, 0.43068535825545173, 0.43068535825545173, None)

Embeddings:
Word_embeddings: 8892
Train_doc_embeddings: 6532
Test_doc_embeddings: 2568

Elapsed time is 85.215836 seconds.
