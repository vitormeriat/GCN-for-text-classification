
==================== Torch Seed: 10792931544900

Model parameters

Layer: layer1.W0 | Size: torch.Size([17992, 200])
Layer: layer2.W0 | Size: torch.Size([200, 52])

Data statistics

  Edges    Classes    Train samples    Val samples    Test samples
-------  ---------  ---------------  -------------  --------------
  17992         52             5879            653            2568

Epoch:0001, train_loss=7.92323, train_acc=0.03929, val_loss=4.30599, val_acc=0.07198, time=0.36901
Epoch:0002, train_loss=6.92875, train_acc=0.09202, val_loss=4.24961, val_acc=0.12864, time=0.42100
Epoch:0003, train_loss=6.09694, train_acc=0.17639, val_loss=4.20477, val_acc=0.18070, time=0.38200
Epoch:0004, train_loss=5.44782, train_acc=0.26535, val_loss=4.17151, val_acc=0.23737, time=0.35400
Epoch:0005, train_loss=4.95820, train_acc=0.34087, val_loss=4.14702, val_acc=0.27259, time=0.43900
Epoch:0006, train_loss=4.56927, train_acc=0.41180, val_loss=4.12736, val_acc=0.29709, time=0.46200
Epoch:0007, train_loss=4.23255, train_acc=0.47423, val_loss=4.11077, val_acc=0.31240, time=0.44000
Epoch:0008, train_loss=3.93142, train_acc=0.53036, val_loss=4.09718, val_acc=0.31547, time=0.39900
Epoch:0009, train_loss=3.67138, train_acc=0.58888, val_loss=4.08654, val_acc=0.32466, time=0.39200
Epoch:0010, train_loss=3.46032, train_acc=0.64467, val_loss=4.07876, val_acc=0.32619, time=0.41599
Epoch:0011, train_loss=3.29815, train_acc=0.69706, val_loss=4.07364, val_acc=0.33691, time=0.47701
Epoch:0012, train_loss=3.17743, train_acc=0.73601, val_loss=4.07065, val_acc=0.32925, time=0.39500
Epoch:0013, train_loss=3.08585, train_acc=0.76731, val_loss=4.06913, val_acc=0.32466, time=0.37401
Epoch:0014, train_loss=3.01156, train_acc=0.80116, val_loss=4.06849, val_acc=0.32772, time=0.42400
Epoch:0015, train_loss=2.94775, train_acc=0.83348, val_loss=4.06821, val_acc=0.32466, time=0.43199
Epoch:0016, train_loss=2.89137, train_acc=0.86545, val_loss=4.06800, val_acc=0.32159, time=0.35401
Epoch:0017, train_loss=2.84198, train_acc=0.89352, val_loss=4.06775, val_acc=0.32619, time=0.43599
Epoch:0018, train_loss=2.80019, train_acc=0.91342, val_loss=4.06742, val_acc=0.33231, time=0.35701
Epoch:0019, train_loss=2.76595, train_acc=0.93366, val_loss=4.06705, val_acc=0.32772, time=0.43700
Epoch:0020, train_loss=2.73879, train_acc=0.94846, val_loss=4.06665, val_acc=0.32772, time=0.41900
Epoch:0021, train_loss=2.71790, train_acc=0.96122, val_loss=4.06624, val_acc=0.32619, time=0.44802
Epoch:0022, train_loss=2.70231, train_acc=0.97125, val_loss=4.06582, val_acc=0.32312, time=0.43098
Epoch:0023, train_loss=2.69091, train_acc=0.98061, val_loss=4.06541, val_acc=0.32006, time=0.35401
Epoch:0024, train_loss=2.68260, train_acc=0.98520, val_loss=4.06500, val_acc=0.32006, time=0.35700
Epoch:0025, train_loss=2.67655, train_acc=0.99013, val_loss=4.06462, val_acc=0.32159, time=0.39399
Epoch:0026, train_loss=2.67214, train_acc=0.99286, val_loss=4.06425, val_acc=0.32619, time=0.35701
Epoch:0027, train_loss=2.66892, train_acc=0.99490, val_loss=4.06390, val_acc=0.32466, time=0.42000
Epoch:0028, train_loss=2.66655, train_acc=0.99592, val_loss=4.06359, val_acc=0.32619, time=0.35901
Epoch:0029, train_loss=2.66478, train_acc=0.99677, val_loss=4.06329, val_acc=0.32925, time=0.36400
Epoch:0030, train_loss=2.66349, train_acc=0.99830, val_loss=4.06302, val_acc=0.32925, time=0.41499
Epoch:0031, train_loss=2.66259, train_acc=0.99881, val_loss=4.06278, val_acc=0.32772, time=0.41601
Epoch:0032, train_loss=2.66196, train_acc=0.99915, val_loss=4.06256, val_acc=0.32925, time=0.36000
Epoch:0033, train_loss=2.66151, train_acc=0.99932, val_loss=4.06235, val_acc=0.33078, time=0.46800
Epoch:0034, train_loss=2.66118, train_acc=0.99932, val_loss=4.06216, val_acc=0.33231, time=0.46299
Epoch:0035, train_loss=2.66091, train_acc=0.99966, val_loss=4.06199, val_acc=0.33384, time=0.44601
Epoch:0036, train_loss=2.66071, train_acc=0.99966, val_loss=4.06182, val_acc=0.33538, time=0.48099
Epoch:0037, train_loss=2.66056, train_acc=0.99983, val_loss=4.06167, val_acc=0.33538, time=0.43501
Epoch:0038, train_loss=2.66046, train_acc=1.00000, val_loss=4.06153, val_acc=0.33691, time=0.35200
Epoch:0039, train_loss=2.66040, train_acc=1.00000, val_loss=4.06140, val_acc=0.33691, time=0.38899
Epoch:0040, train_loss=2.66036, train_acc=1.00000, val_loss=4.06128, val_acc=0.33844, time=0.42601
Epoch:0041, train_loss=2.66033, train_acc=1.00000, val_loss=4.06116, val_acc=0.33844, time=0.40899
Epoch:0042, train_loss=2.66031, train_acc=1.00000, val_loss=4.06105, val_acc=0.34150, time=0.36300
Epoch:0043, train_loss=2.66030, train_acc=1.00000, val_loss=4.06095, val_acc=0.34303, time=0.49801
Epoch:0044, train_loss=2.66029, train_acc=1.00000, val_loss=4.06086, val_acc=0.34609, time=0.35600
Epoch:0045, train_loss=2.66028, train_acc=1.00000, val_loss=4.06076, val_acc=0.34609, time=0.42100
Epoch:0046, train_loss=2.66027, train_acc=1.00000, val_loss=4.06068, val_acc=0.34763, time=0.40600
Epoch:0047, train_loss=2.66026, train_acc=1.00000, val_loss=4.06059, val_acc=0.34763, time=0.39799
Epoch:0048, train_loss=2.66026, train_acc=1.00000, val_loss=4.06051, val_acc=0.34763, time=0.41901
Epoch:0049, train_loss=2.66025, train_acc=1.00000, val_loss=4.06044, val_acc=0.34763, time=0.39000
Epoch:0050, train_loss=2.66025, train_acc=1.00000, val_loss=4.06036, val_acc=0.34763, time=0.38799
Epoch:0051, train_loss=2.66025, train_acc=1.00000, val_loss=4.06029, val_acc=0.34763, time=0.35399
Epoch:0052, train_loss=2.66024, train_acc=1.00000, val_loss=4.06023, val_acc=0.34763, time=0.45100
Epoch:0053, train_loss=2.66024, train_acc=1.00000, val_loss=4.06016, val_acc=0.34763, time=0.48000
Epoch:0054, train_loss=2.66024, train_acc=1.00000, val_loss=4.06010, val_acc=0.34916, time=0.35800
Epoch:0055, train_loss=2.66023, train_acc=1.00000, val_loss=4.06004, val_acc=0.34916, time=0.48900
Epoch:0056, train_loss=2.66023, train_acc=1.00000, val_loss=4.05998, val_acc=0.34916, time=0.35601
Epoch:0057, train_loss=2.66023, train_acc=1.00000, val_loss=4.05993, val_acc=0.34916, time=0.43199
Epoch:0058, train_loss=2.66023, train_acc=1.00000, val_loss=4.05987, val_acc=0.34916, time=0.44201
Epoch:0059, train_loss=2.66023, train_acc=1.00000, val_loss=4.05982, val_acc=0.34916, time=0.36199
Epoch:0060, train_loss=2.66022, train_acc=1.00000, val_loss=4.05977, val_acc=0.34916, time=0.43299
Epoch:0061, train_loss=2.66022, train_acc=1.00000, val_loss=4.05972, val_acc=0.34916, time=0.39500
Epoch:0062, train_loss=2.66022, train_acc=1.00000, val_loss=4.05967, val_acc=0.34916, time=0.39301
Epoch:0063, train_loss=2.66022, train_acc=1.00000, val_loss=4.05963, val_acc=0.34916, time=0.35499
Epoch:0064, train_loss=2.66022, train_acc=1.00000, val_loss=4.05958, val_acc=0.34916, time=0.36200
Epoch:0065, train_loss=2.66022, train_acc=1.00000, val_loss=4.05954, val_acc=0.34916, time=0.51100
Epoch:0066, train_loss=2.66022, train_acc=1.00000, val_loss=4.05950, val_acc=0.34916, time=0.37501
Epoch:0067, train_loss=2.66021, train_acc=1.00000, val_loss=4.05946, val_acc=0.34916, time=0.39700
Epoch:0068, train_loss=2.66021, train_acc=1.00000, val_loss=4.05942, val_acc=0.34916, time=0.41699
Epoch:0069, train_loss=2.66021, train_acc=1.00000, val_loss=4.05938, val_acc=0.34916, time=0.35500
Epoch:0070, train_loss=2.66021, train_acc=1.00000, val_loss=4.05934, val_acc=0.35069, time=0.35600
Epoch:0071, train_loss=2.66021, train_acc=1.00000, val_loss=4.05930, val_acc=0.35069, time=0.35701
Epoch:0072, train_loss=2.66021, train_acc=1.00000, val_loss=4.05926, val_acc=0.35069, time=0.35500
Epoch:0073, train_loss=2.66021, train_acc=1.00000, val_loss=4.05923, val_acc=0.35069, time=0.51799
Epoch:0074, train_loss=2.66021, train_acc=1.00000, val_loss=4.05919, val_acc=0.35069, time=0.35500
Epoch:0075, train_loss=2.66021, train_acc=1.00000, val_loss=4.05916, val_acc=0.35069, time=0.41601
Epoch:0076, train_loss=2.66021, train_acc=1.00000, val_loss=4.05912, val_acc=0.35069, time=0.35500
Epoch:0077, train_loss=2.66021, train_acc=1.00000, val_loss=4.05909, val_acc=0.35069, time=0.35400
Epoch:0078, train_loss=2.66021, train_acc=1.00000, val_loss=4.05906, val_acc=0.35069, time=0.49900
Epoch:0079, train_loss=2.66020, train_acc=1.00000, val_loss=4.05903, val_acc=0.35222, time=0.47799
Epoch:0080, train_loss=2.66020, train_acc=1.00000, val_loss=4.05900, val_acc=0.35222, time=0.40801
Epoch:0081, train_loss=2.66020, train_acc=1.00000, val_loss=4.05897, val_acc=0.35222, time=0.38300
Epoch:0082, train_loss=2.66020, train_acc=1.00000, val_loss=4.05894, val_acc=0.35375, time=0.36099
Epoch:0083, train_loss=2.66020, train_acc=1.00000, val_loss=4.05891, val_acc=0.35375, time=0.50199
Epoch:0084, train_loss=2.66020, train_acc=1.00000, val_loss=4.05888, val_acc=0.35375, time=0.46901
Epoch:0085, train_loss=2.66020, train_acc=1.00000, val_loss=4.05885, val_acc=0.35375, time=0.36000
Epoch:0086, train_loss=2.66020, train_acc=1.00000, val_loss=4.05882, val_acc=0.35528, time=0.35800
Epoch:0087, train_loss=2.66020, train_acc=1.00000, val_loss=4.05879, val_acc=0.35528, time=0.40099
Epoch:0088, train_loss=2.66020, train_acc=1.00000, val_loss=4.05876, val_acc=0.35528, time=0.37701
Epoch:0089, train_loss=2.66020, train_acc=1.00000, val_loss=4.05874, val_acc=0.35528, time=0.35599
Epoch:0090, train_loss=2.66020, train_acc=1.00000, val_loss=4.05871, val_acc=0.35528, time=0.37300
Epoch:0091, train_loss=2.66020, train_acc=1.00000, val_loss=4.05868, val_acc=0.35528, time=0.38401
Epoch:0092, train_loss=2.66020, train_acc=1.00000, val_loss=4.05866, val_acc=0.35528, time=0.35600
Epoch:0093, train_loss=2.66020, train_acc=1.00000, val_loss=4.05863, val_acc=0.35375, time=0.45300
Epoch:0094, train_loss=2.66020, train_acc=1.00000, val_loss=4.05861, val_acc=0.35375, time=0.36300
Epoch:0095, train_loss=2.66020, train_acc=1.00000, val_loss=4.05858, val_acc=0.35375, time=0.37800
Epoch:0096, train_loss=2.66020, train_acc=1.00000, val_loss=4.05856, val_acc=0.35375, time=0.41100
Epoch:0097, train_loss=2.66020, train_acc=1.00000, val_loss=4.05853, val_acc=0.35528, time=0.42200
Epoch:0098, train_loss=2.66020, train_acc=1.00000, val_loss=4.05851, val_acc=0.35528, time=0.50199
Epoch:0099, train_loss=2.66020, train_acc=1.00000, val_loss=4.05848, val_acc=0.35528, time=0.38700
Epoch:0100, train_loss=2.66019, train_acc=1.00000, val_loss=4.05846, val_acc=0.35681, time=0.37701
Epoch:0101, train_loss=2.66019, train_acc=1.00000, val_loss=4.05844, val_acc=0.35681, time=0.46099
Epoch:0102, train_loss=2.66019, train_acc=1.00000, val_loss=4.05841, val_acc=0.35681, time=0.35401
Epoch:0103, train_loss=2.66019, train_acc=1.00000, val_loss=4.05839, val_acc=0.35681, time=0.36499
Epoch:0104, train_loss=2.66019, train_acc=1.00000, val_loss=4.05837, val_acc=0.35835, time=0.35500
Epoch:0105, train_loss=2.66019, train_acc=1.00000, val_loss=4.05834, val_acc=0.35835, time=0.35602
Epoch:0106, train_loss=2.66019, train_acc=1.00000, val_loss=4.05832, val_acc=0.35835, time=0.37199
Epoch:0107, train_loss=2.66019, train_acc=1.00000, val_loss=4.05830, val_acc=0.35835, time=0.35600
Epoch:0108, train_loss=2.66019, train_acc=1.00000, val_loss=4.05828, val_acc=0.35835, time=0.36400
Epoch:0109, train_loss=2.66019, train_acc=1.00000, val_loss=4.05825, val_acc=0.35835, time=0.46200
Epoch:0110, train_loss=2.66019, train_acc=1.00000, val_loss=4.05823, val_acc=0.35835, time=0.38200
Epoch:0111, train_loss=2.66019, train_acc=1.00000, val_loss=4.05821, val_acc=0.35835, time=0.40800
Epoch:0112, train_loss=2.66019, train_acc=1.00000, val_loss=4.05819, val_acc=0.35835, time=0.35900
Epoch:0113, train_loss=2.66019, train_acc=1.00000, val_loss=4.05817, val_acc=0.35835, time=0.36501
Epoch:0114, train_loss=2.66019, train_acc=1.00000, val_loss=4.05815, val_acc=0.35835, time=0.35799
Epoch:0115, train_loss=2.66019, train_acc=1.00000, val_loss=4.05813, val_acc=0.35835, time=0.37800
Epoch:0116, train_loss=2.66019, train_acc=1.00000, val_loss=4.05811, val_acc=0.35835, time=0.37400
Epoch:0117, train_loss=2.66019, train_acc=1.00000, val_loss=4.05809, val_acc=0.35835, time=0.40900
Epoch:0118, train_loss=2.66019, train_acc=1.00000, val_loss=4.05807, val_acc=0.35835, time=0.36001
Epoch:0119, train_loss=2.66019, train_acc=1.00000, val_loss=4.05804, val_acc=0.35835, time=0.47700
Epoch:0120, train_loss=2.66019, train_acc=1.00000, val_loss=4.05802, val_acc=0.35835, time=0.40900
Epoch:0121, train_loss=2.66019, train_acc=1.00000, val_loss=4.05800, val_acc=0.35835, time=0.38700
Epoch:0122, train_loss=2.66019, train_acc=1.00000, val_loss=4.05798, val_acc=0.35835, time=0.44700
Epoch:0123, train_loss=2.66019, train_acc=1.00000, val_loss=4.05797, val_acc=0.35835, time=0.35300
Epoch:0124, train_loss=2.66019, train_acc=1.00000, val_loss=4.05795, val_acc=0.35835, time=0.40500
Epoch:0125, train_loss=2.66019, train_acc=1.00000, val_loss=4.05793, val_acc=0.35835, time=0.42100
Epoch:0126, train_loss=2.66019, train_acc=1.00000, val_loss=4.05791, val_acc=0.35835, time=0.43199
Epoch:0127, train_loss=2.66019, train_acc=1.00000, val_loss=4.05789, val_acc=0.35988, time=0.35400
Epoch:0128, train_loss=2.66019, train_acc=1.00000, val_loss=4.05787, val_acc=0.35988, time=0.35600
Epoch:0129, train_loss=2.66019, train_acc=1.00000, val_loss=4.05785, val_acc=0.35988, time=0.51801
Epoch:0130, train_loss=2.66019, train_acc=1.00000, val_loss=4.05783, val_acc=0.35988, time=0.47300
Epoch:0131, train_loss=2.66019, train_acc=1.00000, val_loss=4.05781, val_acc=0.35988, time=0.50400
Epoch:0132, train_loss=2.66019, train_acc=1.00000, val_loss=4.05779, val_acc=0.35988, time=0.35499
Epoch:0133, train_loss=2.66019, train_acc=1.00000, val_loss=4.05778, val_acc=0.35988, time=0.48900
Epoch:0134, train_loss=2.66019, train_acc=1.00000, val_loss=4.05776, val_acc=0.35988, time=0.39001
Epoch:0135, train_loss=2.66019, train_acc=1.00000, val_loss=4.05774, val_acc=0.35988, time=0.38900
Epoch:0136, train_loss=2.66018, train_acc=1.00000, val_loss=4.05772, val_acc=0.36141, time=0.44599
Epoch:0137, train_loss=2.66018, train_acc=1.00000, val_loss=4.05770, val_acc=0.36141, time=0.39901
Epoch:0138, train_loss=2.66018, train_acc=1.00000, val_loss=4.05769, val_acc=0.36141, time=0.36500
Epoch:0139, train_loss=2.66018, train_acc=1.00000, val_loss=4.05767, val_acc=0.36141, time=0.38800
Epoch:0140, train_loss=2.66018, train_acc=1.00000, val_loss=4.05765, val_acc=0.36141, time=0.44899
Epoch:0141, train_loss=2.66018, train_acc=1.00000, val_loss=4.05763, val_acc=0.36141, time=0.37200
Epoch:0142, train_loss=2.66018, train_acc=1.00000, val_loss=4.05762, val_acc=0.36141, time=0.35401
Epoch:0143, train_loss=2.66018, train_acc=1.00000, val_loss=4.05760, val_acc=0.36141, time=0.43999
Epoch:0144, train_loss=2.66018, train_acc=1.00000, val_loss=4.05758, val_acc=0.36141, time=0.42101
Epoch:0145, train_loss=2.66018, train_acc=1.00000, val_loss=4.05757, val_acc=0.36141, time=0.43900
Epoch:0146, train_loss=2.66018, train_acc=1.00000, val_loss=4.05755, val_acc=0.36141, time=0.40199
Epoch:0147, train_loss=2.66018, train_acc=1.00000, val_loss=4.05753, val_acc=0.36141, time=0.39000
Epoch:0148, train_loss=2.66018, train_acc=1.00000, val_loss=4.05751, val_acc=0.36141, time=0.36700
Epoch:0149, train_loss=2.66018, train_acc=1.00000, val_loss=4.05750, val_acc=0.36141, time=0.47000
Epoch:0150, train_loss=2.66018, train_acc=1.00000, val_loss=4.05748, val_acc=0.36141, time=0.47901
Epoch:0151, train_loss=2.66018, train_acc=1.00000, val_loss=4.05746, val_acc=0.36141, time=0.54100
Epoch:0152, train_loss=2.66018, train_acc=1.00000, val_loss=4.05745, val_acc=0.36141, time=0.38700
Epoch:0153, train_loss=2.66018, train_acc=1.00000, val_loss=4.05743, val_acc=0.36141, time=0.54599
Epoch:0154, train_loss=2.66018, train_acc=1.00000, val_loss=4.05742, val_acc=0.36141, time=0.47801
Epoch:0155, train_loss=2.66018, train_acc=1.00000, val_loss=4.05740, val_acc=0.36141, time=0.41199
Epoch:0156, train_loss=2.66018, train_acc=1.00000, val_loss=4.05738, val_acc=0.36141, time=0.57200
Epoch:0157, train_loss=2.66018, train_acc=1.00000, val_loss=4.05737, val_acc=0.36141, time=0.54101
Epoch:0158, train_loss=2.66018, train_acc=1.00000, val_loss=4.05735, val_acc=0.36141, time=0.60500
Epoch:0159, train_loss=2.66018, train_acc=1.00000, val_loss=4.05734, val_acc=0.36141, time=0.55999
Epoch:0160, train_loss=2.66018, train_acc=1.00000, val_loss=4.05732, val_acc=0.36141, time=0.54601
Epoch:0161, train_loss=2.66018, train_acc=1.00000, val_loss=4.05731, val_acc=0.36294, time=0.55800
Epoch:0162, train_loss=2.66018, train_acc=1.00000, val_loss=4.05729, val_acc=0.36294, time=0.57499
Epoch:0163, train_loss=2.66018, train_acc=1.00000, val_loss=4.05727, val_acc=0.36294, time=0.58201
Epoch:0164, train_loss=2.66018, train_acc=1.00000, val_loss=4.05726, val_acc=0.36294, time=0.42900
Epoch:0165, train_loss=2.66018, train_acc=1.00000, val_loss=4.05724, val_acc=0.36294, time=0.35500
Epoch:0166, train_loss=2.66018, train_acc=1.00000, val_loss=4.05723, val_acc=0.36294, time=0.35600
Epoch:0167, train_loss=2.66018, train_acc=1.00000, val_loss=4.05721, val_acc=0.36294, time=0.41400
Epoch:0168, train_loss=2.66018, train_acc=1.00000, val_loss=4.05720, val_acc=0.36294, time=0.45300
Epoch:0169, train_loss=2.66018, train_acc=1.00000, val_loss=4.05718, val_acc=0.36294, time=0.35300
Epoch:0170, train_loss=2.66018, train_acc=1.00000, val_loss=4.05717, val_acc=0.36294, time=0.39599
Epoch:0171, train_loss=2.66018, train_acc=1.00000, val_loss=4.05715, val_acc=0.36294, time=0.40400
Epoch:0172, train_loss=2.66018, train_acc=1.00000, val_loss=4.05714, val_acc=0.36294, time=0.42801
Epoch:0173, train_loss=2.66018, train_acc=1.00000, val_loss=4.05712, val_acc=0.36294, time=0.50699
Epoch:0174, train_loss=2.66018, train_acc=1.00000, val_loss=4.05711, val_acc=0.36294, time=0.43300
Epoch:0175, train_loss=2.66018, train_acc=1.00000, val_loss=4.05710, val_acc=0.36294, time=0.50601
Epoch:0176, train_loss=2.66018, train_acc=1.00000, val_loss=4.05708, val_acc=0.36294, time=0.42500
Epoch:0177, train_loss=2.66018, train_acc=1.00000, val_loss=4.05707, val_acc=0.36294, time=0.46001
Epoch:0178, train_loss=2.66018, train_acc=1.00000, val_loss=4.05705, val_acc=0.36294, time=0.35500
Epoch:0179, train_loss=2.66018, train_acc=1.00000, val_loss=4.05704, val_acc=0.36294, time=0.37600
Epoch:0180, train_loss=2.66018, train_acc=1.00000, val_loss=4.05702, val_acc=0.36294, time=0.49799
Epoch:0181, train_loss=2.66018, train_acc=1.00000, val_loss=4.05701, val_acc=0.36294, time=0.39200
Epoch:0182, train_loss=2.66018, train_acc=1.00000, val_loss=4.05700, val_acc=0.36294, time=0.39501
Epoch:0183, train_loss=2.66018, train_acc=1.00000, val_loss=4.05698, val_acc=0.36294, time=0.35700
Epoch:0184, train_loss=2.66018, train_acc=1.00000, val_loss=4.05697, val_acc=0.36294, time=0.38700
Epoch:0185, train_loss=2.66018, train_acc=1.00000, val_loss=4.05695, val_acc=0.36294, time=0.37800
Epoch:0186, train_loss=2.66018, train_acc=1.00000, val_loss=4.05694, val_acc=0.36294, time=0.35500
Epoch:0187, train_loss=2.66018, train_acc=1.00000, val_loss=4.05693, val_acc=0.36294, time=0.37199
Epoch:0188, train_loss=2.66018, train_acc=1.00000, val_loss=4.05691, val_acc=0.36294, time=0.35301
Epoch:0189, train_loss=2.66018, train_acc=1.00000, val_loss=4.05690, val_acc=0.36294, time=0.36399
Epoch:0190, train_loss=2.66018, train_acc=1.00000, val_loss=4.05689, val_acc=0.36294, time=0.47200
Epoch:0191, train_loss=2.66018, train_acc=1.00000, val_loss=4.05687, val_acc=0.36294, time=0.39700
Epoch:0192, train_loss=2.66018, train_acc=1.00000, val_loss=4.05686, val_acc=0.36294, time=0.42100
Epoch:0193, train_loss=2.66018, train_acc=1.00000, val_loss=4.05685, val_acc=0.36294, time=0.37401
Epoch:0194, train_loss=2.66018, train_acc=1.00000, val_loss=4.05683, val_acc=0.36294, time=0.36199
Epoch:0195, train_loss=2.66018, train_acc=1.00000, val_loss=4.05682, val_acc=0.36294, time=0.38901
Epoch:0196, train_loss=2.66018, train_acc=1.00000, val_loss=4.05681, val_acc=0.36141, time=0.35500
Epoch:0197, train_loss=2.66018, train_acc=1.00000, val_loss=4.05679, val_acc=0.36141, time=0.36199
Epoch:0198, train_loss=2.66018, train_acc=1.00000, val_loss=4.05678, val_acc=0.36141, time=0.35701
Epoch:0199, train_loss=2.66018, train_acc=1.00000, val_loss=4.05677, val_acc=0.36141, time=0.35500
Epoch:0200, train_loss=2.66018, train_acc=1.00000, val_loss=4.05675, val_acc=0.36141, time=0.40499

Optimization Finished!

Test set results: loss= 4.34724, accuracy= 0.38084, time= 0.14300

Test Precision, Recall and F1-Score...
              precision    recall  f1-score   support

           0     0.7066    0.5069    0.5903      1083
           1     0.1602    0.2397    0.1921       121
           2     0.4647    0.5388    0.4990       696
           3     0.0000    0.0000    0.0000        15
           4     0.0000    0.0000    0.0000        15
           5     0.0000    0.0000    0.0000        17
           6     0.0408    0.0556    0.0471        36
           7     0.0000    0.0000    0.0000        25
           8     0.0000    0.0000    0.0000        19
           9     0.0714    0.0769    0.0741        13
          10     0.1024    0.1494    0.1215        87
          11     0.0000    0.0000    0.0000        20
          12     0.0000    0.0000    0.0000        75
          13     0.0317    0.0714    0.0440        28
          14     0.0000    0.0000    0.0000         9
          15     0.0116    0.0455    0.0185        22
          16     0.0000    0.0000    0.0000         5
          17     0.1000    0.0833    0.0909        12
          18     0.0267    0.0247    0.0256        81
          19     0.0000    0.0000    0.0000        10
          20     0.0000    0.0000    0.0000         2
          21     0.0000    0.0000    0.0000        12
          22     0.0000    0.0000    0.0000         1
          23     0.0000    0.0000    0.0000         9
          24     0.0909    0.0833    0.0870        12
          25     0.0000    0.0000    0.0000         5
          26     0.0000    0.0000    0.0000        10
          27     0.0455    0.0833    0.0588        12
          28     0.0000    0.0000    0.0000         3
          29     0.0000    0.0000    0.0000         3
          30     0.0000    0.0000    0.0000         9
          31     0.0000    0.0000    0.0000         9
          32     0.0000    0.0000    0.0000         8
          33     0.0000    0.0000    0.0000        11
          34     0.0000    0.0000    0.0000         5
          35     0.0000    0.0000    0.0000         4
          36     0.0000    0.0000    0.0000         4
          37     0.0000    0.0000    0.0000         3
          38     0.2000    0.2500    0.2222         4
          39     0.0000    0.0000    0.0000         1
          40     0.0000    0.0000    0.0000         6
          41     0.0000    0.0000    0.0000        11
          42     0.0000    0.0000    0.0000         9
          43     0.0000    0.0000    0.0000         6
          44     0.0000    0.0000    0.0000         1
          45     0.0000    0.0000    0.0000         1
          46     0.0000    0.0000    0.0000         1
          47     0.0000    0.0000    0.0000         7
          48     0.0000    0.0000    0.0000         1
          49     0.0000    0.0000    0.0000         2
          50     0.0000    0.0000    0.0000         4
          51     0.0000    0.0000    0.0000         3

    accuracy                         0.3808      2568
   macro avg     0.0395    0.0425    0.0398      2568
weighted avg     0.4386    0.3808    0.4013      2568


Macro average Test Precision, Recall and F1-Score...
(0.03947077039125779, 0.042478194758362973, 0.03982755257480157, None)

Micro average Test Precision, Recall and F1-Score...
(0.3808411214953271, 0.3808411214953271, 0.38084112149532706, None)

Embeddings:
Word_embeddings: 8892
Train_doc_embeddings: 6532
Test_doc_embeddings: 2568

Elapsed time is 84.643906 seconds.
