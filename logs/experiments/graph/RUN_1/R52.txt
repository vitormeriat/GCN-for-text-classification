
==================== Torch Seed: 10773170984500

Model parameters

Layer: layer1.W0 | Size: torch.Size([17992, 200])
Layer: layer2.W0 | Size: torch.Size([200, 52])

Data statistics

  Edges    Classes    Train samples    Val samples    Test samples
-------  ---------  ---------------  -------------  --------------
  17992         52             5879            653            2568

Epoch:0001, train_loss=8.98898, train_acc=0.00340, val_loss=4.43639, val_acc=0.00459, time=0.37399
Epoch:0002, train_loss=7.88100, train_acc=0.01106, val_loss=4.36410, val_acc=0.01072, time=0.35601
Epoch:0003, train_loss=6.88834, train_acc=0.03062, val_loss=4.29952, val_acc=0.02297, time=0.35500
Epoch:0004, train_loss=6.03028, train_acc=0.07671, val_loss=4.24193, val_acc=0.03675, time=0.43800
Epoch:0005, train_loss=5.29900, train_acc=0.14747, val_loss=4.19138, val_acc=0.07810, time=0.44600
Epoch:0006, train_loss=4.68522, train_acc=0.24069, val_loss=4.14901, val_acc=0.13323, time=0.39300
Epoch:0007, train_loss=4.19900, train_acc=0.35873, val_loss=4.11625, val_acc=0.17917, time=0.44899
Epoch:0008, train_loss=3.85051, train_acc=0.47338, val_loss=4.09324, val_acc=0.22818, time=0.41900
Epoch:0009, train_loss=3.62304, train_acc=0.56438, val_loss=4.07853, val_acc=0.28331, time=0.52000
Epoch:0010, train_loss=3.47614, train_acc=0.62936, val_loss=4.06997, val_acc=0.32006, time=0.41001
Epoch:0011, train_loss=3.37133, train_acc=0.67375, val_loss=4.06505, val_acc=0.34916, time=0.41899
Epoch:0012, train_loss=3.28119, train_acc=0.70879, val_loss=4.06186, val_acc=0.36753, time=0.41699
Epoch:0013, train_loss=3.19282, train_acc=0.74502, val_loss=4.05927, val_acc=0.39051, time=0.46400
Epoch:0014, train_loss=3.10492, train_acc=0.78092, val_loss=4.05679, val_acc=0.39204, time=0.45300
Epoch:0015, train_loss=3.02077, train_acc=0.81596, val_loss=4.05431, val_acc=0.39816, time=0.35601
Epoch:0016, train_loss=2.94446, train_acc=0.84708, val_loss=4.05186, val_acc=0.39663, time=0.42399
Epoch:0017, train_loss=2.87898, train_acc=0.87719, val_loss=4.04955, val_acc=0.39663, time=0.35500
Epoch:0018, train_loss=2.82567, train_acc=0.90219, val_loss=4.04746, val_acc=0.39204, time=0.36200
Epoch:0019, train_loss=2.78406, train_acc=0.92584, val_loss=4.04565, val_acc=0.38897, time=0.35400
Epoch:0020, train_loss=2.75260, train_acc=0.94302, val_loss=4.04413, val_acc=0.39051, time=0.35200
Epoch:0021, train_loss=2.72925, train_acc=0.95782, val_loss=4.04290, val_acc=0.38897, time=0.39688
Epoch:0022, train_loss=2.71190, train_acc=0.96632, val_loss=4.04192, val_acc=0.39051, time=0.43000
Epoch:0023, train_loss=2.69887, train_acc=0.97415, val_loss=4.04116, val_acc=0.39204, time=0.38901
Epoch:0024, train_loss=2.68908, train_acc=0.98197, val_loss=4.04060, val_acc=0.38744, time=0.53900
Epoch:0025, train_loss=2.68175, train_acc=0.98656, val_loss=4.04018, val_acc=0.38438, time=0.39700
Epoch:0026, train_loss=2.67631, train_acc=0.99064, val_loss=4.03990, val_acc=0.38285, time=0.36700
Epoch:0027, train_loss=2.67231, train_acc=0.99235, val_loss=4.03971, val_acc=0.38591, time=0.35700
Epoch:0028, train_loss=2.66933, train_acc=0.99456, val_loss=4.03961, val_acc=0.38591, time=0.35400
Epoch:0029, train_loss=2.66708, train_acc=0.99541, val_loss=4.03958, val_acc=0.38285, time=0.47698
Epoch:0030, train_loss=2.66539, train_acc=0.99694, val_loss=4.03960, val_acc=0.37672, time=0.39401
Epoch:0031, train_loss=2.66412, train_acc=0.99779, val_loss=4.03965, val_acc=0.37519, time=0.46600
Epoch:0032, train_loss=2.66317, train_acc=0.99847, val_loss=4.03974, val_acc=0.37519, time=0.39399
Epoch:0033, train_loss=2.66246, train_acc=0.99881, val_loss=4.03985, val_acc=0.37366, time=0.36400
Epoch:0034, train_loss=2.66193, train_acc=0.99898, val_loss=4.03997, val_acc=0.37213, time=0.41999
Early stopping...

Optimization Finished!

Test set results: loss= 4.30037, accuracy= 0.39720, time= 0.11000

Test Precision, Recall and F1-Score...
              precision    recall  f1-score   support

           0     0.6553    0.6214    0.6379      1083
           1     0.1654    0.1818    0.1732       121
           2     0.5017    0.4239    0.4595       696
           3     0.0286    0.0667    0.0400        15
           4     0.0000    0.0000    0.0000        15
           5     0.0000    0.0000    0.0000        17
           6     0.0217    0.0278    0.0244        36
           7     0.0227    0.0400    0.0290        25
           8     0.1429    0.0526    0.0769        19
           9     0.0000    0.0000    0.0000        13
          10     0.1250    0.0805    0.0979        87
          11     0.0333    0.0500    0.0400        20
          12     0.0526    0.0667    0.0588        75
          13     0.0610    0.1786    0.0909        28
          14     0.0000    0.0000    0.0000         9
          15     0.0000    0.0000    0.0000        22
          16     0.0000    0.0000    0.0000         5
          17     0.0000    0.0000    0.0000        12
          18     0.0656    0.0494    0.0563        81
          19     0.0000    0.0000    0.0000        10
          20     0.0000    0.0000    0.0000         2
          21     0.0000    0.0000    0.0000        12
          22     0.0000    0.0000    0.0000         1
          23     0.0000    0.0000    0.0000         9
          24     0.0000    0.0000    0.0000        12
          25     0.0000    0.0000    0.0000         5
          26     0.0000    0.0000    0.0000        10
          27     0.0588    0.0833    0.0690        12
          28     0.0000    0.0000    0.0000         3
          29     0.1667    0.3333    0.2222         3
          30     0.0000    0.0000    0.0000         9
          31     0.0000    0.0000    0.0000         9
          32     0.0000    0.0000    0.0000         8
          33     0.0435    0.0909    0.0588        11
          34     0.0000    0.0000    0.0000         5
          35     0.0000    0.0000    0.0000         4
          36     0.0000    0.0000    0.0000         4
          37     1.0000    0.3333    0.5000         3
          38     0.0000    0.0000    0.0000         4
          39     0.0000    0.0000    0.0000         1
          40     0.0000    0.0000    0.0000         6
          41     0.0000    0.0000    0.0000        11
          42     0.0000    0.0000    0.0000         9
          43     0.0000    0.0000    0.0000         6
          44     0.0000    0.0000    0.0000         1
          45     0.0000    0.0000    0.0000         1
          46     0.0000    0.0000    0.0000         1
          47     0.0000    0.0000    0.0000         7
          48     0.0000    0.0000    0.0000         1
          49     0.0000    0.0000    0.0000         2
          50     0.0000    0.0000    0.0000         4
          51     0.0000    0.0000    0.0000         3

    accuracy                         0.3972      2568
   macro avg     0.0605    0.0515    0.0507      2568
weighted avg     0.4325    0.3972    0.4127      2568


Macro average Test Precision, Recall and F1-Score...
(0.06047689724561113, 0.05154146977359159, 0.05067168152637396, None)

Micro average Test Precision, Recall and F1-Score...
(0.397196261682243, 0.397196261682243, 0.397196261682243, None)

Embeddings:
Word_embeddings: 8892
Train_doc_embeddings: 6532
Test_doc_embeddings: 2568

Elapsed time is 15.857893 seconds.
