
==================== Torch Seed: 14221122196000

Model parameters

Layer: layer1.W0 | Size: torch.Size([6827, 200])
Layer: layer2.W0 | Size: torch.Size([200, 6])

Data statistics

  Edges    Classes    Train samples    Val samples    Test samples
-------  ---------  ---------------  -------------  --------------
   6827          6             2088            231             993

Epoch:0001, train_loss=4.19598, train_acc=0.16954, val_loss=2.03224, val_acc=0.16450, time=0.08800
Epoch:0002, train_loss=3.56559, train_acc=0.21695, val_loss=2.01464, val_acc=0.19048, time=0.06301
Epoch:0003, train_loss=3.10019, train_acc=0.27634, val_loss=2.00541, val_acc=0.19481, time=0.05802
Epoch:0004, train_loss=2.76354, train_acc=0.35776, val_loss=2.00079, val_acc=0.20779, time=0.05801
Epoch:0005, train_loss=2.49347, train_acc=0.42050, val_loss=1.99780, val_acc=0.20779, time=0.05800
Epoch:0006, train_loss=2.24679, train_acc=0.49617, val_loss=1.99532, val_acc=0.20346, time=0.05702
Epoch:0007, train_loss=2.02088, train_acc=0.57998, val_loss=1.99338, val_acc=0.20346, time=0.05801
Epoch:0008, train_loss=1.82572, train_acc=0.65661, val_loss=1.99211, val_acc=0.21645, time=0.05702
Epoch:0009, train_loss=1.66914, train_acc=0.73372, val_loss=1.99157, val_acc=0.20779, time=0.05802
Epoch:0010, train_loss=1.55163, train_acc=0.79262, val_loss=1.99168, val_acc=0.20346, time=0.05900
Epoch:0011, train_loss=1.46663, train_acc=0.84914, val_loss=1.99217, val_acc=0.18182, time=0.05802
Epoch:0012, train_loss=1.40653, train_acc=0.88506, val_loss=1.99269, val_acc=0.17749, time=0.05802
Epoch:0013, train_loss=1.36267, train_acc=0.91140, val_loss=1.99303, val_acc=0.17749, time=0.05802
Epoch:0014, train_loss=1.32934, train_acc=0.93295, val_loss=1.99310, val_acc=0.18182, time=0.05802
Epoch:0015, train_loss=1.30372, train_acc=0.95115, val_loss=1.99295, val_acc=0.18182, time=0.06100
Epoch:0016, train_loss=1.28444, train_acc=0.96504, val_loss=1.99265, val_acc=0.17316, time=0.05800
Epoch:0017, train_loss=1.27039, train_acc=0.97749, val_loss=1.99229, val_acc=0.17316, time=0.05801
Epoch:0018, train_loss=1.26058, train_acc=0.98659, val_loss=1.99191, val_acc=0.17316, time=0.05800
Epoch:0019, train_loss=1.25407, train_acc=0.99042, val_loss=1.99155, val_acc=0.17316, time=0.07101
Epoch:0020, train_loss=1.24992, train_acc=0.99569, val_loss=1.99123, val_acc=0.17316, time=0.07902
Epoch:0021, train_loss=1.24739, train_acc=0.99808, val_loss=1.99094, val_acc=0.17316, time=0.07800
Epoch:0022, train_loss=1.24596, train_acc=0.99952, val_loss=1.99069, val_acc=0.17316, time=0.08700
Epoch:0023, train_loss=1.24517, train_acc=0.99952, val_loss=1.99047, val_acc=0.17316, time=0.08500
Epoch:0024, train_loss=1.24469, train_acc=0.99952, val_loss=1.99028, val_acc=0.17316, time=0.08700
Epoch:0025, train_loss=1.24437, train_acc=0.99952, val_loss=1.99011, val_acc=0.17316, time=0.06501
Epoch:0026, train_loss=1.24412, train_acc=0.99952, val_loss=1.98997, val_acc=0.17749, time=0.05901
Epoch:0027, train_loss=1.24395, train_acc=1.00000, val_loss=1.98985, val_acc=0.17749, time=0.05803
Epoch:0028, train_loss=1.24386, train_acc=1.00000, val_loss=1.98974, val_acc=0.17749, time=0.05803
Epoch:0029, train_loss=1.24382, train_acc=1.00000, val_loss=1.98965, val_acc=0.17749, time=0.05802
Epoch:0030, train_loss=1.24381, train_acc=1.00000, val_loss=1.98956, val_acc=0.17749, time=0.05801
Epoch:0031, train_loss=1.24380, train_acc=1.00000, val_loss=1.98949, val_acc=0.17749, time=0.05801
Epoch:0032, train_loss=1.24379, train_acc=1.00000, val_loss=1.98943, val_acc=0.17749, time=0.07499
Epoch:0033, train_loss=1.24379, train_acc=1.00000, val_loss=1.98937, val_acc=0.17749, time=0.08900
Epoch:0034, train_loss=1.24379, train_acc=1.00000, val_loss=1.98932, val_acc=0.17749, time=0.05900
Epoch:0035, train_loss=1.24379, train_acc=1.00000, val_loss=1.98928, val_acc=0.17749, time=0.05800
Epoch:0036, train_loss=1.24378, train_acc=1.00000, val_loss=1.98924, val_acc=0.17749, time=0.05801
Epoch:0037, train_loss=1.24378, train_acc=1.00000, val_loss=1.98920, val_acc=0.17749, time=0.05801
Epoch:0038, train_loss=1.24378, train_acc=1.00000, val_loss=1.98917, val_acc=0.17749, time=0.05801
Epoch:0039, train_loss=1.24378, train_acc=1.00000, val_loss=1.98914, val_acc=0.17749, time=0.05900
Epoch:0040, train_loss=1.24378, train_acc=1.00000, val_loss=1.98911, val_acc=0.17749, time=0.05800
Epoch:0041, train_loss=1.24378, train_acc=1.00000, val_loss=1.98909, val_acc=0.17749, time=0.05800
Epoch:0042, train_loss=1.24378, train_acc=1.00000, val_loss=1.98907, val_acc=0.17749, time=0.05902
Epoch:0043, train_loss=1.24378, train_acc=1.00000, val_loss=1.98905, val_acc=0.18182, time=0.05802
Epoch:0044, train_loss=1.24378, train_acc=1.00000, val_loss=1.98903, val_acc=0.18182, time=0.05801
Epoch:0045, train_loss=1.24378, train_acc=1.00000, val_loss=1.98902, val_acc=0.18182, time=0.05801
Epoch:0046, train_loss=1.24378, train_acc=1.00000, val_loss=1.98900, val_acc=0.18182, time=0.05801
Epoch:0047, train_loss=1.24378, train_acc=1.00000, val_loss=1.98899, val_acc=0.18182, time=0.05801
Epoch:0048, train_loss=1.24377, train_acc=1.00000, val_loss=1.98898, val_acc=0.18182, time=0.05801
Epoch:0049, train_loss=1.24377, train_acc=1.00000, val_loss=1.98897, val_acc=0.18182, time=0.05900
Epoch:0050, train_loss=1.24377, train_acc=1.00000, val_loss=1.98896, val_acc=0.18182, time=0.06301
Epoch:0051, train_loss=1.24377, train_acc=1.00000, val_loss=1.98895, val_acc=0.18182, time=0.05803
Epoch:0052, train_loss=1.24377, train_acc=1.00000, val_loss=1.98894, val_acc=0.18182, time=0.05800
Epoch:0053, train_loss=1.24377, train_acc=1.00000, val_loss=1.98893, val_acc=0.18182, time=0.05701
Epoch:0054, train_loss=1.24377, train_acc=1.00000, val_loss=1.98892, val_acc=0.18182, time=0.05800
Epoch:0055, train_loss=1.24377, train_acc=1.00000, val_loss=1.98892, val_acc=0.18182, time=0.05702
Epoch:0056, train_loss=1.24377, train_acc=1.00000, val_loss=1.98891, val_acc=0.18182, time=0.05801
Epoch:0057, train_loss=1.24377, train_acc=1.00000, val_loss=1.98891, val_acc=0.18182, time=0.05702
Epoch:0058, train_loss=1.24377, train_acc=1.00000, val_loss=1.98890, val_acc=0.18182, time=0.05802
Epoch:0059, train_loss=1.24377, train_acc=1.00000, val_loss=1.98889, val_acc=0.18182, time=0.05801
Epoch:0060, train_loss=1.24377, train_acc=1.00000, val_loss=1.98889, val_acc=0.18182, time=0.05800
Epoch:0061, train_loss=1.24377, train_acc=1.00000, val_loss=1.98889, val_acc=0.18182, time=0.05702
Epoch:0062, train_loss=1.24377, train_acc=1.00000, val_loss=1.98888, val_acc=0.18182, time=0.05800
Epoch:0063, train_loss=1.24377, train_acc=1.00000, val_loss=1.98888, val_acc=0.18182, time=0.05702
Epoch:0064, train_loss=1.24377, train_acc=1.00000, val_loss=1.98887, val_acc=0.18182, time=0.05801
Epoch:0065, train_loss=1.24377, train_acc=1.00000, val_loss=1.98887, val_acc=0.18182, time=0.05802
Epoch:0066, train_loss=1.24377, train_acc=1.00000, val_loss=1.98887, val_acc=0.18182, time=0.05802
Epoch:0067, train_loss=1.24377, train_acc=1.00000, val_loss=1.98886, val_acc=0.18182, time=0.05702
Epoch:0068, train_loss=1.24377, train_acc=1.00000, val_loss=1.98886, val_acc=0.18182, time=0.07502
Epoch:0069, train_loss=1.24377, train_acc=1.00000, val_loss=1.98886, val_acc=0.18182, time=0.05800
Epoch:0070, train_loss=1.24377, train_acc=1.00000, val_loss=1.98885, val_acc=0.18182, time=0.07900
Epoch:0071, train_loss=1.24377, train_acc=1.00000, val_loss=1.98885, val_acc=0.18182, time=0.08100
Epoch:0072, train_loss=1.24377, train_acc=1.00000, val_loss=1.98885, val_acc=0.18182, time=0.05800
Epoch:0073, train_loss=1.24377, train_acc=1.00000, val_loss=1.98885, val_acc=0.18182, time=0.05702
Epoch:0074, train_loss=1.24377, train_acc=1.00000, val_loss=1.98884, val_acc=0.18182, time=0.05900
Epoch:0075, train_loss=1.24377, train_acc=1.00000, val_loss=1.98884, val_acc=0.18182, time=0.05800
Epoch:0076, train_loss=1.24377, train_acc=1.00000, val_loss=1.98884, val_acc=0.18182, time=0.06002
Epoch:0077, train_loss=1.24377, train_acc=1.00000, val_loss=1.98884, val_acc=0.18182, time=0.05802
Epoch:0078, train_loss=1.24377, train_acc=1.00000, val_loss=1.98883, val_acc=0.18182, time=0.05802
Epoch:0079, train_loss=1.24377, train_acc=1.00000, val_loss=1.98883, val_acc=0.18182, time=0.05802
Epoch:0080, train_loss=1.24377, train_acc=1.00000, val_loss=1.98883, val_acc=0.18182, time=0.05900
Epoch:0081, train_loss=1.24377, train_acc=1.00000, val_loss=1.98883, val_acc=0.18182, time=0.05800
Epoch:0082, train_loss=1.24377, train_acc=1.00000, val_loss=1.98883, val_acc=0.18182, time=0.05900
Epoch:0083, train_loss=1.24377, train_acc=1.00000, val_loss=1.98882, val_acc=0.18182, time=0.05900
Epoch:0084, train_loss=1.24377, train_acc=1.00000, val_loss=1.98882, val_acc=0.18182, time=0.07700
Epoch:0085, train_loss=1.24377, train_acc=1.00000, val_loss=1.98882, val_acc=0.18182, time=0.08700
Epoch:0086, train_loss=1.24377, train_acc=1.00000, val_loss=1.98882, val_acc=0.18182, time=0.08800
Epoch:0087, train_loss=1.24377, train_acc=1.00000, val_loss=1.98882, val_acc=0.18182, time=0.08800
Epoch:0088, train_loss=1.24377, train_acc=1.00000, val_loss=1.98881, val_acc=0.18182, time=0.08700
Epoch:0089, train_loss=1.24377, train_acc=1.00000, val_loss=1.98881, val_acc=0.18182, time=0.08601
Epoch:0090, train_loss=1.24377, train_acc=1.00000, val_loss=1.98881, val_acc=0.18182, time=0.08700
Epoch:0091, train_loss=1.24377, train_acc=1.00000, val_loss=1.98881, val_acc=0.18182, time=0.08700
Epoch:0092, train_loss=1.24377, train_acc=1.00000, val_loss=1.98881, val_acc=0.18182, time=0.08799
Epoch:0093, train_loss=1.24377, train_acc=1.00000, val_loss=1.98881, val_acc=0.18182, time=0.06700
Epoch:0094, train_loss=1.24377, train_acc=1.00000, val_loss=1.98880, val_acc=0.18182, time=0.05801
Epoch:0095, train_loss=1.24377, train_acc=1.00000, val_loss=1.98880, val_acc=0.18182, time=0.09000
Epoch:0096, train_loss=1.24377, train_acc=1.00000, val_loss=1.98880, val_acc=0.18182, time=0.06900
Epoch:0097, train_loss=1.24377, train_acc=1.00000, val_loss=1.98880, val_acc=0.18182, time=0.08700
Epoch:0098, train_loss=1.24377, train_acc=1.00000, val_loss=1.98880, val_acc=0.18182, time=0.06200
Epoch:0099, train_loss=1.24377, train_acc=1.00000, val_loss=1.98880, val_acc=0.18182, time=0.08500
Epoch:0100, train_loss=1.24377, train_acc=1.00000, val_loss=1.98879, val_acc=0.18182, time=0.06701
Epoch:0101, train_loss=1.24377, train_acc=1.00000, val_loss=1.98879, val_acc=0.18182, time=0.06302
Epoch:0102, train_loss=1.24377, train_acc=1.00000, val_loss=1.98879, val_acc=0.18182, time=0.05802
Epoch:0103, train_loss=1.24377, train_acc=1.00000, val_loss=1.98879, val_acc=0.18182, time=0.05902
Epoch:0104, train_loss=1.24377, train_acc=1.00000, val_loss=1.98879, val_acc=0.18182, time=0.06000
Epoch:0105, train_loss=1.24377, train_acc=1.00000, val_loss=1.98879, val_acc=0.18182, time=0.05801
Epoch:0106, train_loss=1.24377, train_acc=1.00000, val_loss=1.98879, val_acc=0.18182, time=0.05802
Epoch:0107, train_loss=1.24377, train_acc=1.00000, val_loss=1.98878, val_acc=0.18182, time=0.05800
Epoch:0108, train_loss=1.24377, train_acc=1.00000, val_loss=1.98878, val_acc=0.18182, time=0.05702
Epoch:0109, train_loss=1.24377, train_acc=1.00000, val_loss=1.98878, val_acc=0.18182, time=0.05800
Epoch:0110, train_loss=1.24377, train_acc=1.00000, val_loss=1.98878, val_acc=0.18182, time=0.05702
Epoch:0111, train_loss=1.24377, train_acc=1.00000, val_loss=1.98878, val_acc=0.18182, time=0.05802
Epoch:0112, train_loss=1.24377, train_acc=1.00000, val_loss=1.98878, val_acc=0.18182, time=0.07899
Epoch:0113, train_loss=1.24377, train_acc=1.00000, val_loss=1.98877, val_acc=0.18182, time=0.05900
Epoch:0114, train_loss=1.24377, train_acc=1.00000, val_loss=1.98877, val_acc=0.18182, time=0.05800
Epoch:0115, train_loss=1.24377, train_acc=1.00000, val_loss=1.98877, val_acc=0.18182, time=0.05801
Epoch:0116, train_loss=1.24377, train_acc=1.00000, val_loss=1.98877, val_acc=0.18182, time=0.05801
Epoch:0117, train_loss=1.24377, train_acc=1.00000, val_loss=1.98877, val_acc=0.18182, time=0.05801
Epoch:0118, train_loss=1.24377, train_acc=1.00000, val_loss=1.98877, val_acc=0.18182, time=0.05800
Epoch:0119, train_loss=1.24377, train_acc=1.00000, val_loss=1.98877, val_acc=0.18182, time=0.05801
Epoch:0120, train_loss=1.24377, train_acc=1.00000, val_loss=1.98876, val_acc=0.18182, time=0.05703
Epoch:0121, train_loss=1.24377, train_acc=1.00000, val_loss=1.98876, val_acc=0.18182, time=0.06002
Epoch:0122, train_loss=1.24377, train_acc=1.00000, val_loss=1.98876, val_acc=0.18182, time=0.05801
Epoch:0123, train_loss=1.24377, train_acc=1.00000, val_loss=1.98876, val_acc=0.18182, time=0.05802
Epoch:0124, train_loss=1.24377, train_acc=1.00000, val_loss=1.98876, val_acc=0.18182, time=0.05802
Epoch:0125, train_loss=1.24377, train_acc=1.00000, val_loss=1.98876, val_acc=0.18182, time=0.05702
Epoch:0126, train_loss=1.24377, train_acc=1.00000, val_loss=1.98875, val_acc=0.18182, time=0.05802
Epoch:0127, train_loss=1.24377, train_acc=1.00000, val_loss=1.98875, val_acc=0.18182, time=0.05802
Epoch:0128, train_loss=1.24377, train_acc=1.00000, val_loss=1.98875, val_acc=0.18182, time=0.05801
Epoch:0129, train_loss=1.24377, train_acc=1.00000, val_loss=1.98875, val_acc=0.18182, time=0.06299
Epoch:0130, train_loss=1.24377, train_acc=1.00000, val_loss=1.98875, val_acc=0.18182, time=0.06001
Epoch:0131, train_loss=1.24377, train_acc=1.00000, val_loss=1.98875, val_acc=0.18182, time=0.05701
Epoch:0132, train_loss=1.24377, train_acc=1.00000, val_loss=1.98875, val_acc=0.18182, time=0.05800
Epoch:0133, train_loss=1.24377, train_acc=1.00000, val_loss=1.98874, val_acc=0.18182, time=0.05700
Epoch:0134, train_loss=1.24377, train_acc=1.00000, val_loss=1.98874, val_acc=0.18182, time=0.05800
Epoch:0135, train_loss=1.24377, train_acc=1.00000, val_loss=1.98874, val_acc=0.18182, time=0.05701
Epoch:0136, train_loss=1.24377, train_acc=1.00000, val_loss=1.98874, val_acc=0.18182, time=0.05702
Epoch:0137, train_loss=1.24377, train_acc=1.00000, val_loss=1.98874, val_acc=0.18182, time=0.05803
Epoch:0138, train_loss=1.24377, train_acc=1.00000, val_loss=1.98874, val_acc=0.18182, time=0.05702
Epoch:0139, train_loss=1.24377, train_acc=1.00000, val_loss=1.98873, val_acc=0.18615, time=0.06100
Epoch:0140, train_loss=1.24377, train_acc=1.00000, val_loss=1.98873, val_acc=0.18615, time=0.05701
Epoch:0141, train_loss=1.24377, train_acc=1.00000, val_loss=1.98873, val_acc=0.18615, time=0.05702
Epoch:0142, train_loss=1.24377, train_acc=1.00000, val_loss=1.98873, val_acc=0.18615, time=0.05800
Epoch:0143, train_loss=1.24377, train_acc=1.00000, val_loss=1.98873, val_acc=0.18615, time=0.05800
Epoch:0144, train_loss=1.24377, train_acc=1.00000, val_loss=1.98873, val_acc=0.18615, time=0.05802
Epoch:0145, train_loss=1.24377, train_acc=1.00000, val_loss=1.98873, val_acc=0.18615, time=0.05701
Epoch:0146, train_loss=1.24377, train_acc=1.00000, val_loss=1.98872, val_acc=0.18615, time=0.07900
Epoch:0147, train_loss=1.24377, train_acc=1.00000, val_loss=1.98872, val_acc=0.18615, time=0.07202
Epoch:0148, train_loss=1.24377, train_acc=1.00000, val_loss=1.98872, val_acc=0.18615, time=0.05801
Epoch:0149, train_loss=1.24377, train_acc=1.00000, val_loss=1.98872, val_acc=0.18615, time=0.05802
Epoch:0150, train_loss=1.24377, train_acc=1.00000, val_loss=1.98872, val_acc=0.18615, time=0.05802
Epoch:0151, train_loss=1.24377, train_acc=1.00000, val_loss=1.98872, val_acc=0.18615, time=0.05800
Epoch:0152, train_loss=1.24377, train_acc=1.00000, val_loss=1.98872, val_acc=0.18615, time=0.05800
Epoch:0153, train_loss=1.24377, train_acc=1.00000, val_loss=1.98871, val_acc=0.18615, time=0.05900
Epoch:0154, train_loss=1.24377, train_acc=1.00000, val_loss=1.98871, val_acc=0.18615, time=0.05902
Epoch:0155, train_loss=1.24377, train_acc=1.00000, val_loss=1.98871, val_acc=0.18615, time=0.05901
Epoch:0156, train_loss=1.24377, train_acc=1.00000, val_loss=1.98871, val_acc=0.18615, time=0.05800
Epoch:0157, train_loss=1.24377, train_acc=1.00000, val_loss=1.98871, val_acc=0.18615, time=0.05802
Epoch:0158, train_loss=1.24377, train_acc=1.00000, val_loss=1.98871, val_acc=0.18615, time=0.05801
Epoch:0159, train_loss=1.24377, train_acc=1.00000, val_loss=1.98871, val_acc=0.18615, time=0.05802
Epoch:0160, train_loss=1.24377, train_acc=1.00000, val_loss=1.98870, val_acc=0.18615, time=0.05802
Epoch:0161, train_loss=1.24377, train_acc=1.00000, val_loss=1.98870, val_acc=0.18615, time=0.05802
Epoch:0162, train_loss=1.24377, train_acc=1.00000, val_loss=1.98870, val_acc=0.18615, time=0.06000
Epoch:0163, train_loss=1.24377, train_acc=1.00000, val_loss=1.98870, val_acc=0.18615, time=0.05800
Epoch:0164, train_loss=1.24377, train_acc=1.00000, val_loss=1.98870, val_acc=0.18615, time=0.05902
Epoch:0165, train_loss=1.24377, train_acc=1.00000, val_loss=1.98870, val_acc=0.18615, time=0.05802
Epoch:0166, train_loss=1.24377, train_acc=1.00000, val_loss=1.98869, val_acc=0.18615, time=0.05701
Epoch:0167, train_loss=1.24377, train_acc=1.00000, val_loss=1.98869, val_acc=0.18615, time=0.05800
Epoch:0168, train_loss=1.24377, train_acc=1.00000, val_loss=1.98869, val_acc=0.18615, time=0.05699
Epoch:0169, train_loss=1.24377, train_acc=1.00000, val_loss=1.98869, val_acc=0.18615, time=0.05800
Epoch:0170, train_loss=1.24377, train_acc=1.00000, val_loss=1.98869, val_acc=0.18615, time=0.05801
Epoch:0171, train_loss=1.24377, train_acc=1.00000, val_loss=1.98869, val_acc=0.18615, time=0.05801
Epoch:0172, train_loss=1.24377, train_acc=1.00000, val_loss=1.98869, val_acc=0.18615, time=0.05800
Epoch:0173, train_loss=1.24377, train_acc=1.00000, val_loss=1.98868, val_acc=0.18615, time=0.06001
Epoch:0174, train_loss=1.24377, train_acc=1.00000, val_loss=1.98868, val_acc=0.18615, time=0.06800
Epoch:0175, train_loss=1.24377, train_acc=1.00000, val_loss=1.98868, val_acc=0.18615, time=0.08700
Epoch:0176, train_loss=1.24377, train_acc=1.00000, val_loss=1.98868, val_acc=0.18615, time=0.08700
Epoch:0177, train_loss=1.24377, train_acc=1.00000, val_loss=1.98868, val_acc=0.18615, time=0.06101
Epoch:0178, train_loss=1.24377, train_acc=1.00000, val_loss=1.98868, val_acc=0.18615, time=0.06101
Epoch:0179, train_loss=1.24377, train_acc=1.00000, val_loss=1.98868, val_acc=0.18615, time=0.08500
Epoch:0180, train_loss=1.24377, train_acc=1.00000, val_loss=1.98867, val_acc=0.18615, time=0.07500
Epoch:0181, train_loss=1.24377, train_acc=1.00000, val_loss=1.98867, val_acc=0.18615, time=0.05801
Epoch:0182, train_loss=1.24377, train_acc=1.00000, val_loss=1.98867, val_acc=0.18615, time=0.05801
Epoch:0183, train_loss=1.24377, train_acc=1.00000, val_loss=1.98867, val_acc=0.18615, time=0.05800
Epoch:0184, train_loss=1.24377, train_acc=1.00000, val_loss=1.98867, val_acc=0.18615, time=0.05802
Epoch:0185, train_loss=1.24377, train_acc=1.00000, val_loss=1.98867, val_acc=0.18615, time=0.05802
Epoch:0186, train_loss=1.24377, train_acc=1.00000, val_loss=1.98867, val_acc=0.18615, time=0.05801
Epoch:0187, train_loss=1.24377, train_acc=1.00000, val_loss=1.98866, val_acc=0.18615, time=0.05801
Epoch:0188, train_loss=1.24377, train_acc=1.00000, val_loss=1.98866, val_acc=0.18615, time=0.05900
Epoch:0189, train_loss=1.24377, train_acc=1.00000, val_loss=1.98866, val_acc=0.18615, time=0.05902
Epoch:0190, train_loss=1.24377, train_acc=1.00000, val_loss=1.98866, val_acc=0.18615, time=0.05800
Epoch:0191, train_loss=1.24377, train_acc=1.00000, val_loss=1.98866, val_acc=0.18615, time=0.05802
Epoch:0192, train_loss=1.24377, train_acc=1.00000, val_loss=1.98866, val_acc=0.18615, time=0.05801
Epoch:0193, train_loss=1.24377, train_acc=1.00000, val_loss=1.98866, val_acc=0.18615, time=0.05900
Epoch:0194, train_loss=1.24377, train_acc=1.00000, val_loss=1.98865, val_acc=0.18615, time=0.05900
Epoch:0195, train_loss=1.24377, train_acc=1.00000, val_loss=1.98865, val_acc=0.18615, time=0.08000
Epoch:0196, train_loss=1.24377, train_acc=1.00000, val_loss=1.98865, val_acc=0.18615, time=0.08800
Epoch:0197, train_loss=1.24377, train_acc=1.00000, val_loss=1.98865, val_acc=0.18615, time=0.05901
Epoch:0198, train_loss=1.24377, train_acc=1.00000, val_loss=1.98865, val_acc=0.18615, time=0.08800
Epoch:0199, train_loss=1.24377, train_acc=1.00000, val_loss=1.98865, val_acc=0.18615, time=0.08301
Epoch:0200, train_loss=1.24377, train_acc=1.00000, val_loss=1.98865, val_acc=0.18615, time=0.05801

Optimization Finished!

Test set results: loss= 2.67216, accuracy= 0.20342, time= 0.01600

Test Precision, Recall and F1-Score...
              precision    recall  f1-score   support

           0     0.2353    0.1765    0.2017       204
           1     0.2090    0.1779    0.1922       208
           2     0.1881    0.2733    0.2228       150
           3     0.2515    0.2222    0.2360       189
           4     0.1031    0.1449    0.1205        69
           5     0.1989    0.2081    0.2034       173

    accuracy                         0.2034       993
   macro avg     0.1976    0.2005    0.1961       993
weighted avg     0.2102    0.2034    0.2041       993


Macro average Test Precision, Recall and F1-Score...
(0.19764864621375974, 0.200488463492747, 0.1960902276387191, None)

Micro average Test Precision, Recall and F1-Score...
(0.20342396777442096, 0.20342396777442096, 0.20342396777442096, None)

Embeddings:
Word_embeddings: 3515
Train_doc_embeddings: 2319
Test_doc_embeddings: 993

Elapsed time is 13.200997 seconds.
