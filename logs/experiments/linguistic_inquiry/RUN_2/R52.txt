
==================== Torch Seed: 8197749607500

Model parameters

Layer: layer1.W0 | Size: torch.Size([17992, 200])
Layer: layer2.W0 | Size: torch.Size([200, 52])

Data statistics

  Edges    Classes    Train samples    Val samples    Test samples
-------  ---------  ---------------  -------------  --------------
  17992         52             5879            653            2568

Epoch:0001, train_loss=4.27194, train_acc=0.00357, val_loss=3.94876, val_acc=0.18683, time=0.42200
Epoch:0002, train_loss=3.93287, train_acc=0.17690, val_loss=3.91619, val_acc=0.44104, time=0.47400
Epoch:0003, train_loss=3.63598, train_acc=0.43919, val_loss=3.88960, val_acc=0.59418, time=0.47899
Epoch:0004, train_loss=3.39549, train_acc=0.59568, val_loss=3.87193, val_acc=0.64165, time=0.43600
Epoch:0005, train_loss=3.23642, train_acc=0.65062, val_loss=3.86233, val_acc=0.66616, time=0.39400
Epoch:0006, train_loss=3.15060, train_acc=0.67256, val_loss=3.85701, val_acc=0.67841, time=0.36501
Epoch:0007, train_loss=3.10287, train_acc=0.68991, val_loss=3.85315, val_acc=0.69525, time=0.40800
Epoch:0008, train_loss=3.06678, train_acc=0.71356, val_loss=3.84959, val_acc=0.72435, time=0.35800
Epoch:0009, train_loss=3.03221, train_acc=0.73567, val_loss=3.84615, val_acc=0.74119, time=0.42599
Epoch:0010, train_loss=2.99812, train_acc=0.76084, val_loss=3.84299, val_acc=0.77182, time=0.35600
Epoch:0011, train_loss=2.96647, train_acc=0.78993, val_loss=3.84033, val_acc=0.79020, time=0.35801
Epoch:0012, train_loss=2.93938, train_acc=0.82021, val_loss=3.83824, val_acc=0.81930, time=0.37001
Epoch:0013, train_loss=2.91747, train_acc=0.84385, val_loss=3.83657, val_acc=0.82389, time=0.35399
Epoch:0014, train_loss=2.89955, train_acc=0.86052, val_loss=3.83515, val_acc=0.83002, time=0.41697
Epoch:0015, train_loss=2.88399, train_acc=0.87362, val_loss=3.83386, val_acc=0.84533, time=0.48300
Epoch:0016, train_loss=2.86970, train_acc=0.88093, val_loss=3.83265, val_acc=0.85605, time=0.35900
Epoch:0017, train_loss=2.85619, train_acc=0.88927, val_loss=3.83151, val_acc=0.86983, time=0.43799
Epoch:0018, train_loss=2.84338, train_acc=0.89471, val_loss=3.83044, val_acc=0.87443, time=0.35501
Epoch:0019, train_loss=2.83136, train_acc=0.90424, val_loss=3.82947, val_acc=0.88361, time=0.44099
Epoch:0020, train_loss=2.82024, train_acc=0.91121, val_loss=3.82860, val_acc=0.89280, time=0.35799
Epoch:0021, train_loss=2.81008, train_acc=0.91903, val_loss=3.82782, val_acc=0.90199, time=0.36499
Epoch:0022, train_loss=2.80083, train_acc=0.92533, val_loss=3.82712, val_acc=0.90352, time=0.40200
Epoch:0023, train_loss=2.79240, train_acc=0.93043, val_loss=3.82648, val_acc=0.91118, time=0.39400
Epoch:0024, train_loss=2.78465, train_acc=0.93553, val_loss=3.82588, val_acc=0.91271, time=0.43000
Epoch:0025, train_loss=2.77743, train_acc=0.93979, val_loss=3.82529, val_acc=0.91118, time=0.46000
Epoch:0026, train_loss=2.77064, train_acc=0.94438, val_loss=3.82471, val_acc=0.91271, time=0.45001
Epoch:0027, train_loss=2.76424, train_acc=0.94761, val_loss=3.82416, val_acc=0.91577, time=0.50400
Epoch:0028, train_loss=2.75823, train_acc=0.95016, val_loss=3.82363, val_acc=0.91884, time=0.51301
Epoch:0029, train_loss=2.75262, train_acc=0.95305, val_loss=3.82314, val_acc=0.92037, time=0.37100
Epoch:0030, train_loss=2.74739, train_acc=0.95594, val_loss=3.82268, val_acc=0.92190, time=0.35600
Epoch:0031, train_loss=2.74252, train_acc=0.95765, val_loss=3.82227, val_acc=0.92190, time=0.42601
Epoch:0032, train_loss=2.73798, train_acc=0.95986, val_loss=3.82191, val_acc=0.92037, time=0.44100
Epoch:0033, train_loss=2.73376, train_acc=0.96258, val_loss=3.82158, val_acc=0.92037, time=0.38901
Epoch:0034, train_loss=2.72983, train_acc=0.96462, val_loss=3.82128, val_acc=0.91884, time=0.38399
Epoch:0035, train_loss=2.72614, train_acc=0.96564, val_loss=3.82101, val_acc=0.91884, time=0.46900
Epoch:0036, train_loss=2.72268, train_acc=0.96649, val_loss=3.82076, val_acc=0.91730, time=0.35801
Epoch:0037, train_loss=2.71941, train_acc=0.96870, val_loss=3.82052, val_acc=0.91730, time=0.35401
Epoch:0038, train_loss=2.71631, train_acc=0.97108, val_loss=3.82029, val_acc=0.92037, time=0.35300
Epoch:0039, train_loss=2.71338, train_acc=0.97346, val_loss=3.82007, val_acc=0.92190, time=0.36001
Epoch:0040, train_loss=2.71059, train_acc=0.97585, val_loss=3.81986, val_acc=0.92496, time=0.35298
Epoch:0041, train_loss=2.70796, train_acc=0.97755, val_loss=3.81965, val_acc=0.92649, time=0.35701
Epoch:0042, train_loss=2.70549, train_acc=0.97806, val_loss=3.81945, val_acc=0.92649, time=0.43601
Epoch:0043, train_loss=2.70317, train_acc=0.97959, val_loss=3.81927, val_acc=0.92956, time=0.38699
Epoch:0044, train_loss=2.70102, train_acc=0.98112, val_loss=3.81909, val_acc=0.92956, time=0.38700
Epoch:0045, train_loss=2.69904, train_acc=0.98265, val_loss=3.81892, val_acc=0.93109, time=0.38803
Epoch:0046, train_loss=2.69721, train_acc=0.98401, val_loss=3.81875, val_acc=0.93109, time=0.35599
Epoch:0047, train_loss=2.69552, train_acc=0.98537, val_loss=3.81860, val_acc=0.93415, time=0.45201
Epoch:0048, train_loss=2.69397, train_acc=0.98639, val_loss=3.81845, val_acc=0.93415, time=0.39400
Epoch:0049, train_loss=2.69254, train_acc=0.98707, val_loss=3.81831, val_acc=0.93568, time=0.35000
Epoch:0050, train_loss=2.69121, train_acc=0.98775, val_loss=3.81818, val_acc=0.93721, time=0.42600
Epoch:0051, train_loss=2.68996, train_acc=0.98860, val_loss=3.81805, val_acc=0.94028, time=0.44599
Epoch:0052, train_loss=2.68879, train_acc=0.98911, val_loss=3.81793, val_acc=0.94028, time=0.46600
Epoch:0053, train_loss=2.68768, train_acc=0.98945, val_loss=3.81782, val_acc=0.94028, time=0.39600
Epoch:0054, train_loss=2.68662, train_acc=0.98996, val_loss=3.81771, val_acc=0.94181, time=0.35401
Epoch:0055, train_loss=2.68562, train_acc=0.99013, val_loss=3.81760, val_acc=0.94181, time=0.42600
Epoch:0056, train_loss=2.68466, train_acc=0.99030, val_loss=3.81751, val_acc=0.94181, time=0.41501
Epoch:0057, train_loss=2.68374, train_acc=0.99098, val_loss=3.81741, val_acc=0.94181, time=0.40499
Epoch:0058, train_loss=2.68287, train_acc=0.99133, val_loss=3.81733, val_acc=0.94181, time=0.50801
Epoch:0059, train_loss=2.68205, train_acc=0.99167, val_loss=3.81725, val_acc=0.94181, time=0.42599
Epoch:0060, train_loss=2.68126, train_acc=0.99201, val_loss=3.81717, val_acc=0.94181, time=0.45600
Epoch:0061, train_loss=2.68052, train_acc=0.99201, val_loss=3.81710, val_acc=0.94181, time=0.35200
Epoch:0062, train_loss=2.67982, train_acc=0.99269, val_loss=3.81703, val_acc=0.94181, time=0.35601
Epoch:0063, train_loss=2.67916, train_acc=0.99269, val_loss=3.81696, val_acc=0.94028, time=0.36700
Epoch:0064, train_loss=2.67853, train_acc=0.99303, val_loss=3.81689, val_acc=0.94181, time=0.37000
Epoch:0065, train_loss=2.67793, train_acc=0.99337, val_loss=3.81683, val_acc=0.94181, time=0.41900
Epoch:0066, train_loss=2.67736, train_acc=0.99354, val_loss=3.81676, val_acc=0.94181, time=0.36201
Epoch:0067, train_loss=2.67682, train_acc=0.99388, val_loss=3.81670, val_acc=0.94181, time=0.48900
Epoch:0068, train_loss=2.67630, train_acc=0.99388, val_loss=3.81664, val_acc=0.94028, time=0.47900
Epoch:0069, train_loss=2.67581, train_acc=0.99405, val_loss=3.81658, val_acc=0.94028, time=0.38000
Epoch:0070, train_loss=2.67533, train_acc=0.99422, val_loss=3.81651, val_acc=0.94028, time=0.36801
Epoch:0071, train_loss=2.67487, train_acc=0.99490, val_loss=3.81645, val_acc=0.94028, time=0.45299
Epoch:0072, train_loss=2.67443, train_acc=0.99507, val_loss=3.81639, val_acc=0.93874, time=0.48200
Epoch:0073, train_loss=2.67401, train_acc=0.99524, val_loss=3.81634, val_acc=0.93874, time=0.45301
Epoch:0074, train_loss=2.67361, train_acc=0.99541, val_loss=3.81628, val_acc=0.94028, time=0.44700
Epoch:0075, train_loss=2.67322, train_acc=0.99558, val_loss=3.81622, val_acc=0.94181, time=0.38400
Epoch:0076, train_loss=2.67285, train_acc=0.99558, val_loss=3.81617, val_acc=0.94181, time=0.37700
Epoch:0077, train_loss=2.67250, train_acc=0.99575, val_loss=3.81612, val_acc=0.94181, time=0.40100
Epoch:0078, train_loss=2.67217, train_acc=0.99592, val_loss=3.81608, val_acc=0.94181, time=0.46401
Epoch:0079, train_loss=2.67184, train_acc=0.99592, val_loss=3.81603, val_acc=0.94334, time=0.40099
Epoch:0080, train_loss=2.67154, train_acc=0.99609, val_loss=3.81599, val_acc=0.94487, time=0.41999
Epoch:0081, train_loss=2.67125, train_acc=0.99609, val_loss=3.81596, val_acc=0.94487, time=0.39701
Epoch:0082, train_loss=2.67097, train_acc=0.99609, val_loss=3.81592, val_acc=0.94487, time=0.49399
Epoch:0083, train_loss=2.67070, train_acc=0.99609, val_loss=3.81589, val_acc=0.94487, time=0.35601
Epoch:0084, train_loss=2.67045, train_acc=0.99609, val_loss=3.81586, val_acc=0.94487, time=0.37500
Epoch:0085, train_loss=2.67020, train_acc=0.99626, val_loss=3.81583, val_acc=0.94487, time=0.36000
Epoch:0086, train_loss=2.66997, train_acc=0.99626, val_loss=3.81581, val_acc=0.94487, time=0.36300
Epoch:0087, train_loss=2.66974, train_acc=0.99643, val_loss=3.81579, val_acc=0.94487, time=0.41900
Epoch:0088, train_loss=2.66953, train_acc=0.99660, val_loss=3.81576, val_acc=0.94487, time=0.35400
Epoch:0089, train_loss=2.66932, train_acc=0.99677, val_loss=3.81574, val_acc=0.94487, time=0.35200
Epoch:0090, train_loss=2.66912, train_acc=0.99694, val_loss=3.81572, val_acc=0.94487, time=0.47201
Epoch:0091, train_loss=2.66893, train_acc=0.99728, val_loss=3.81571, val_acc=0.94334, time=0.53899
Epoch:0092, train_loss=2.66874, train_acc=0.99762, val_loss=3.81569, val_acc=0.94181, time=0.51198
Epoch:0093, train_loss=2.66857, train_acc=0.99762, val_loss=3.81567, val_acc=0.94181, time=0.36201
Epoch:0094, train_loss=2.66840, train_acc=0.99779, val_loss=3.81566, val_acc=0.94181, time=0.45801
Epoch:0095, train_loss=2.66823, train_acc=0.99779, val_loss=3.81565, val_acc=0.94181, time=0.41999
Epoch:0096, train_loss=2.66807, train_acc=0.99779, val_loss=3.81563, val_acc=0.94181, time=0.35900
Epoch:0097, train_loss=2.66792, train_acc=0.99796, val_loss=3.81562, val_acc=0.94181, time=0.42800
Epoch:0098, train_loss=2.66778, train_acc=0.99796, val_loss=3.81561, val_acc=0.94181, time=0.42700
Epoch:0099, train_loss=2.66764, train_acc=0.99796, val_loss=3.81560, val_acc=0.94181, time=0.58902
Epoch:0100, train_loss=2.66750, train_acc=0.99796, val_loss=3.81559, val_acc=0.94181, time=0.37098
Epoch:0101, train_loss=2.66737, train_acc=0.99796, val_loss=3.81558, val_acc=0.94181, time=0.38300
Epoch:0102, train_loss=2.66724, train_acc=0.99813, val_loss=3.81557, val_acc=0.94181, time=0.39601
Epoch:0103, train_loss=2.66712, train_acc=0.99813, val_loss=3.81557, val_acc=0.94181, time=0.40099
Epoch:0104, train_loss=2.66701, train_acc=0.99813, val_loss=3.81556, val_acc=0.94181, time=0.50100
Epoch:0105, train_loss=2.66689, train_acc=0.99813, val_loss=3.81555, val_acc=0.94181, time=0.35401
Epoch:0106, train_loss=2.66678, train_acc=0.99813, val_loss=3.81554, val_acc=0.94181, time=0.35899
Epoch:0107, train_loss=2.66668, train_acc=0.99813, val_loss=3.81554, val_acc=0.94181, time=0.37701
Epoch:0108, train_loss=2.66658, train_acc=0.99813, val_loss=3.81553, val_acc=0.94334, time=0.36000
Epoch:0109, train_loss=2.66648, train_acc=0.99813, val_loss=3.81552, val_acc=0.94334, time=0.37800
Epoch:0110, train_loss=2.66638, train_acc=0.99813, val_loss=3.81552, val_acc=0.94334, time=0.36701
Epoch:0111, train_loss=2.66629, train_acc=0.99796, val_loss=3.81551, val_acc=0.94334, time=0.35200
Epoch:0112, train_loss=2.66619, train_acc=0.99796, val_loss=3.81550, val_acc=0.94334, time=0.39101
Epoch:0113, train_loss=2.66611, train_acc=0.99813, val_loss=3.81550, val_acc=0.94334, time=0.52200
Epoch:0114, train_loss=2.66602, train_acc=0.99813, val_loss=3.81549, val_acc=0.94334, time=0.45100
Epoch:0115, train_loss=2.66594, train_acc=0.99813, val_loss=3.81549, val_acc=0.94334, time=0.35299
Epoch:0116, train_loss=2.66585, train_acc=0.99813, val_loss=3.81548, val_acc=0.94334, time=0.35199
Epoch:0117, train_loss=2.66578, train_acc=0.99813, val_loss=3.81548, val_acc=0.94334, time=0.37900
Epoch:0118, train_loss=2.66570, train_acc=0.99813, val_loss=3.81547, val_acc=0.94334, time=0.35400
Epoch:0119, train_loss=2.66562, train_acc=0.99813, val_loss=3.81547, val_acc=0.94334, time=0.36801
Epoch:0120, train_loss=2.66555, train_acc=0.99813, val_loss=3.81546, val_acc=0.94334, time=0.38900
Epoch:0121, train_loss=2.66548, train_acc=0.99813, val_loss=3.81546, val_acc=0.94334, time=0.38201
Epoch:0122, train_loss=2.66541, train_acc=0.99813, val_loss=3.81546, val_acc=0.94334, time=0.35799
Epoch:0123, train_loss=2.66534, train_acc=0.99813, val_loss=3.81545, val_acc=0.94334, time=0.35300
Epoch:0124, train_loss=2.66527, train_acc=0.99813, val_loss=3.81545, val_acc=0.94334, time=0.41301
Epoch:0125, train_loss=2.66521, train_acc=0.99813, val_loss=3.81545, val_acc=0.94334, time=0.45601
Epoch:0126, train_loss=2.66514, train_acc=0.99813, val_loss=3.81545, val_acc=0.94334, time=0.35900
Epoch:0127, train_loss=2.66508, train_acc=0.99813, val_loss=3.81544, val_acc=0.94334, time=0.40901
Epoch:0128, train_loss=2.66502, train_acc=0.99813, val_loss=3.81544, val_acc=0.94334, time=0.50699
Epoch:0129, train_loss=2.66496, train_acc=0.99830, val_loss=3.81544, val_acc=0.94334, time=0.39501
Epoch:0130, train_loss=2.66490, train_acc=0.99830, val_loss=3.81544, val_acc=0.94334, time=0.43500
Epoch:0131, train_loss=2.66485, train_acc=0.99830, val_loss=3.81544, val_acc=0.94334, time=0.38500
Epoch:0132, train_loss=2.66479, train_acc=0.99830, val_loss=3.81543, val_acc=0.94334, time=0.41399
Epoch:0133, train_loss=2.66474, train_acc=0.99830, val_loss=3.81543, val_acc=0.94334, time=0.38800
Epoch:0134, train_loss=2.66468, train_acc=0.99830, val_loss=3.81543, val_acc=0.94334, time=0.37099
Epoch:0135, train_loss=2.66463, train_acc=0.99830, val_loss=3.81543, val_acc=0.94334, time=0.45901
Epoch:0136, train_loss=2.66458, train_acc=0.99830, val_loss=3.81543, val_acc=0.94334, time=0.36901
Epoch:0137, train_loss=2.66453, train_acc=0.99830, val_loss=3.81543, val_acc=0.94334, time=0.35301
Epoch:0138, train_loss=2.66448, train_acc=0.99830, val_loss=3.81542, val_acc=0.94334, time=0.35801
Epoch:0139, train_loss=2.66443, train_acc=0.99830, val_loss=3.81542, val_acc=0.94334, time=0.39300
Epoch:0140, train_loss=2.66439, train_acc=0.99830, val_loss=3.81542, val_acc=0.94334, time=0.38300
Epoch:0141, train_loss=2.66434, train_acc=0.99830, val_loss=3.81542, val_acc=0.94334, time=0.41200
Epoch:0142, train_loss=2.66430, train_acc=0.99830, val_loss=3.81542, val_acc=0.94334, time=0.41300
Epoch:0143, train_loss=2.66425, train_acc=0.99830, val_loss=3.81542, val_acc=0.94334, time=0.41700
Epoch:0144, train_loss=2.66421, train_acc=0.99830, val_loss=3.81542, val_acc=0.94334, time=0.35600
Epoch:0145, train_loss=2.66417, train_acc=0.99830, val_loss=3.81542, val_acc=0.94334, time=0.36901
Epoch:0146, train_loss=2.66413, train_acc=0.99830, val_loss=3.81542, val_acc=0.94334, time=0.39501
Epoch:0147, train_loss=2.66408, train_acc=0.99830, val_loss=3.81542, val_acc=0.94334, time=0.38701
Epoch:0148, train_loss=2.66404, train_acc=0.99830, val_loss=3.81542, val_acc=0.94334, time=0.45400
Epoch:0149, train_loss=2.66401, train_acc=0.99830, val_loss=3.81542, val_acc=0.94334, time=0.56001
Epoch:0150, train_loss=2.66397, train_acc=0.99830, val_loss=3.81541, val_acc=0.94334, time=0.35499
Epoch:0151, train_loss=2.66393, train_acc=0.99830, val_loss=3.81541, val_acc=0.94334, time=0.35600
Epoch:0152, train_loss=2.66389, train_acc=0.99830, val_loss=3.81541, val_acc=0.94334, time=0.39801
Epoch:0153, train_loss=2.66386, train_acc=0.99830, val_loss=3.81541, val_acc=0.94334, time=0.35800
Epoch:0154, train_loss=2.66382, train_acc=0.99830, val_loss=3.81541, val_acc=0.94334, time=0.35700
Epoch:0155, train_loss=2.66378, train_acc=0.99830, val_loss=3.81541, val_acc=0.94334, time=0.37300
Epoch:0156, train_loss=2.66375, train_acc=0.99830, val_loss=3.81541, val_acc=0.94334, time=0.43101
Epoch:0157, train_loss=2.66372, train_acc=0.99830, val_loss=3.81541, val_acc=0.94334, time=0.35299
Epoch:0158, train_loss=2.66368, train_acc=0.99830, val_loss=3.81541, val_acc=0.94334, time=0.35100
Epoch:0159, train_loss=2.66365, train_acc=0.99830, val_loss=3.81541, val_acc=0.94334, time=0.44399
Epoch:0160, train_loss=2.66362, train_acc=0.99830, val_loss=3.81541, val_acc=0.94334, time=0.35399
Epoch:0161, train_loss=2.66358, train_acc=0.99830, val_loss=3.81541, val_acc=0.94334, time=0.36401
Epoch:0162, train_loss=2.66355, train_acc=0.99830, val_loss=3.81541, val_acc=0.94334, time=0.35300
Epoch:0163, train_loss=2.66352, train_acc=0.99847, val_loss=3.81541, val_acc=0.94334, time=0.35399
Epoch:0164, train_loss=2.66349, train_acc=0.99847, val_loss=3.81541, val_acc=0.94334, time=0.35701
Epoch:0165, train_loss=2.66346, train_acc=0.99847, val_loss=3.81541, val_acc=0.94334, time=0.42098
Epoch:0166, train_loss=2.66343, train_acc=0.99847, val_loss=3.81541, val_acc=0.94334, time=0.35501
Epoch:0167, train_loss=2.66340, train_acc=0.99847, val_loss=3.81541, val_acc=0.94334, time=0.43200
Epoch:0168, train_loss=2.66338, train_acc=0.99847, val_loss=3.81541, val_acc=0.94334, time=0.46400
Epoch:0169, train_loss=2.66335, train_acc=0.99847, val_loss=3.81541, val_acc=0.94334, time=0.44100
Epoch:0170, train_loss=2.66332, train_acc=0.99847, val_loss=3.81541, val_acc=0.94334, time=0.37300
Epoch:0171, train_loss=2.66329, train_acc=0.99847, val_loss=3.81541, val_acc=0.94334, time=0.35500
Epoch:0172, train_loss=2.66327, train_acc=0.99847, val_loss=3.81541, val_acc=0.94334, time=0.35701
Epoch:0173, train_loss=2.66324, train_acc=0.99847, val_loss=3.81541, val_acc=0.94334, time=0.42301
Epoch:0174, train_loss=2.66322, train_acc=0.99847, val_loss=3.81541, val_acc=0.94334, time=0.51798
Epoch:0175, train_loss=2.66319, train_acc=0.99847, val_loss=3.81541, val_acc=0.94334, time=0.39601
Epoch:0176, train_loss=2.66317, train_acc=0.99847, val_loss=3.81541, val_acc=0.94334, time=0.49198
Epoch:0177, train_loss=2.66314, train_acc=0.99847, val_loss=3.81541, val_acc=0.94334, time=0.37301
Epoch:0178, train_loss=2.66312, train_acc=0.99847, val_loss=3.81541, val_acc=0.94334, time=0.35199
Epoch:0179, train_loss=2.66309, train_acc=0.99847, val_loss=3.81541, val_acc=0.94334, time=0.35700
Epoch:0180, train_loss=2.66307, train_acc=0.99864, val_loss=3.81541, val_acc=0.94334, time=0.42999
Epoch:0181, train_loss=2.66305, train_acc=0.99864, val_loss=3.81541, val_acc=0.94334, time=0.35800
Early stopping...

Optimization Finished!

Test set results: loss= 3.44284, accuracy= 0.91706, time= 0.14501

Test Precision, Recall and F1-Score...
              precision    recall  f1-score   support

           0     0.9537    0.9889    0.9710      1083
           1     0.8507    0.9421    0.8941       121
           2     0.9554    0.9224    0.9386       696
           3     1.0000    0.8667    0.9286        15
           4     0.8824    1.0000    0.9375        15
           5     1.0000    0.7647    0.8667        17
           6     0.8387    0.7222    0.7761        36
           7     0.9200    0.9200    0.9200        25
           8     0.9286    0.6842    0.7879        19
           9     0.8462    0.8462    0.8462        13
          10     0.7849    0.8391    0.8111        87
          11     0.8500    0.8500    0.8500        20
          12     0.7391    0.9067    0.8144        75
          13     0.8387    0.9286    0.8814        28
          14     1.0000    0.8889    0.9412         9
          15     0.9565    1.0000    0.9778        22
          16     0.8333    1.0000    0.9091         5
          17     0.9091    0.8333    0.8696        12
          18     0.7654    0.7654    0.7654        81
          19     0.6667    1.0000    0.8000        10
          20     1.0000    1.0000    1.0000         2
          21     0.8571    1.0000    0.9231        12
          22     1.0000    1.0000    1.0000         1
          23     1.0000    0.7778    0.8750         9
          24     0.8000    0.3333    0.4706        12
          25     1.0000    0.6000    0.7500         5
          26     1.0000    0.8000    0.8889        10
          27     1.0000    0.9167    0.9565        12
          28     0.0000    0.0000    0.0000         3
          29     1.0000    1.0000    1.0000         3
          30     0.7143    0.5556    0.6250         9
          31     1.0000    1.0000    1.0000         9
          32     0.7778    0.8750    0.8235         8
          33     0.7333    1.0000    0.8462        11
          34     1.0000    0.2000    0.3333         5
          35     1.0000    0.5000    0.6667         4
          36     0.7500    0.7500    0.7500         4
          37     1.0000    0.3333    0.5000         3
          38     1.0000    1.0000    1.0000         4
          39     0.0000    0.0000    0.0000         1
          40     0.2500    0.1667    0.2000         6
          41     1.0000    0.7273    0.8421        11
          42     1.0000    0.8889    0.9412         9
          43     0.0000    0.0000    0.0000         6
          44     1.0000    1.0000    1.0000         1
          45     0.0000    0.0000    0.0000         1
          46     0.0000    0.0000    0.0000         1
          47     1.0000    0.1429    0.2500         7
          48     0.0000    0.0000    0.0000         1
          49     0.0000    0.0000    0.0000         2
          50     0.0000    0.0000    0.0000         4
          51     0.0000    0.0000    0.0000         3

    accuracy                         0.9171      2568
   macro avg     0.7385    0.6584    0.6755      2568
weighted avg     0.9135    0.9171    0.9115      2568


Macro average Test Precision, Recall and F1-Score...
(0.7384991834945303, 0.6583993525592977, 0.6755483468648625, None)

Micro average Test Precision, Recall and F1-Score...
(0.9170560747663551, 0.9170560747663551, 0.9170560747663551, None)

Embeddings:
Word_embeddings: 8892
Train_doc_embeddings: 6532
Test_doc_embeddings: 2568

Elapsed time is 74.829842 seconds.
