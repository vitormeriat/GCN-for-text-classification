
==================== Torch Seed: 9066940178100

Model parameters

Layer: layer1.W0 | Size: torch.Size([61603, 200])
Layer: layer2.W0 | Size: torch.Size([200, 20])

Data statistics

  Edges    Classes    Train samples    Val samples    Test samples
-------  ---------  ---------------  -------------  --------------
  61603         20            10183           1131            7532

Epoch:0001, train_loss=3.10439, train_acc=0.05048, val_loss=2.99737, val_acc=0.10610, time=3.75398
Epoch:0002, train_loss=3.00783, train_acc=0.10056, val_loss=2.99108, val_acc=0.23784, time=3.83799
Epoch:0003, train_loss=2.94747, train_acc=0.25582, val_loss=2.98616, val_acc=0.36516, time=3.76399
Epoch:0004, train_loss=2.89927, train_acc=0.39949, val_loss=2.98183, val_acc=0.51547, time=3.85597
Epoch:0005, train_loss=2.85676, train_acc=0.56251, val_loss=2.97794, val_acc=0.59593, time=3.89501
Epoch:0006, train_loss=2.81867, train_acc=0.66542, val_loss=2.97442, val_acc=0.65606, time=3.77999
Epoch:0007, train_loss=2.78438, train_acc=0.72503, val_loss=2.97111, val_acc=0.71088, time=3.83499
Epoch:0008, train_loss=2.75275, train_acc=0.76755, val_loss=2.96795, val_acc=0.76039, time=3.73600
Epoch:0009, train_loss=2.72311, train_acc=0.81165, val_loss=2.96500, val_acc=0.80106, time=3.70898
Epoch:0010, train_loss=2.69579, train_acc=0.85456, val_loss=2.96236, val_acc=0.82759, time=3.51701
Epoch:0011, train_loss=2.67148, train_acc=0.88638, val_loss=2.96009, val_acc=0.85676, time=3.67697
Epoch:0012, train_loss=2.65052, train_acc=0.90435, val_loss=2.95816, val_acc=0.86295, time=3.69500
Epoch:0013, train_loss=2.63275, train_acc=0.91879, val_loss=2.95652, val_acc=0.87710, time=3.55201
Epoch:0014, train_loss=2.61762, train_acc=0.92762, val_loss=2.95510, val_acc=0.87975, time=3.92401
Epoch:0015, train_loss=2.60456, train_acc=0.93381, val_loss=2.95385, val_acc=0.88417, time=3.74299
Epoch:0016, train_loss=2.59313, train_acc=0.94039, val_loss=2.95274, val_acc=0.89744, time=3.70600
Epoch:0017, train_loss=2.58309, train_acc=0.94560, val_loss=2.95178, val_acc=0.90009, time=3.70299
Epoch:0018, train_loss=2.57433, train_acc=0.94992, val_loss=2.95095, val_acc=0.90097, time=3.68698
Epoch:0019, train_loss=2.56678, train_acc=0.95218, val_loss=2.95024, val_acc=0.90628, time=3.82999
Epoch:0020, train_loss=2.56035, train_acc=0.95728, val_loss=2.94965, val_acc=0.91158, time=3.77900
Epoch:0021, train_loss=2.55489, train_acc=0.95964, val_loss=2.94917, val_acc=0.91335, time=3.78198
Epoch:0022, train_loss=2.55026, train_acc=0.96200, val_loss=2.94876, val_acc=0.91070, time=3.83599
Epoch:0023, train_loss=2.54628, train_acc=0.96406, val_loss=2.94842, val_acc=0.91247, time=4.05501
Epoch:0024, train_loss=2.54281, train_acc=0.96563, val_loss=2.94812, val_acc=0.91247, time=3.64399
Epoch:0025, train_loss=2.53972, train_acc=0.96730, val_loss=2.94786, val_acc=0.91512, time=3.80401
Epoch:0026, train_loss=2.53695, train_acc=0.96867, val_loss=2.94762, val_acc=0.91777, time=3.53600
Epoch:0027, train_loss=2.53442, train_acc=0.97083, val_loss=2.94740, val_acc=0.91689, time=3.64000
Epoch:0028, train_loss=2.53210, train_acc=0.97368, val_loss=2.94720, val_acc=0.91600, time=3.70299
Epoch:0029, train_loss=2.52998, train_acc=0.97574, val_loss=2.94702, val_acc=0.91954, time=3.72501
Epoch:0030, train_loss=2.52806, train_acc=0.97732, val_loss=2.94686, val_acc=0.91866, time=3.69001
Epoch:0031, train_loss=2.52631, train_acc=0.97908, val_loss=2.94672, val_acc=0.92396, time=3.68001
Epoch:0032, train_loss=2.52474, train_acc=0.98006, val_loss=2.94660, val_acc=0.92396, time=3.55998
Epoch:0033, train_loss=2.52332, train_acc=0.98124, val_loss=2.94650, val_acc=0.92661, time=3.66799
Epoch:0034, train_loss=2.52203, train_acc=0.98252, val_loss=2.94641, val_acc=0.92396, time=3.59400
Epoch:0035, train_loss=2.52085, train_acc=0.98380, val_loss=2.94633, val_acc=0.92396, time=3.49999
Epoch:0036, train_loss=2.51977, train_acc=0.98429, val_loss=2.94626, val_acc=0.92308, time=3.58100
Epoch:0037, train_loss=2.51876, train_acc=0.98478, val_loss=2.94621, val_acc=0.92396, time=3.67799
Epoch:0038, train_loss=2.51783, train_acc=0.98527, val_loss=2.94616, val_acc=0.92219, time=3.81700
Epoch:0039, train_loss=2.51696, train_acc=0.98615, val_loss=2.94612, val_acc=0.92219, time=3.60200
Epoch:0040, train_loss=2.51615, train_acc=0.98684, val_loss=2.94608, val_acc=0.92308, time=3.74600
Epoch:0041, train_loss=2.51540, train_acc=0.98743, val_loss=2.94605, val_acc=0.92308, time=3.72498
Epoch:0042, train_loss=2.51470, train_acc=0.98782, val_loss=2.94602, val_acc=0.92308, time=3.78799
Epoch:0043, train_loss=2.51405, train_acc=0.98871, val_loss=2.94599, val_acc=0.92219, time=3.75899
Epoch:0044, train_loss=2.51345, train_acc=0.98930, val_loss=2.94597, val_acc=0.92485, time=3.74599
Epoch:0045, train_loss=2.51288, train_acc=0.99008, val_loss=2.94595, val_acc=0.92485, time=3.81301
Epoch:0046, train_loss=2.51235, train_acc=0.99087, val_loss=2.94593, val_acc=0.92485, time=3.57700
Epoch:0047, train_loss=2.51185, train_acc=0.99175, val_loss=2.94591, val_acc=0.92573, time=3.68199
Epoch:0048, train_loss=2.51138, train_acc=0.99293, val_loss=2.94589, val_acc=0.92573, time=3.84301
Epoch:0049, train_loss=2.51094, train_acc=0.99342, val_loss=2.94587, val_acc=0.92661, time=3.53800
Epoch:0050, train_loss=2.51053, train_acc=0.99372, val_loss=2.94585, val_acc=0.92750, time=3.80798
Epoch:0051, train_loss=2.51014, train_acc=0.99411, val_loss=2.94584, val_acc=0.92750, time=3.77501
Epoch:0052, train_loss=2.50977, train_acc=0.99450, val_loss=2.94583, val_acc=0.92750, time=3.80000
Epoch:0053, train_loss=2.50942, train_acc=0.99450, val_loss=2.94582, val_acc=0.92661, time=3.74697
Epoch:0054, train_loss=2.50909, train_acc=0.99489, val_loss=2.94582, val_acc=0.92573, time=3.57299
Epoch:0055, train_loss=2.50878, train_acc=0.99519, val_loss=2.94581, val_acc=0.92485, time=3.66000
Epoch:0056, train_loss=2.50848, train_acc=0.99548, val_loss=2.94581, val_acc=0.92485, time=3.67999
Epoch:0057, train_loss=2.50821, train_acc=0.99548, val_loss=2.94581, val_acc=0.92485, time=3.86702
Epoch:0058, train_loss=2.50794, train_acc=0.99607, val_loss=2.94582, val_acc=0.92485, time=3.78299
Epoch:0059, train_loss=2.50770, train_acc=0.99646, val_loss=2.94582, val_acc=0.92485, time=3.69701
Epoch:0060, train_loss=2.50746, train_acc=0.99676, val_loss=2.94582, val_acc=0.92485, time=3.73098
Epoch:0061, train_loss=2.50724, train_acc=0.99705, val_loss=2.94583, val_acc=0.92485, time=3.63299
Early stopping...

Optimization Finished!

Test set results: loss= 2.70018, accuracy= 0.84280, time= 1.12901

Test Precision, Recall and F1-Score...
              precision    recall  f1-score   support

           0     0.8506    0.9296    0.8884       398
           1     0.7102    0.7686    0.7383       389
           2     0.8567    0.8056    0.8304       319
           3     0.9112    0.8813    0.8960       396
           4     0.8127    0.6581    0.7273       310
           5     0.7879    0.6599    0.7182       394
           6     0.9378    0.9496    0.9437       397
           7     0.8911    0.9137    0.9023       394
           8     0.8886    0.9268    0.9073       396
           9     0.9599    0.9599    0.9599       399
          10     0.9861    0.9415    0.9633       376
          11     0.8172    0.7696    0.7927       395
          12     0.7672    0.8282    0.7965       390
          13     0.7920    0.8041    0.7980       393
          14     0.6825    0.7679    0.7227       392
          15     0.7773    0.9011    0.8346       364
          16     0.8753    0.8510    0.8630       396
          17     0.8268    0.8182    0.8225       385
          18     0.9550    0.9598    0.9574       398
          19     0.7570    0.6454    0.6968       251

    accuracy                         0.8428      7532
   macro avg     0.8422    0.8370    0.8380      7532
weighted avg     0.8443    0.8428    0.8420      7532


Macro average Test Precision, Recall and F1-Score...
(0.8421551939609542, 0.8369975491009797, 0.8379527993825573, None)

Micro average Test Precision, Recall and F1-Score...
(0.8428040361125863, 0.8428040361125863, 0.8428040361125863, None)

Embeddings:
Word_embeddings: 42757
Train_doc_embeddings: 11314
Test_doc_embeddings: 7532

Elapsed time is 240.600500 seconds.
