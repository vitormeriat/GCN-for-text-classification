
==================== Torch Seed: 8035199263500

Model parameters

Layer: layer1.W0 | Size: torch.Size([17992, 200])
Layer: layer2.W0 | Size: torch.Size([200, 52])

Data statistics

  Edges    Classes    Train samples    Val samples    Test samples
-------  ---------  ---------------  -------------  --------------
  17992         52             5879            653            2568

Epoch:0001, train_loss=4.14651, train_acc=0.04065, val_loss=3.93104, val_acc=0.37060, time=0.56200
Epoch:0002, train_loss=3.79175, train_acc=0.35159, val_loss=3.90021, val_acc=0.49923, time=0.53200
Epoch:0003, train_loss=3.51546, train_acc=0.47066, val_loss=3.88035, val_acc=0.56815, time=0.45300
Epoch:0004, train_loss=3.33469, train_acc=0.55707, val_loss=3.86905, val_acc=0.62634, time=0.43300
Epoch:0005, train_loss=3.22814, train_acc=0.61898, val_loss=3.86262, val_acc=0.65850, time=0.41700
Epoch:0006, train_loss=3.16522, train_acc=0.64739, val_loss=3.85790, val_acc=0.66922, time=0.41400
Epoch:0007, train_loss=3.11733, train_acc=0.66950, val_loss=3.85340, val_acc=0.70138, time=0.49100
Epoch:0008, train_loss=3.07093, train_acc=0.69468, val_loss=3.84899, val_acc=0.72282, time=0.46901
Epoch:0009, train_loss=3.02559, train_acc=0.73346, val_loss=3.84499, val_acc=0.75038, time=0.42900
Epoch:0010, train_loss=2.98488, train_acc=0.77105, val_loss=3.84166, val_acc=0.79479, time=0.45500
Epoch:0011, train_loss=2.95116, train_acc=0.81544, val_loss=3.83904, val_acc=0.82236, time=0.45101
Epoch:0012, train_loss=2.92434, train_acc=0.84368, val_loss=3.83699, val_acc=0.84686, time=0.52500
Epoch:0013, train_loss=2.90307, train_acc=0.86137, val_loss=3.83536, val_acc=0.85299, time=0.44000
Epoch:0014, train_loss=2.88568, train_acc=0.87719, val_loss=3.83401, val_acc=0.86217, time=0.49700
Epoch:0015, train_loss=2.87082, train_acc=0.88706, val_loss=3.83282, val_acc=0.87289, time=0.52300
Epoch:0016, train_loss=2.85761, train_acc=0.89386, val_loss=3.83175, val_acc=0.87596, time=0.53400
Epoch:0017, train_loss=2.84548, train_acc=0.89981, val_loss=3.83074, val_acc=0.88208, time=0.50600
Epoch:0018, train_loss=2.83405, train_acc=0.90832, val_loss=3.82975, val_acc=0.89127, time=0.51301
Epoch:0019, train_loss=2.82307, train_acc=0.91478, val_loss=3.82878, val_acc=0.89127, time=0.44599
Epoch:0020, train_loss=2.81248, train_acc=0.91988, val_loss=3.82784, val_acc=0.89433, time=0.45200
Epoch:0021, train_loss=2.80235, train_acc=0.92584, val_loss=3.82695, val_acc=0.89740, time=0.47900
Epoch:0022, train_loss=2.79282, train_acc=0.93145, val_loss=3.82611, val_acc=0.89893, time=0.49599
Epoch:0023, train_loss=2.78396, train_acc=0.93485, val_loss=3.82534, val_acc=0.90505, time=0.46501
Epoch:0024, train_loss=2.77581, train_acc=0.94081, val_loss=3.82464, val_acc=0.90812, time=0.49901
Epoch:0025, train_loss=2.76840, train_acc=0.94506, val_loss=3.82402, val_acc=0.90965, time=0.47598
Epoch:0026, train_loss=2.76171, train_acc=0.94863, val_loss=3.82347, val_acc=0.92037, time=0.55701
Epoch:0027, train_loss=2.75569, train_acc=0.95101, val_loss=3.82300, val_acc=0.92802, time=0.47901
Epoch:0028, train_loss=2.75024, train_acc=0.95339, val_loss=3.82258, val_acc=0.92496, time=0.52100
Epoch:0029, train_loss=2.74526, train_acc=0.95560, val_loss=3.82220, val_acc=0.92190, time=0.42700
Epoch:0030, train_loss=2.74063, train_acc=0.95765, val_loss=3.82184, val_acc=0.92496, time=0.46100
Epoch:0031, train_loss=2.73626, train_acc=0.96088, val_loss=3.82150, val_acc=0.92343, time=0.45501
Epoch:0032, train_loss=2.73209, train_acc=0.96343, val_loss=3.82117, val_acc=0.92343, time=0.44400
Epoch:0033, train_loss=2.72812, train_acc=0.96547, val_loss=3.82086, val_acc=0.92802, time=0.40901
Epoch:0034, train_loss=2.72436, train_acc=0.96717, val_loss=3.82056, val_acc=0.92956, time=0.50300
Epoch:0035, train_loss=2.72082, train_acc=0.96921, val_loss=3.82027, val_acc=0.92956, time=0.40200
Epoch:0036, train_loss=2.71752, train_acc=0.97023, val_loss=3.82000, val_acc=0.92956, time=0.46401
Epoch:0037, train_loss=2.71445, train_acc=0.97176, val_loss=3.81973, val_acc=0.92956, time=0.47299
Epoch:0038, train_loss=2.71161, train_acc=0.97329, val_loss=3.81948, val_acc=0.93109, time=0.37701
Epoch:0039, train_loss=2.70898, train_acc=0.97466, val_loss=3.81924, val_acc=0.92802, time=0.40501
Epoch:0040, train_loss=2.70653, train_acc=0.97602, val_loss=3.81900, val_acc=0.92802, time=0.46300
Epoch:0041, train_loss=2.70425, train_acc=0.97755, val_loss=3.81877, val_acc=0.93262, time=0.52000
Epoch:0042, train_loss=2.70213, train_acc=0.97891, val_loss=3.81854, val_acc=0.93262, time=0.49301
Epoch:0043, train_loss=2.70014, train_acc=0.97942, val_loss=3.81833, val_acc=0.93262, time=0.46500
Epoch:0044, train_loss=2.69828, train_acc=0.98061, val_loss=3.81813, val_acc=0.93109, time=0.48501
Epoch:0045, train_loss=2.69653, train_acc=0.98112, val_loss=3.81794, val_acc=0.93262, time=0.49700
Epoch:0046, train_loss=2.69488, train_acc=0.98299, val_loss=3.81775, val_acc=0.93262, time=0.52301
Epoch:0047, train_loss=2.69332, train_acc=0.98384, val_loss=3.81758, val_acc=0.93262, time=0.38100
Epoch:0048, train_loss=2.69185, train_acc=0.98537, val_loss=3.81743, val_acc=0.93415, time=0.42100
Epoch:0049, train_loss=2.69047, train_acc=0.98673, val_loss=3.81728, val_acc=0.93568, time=0.50800
Epoch:0050, train_loss=2.68917, train_acc=0.98758, val_loss=3.81715, val_acc=0.94028, time=0.43601
Epoch:0051, train_loss=2.68794, train_acc=0.98860, val_loss=3.81702, val_acc=0.93874, time=0.49497
Epoch:0052, train_loss=2.68679, train_acc=0.98928, val_loss=3.81691, val_acc=0.94181, time=0.48401
Epoch:0053, train_loss=2.68571, train_acc=0.98996, val_loss=3.81680, val_acc=0.94334, time=0.45901
Epoch:0054, train_loss=2.68471, train_acc=0.99047, val_loss=3.81670, val_acc=0.94487, time=0.40000
Epoch:0055, train_loss=2.68376, train_acc=0.99064, val_loss=3.81661, val_acc=0.94487, time=0.38999
Epoch:0056, train_loss=2.68287, train_acc=0.99115, val_loss=3.81653, val_acc=0.94487, time=0.45601
Epoch:0057, train_loss=2.68203, train_acc=0.99133, val_loss=3.81646, val_acc=0.94793, time=0.51900
Epoch:0058, train_loss=2.68125, train_acc=0.99150, val_loss=3.81639, val_acc=0.94793, time=0.49899
Epoch:0059, train_loss=2.68050, train_acc=0.99201, val_loss=3.81633, val_acc=0.94793, time=0.53900
Epoch:0060, train_loss=2.67979, train_acc=0.99201, val_loss=3.81627, val_acc=0.94640, time=0.53498
Epoch:0061, train_loss=2.67912, train_acc=0.99269, val_loss=3.81622, val_acc=0.94640, time=0.40999
Epoch:0062, train_loss=2.67848, train_acc=0.99269, val_loss=3.81617, val_acc=0.94793, time=0.47503
Epoch:0063, train_loss=2.67787, train_acc=0.99320, val_loss=3.81613, val_acc=0.94793, time=0.46498
Epoch:0064, train_loss=2.67729, train_acc=0.99320, val_loss=3.81609, val_acc=0.94793, time=0.48198
Epoch:0065, train_loss=2.67674, train_acc=0.99354, val_loss=3.81605, val_acc=0.94793, time=0.49501
Epoch:0066, train_loss=2.67622, train_acc=0.99371, val_loss=3.81601, val_acc=0.94487, time=0.42000
Epoch:0067, train_loss=2.67571, train_acc=0.99388, val_loss=3.81597, val_acc=0.94640, time=0.50200
Epoch:0068, train_loss=2.67523, train_acc=0.99456, val_loss=3.81593, val_acc=0.94640, time=0.50199
Epoch:0069, train_loss=2.67478, train_acc=0.99456, val_loss=3.81590, val_acc=0.94640, time=0.41401
Epoch:0070, train_loss=2.67434, train_acc=0.99507, val_loss=3.81586, val_acc=0.94793, time=0.50200
Epoch:0071, train_loss=2.67392, train_acc=0.99507, val_loss=3.81583, val_acc=0.94793, time=0.46800
Epoch:0072, train_loss=2.67352, train_acc=0.99507, val_loss=3.81579, val_acc=0.94793, time=0.44402
Epoch:0073, train_loss=2.67314, train_acc=0.99524, val_loss=3.81576, val_acc=0.94793, time=0.52998
Epoch:0074, train_loss=2.67278, train_acc=0.99541, val_loss=3.81573, val_acc=0.94793, time=0.46300
Epoch:0075, train_loss=2.67242, train_acc=0.99541, val_loss=3.81570, val_acc=0.94793, time=0.44000
Epoch:0076, train_loss=2.67209, train_acc=0.99592, val_loss=3.81568, val_acc=0.94946, time=0.43100
Epoch:0077, train_loss=2.67177, train_acc=0.99592, val_loss=3.81565, val_acc=0.94946, time=0.45700
Epoch:0078, train_loss=2.67146, train_acc=0.99609, val_loss=3.81562, val_acc=0.94946, time=0.50500
Epoch:0079, train_loss=2.67116, train_acc=0.99626, val_loss=3.81559, val_acc=0.94946, time=0.42500
Epoch:0080, train_loss=2.67087, train_acc=0.99660, val_loss=3.81557, val_acc=0.94946, time=0.43801
Epoch:0081, train_loss=2.67060, train_acc=0.99677, val_loss=3.81554, val_acc=0.94946, time=0.43101
Epoch:0082, train_loss=2.67033, train_acc=0.99694, val_loss=3.81552, val_acc=0.94946, time=0.50798
Epoch:0083, train_loss=2.67008, train_acc=0.99694, val_loss=3.81549, val_acc=0.94946, time=0.46600
Epoch:0084, train_loss=2.66983, train_acc=0.99694, val_loss=3.81547, val_acc=0.94946, time=0.51500
Epoch:0085, train_loss=2.66960, train_acc=0.99694, val_loss=3.81545, val_acc=0.94946, time=0.49001
Epoch:0086, train_loss=2.66937, train_acc=0.99694, val_loss=3.81543, val_acc=0.94946, time=0.49499
Epoch:0087, train_loss=2.66915, train_acc=0.99711, val_loss=3.81541, val_acc=0.94946, time=0.46301
Epoch:0088, train_loss=2.66894, train_acc=0.99711, val_loss=3.81539, val_acc=0.94946, time=0.46200
Epoch:0089, train_loss=2.66874, train_acc=0.99711, val_loss=3.81537, val_acc=0.94946, time=0.49501
Epoch:0090, train_loss=2.66855, train_acc=0.99711, val_loss=3.81535, val_acc=0.94946, time=0.42101
Epoch:0091, train_loss=2.66836, train_acc=0.99728, val_loss=3.81534, val_acc=0.94946, time=0.41398
Epoch:0092, train_loss=2.66819, train_acc=0.99728, val_loss=3.81532, val_acc=0.94946, time=0.48700
Epoch:0093, train_loss=2.66802, train_acc=0.99745, val_loss=3.81531, val_acc=0.94946, time=0.50099
Epoch:0094, train_loss=2.66786, train_acc=0.99745, val_loss=3.81529, val_acc=0.94946, time=0.45900
Epoch:0095, train_loss=2.66770, train_acc=0.99779, val_loss=3.81528, val_acc=0.94946, time=0.44600
Epoch:0096, train_loss=2.66755, train_acc=0.99796, val_loss=3.81526, val_acc=0.94946, time=0.45900
Epoch:0097, train_loss=2.66741, train_acc=0.99813, val_loss=3.81525, val_acc=0.94946, time=0.58101
Epoch:0098, train_loss=2.66728, train_acc=0.99813, val_loss=3.81524, val_acc=0.94793, time=0.54001
Epoch:0099, train_loss=2.66714, train_acc=0.99813, val_loss=3.81523, val_acc=0.94793, time=0.40100
Epoch:0100, train_loss=2.66702, train_acc=0.99830, val_loss=3.81522, val_acc=0.94793, time=0.45100
Epoch:0101, train_loss=2.66690, train_acc=0.99830, val_loss=3.81520, val_acc=0.94793, time=0.44001
Epoch:0102, train_loss=2.66678, train_acc=0.99830, val_loss=3.81519, val_acc=0.94793, time=0.45901
Epoch:0103, train_loss=2.66667, train_acc=0.99830, val_loss=3.81518, val_acc=0.94793, time=0.49799
Epoch:0104, train_loss=2.66656, train_acc=0.99830, val_loss=3.81518, val_acc=0.94793, time=0.50501
Epoch:0105, train_loss=2.66645, train_acc=0.99830, val_loss=3.81517, val_acc=0.94793, time=0.40099
Epoch:0106, train_loss=2.66635, train_acc=0.99830, val_loss=3.81516, val_acc=0.94793, time=0.49802
Epoch:0107, train_loss=2.66625, train_acc=0.99830, val_loss=3.81515, val_acc=0.94640, time=0.49698
Epoch:0108, train_loss=2.66616, train_acc=0.99830, val_loss=3.81514, val_acc=0.94640, time=0.52401
Epoch:0109, train_loss=2.66606, train_acc=0.99830, val_loss=3.81514, val_acc=0.94640, time=0.43399
Epoch:0110, train_loss=2.66597, train_acc=0.99830, val_loss=3.81513, val_acc=0.94640, time=0.39500
Epoch:0111, train_loss=2.66589, train_acc=0.99830, val_loss=3.81512, val_acc=0.94640, time=0.47699
Epoch:0112, train_loss=2.66580, train_acc=0.99830, val_loss=3.81512, val_acc=0.94640, time=0.40600
Epoch:0113, train_loss=2.66572, train_acc=0.99830, val_loss=3.81511, val_acc=0.94640, time=0.41600
Epoch:0114, train_loss=2.66564, train_acc=0.99830, val_loss=3.81511, val_acc=0.94640, time=0.40500
Epoch:0115, train_loss=2.66556, train_acc=0.99830, val_loss=3.81510, val_acc=0.94640, time=0.51001
Epoch:0116, train_loss=2.66548, train_acc=0.99830, val_loss=3.81510, val_acc=0.94640, time=0.49400
Epoch:0117, train_loss=2.66541, train_acc=0.99830, val_loss=3.81509, val_acc=0.94640, time=0.44300
Epoch:0118, train_loss=2.66534, train_acc=0.99830, val_loss=3.81509, val_acc=0.94640, time=0.40998
Epoch:0119, train_loss=2.66527, train_acc=0.99830, val_loss=3.81509, val_acc=0.94640, time=0.42301
Epoch:0120, train_loss=2.66520, train_acc=0.99830, val_loss=3.81508, val_acc=0.94640, time=0.43200
Epoch:0121, train_loss=2.66513, train_acc=0.99830, val_loss=3.81508, val_acc=0.94640, time=0.43002
Epoch:0122, train_loss=2.66506, train_acc=0.99830, val_loss=3.81508, val_acc=0.94640, time=0.48600
Epoch:0123, train_loss=2.66500, train_acc=0.99830, val_loss=3.81507, val_acc=0.94640, time=0.40901
Epoch:0124, train_loss=2.66494, train_acc=0.99830, val_loss=3.81507, val_acc=0.94640, time=0.42201
Epoch:0125, train_loss=2.66488, train_acc=0.99830, val_loss=3.81507, val_acc=0.94640, time=0.46199
Epoch:0126, train_loss=2.66481, train_acc=0.99830, val_loss=3.81507, val_acc=0.94640, time=0.46901
Epoch:0127, train_loss=2.66476, train_acc=0.99830, val_loss=3.81506, val_acc=0.94640, time=0.48098
Epoch:0128, train_loss=2.66470, train_acc=0.99847, val_loss=3.81506, val_acc=0.94640, time=0.39001
Epoch:0129, train_loss=2.66464, train_acc=0.99847, val_loss=3.81506, val_acc=0.94640, time=0.41900
Epoch:0130, train_loss=2.66459, train_acc=0.99847, val_loss=3.81506, val_acc=0.94640, time=0.50301
Epoch:0131, train_loss=2.66454, train_acc=0.99847, val_loss=3.81506, val_acc=0.94640, time=0.35301
Epoch:0132, train_loss=2.66448, train_acc=0.99847, val_loss=3.81506, val_acc=0.94640, time=0.44398
Epoch:0133, train_loss=2.66443, train_acc=0.99847, val_loss=3.81505, val_acc=0.94640, time=0.43601
Epoch:0134, train_loss=2.66438, train_acc=0.99847, val_loss=3.81505, val_acc=0.94640, time=0.41900
Epoch:0135, train_loss=2.66433, train_acc=0.99847, val_loss=3.81505, val_acc=0.94640, time=0.40200
Epoch:0136, train_loss=2.66429, train_acc=0.99847, val_loss=3.81505, val_acc=0.94640, time=0.43998
Epoch:0137, train_loss=2.66424, train_acc=0.99847, val_loss=3.81505, val_acc=0.94640, time=0.35801
Epoch:0138, train_loss=2.66419, train_acc=0.99847, val_loss=3.81505, val_acc=0.94640, time=0.37099
Epoch:0139, train_loss=2.66415, train_acc=0.99847, val_loss=3.81505, val_acc=0.94640, time=0.47700
Epoch:0140, train_loss=2.66410, train_acc=0.99847, val_loss=3.81505, val_acc=0.94640, time=0.36800
Epoch:0141, train_loss=2.66406, train_acc=0.99847, val_loss=3.81504, val_acc=0.94640, time=0.37102
Epoch:0142, train_loss=2.66402, train_acc=0.99847, val_loss=3.81504, val_acc=0.94640, time=0.38501
Epoch:0143, train_loss=2.66398, train_acc=0.99847, val_loss=3.81504, val_acc=0.94640, time=0.35400
Epoch:0144, train_loss=2.66394, train_acc=0.99847, val_loss=3.81504, val_acc=0.94640, time=0.35899
Epoch:0145, train_loss=2.66390, train_acc=0.99847, val_loss=3.81504, val_acc=0.94640, time=0.35300
Epoch:0146, train_loss=2.66386, train_acc=0.99847, val_loss=3.81504, val_acc=0.94640, time=0.35400
Epoch:0147, train_loss=2.66382, train_acc=0.99847, val_loss=3.81504, val_acc=0.94640, time=0.45800
Epoch:0148, train_loss=2.66378, train_acc=0.99847, val_loss=3.81504, val_acc=0.94640, time=0.35801
Epoch:0149, train_loss=2.66375, train_acc=0.99847, val_loss=3.81504, val_acc=0.94640, time=0.36500
Epoch:0150, train_loss=2.66371, train_acc=0.99847, val_loss=3.81504, val_acc=0.94640, time=0.47002
Epoch:0151, train_loss=2.66367, train_acc=0.99847, val_loss=3.81504, val_acc=0.94640, time=0.35800
Epoch:0152, train_loss=2.66364, train_acc=0.99847, val_loss=3.81504, val_acc=0.94640, time=0.43298
Epoch:0153, train_loss=2.66360, train_acc=0.99847, val_loss=3.81504, val_acc=0.94640, time=0.39601
Epoch:0154, train_loss=2.66357, train_acc=0.99847, val_loss=3.81504, val_acc=0.94640, time=0.35400
Epoch:0155, train_loss=2.66354, train_acc=0.99847, val_loss=3.81504, val_acc=0.94640, time=0.42201
Epoch:0156, train_loss=2.66351, train_acc=0.99847, val_loss=3.81504, val_acc=0.94640, time=0.35500
Epoch:0157, train_loss=2.66347, train_acc=0.99847, val_loss=3.81504, val_acc=0.94640, time=0.41098
Epoch:0158, train_loss=2.66344, train_acc=0.99847, val_loss=3.81504, val_acc=0.94640, time=0.36901
Epoch:0159, train_loss=2.66341, train_acc=0.99847, val_loss=3.81504, val_acc=0.94640, time=0.35300
Epoch:0160, train_loss=2.66338, train_acc=0.99847, val_loss=3.81504, val_acc=0.94487, time=0.35698
Early stopping...

Optimization Finished!

Test set results: loss= 3.43968, accuracy= 0.91861, time= 0.10801

Test Precision, Recall and F1-Score...
              precision    recall  f1-score   support

           0     0.9589    0.9898    0.9741      1083
           1     0.8286    0.9587    0.8889       121
           2     0.9586    0.9325    0.9454       696
           3     1.0000    0.8667    0.9286        15
           4     0.8125    0.8667    0.8387        15
           5     1.0000    0.8235    0.9032        17
           6     0.8929    0.6944    0.7812        36
           7     0.8846    0.9200    0.9020        25
           8     0.9286    0.6842    0.7879        19
           9     0.7692    0.7692    0.7692        13
          10     0.7766    0.8391    0.8066        87
          11     0.8421    0.8000    0.8205        20
          12     0.7216    0.9333    0.8140        75
          13     0.8333    0.8929    0.8621        28
          14     1.0000    0.8889    0.9412         9
          15     0.9565    1.0000    0.9778        22
          16     1.0000    1.0000    1.0000         5
          17     0.9000    0.7500    0.8182        12
          18     0.7692    0.7407    0.7547        81
          19     0.5714    0.8000    0.6667        10
          20     1.0000    1.0000    1.0000         2
          21     0.9231    1.0000    0.9600        12
          22     0.0000    0.0000    0.0000         1
          23     0.8750    0.7778    0.8235         9
          24     0.8000    0.3333    0.4706        12
          25     0.6000    0.6000    0.6000         5
          26     1.0000    1.0000    1.0000        10
          27     1.0000    0.9167    0.9565        12
          28     0.0000    0.0000    0.0000         3
          29     1.0000    1.0000    1.0000         3
          30     0.6667    0.4444    0.5333         9
          31     1.0000    1.0000    1.0000         9
          32     0.8750    0.8750    0.8750         8
          33     0.9167    1.0000    0.9565        11
          34     1.0000    0.2000    0.3333         5
          35     1.0000    0.5000    0.6667         4
          36     0.6000    0.7500    0.6667         4
          37     1.0000    0.3333    0.5000         3
          38     1.0000    1.0000    1.0000         4
          39     0.0000    0.0000    0.0000         1
          40     0.3333    0.1667    0.2222         6
          41     0.9000    0.8182    0.8571        11
          42     1.0000    0.8889    0.9412         9
          43     0.0000    0.0000    0.0000         6
          44     1.0000    1.0000    1.0000         1
          45     1.0000    1.0000    1.0000         1
          46     0.0000    0.0000    0.0000         1
          47     1.0000    0.1429    0.2500         7
          48     0.0000    0.0000    0.0000         1
          49     0.0000    0.0000    0.0000         2
          50     0.0000    0.0000    0.0000         4
          51     0.0000    0.0000    0.0000         3

    accuracy                         0.9186      2568
   macro avg     0.7287    0.6519    0.6691      2568
weighted avg     0.9138    0.9186    0.9125      2568


Macro average Test Precision, Recall and F1-Score...
(0.7287394386729458, 0.6518805981109954, 0.6691073348590708, None)

Micro average Test Precision, Recall and F1-Score...
(0.918613707165109, 0.918613707165109, 0.918613707165109, None)

Embeddings:
Word_embeddings: 8892
Train_doc_embeddings: 6532
Test_doc_embeddings: 2568

Elapsed time is 74.014818 seconds.
